<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Distributed Computing - 分类 - yejian's blog</title><link>https://lruihao.cn/categories/distributed-computing/</link><description>Distributed Computing - 分类 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Fri, 14 Jul 2023 09:23:07 +0800</lastBuildDate><atom:link href="https://lruihao.cn/categories/distributed-computing/" rel="self" type="application/rss+xml"/><item><title>TensorRT Introduction</title><link>https://lruihao.cn/posts/tensorrt_introduction/</link><pubDate>Fri, 14 Jul 2023 09:23:07 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/tensorrt_introduction/</guid><description><![CDATA[<h3 id="tensorrt-介绍">TensorRT 介绍</h3>
<p>TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。</p>
<p>TensorRT 是一个C++库，从 TensorRT 3 开始提供C++ API和Python API，主要用来针对 NVIDIA GPU进行 高性能推理（Inference）加速。</p>
<p></p>
<p>由以上图可以很清楚的看出，训练(training)和 推理(inference)的区别：</p>
<ul>
<li>**训练(training)**包含了前向传播和后向传播两个阶段，针对的是训练集。训练时通过误差反向传播来不断修改网络权值(weights)。</li>
<li>**推理(inference)**只包含前向传播一个阶段，针对的是除了训练集之外的新数据。可以是测试集，但不完全是，更多的是整个数据集之外的数据。其实就是针对新数据进行预测，预测时，速度是一个很重要的因素。</li>
</ul>
<p>一般的深度学习项目，训练时为了加快速度，会使用多GPU分布式训练。但在部署推理时，为了降低成本，往往使用单个GPU机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如caffe，TensorFlow等。</p>
<p>由于训练的网络模型可能会很大（比如，inception，resnet等），参数很多，而且部署端的机器性能存在差异，就会导致推理速度慢，延迟高。这对于那些高实时性的应用场合是致命的，比如自动驾驶要求实时目标检测，目标追踪等。所以为了提高部署推理的速度，出现了很多轻量级神经网络，比如squeezenet，mobilenet，shufflenet等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。</p>
<p>而tensorRT 则是对训练好的模型进行优化。 tensorRT就只是 推理优化器。当你的网络训练完之后，可以将训练模型文件直接丢进tensorRT中，而不再需要依赖深度学习框架（Caffe，TensorFlow等），如下:</p>
<p></p>
<p>可以认为tensorRT是一个只有前向传播的深度学习框架，这个框架可以将 Caffe，TensorFlow的网络模型解析，然后与tensorRT中对应的层进行一一映射，把其他框架的模型统一全部 转换到tensorRT中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略，并进行部署加速。</p>
<p>目前TensorRT8.0 几乎可以支持所有常用的深度学习框架，对于caffe和TensorFlow来说，tensorRT可以直接解析他们的网络模型；对于caffe2，pytorch，mxnet，chainer，CNTK等框架则是首先要将模型转为 ONNX 的通用深度学习模型，然后对ONNX模型做解析。而tensorflow和MATLAB已经将TensorRT集成到框架中去了。</p>
<p>**ONNX(Open Neural Network Exchange)**是微软和Facebook携手开发的开放式神经网络交换工具，也就是说不管用什么框架训练，只要转换为ONNX模型，就可以放在其他框架上面去inference。这是一种统一的神经网络模型定义和保存方式，上面提到的除了tensorflow之外的其他框架官方应该都对onnx做了支持，而ONNX自己开发了对tensorflow的支持。从深度学习框架方面来说，这是各大厂商对抗谷歌tensorflow垄断地位的一种有效方式；从研究人员和开发者方面来说，这可以使开发者轻易地在不同机器学习工具之间进行转换，并为项目选择最好的组合方式，加快从研究到生产的速度。</p>
<p>ONNX / TensorFlow / Custom deep-learning frame模型的工作方式：
</p>
<p>tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层。
TensorRT-plugin
</p>
<p>目前TensorRT支持的层有:https://github.com/onnx/onnx-tensorrt/blob/main/docs/operators.md
目前ONNX支持的算子:https://github.com/onnx/onnx/blob/main/docs/Operators.md</p>
<h3 id="tensorrt-优化方式">TensorRT 优化方式</h3>
<p></p>
<p>TensorRT优化方法主要有以下几种方式，最主要的是前面两种。</p>
<ul>
<li>
<p><strong>层间融合或张量融合(Layer &amp; Tensor Fusion)</strong></p>
<p>如下图左侧是GoogLeNetInception模块的计算图。这个结构中有很多层，在部署模型推理时，这每一层的运算操作都是由GPU完成的，但实际上是GPU通过启动不同的CUDA（Compute unified device architecture）核心来完成计算的，CUDA核心计算张量的速度是很快的，但是往往大量的时间是浪费在CUDA核心的启动和对每一层输入/输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费。TensorRT通过对层间的横向或纵向合并（合并后的结构称为CBR，意指 convolution, bias, and ReLU layers are fused to form a single layer），使得层的数量大大减少。横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心。合并之后的计算图（图4右侧）的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效。</p>
<p></p>
</li>
<li>
<p><strong>数据精度校准(Weight &amp;Activation Precision Calibration)</strong></p>
<p>大部分深度学习框架在训练神经网络时网络中的张量（Tensor）都是32位浮点数的精度（Full 32-bit precision，FP32），一旦网络训练完成，在部署推理的过程中由于不需要反向传播，完全可以适当降低数据精度，比如降为FP16或INT8的精度。更低的数据精度将会使得内存占用和延迟更低，模型体积更小。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Dynamic Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">FP32</td>
<td style="text-align:center">−3.4×1038 +3.4×1038</td>
</tr>
<tr>
<td style="text-align:center">FP16</td>
<td style="text-align:center">−65504 +65504</td>
</tr>
<tr>
<td style="text-align:center">INT8</td>
<td style="text-align:center">−128 +127</td>
</tr>
</tbody>
</table>
<p>INT8只有256个不同的数值，使用INT8来表示 FP32精度的数值，肯定会丢失信息，造成性能下降。不过TensorRT会提供完全自动化的校准（Calibration ）过程，会以最好的匹配性能将FP32精度的数据降低为INT8精度，最小化性能损失。</p>
</li>
<li>
<p><strong>Kernel Auto-Tuning</strong>
网络模型在推理计算时，是调用GPU的CUDA核进行计算的。TensorRT可以针对不同的算法，不同的网络模型，不同的GPU平台，进行 CUDA核的调整（怎么调整的还不清楚），以保证当前模型在特定平台上以最优性能计算。</p>
<p>TensorRT will pick the implementation from a library of kernels that delivers the best performance for the target GPU, input data size, filter size, tensor layout, batch size and other parameters.</p>
</li>
<li>
<p><strong>Dynamic Tensor Memory</strong>
在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。</p>
</li>
<li>
<p><strong>Multi-Stream Execution</strong>
Scalable design to process multiple input streams in parallel，这个应该就是GPU底层的优化了。</p>
</li>
</ul>
<h3 id="tensorrt-安装">TensorRT 安装</h3>
<p><strong><a href="https://zhuanlan.zhihu.com/p/72298520"target="_blank" rel="external nofollow noopener noreferrer">CUDA的安装<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong></p>
<ol>
<li>
<p>安装显卡驱动</p>
</li>
<li>
<p>安装cuda
2.1 进入<a href="https://developer.nvidia.com/cuda-toolkit-archive"target="_blank" rel="external nofollow noopener noreferrer">nvidia开发者网站的CUDA下载页面<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>选择runfile格式的文件下载。</p>
<p>2.2 下载完成后，解压，并运行上图中的命令，会有条款，接受即可，注意安装CUDA的时候不要安装驱动

2.3 路径设置</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">$ <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/nsight-compute-2019.5.0<span class="si">${</span><span class="nv">PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">PATH</span><span class="si">}}</span>
</span></span><span class="line"><span class="cl">$ <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-10.2/lib64/<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>并使设置生效:</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">source</span> ~/.bashrc</span></span></code></pre></td></tr></table>
</div>
</div><p>2.4 验证安装是否成功
进入/usr/local/cuda-10.1/samples/1_Utilities/目录，</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> deviceQuery
</span></span><span class="line"><span class="cl">sudo make
</span></span><span class="line"><span class="cl">./deviceQuery</span></span></code></pre></td></tr></table>
</div>
</div><p>出现如下输出，则CUDA安装成功。
</p>
</li>
<li>
<p>安装cuDNN
3.1进入<a href="https://developer.nvidia.com/cudnn"target="_blank" rel="external nofollow noopener noreferrer">cudnn下载<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>页面，下载版本合适的版
3.2 解压，并进入到相应目录，运行以下命令：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">sudo cp cuda/include/cudnn*.h /usr/local/cuda-10.2/include
</span></span><span class="line"><span class="cl">sudo cp cuda/lib64/libcudnn* /usr/local/cuda-10.2/lib64
</span></span><span class="line"><span class="cl">sudo chmod a+r /usr/local/cuda-10.2/include/cudnn*.h
</span></span><span class="line"><span class="cl">sudo chmod a+r /usr/local/cuda-10.2/lib64/libcudnn*</span></span></code></pre></td></tr></table>
</div>
</div><p>3.3 查看cudnn版本</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">cat /usr/local/cuda-10.2/include/cudnn.h <span class="p">|</span> grep CUDNN_MAJOR -A <span class="m">2</span></span></span></code></pre></td></tr></table>
</div>
</div><p>新版本:</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">cat /usr/local/cuda-10.2/include/cudnn_version.h <span class="p">|</span> grep CUDNN_MAJOR -A <span class="m">2</span></span></span></code></pre></td></tr></table>
</div>
</div><p>ref: <a href="https://blog.csdn.net/weixin_43592742/article/details/115689886?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&amp;spm=1001.2101.3001.4242"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_43592742/article/details/115689886?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
</ol>
<p><strong><a href="https://github.com/nvidia/TensorRT"target="_blank" rel="external nofollow noopener noreferrer">TensorRT的安装<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong></p>
<p><a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html"target="_blank" rel="external nofollow noopener noreferrer">英伟达提供的安装指导<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>tensorRT 要匹配cuda和cudnn版本。在安装之前请匹配。</p>
</blockquote>
<p>OSS 和 GA 两个版本:</p>
<ol>
<li>
<p>TensorRT OSS:</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone -b master https://github.com/nvidia/TensorRT TensorRT
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> TensorRT
</span></span><span class="line"><span class="cl">git submodule update --init --recursive</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>GA 版本(<a href="https://developer.nvidia.com/nvidia-tensorrt-download"target="_blank" rel="external nofollow noopener noreferrer">下载地址<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
</li>
<li>
<p>对GA版本和OSS版本在<code>~/.bashrc</code>文件中声明路径:
(GA: General Availability Stable Version)
(OSS: OPEN SOURCE)</p>
<ol>
<li>[oss版本路径]export TRT_SOURCE=/home/yejian/TensorRT/TensorRT_7.2.1</li>
<li>[GA Release 版本路径]export TRT_RELEASE=/home/yejian/TensorRT/TensorRT_7.2.1/TensorRT-7.2.1.6/TensorRT-7.2.1.6</li>
</ol>
</li>
<li>
<p>Build TensorRT RSS (这一步需要在编写自定义算子的时候编译通过，参能调用自定义算子)</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mysql" data-lang="mysql"><span class="line"><span class="cl"><span class="n">cd</span><span class="w"> </span><span class="err">$</span><span class="n">TRT_OSSPATH</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">mkdir</span><span class="w"> </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="n">build</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">cmake</span><span class="w"> </span><span class="p">..</span><span class="w"> </span><span class="o">-</span><span class="n">DTRT_LIB_DIR</span><span class="o">=</span><span class="err">$</span><span class="n">TRT_LIBPATH</span><span class="w"> </span><span class="o">-</span><span class="n">DTRT_OUT_DIR</span><span class="o">=`</span><span class="n">pwd</span><span class="o">`/</span><span class="k">out</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">make</span><span class="w"> </span><span class="o">-</span><span class="n">j</span><span class="err">$</span><span class="p">(</span><span class="n">nproc</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ol>
<h2 id="自定义算子开发----scatterelements">自定义算子开发 &ndash; ScatterElements</h2>
<p>在自定义算子开发过程中，需要撰写一下4个文件，并且把文件放在scatterElementsPlugin文件夹中:</p>
<ul>
<li><code>CmakeLists.txt</code></li>
<li><code>scatterElements.cu</code></li>
<li><code>scatterElementsPlugin.cpp</code></li>
<li><code>scatterElementsPlugin.h</code></li>
</ul>
<p>如图所示:</p>
<p></p>
<p><strong>自定义算子的生成与注册</strong></p>
<ul>
<li>将以上四个文件报括文件夹复制到TensorRT(OOS)下的plugin文件夹下;</li>
<li>然后修改注册信息文件:(这些文件也在plugin文件夹下)
<ul>
<li><code>${TRT_SOURCE}/plugin: CMakeLists.txt</code></li>
<li><code>${TRT_SOURCE}/InferPlugin.cpp</code></li>
<li><code>${TRT_SOURCE}/common/kernels/kernel.h</code></li>
<li><code>${TRT_SOURCE}/parsers/onnx/builtin_op_importers.cpp</code></li>
</ul>
</li>
</ul>
<p>执行完以上步骤以后，重新编译OOS版本，然后就可以调用自定义算子:</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="nv">$TRT_OSSPATH</span>
</span></span><span class="line"><span class="cl">mkdir -p build <span class="o">&amp;&amp;</span> <span class="nb">cd</span> build
</span></span><span class="line"><span class="cl">cmake .. -DTRT_LIB_DIR<span class="o">=</span><span class="nv">$TRT_LIBPATH</span> -DTRT_OUT_DIR<span class="o">=</span><span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>/out
</span></span><span class="line"><span class="cl">make -j<span class="k">$(</span>nproc<span class="k">)</span></span></span></code></pre></td></tr></table>
</div>
</div>]]></description></item><item><title>分布式训练 - 第5篇 - 分布式训练服务框架基本原理与架构解析</title><link>https://lruihao.cn/posts/distributedtraining_5/</link><pubDate>Thu, 13 Jul 2023 08:35:54 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/distributedtraining_5/</guid><description><![CDATA[<h2 id="1-概述">1. 概述</h2>
<p>分布式训练服务框架与集合通信库的组合构成了分布式训练的整体服务软件栈，在第3篇、第4篇文章里已经剖析完集合通信的相关内容，而本文会以Horovod为例介绍数据并行下分布式训练服务框架的基本原理以及进行架构解析。当前，在分布式训练里分布式训练服务框架需要解决以下几个核心问题 ：</p>
<ul>
<li>计算与通信同步耦合问题：如果反向传播一产生一份梯度，就马上对其调用全局AllReduce，计算与通信同步耦合，容易造成死锁同时性能也会很不如意；</li>
<li>计算时间与通信时间串行问题：神经网络是分层的，梯度计算的过程是数据加载，然后前向传播算出损失值，再反向传播算出梯度，而反向计算时梯度是从输出层往输入层方向一层一层产生的，在有些模型里，如果需要等所有的梯度都计算完毕才能触发全局AllReduce，那么对性能的影响也会很大；</li>
<li>梯度生成的落后者问题：集群内每个计算节点的同一份梯度的产生不一定都是同一时刻的，如果梯度没有全部生成就发起对这个梯度的全局规约，否则容易造成训练出来的模型精度不达标或者不收敛的问题；</li>
<li>梯度融合问题：如果每一份梯度都触发一次全局AllReduce，在梯度Tensor较多的神经网络训练里，整体的训练系统性能会变得极低；</li>
<li>易用性问题：从TensorFlow，PyTorch迁移过来需要改的代码需要极少，从单卡训练迁移到多卡训练需要改动的代码也需要极少；</li>
<li>可移植问题：支持多种多样的深度学习训练框架，比如 TensorFlow、PyTorch、MxNet等，也能支持多种多样的通信库，比如openMPI、NCCL、Gloo、CCL、RCCL等；</li>
<li>可靠性问题：在集群训练的过程中网络时不可靠的、计算卡是会出故障的、服务器是会出故障的、系统软件也是会出Bug的，这些因素造成了分布式训练过程中还存在可靠性问题，如何解决这个问题也是一个难题。</li>
</ul>
<p>软件是由人实现的，解析一个软件系统最难的地方在于从庞杂的代码里倒推出背后实现它的人的设计意图，为了更好的理解Horovod，本文会基于以上这几个分布式训练的核心问题，以Horovod为例介绍分布式训练服务框架的基本原理以及进行架构解析。</p>
<h2 id="2-基础知识">2. 基础知识</h2>
<h3 id="21-单卡训练">2.1 单卡训练</h3>
<p>神经网络的训练，本质上就是Y=F(x)的迭代，通过反复输入X、输出Y，使得神经网络的参数变化与输入输出间的复杂关系拟合。在神经网络训练的过程中，通过输入数据利用梯度下降的方法进行迭代从而优化神经网络参数，并最终输出神经网络模型。而神经网络可以看作一种运算模型，其由大量的神经元（节点）相互联接构成，其由输入层、隐藏层以及输出层组合而成（如下图左侧所示）。神经元(neuron)是神经网络的基本计算单元，也被称作节点(node)，它可以接受来自其他神经元或外部数据的输入，然后计算出一个输出（如下图右上角所示）。</p>
<p></p>
<p>如上图右下角所示，在单卡训练迭代中，基于并行梯度下降法，会有以下操作：</p>
<p>第一步，读取部分数据，并且将数据加载进训练卡的存储空间；</p>
<p>第二步，对模型进行前向传播计算，从输入层往输出层一层一层的进行计算，得到损失差LOSS；</p>
<p>第三步，对模型进行反向传播计算，从输出层往输入层一层一层的进行计算，得到梯度值，注意这一步会把每一层都计算出一个梯度张量（Gradient Tensor）出来；</p>
<p>第四步，将新的到的梯度与部分数据 作为新的输入，重新开始以上步骤的迭代。</p>
<p>在这一步里有一个很重要的与性能优化相关的信息是反向传播是每一层输出一个梯度张量，以及反向传播是从输出层往输入层一层一层的进行计算的，这一点信息可以用通信隐藏性能优化与梯度融合优化。</p>
<h3 id="22-多卡训练">2.2 多卡训练</h3>
<p>以数据并行随机梯度下降法( SGD )为例，多卡神经网络的训练过程如下图，与单卡训练相比，多卡训练多了梯度全局规约的过程：</p>
<p></p>
<p>第一步，通过Broadcast操作将第一个节点参数同步到集群内的所有的训练卡上，保证每个计算节点的初始参数是一致的，同时训练脚本在多个计算节点上运行，每个计算节点包含了整体的模型参数；</p>
<p>第二步，将数据样本切片分发到整个集群内的个计算节点（训练卡）上并且通过数据流水技术将数据样本加载进训练卡的高速内存空间内，作为输入X;</p>
<p>第三步，每个训练卡在其数据样本上运行前向传播，计算出损失差LOSSi；</p>
<p>第四步，对计算出的LOSSi进行反向传播，得到梯度GRADi，这一步也需要注意得是每一层都会计算出一个梯度，同时梯度是以输出的Tensor来表示的；</p>
<p>第五步，所有的训练卡计算出来的部分梯度，在主机内及主机之间通过集合通信进行全局归约(AllReduce)得到全局梯度；</p>
<p>第六步，最后再将这个全局梯度作为参数进行更新，再进行以上2-5步骤的迭代从而获得新的梯度。</p>
<p>以上2-6步骤就是多卡并行梯度下降的基本思想，即多个计算节点通过分片的数据样本进行梯度计算，得到分区梯度后，再通过全局梯度规约以及将这个聚合好的梯度作为新的参数进行更新，从而实现并行梯度下降。</p>
<h2 id="3-几个核心问题">3. 几个核心问题</h2>
<p>在本章节里会解读本文概述里提到的分布式服务框架需要解决的几个与性能、易用性等相关的几个核心问题，并且以Horovod为例讲述Horovod是如何解决这个几个难题的。</p>
<h3 id="31-计算与通信解耦">3.1 计算与通信解耦</h3>
<p>在神经网络的训练过程中，每一神经网络层都会计算出一个梯度，同时梯度是以输出Tensor来表示的，如果反向传播一计算出一个梯度就马上调用通信去做梯度规约，将计算与通信同步耦合，那么整体的性能的表现就会很差。比如一个ResNet-50 v3的梯度张量个数是153个，如果一计算出一个梯度就马上进行通信，假设计算梯度花了1ms，通信这个梯度花了 500ms，那么这个过程就是 501ms，总体上就需要501x153 = 76653ms，即近76.6s才能完成一次梯度迭代。而将计算与通信解耦，计算的归计算，通信的归通信，通过性能优化策略减少通信的次数，既能提升整体训练性能也能避免某些死锁问题，比如计算梯度grad i的时候花了很长时间，而通信线程一直在等待这个梯度，表现出来就是死锁现象。</p>
<p>Horovod采用计算与通信分离的设计思想，解耦了计算过程与通信过程，从而提升了整体训练的性能与可靠性。如下图的Horovod逻辑架构图所示，从图中可以看出Horovod解耦了计算与通信，其将框架层计算出来的梯度request信息push 到一个消息队列message_queue里，同时将梯度信息push到一个Tensor_table里，再通过控制层在后台起一个loop线程，周期性的从消息队列里读取梯度消息，在控制层集群的节点之间协商达成一致后，再进行消息分发触发训练行为。</p>
<p></p>
<p>如上图可看出，Horovod从下到上分为7层：物理层、链路层、数据传输层、控制层、消息层、框架层以及用户层。框架层，控制层以及数据传输层体现了Horovod的核心设计理念，即：框架层，用户可以自定义Op，以插件的形式hack进框架；在控制层，worker节点与master节点之间协商达成触发训练行为的约定；在数据传输层，服务器内以及服务器之间采用集合通信库传输数据。</p>
<p>本质上Horovod的整体设计理念之一遵循的是生产者消费者模式，如下图所示：</p>
<p></p>
<p>在Horovod里每个计算节点都会有有两个核心线程：Execution thread 和 Background thread ：</p>
<ul>
<li>生产者Execution Thread 是用来做梯度计算的，在TensorFlow、PyTorch之类的之类的训练框架计算出梯度Tensor后，将Tensor 信息push进tenor_table队列，同时将Tensor的request信息push进message_queue队列;</li>
<li>消费者Background thread 是做集合通讯以及全局Allreduce的，后台线程会每隔一段时间轮询消息队列，拿到一批Tensor信息之后，会进行相应的操作。</li>
</ul>
<h3 id="32-通信隐藏">3.2 通信隐藏</h3>
<p>神经网络是分层的，在训练的过程中，先是数据加载，然后前向传播算出LOSS，再反向传播算出梯度，而反向计算时梯度是从输出层往输入层方向一层一层产生的，如果需要等所有的梯度都计算完毕才能触发全局AllReduce，对性能不是很友好。如下图所示，计算时间与通信时间是串行的，如果能将全局梯度规约的通信时间与计算时间想办法并行起来，将通信时间隐藏在计算时间之内，那么就能节约梯度的训练时间从而提升分布式训练系统整体的训练性能。</p>
<p></p>
<p>如下图所示，将计算出来的梯度进行分桶触发异步Allreduce，一边反向传播计算梯度，一边做部分梯度的全局规约通信，从而达到将通信时间隐藏在计算时间内的效果。而Horovod为达成这一效果，Background thread 会每隔一段时间轮询梯度消息队列里的梯度信息，获取了可以过全局规约的梯度后，就进行全局规约操作，而这个时间其他的梯度还在计算过程中，通过调整轮询的时间间隔从而达到调整梯度分桶的效果。</p>
<p></p>
<h3 id="33-梯度协商">3.3 梯度协商</h3>
<p>神经网络的每一层对应一个梯度Tensor，在分布式训练集群里每张训练卡对同一份梯度计算产生的时间是有差异的，当集群内每个计算节点的同一神经网络层的同一梯度都产生时，才能发起对这个梯度的全局AllReduce规约，否则容易造成丢梯度，训练出来模型精度不达标或者模型不收敛。比如在一个128卡的训练集群里，同一份梯度是对应同一个神经网络模型里的同一层神经网络的，只有每张训练卡上都计算出了同一层神经网络的梯度 才能对这一层神经网络的梯度进行全局规约，如下图所示：</p>
<p></p>
<p>Horovod设计了一种梯度状态协商机制，它将 计算节点Rank0 作为coordinator（master），其余的rank1-N节点进程为worker，由coordinator来协商确定同一份梯度是否在每个计算节点上都已经计算出来，只有在每个计算节点上都计算出来的同一梯度才可以进行全局规约操作。在Horovod里每个计算节点上都有一个message_queue以及tensor_table，而在coordinator节点上除此之外，还有一个message_table用于保存可以进行全局Allreduce的梯度请求次数信息。Horovod 控制面的ComputeResponseList 函数里实现了这一梯度的协商过程，在从message_queue获取了本节点生成的梯度信息后，coordinator会与其他节点协商这个梯度是否都计算出来，这一过程是阻塞进行的，这个协商过程如下图：</p>
<p></p>
<p>一个梯度是否能满足全局规约AllReduce的协商过程如下：</p>
<p>首先，集群内的每个计算节点进程都会往coordinator Rank0发送一个 tensor的请求request，表示说本节点这一层神经网络的梯度已经生成，比如tensor1，每个rank都会往rank0 发送一个本梯度tensor1已经计算出来的请求信息；</p>
<p>第二步，coordinator接收到节点的梯度协商请求后（包括本节点），会把收到的tensor请求次数进行累加，并将这个信息记录在message_table里，当这个梯度的请求信息达到集群内节点的个数时，比如在N个节点的集群，一个神经网络层的梯度tensor的通信请求出现了N次，那就表示在本集群里所有的计算节点都已经发出了对该梯度tensor的通信request，这就表明这个梯度tensor是符合全局规约要求的，就能进行集合通信全局规约，不符合要求的梯度tensor将继续留在message_table中，直到条件符合为止；</p>
<p>第三步，再接着coordinator会将满足全局allreduce规约条件的梯度Tensor通过response返回给其他节点，告诉其他节点这个梯度可以启动全局规约AllReduce。</p>
<p>经过这几步的协商达成梯度全局状态一致的目的，从而避免梯度丢失造成的模型精度不达标、不收敛或者进程死锁问题。</p>
<h3 id="34-梯度融合">3.4 梯度融合</h3>
<p>神经网络的每一层都能对应一个梯度，假设每生成一个梯度就进行一次全局规约时，100个梯度就需要进行100次全局通信100次全局规约，而通信对训练的性能有巨大的影响，这种情况表现出来的效果就是分布式训练集群的整体性能极差。通过梯度融合计算将多个梯度合成一个，从而减少全局规约的次数能大幅提高分布式训练的训练性能，如下图所示，将N个小梯度Tensor合成两个，能将全局通信的次数减少到2次，从而大幅提升训练性能，在Horovod里这个功能对TensorFusion特性。但这个特性也会与3.2通信隐藏特性相冲突，需要根据具体情况进行合理的调试优化。</p>
<p></p>
<h3 id="35-易用性">3.5 易用性</h3>
<p>从TensorFlow，PyTorch等框架迁移到Horovod需要改的的代码极少，horovod接入方式比较简单，与原生训练框架对比，主要的区别在于：</p>
<ul>
<li>
<p>1，初始化 Horovod，包括机器资源的分配：
<code>horovod.init()</code></p>
</li>
<li>
<p>2，向每个进程分配XPU资源， 典型的设置是 1 个 XPU 一个进程，即设置 local rank：
<code>config.gpu_options.visible_device_list = str(hvd.local_rank())</code></p>
</li>
<li>
<p>3，对原优化器进行包装，分布式优化器将梯度计算委托给原始优化器，使用allreduce或allgather对梯度求平均，然后应用这些平均梯度：
<code>opt=hvd.DistributedOptimizer(opt)</code></p>
</li>
<li>
<p>4， 将初始化参数从rank 0广播给其他进程(rank表示进程序号)，实现参数的初始化，确保所有节点的初始化参数保持一致：
<code>hvd.BroadcastGlobalVariablesHook(0)：</code></p>
</li>
</ul>
<h3 id="36-可移植">3.6 可移植</h3>
<p>可移植问题，Horovod通过 OP和OpKernels的插件化机制支持多种多样的深度学习训练框架，比如 TensorFlow、PyTorch、MxNet等。基于的opKernels的可定制化机制，Horovod自定义了Op然后hack了数据链路层的通信协议，从而达到在多个深度学习框架之间可移植。</p>
<h3 id="37-可靠性问题">3.7 可靠性问题</h3>
<p>在集群训练的过程中网络时不可靠的、计算卡是会出故障的、服务器是会出故障的的，这些因素造成了分布式训练过程中需要考虑训练集群的可靠性，Horovod结合集合通信库Gloo对外提供了弹性训练的特性，但可靠性不只是弹性训练就能完全解决的，它还有更多的系统级的问题需要解决，因此可靠性问题留着一个后续研究问题，不在本文阐述。</p>
<h2 id="4-优点缺点改进点">4. 优点缺点、改进点</h2>
<ul>
<li>简单易用、可移植，并且支持弹性训练提升了可靠性；</li>
<li>不依赖于某个框架，其通过MPI机制独立建立了一套分布式训练服务系统；</li>
<li>将计算与通信分离，完成了allreduce、allgather等集合通信工作，实现了规模可扩展；</li>
<li>巧妙的通过间隔轮询的机制支持通信时间隐藏，并且完成了梯度协商从而保证训练出来的模型是可收敛、精度达标的；</li>
<li>支持梯度融合，支持将小的tensor合并成一个大的tensor再进行通信传递，从而减小通信操作的额外开销；</li>
<li>自带压缩算法，可以减少集合通信的数据量；</li>
</ul>
<h2 id="5-思考题">5. 思考题</h2>
<ul>
<li>问题1，将通信时间隐藏在计算时间内能有助于提升训练系统的整体性能，但这一特性是针对SIMT芯片的架构的进行性能优化的，如果DSA芯片不能支持这一特性，那应该如何优化Horovod从而大幅提升整体的训练性能？（可以确定这一定是能做到的）</li>
<li>问题2，梯度协商的过程中，每个梯度都需要协商一次，在梯度较多，网络规模较大的集群里，这一特性也会影响性能，如何进行优化才能有效提升Horovod性能？\</li>
<li>问题3，不同的模型对梯度融合有不同的要求，那么梯度融合需要融合到什么程度才能有效提升性能？</li>
</ul>
<p>可以说明的是，这三个问题解决后还能继续提升Horovod在DSA架构芯片上的整体的分布式训练系统级性能。</p>
<h2 id="6-小结">6. 小结</h2>
<p>本文介绍了分布式训练的基础知识以及剖析了分布式训练服务框架所面临的几个核心问题，以Horovod为例从计算与通信解耦、通信隐藏、梯度协商、梯度融合、易用性以及可移植这几个角度倒推了分布式训练服务框架背后的设计意图，从而帮助大家能更好的理解分布式训练服务框架。</p>
<p>ref:
[1] <a href="https://www.changping.me"target="_blank" rel="external nofollow noopener noreferrer">https://www.changping.me<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2] <a href="https://horovod.ai"target="_blank" rel="external nofollow noopener noreferrer">https://horovod.ai<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3] <a href="https://www.cnblogs.com/rossiXYZ/p/14910959.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14910959.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[4] <a href="https://zhuanlan.zhihu.com/p/374575049"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/374575049<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>分布式训练 – 第4篇 - 分布式训练常用的网络结构及集合通信拓扑算法</title><link>https://lruihao.cn/posts/distributedtraining_4/</link><pubDate>Thu, 13 Jul 2023 08:35:50 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/distributedtraining_4/</guid><description><![CDATA[<p>ref:
[1]. <a href="https://www.changping.me/2022/04/10/ai-distributed-training-coll-topo/"target="_blank" rel="external nofollow noopener noreferrer">https://www.changping.me/2022/04/10/ai-distributed-training-coll-topo/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="1-概述">1. 概述</h2>
<p>在深度学习的分布式训练里，Ring AllReduce拓扑算法奠定了数据并行训练的集合通信基础，但集合通信拓扑不只是仅有Ring Allreduce，经典的集合通信拓扑算法还有2D-Ring/Hierarchical Ring AllReduce，halving and doubling AllReduce，Butterfly AllReduce，2D-Torus AllReduce，2D-Mesh AllReduce，double binary tree等。拓扑算法很多，但也不是所有的拓扑算法都能满足实际的生产需求的，这需要具体问题具体分析、具体场景具体设计。</p>
<p>集合通信的<strong>难点</strong>在于需要在固定的网络互联结构的约束下进行高效的通信，集合通信拓扑算法与物理网络互联结构强相关，为了发挥网络通信的效率，也不是说就能随意发挥通信拓扑算法，更多的是在<strong>效率与成本</strong>、<strong>带宽与时延</strong>、<strong>客户要求与质量</strong>、<strong>创新与产品化</strong>等之间进行合理取舍。</p>
<p>充分发挥训练加速卡与网络的效率是通信拓扑算法的初衷，但除了设计高效的集合通信拓扑算法外，分布式训练中需要解决的通信难题还有：网络是异构的，网络带宽是有限的，主机内PCIE SWITCH是有亲和性的，网络是会出故障的，节点是有落后者效应的，设备成本是需要考虑的，数据中心是有部署约束的，用户是有多租户要求的等，这些属于产品化的范畴不在本文阐述。</p>
<h2 id="2-网络互联结构">2. 网络互联结构</h2>
<p>分布式训练的集合通信拓扑算法与物理的网络互联结构强相关，而网络互联结构又多种多样，因此，本文需要先对网络互联结构进行约束，依据生产中常用的、既定的互联结构设计集合通信算法，网络互联结构描述如下：</p>
<h3 id="21-服务内网络互联结构">2.1 服务内网络互联结构</h3>
<p>以一台集成了8张训练加速卡的服务器为例，如下图:</p>
<p></p>
<p>这台服务器内的网络互联情况如下：</p>
<p>1）在这台服务器内，8张训练加速卡通过私有协议连接组成多个主机内的物理ring环，且可双工；</p>
<p>2）服务期内网络带宽 NVLINK&gt;PCIE switch &gt; QPI；</p>
<p>3）加速卡1、2、3、4之间两两全互联，加速卡5,、6、7、8之间两两全互联，2、5、3、8之间非全互联；</p>
<p>4）加速卡1、4与网卡NIC1 挂在同一个PCIE Switch上，具有亲和性，加速卡2、3与网卡NIC2挂在同一个PCIE Switch上，具有亲和性，而PCIE Switch之间也互联，因此 加速卡 1、2、3、4 与网卡NIC 1、NIC2具备亲和性，它们无需通过CPU的QPI线进行通信；</p>
<p>5）加速卡5、8与网卡NIC3 挂在同一个PCIE Switch上，具有亲和性，加速卡6、7与网卡NIC4挂在同一个PCIE Switch上，具有亲和性，而PCIE Switch之间也互联的，因此 加速卡 5、6、7、8 与网卡NIC 3、NIC4具备亲和性，它们也无需通过CPU的QPI线进行通信；</p>
<p>6）网卡可根据需要 选择 1张、2张、4张或8张，最多可以采用8张RDMA物理网卡；</p>
<h3 id="22-服务器间网络互联结构">2.2 服务器间网络互联结构</h3>
<p>以一个训练加速卡集群为例，如下图是一个常用的CLOS互联架构方案:</p>
<p></p>
<p>在这个集群内，其网络互联情况如下：</p>
<p>1）集群内每台服务器自带高速RDMA网卡，通过RDMA 交换机在主机间两两全互联；</p>
<p>2）交换机组成CLOS架构，分为Spine与Leaf交换机，当然也可以是更为高端的Spine、Leaf合一的高端交换机；</p>
<p>3）RDMA网卡与Leaf交换机互联，每台服务器的RDMA网卡数量根据成本与性能考虑，可以是1张、2张+每卡虚拟化4卡、4张+每卡虚拟化2卡或8张；</p>
<h3 id="23-高速网卡及其虚拟化使用">2.3 高速网卡及其虚拟化使用</h3>
<p>RDMA网卡是双工的且可虚拟化，在这里每台服务器可根据成本、性能的考虑选用1张、2张、4张或8张，且在服务器内左右对称，如下图：</p>
<p></p>
<p>从成本与效率的角度考虑，每台服务器内的网卡可以是以下配置：</p>
<ul>
<li>1张物理RDMA网卡，不进行虚拟化，直接用双工通道，适合选用2D/Hierarchical Ring拓扑算法；</li>
<li>2张物理RDMA网卡，可以每张虚拟化出4个虚拟网卡，2X4共8卡，适合选用2D-MESH、2D-Torus拓扑算法；</li>
<li>4张物理RDMA网卡，可每张虚拟化出2个虚拟网卡，4X2共8卡，适合选用2D-MESH、2D-Torus拓扑算法；</li>
<li>8张物理RDMA网卡，不需要虚拟化，直接采用双工通道，适合选用2D-MESH、2D-Torus拓扑算法；</li>
</ul>
<p>在实际的分布式训练生产集群中，集合通信算法也可以结合RDMA网卡端口（包括虚拟化的）的具体个数进行设计，而拓扑算法的选择也是需要根据成本与效率的进行合理取舍的。</p>
<h3 id="24-网络结构抽象">2.4 网络结构抽象</h3>
<p>网络根据连接情况可分为<strong>ring结构</strong>、<strong>mesh结构</strong>、 <strong>torus 结构</strong>以及<strong>tree结构</strong>，基于以上的服务器内网络互联结构、服务器间网络互联结构以及网卡的具体情况，可以抽象出一个网络结构，即<strong>二维环面网络</strong>：Torus 网络，而Torus网络横向与纵向都可以看成ring结构，因此相应的拓扑算法基本上就是Ring-Based 集合通信拓扑算法。如下图：</p>
<p></p>
<p>TORUS网络是常见的大规模并行计算机的互连网络，在上图这个Torus网络里：</p>
<p>1）横向：主机内8卡通过私有连接协议，比如CXL/CCIX/NVLINK等组成一个或多个ring，如上图的黄色连接线，横向8卡组成二维Torus的横向维度；</p>
<p>2）纵向：主机间通过RDMA（RoCE/IB）网卡、交换机互联组成1到8个ring，如上图的红色连接线，纵向采用RDMA网卡组成二维Torus的纵向维度；</p>
<p>3）根据物理网卡数量、网卡虚拟化以及PCIe Switch亲和性的实际情况：</p>
<ul>
<li>每台服务器1张网卡可组成主机间一个ring，网卡与XPU0 挂载同一个PCIE switch上，依据最佳实践原则（比如性能、成本、客户要求等），适合选用2D/Hierarchical Ring拓扑算法；</li>
<li>两张网卡可组成主机间两个ring或者经过虚拟化组成8个ring，根据PCIE SWITCH亲和性原则，一张网卡与XPU0挂在同一个pcie switch，另一张网卡与XPU4挂在同一个pcie switch，依据最佳实践原则（比如性能、成本、客户要求等），适合选用2D-MESH、2D-Torus拓扑算法；</li>
<li>4张网卡、8张网卡以此类推，也是根据PCIE SWITCH亲和性原则进行连接，主机间RDMA物理网卡不够就虚拟化网口来凑，并且要服务器内的RDMA出口端口数左右平衡，依据最佳实践原则（比如性能、成本、客户要求等），也是适合选用2D-MESH、2D-Torus拓扑算法，这样才能发挥多张网卡以及XPU的算力优势。</li>
</ul>
<p>4）更复杂的Torus网络组合关系还可以如下图，从横向只有 主机内的8卡纵向只有主机间的RDMA互联，扩展到 横向与纵向 主机内互联与主机间互联混合，但本文仅限于在横向8卡的二维Torus网络下进行拓扑算法选择与设计，因此不展开讲述。</p>
<p></p>
<h2 id="3-常用的通信拓扑算法">3. 常用的通信拓扑算法</h2>
<p>Torus 网络结构可以解读本文中的物理网络互联结构的一切，而Torus网络的横向与纵向都可以看成ring结构，因此，相应的集合通信拓扑算法都可以看成是Ring-Based 集合通信拓扑算法。</p>
<h3 id="31-ring-allreduce">3.1 Ring AllReduce</h3>
<p>在分布式训练中，Ring 是最基础的互联结构，在本文中Ring AllReduce的应用场景是在服务器内将8张加速卡组环通信进行分布式训练。每个XPU都是这个主机内互联环上的一个计算节点，每个节点都有一个前向和一个后向，它只会向它的前向接收数据，并向它的右向发送数据，如下图所示，8张XPU 通过主机内的私有互联网络组成一个环，当然因为这些通信网络是双工的，这8张XPU训练加速卡也可以看成是通过多个逻辑环互联起来的，同时缺点是，如果这个ring太大，Ring Allreduce的效率也会变得很低。</p>
<p></p>
<p>Ring Allreduce 有两种组合实现策略：
1）先Reduce后broadcast；
2）先ScatterReduce后AllGather，这两个策略执行后都会让每个XPU节点得到一样的平均梯度，如下图所示：</p>
<p></p>
<h4 id="311-reduce-broadcast">3.1.1 Reduce +broadcast</h4>
<p>在Reduce + broadcast里，reduce先将8张卡的梯度reduce sum到master节点 XPU0 上，再通过broadcast将这个总的平均梯度复制给其他XPU，如下图：</p>
<p></p>
<p>Reduce + broadcast这种策略有几个比较大的缺点：
1）8张卡的数据都reduce sum到一张卡，假设每张卡的梯度是100MB，8张卡就是800MB，这可能存在XPU 0计算很久，而其他7张卡空闲的情况存在，整体效率不高；
2）XPU0 的网络带宽可能会成为瓶颈，8张卡的数据都只能通过XPU0的互联网络进行reduce和broadcast，在数据量比较大的场景 XPU0的带宽成为瓶颈；
3）8张XPU不都是两两全互联的，因此，要把8张卡的数据一次Reduce或broadcast，这一点受限于网络互联条件做不到，那么就需要采用 ring或tree的策略进行reduce或broadcast，这样效率也不高。</p>
<h4 id="312-scatterreduce--allgather">3.1.2 ScatterReduce + AllGather</h4>
<p>Ring AllReduce 的Ring ScatterReduce + Ring AllGather策略组合里，每个 XPU只会从前向接受数据，并发送数据给后向，其算法主要分为：</p>
<ul>
<li>ScatterReduce：这一步会先scatter拆分数据块再进行reduce，并且在执行完毕后，每张XPU都会包括一个完整的经过融合的同维梯度；</li>
<li>AllGather：这一步会进行全局Gather同步，最后所有 XPU都会得到完整的大的整个梯度；</li>
</ul>
<p>Ring ScatterReduce + Ring AllGather是效率比较高的 Ring AllReduce 组合策略，这个策略考虑到了XPU上的梯度可能很大的情况，比如一个梯度有400MB，在scatterreduce阶段就会先被拆分成 ring上XPU个数份，比如主机内XPU个数等于8，那么 这400MB 就会被 拆分成8份，每份50MB，从而减少了加速卡的计算量以及节约带宽。此外，scatterReduce通过将数据拆分成小块，同时并发进行scatterReduce，从而将通信时间隐藏在计算时间内进而提高Ring AllReduce的效率。</p>
<h5 id="3121-scatterreduce">3.1.2.1 ScatterReduce</h5>
<p>首先， ScatterReduce先将梯度拆分为N个更小的块，N等于ring里XPU个数，8张卡就拆分成8份，然后进行N-1次scatterreduce迭代。在第一轮迭代中XPU 0上的A0传递给XPU1上A1并相加，XPU1上的B1传递给XPU2上的B2并相加，XPU 2上的C2传递给XPU3上C3并相加，XPU3上的D3传递给XPU4上的D4并相加，以此类推，过程如下图左侧：</p>
<p></p>
<p>接下来，XPU还会进行N-2次 ScatterReduce 迭代，在每次迭代过程中，XPU都会从前向接收一个小梯度块并累加到自己的梯度块中，并且也会向其后向发送一个小梯度块，每个XPU接收和发送的小梯度块在每次迭代中都是不同的，这样经过迭代，到最后，每个XPU将有一个完整的同维梯度，该块梯度中包含所有XPU中该块对应的所有梯度的总和，如上图右侧的累加和部分。</p>
<h5 id="3122-allgather">3.1.2.2 Allgather</h5>
<p>在scatterReduce迭代完成之后，每个XPU都会得到一个同维度的完整的梯度累加值，将这些完整的累加值复制到其他的加速卡后，才算完成allReduce。Allgather的迭代次数与scatterReduce是相同的，也都需要进行N-1次（N是ring上的XPU卡数）迭代，但是不同于ScatterReduce的是allGather没有reduce的过程，只有数值的复制。这样迭代到最后，每个XPU都得到大的拆分前的梯度的完整累加值，如下图演示了这一过程，从第一次迭代开始，到最后AllGather拿到整体的结果。这里头的具体过程就不在这里描述了，可以查相关资料。</p>
<p></p>
<p>Ring AllReduce 实现简单，在ring较少时，效率也较高，但是在ring比较大时需要的网络节点跳数变得比较大，通信时延增加，因此效率也会降低。比如，一个1000张XPU的 ring，这里头网络的跳数 是N-1= 1000-1 =999， 同时传输的过程中，传输效率还受效率最低、带宽最低的XPU的限制，这时网络上的时延会变得巨高，这个时候ring allreduce拓扑算法就变得不大适用这个场景，同时如果在异构网络里涉及网络的不同连接方式，Ring AllReduce也不大适合使用，因此就需要采用另外的更适合网络结构的更高效的集合通信拓扑算法来进行优化。</p>
<h3 id="32-2d-ring-allreduce">3.2 2D-Ring AllReduce</h3>
<p>如果一台2.1里的服务器只配置了一张RDMA网卡，每台服务器通过RDMA交换机互联，这个集群的网络是异构的（如下图），那么Ring AllReduce拓扑算法就不适用了，这个时候，对于这个网络拓扑结构比较适合的是2D-Ring AllReduce也叫Hierarchical Ring AllReduce。</p>
<p></p>
<p>经过抽象，可以将这个网络结构表达成如下的Torus结构：</p>
<p>横向：每台服务器8个XPU节点，每个XPU节点通过私有协议网络互联；</p>
<p>纵向：每台服务器通过一张RDMA网卡NIC 0 通过交换机互联，这个网卡NIC0 与XPU0 挂在同一个PCIE switch上，满足具备亲和性条件，XPU0上的梯度可以通过NIC 0 与其他服务器上的XPU进行全局规约。</p>
<p></p>
<p>2D-Ring AllReduce的过程如下图所示：</p>
<p></p>
<p>第1步，先进行主机内Ring AllReduce，也可以是 Ring Reduce或者根据主机内的互联情况选用的分层reduce方式，将8张卡上的梯度累加到Master节点 XPU0 上；</p>
<p>第2步，进行主机间XPU 0的 Ring AllReduce，将每台服务器的XPU0上的数据进行全局规约；</p>
<p>第3步，进行主机内Broadcast，将XPU0上的梯度复制到服务器内的其他XPU上</p>
<p>2D-Ring AllReduce能充分发挥异构网络的优势，将主机内、主机间的网络带宽充分利用起来。但是XPU的利用率也不是很高，比如在做主机间的Ring AllReduce，每台服务器内的其他7张XPU是处于空闲状态的。</p>
<p>再假设，如果每台服务器配置了 2张/4张/8张RDMA网卡，这个时候 2D-RING AllReduce又难以将网络的优势发挥出来，那么就需要选用 2D-Torus/2D-Mesh AllReduce拓扑算法。</p>
<h3 id="33-2d-torus-allreduce">3.3 2D-Torus AllReduce</h3>
<p>考虑到服务器内PCIE SWITCH 的亲和性问题，2D-Torus至少需要配备2张 左右对称的RDMA网卡才能发挥这个拓扑算法的优势。在这个集群里主机内每张卡都通过私有的通信协议组成Ring，而主机间，可以通过RDMA网卡（包括虚拟化出来的）与RDMA交换机将XPU两两互联，这个网络也是异构的，如下图所示：</p>
<p></p>
<p>经过抽象，可以将这个网络结构表达成如下的Torus结构：</p>
<ul>
<li>横向：每台服务器8个XPU节点，每个XPU节点通过私有协议网络互联；</li>
<li>纵向：每台服务器通过至少2张RDMA网卡NIC 0 /NIC 1通过交换机互联，这个网卡NIC0 与XPU0、1、2、3 挂在同一个PCIE switch上，具备亲和性条件，XPU0、1、2、3上的梯度数据可以通过NIC 0 与其他服务器上的XPU进行交换。网卡NIC1 与XPU4、5、6、7 挂在同一个PCIE switch上，具备亲和性条件，XPU4、5、6、7上的梯度数据可以通过NIC 1 与其他服务器上的XPU进行交换；</li>
<li>当然如果网卡是4个或者8个，也可以根据PCIE SWITCH的亲和性情况合理安排XPU与NIC的对应关系。</li>
</ul>
<p></p>
<p>2D-Torus AllReduce的过程如下图所示：</p>
<p></p>
<p>第1步，横向，先进行主机内Ring ScatterReduce，将主机内8张卡上的梯度进行拆分与规约，这样经过迭代，到最后每个XPU将有一个完整的同维梯度，该块梯度包含所有XPU中该块所对应的所有梯度的总和（参考3.1.2.1 scatterReduce)</p>
<p>第2步，纵向，进行主机间N个（N等于服务器内XPU个数，这里是8个）纵向的 Ring AllReduce，将每台服务器的XPU0-XPU7上的数据进行集群内纵向全局规约；</p>
<p>第3步，横向，进行主机内AllGather，将XPUi(i=0-7)上的梯度复制到服务器内的其他XPU上；</p>
<p>2D-Torus AllReduce能充分挖掘XPU的效率以及发挥异构网络里多网卡的优势，将XPU以及主机内、主机间的网络带宽优势充分利用起来。此外，除了 2D-Torus AllReduce外，2D-Mesh AllReduce也能发挥类似效率。</p>
<h3 id="34-2d-mesh-allreduce">3.4 2D-Mesh AllReduce</h3>
<p>2D-Mesh AllReduce的主要思想也是分层，与2D-Torus AllReduce类似，都是水平和垂直两个方向，但是有点差异，如下图所示：</p>
<p></p>
<p>不同于2D-Torus AllReduce的拓扑算法，2D-Mesh AllReduce 过程是：</p>
<p>第1步，横向，先进行主机内Ring AllReduce 将主机内的8张XPU的梯度都进行规约；</p>
<p>第2步，纵向，进行主机间N个（N等于主机内XPU个数，这里是8个）纵向的 Ring AllReduce；</p>
<p>经过这两步，完成了整体的梯度累加，2D-Mesh AllReduce 也能充分发挥XPU与多网卡异构网络的优势，将XPU与主机内、主机间的网络带宽优势充分利用起来。这里的2D-Mesh与Google论文上的有点差异，主要是吸取了其分层的思想而不是复制其一样的设计。理论上2D-Mesh AllReduce对比 2D-Torus AllReduce，主机间AllReduce用的是 主机内8卡的全局梯度，数据量会比ScatterReduce部分来的大点，因此效率也会相应降低一点。</p>
<h2 id="4-问题探讨">4. 问题探讨</h2>
<p>如下图所示，基于Torus网络的结构，组合Ring AllReduce，2D-Ring AllReduce, 2D-Mesh AllReduce，2D-Torus AllReduce还能构建 3D-Ring/Mesh/Torus AllReduce拓扑算法，但是这些拓扑算法的效率需要进行实践才能证实，也许在规模较大的集群里才能发挥出3D 拓扑算法的优势。</p>
<p></p>
<p>关于 3D-Ring/Mesh/Torus AllReduce的拓扑算法，这里就不在阐述，可作为研究使用。</p>
<h2 id="5-小结">5. 小结</h2>
<p>本文讲述了分布式训练里最常用的几个网络结构以及通信拓扑算法：</p>
<ul>
<li>Ring AllReduce 的最佳组合是 ScatterReduce + AllGather；</li>
<li>2D-Ring AllReduce = 主机内 ringAllReduce/Ring Reduce +主机间 RingAllReduce + 主机内Broadcast；</li>
<li>2D-Torus AllReduce = 主机内 Ring ReduceScatter + 主机间N个Ring AllReduce + 主机内Ring AllGather；</li>
<li>2D-Mesh AllReduce = 主机内Ring AllReduce + 主机间N个Ring AllReduce;</li>
</ul>
<p>Ring AllReduce适合主机内互联Ring的情况使用，2D-Ring AllReduce适合一台服务器配置了一张网卡的异构网络场景，2D-Torus AllReduce与2D-Mesh AllReduce适合一台服务器配置了2/4/8张网卡的异构网络场景。</p>
<p>集合通信拓扑算法多种多样，但基于成本以及效率的取舍考虑，可生产适用的其实也不多，除了理论上的理解之外更重要的是自己编写代码去实践落地。除此之外，还需要解决网络带宽有限、网络容易出故障、落后者效应、部署约束、多租户等产品化的质量要求。</p>
<p>REF:
[1] <a href="https://www.changping.me"target="_blank" rel="external nofollow noopener noreferrer">https://www.changping.me<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[2] 《volta-architecture-whitepaper》</p>
<p>[3] 2D-HRA: Two-Dimensional Hierarchical Ring-based All-reduce Algorithm in Large-Scale Distributed Machine Learning</p>
<p>[4] Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash</p>
<p>[5] <a href="https://zhuanlan.zhihu.com/p/79030485"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/79030485<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> , 腾讯机智团队分享–AllReduce算法的前世今生</p>
<p>[6] <a href="https://zhuanlan.zhihu.com/p/370548366"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/370548366<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, ring allreduce和tree allreduce的具体区别是什么？</p>
<p>[7] <a href="https://zhuanlan.zhihu.com/p/184942777"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/184942777<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> , 分布式深度学习初探</p>
<p>[8] <a href="https://arxiv.org/abs/1811.06992"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/1811.06992<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> ， Image Classification at Supercomputer Scale</p>
]]></description></item><item><title>分布式训练 – 第3篇 - 集合通信及其通信原语</title><link>https://lruihao.cn/posts/distributedtraining_3/</link><pubDate>Thu, 13 Jul 2023 08:35:39 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/distributedtraining_3/</guid><description><![CDATA[<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/493092647"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/493092647<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="概述">概述</h2>
<p>集合通信（Collective Communications）是一个进程组的所有进程都参与的全局通信操作，其最为基础的操作有 <strong>发送send</strong>、<strong>接收receive</strong>、<strong>复制copy</strong>、<strong>组内进程栅障同步Barrier</strong>以及<strong>节点间进程同步(signal+wait)</strong>，这几个最基本的操作经过组合构成了一组通信模板也叫通信原语，比如：<u>1对多的广播broadcast</u>、<u>多对1的收集gather</u>、<u>多对多的收集all-gather</u>、<u>1对多的发散scatter</u>、<u>多对1的规约reduce</u>、<u>多对多的规约all-reduce</u>、<u>组合的规约与发散reduce-scatter</u>、<u>多对多的all-to-all</u>等，<font color=red>集合通信的难点在于通信效率以及网络硬件连接拓扑结构的最佳适用</font>。</p>
<h2 id="通信原语">通信原语</h2>
<p>以一台集成了4张训练加速卡的服务器为例，如下图，服务器内四张训练加速卡是全连接的，物理连接方式可以是私有物理互联协议，比如CXL、NVLINK，也可以是PCIe、InfiniBand、Ethernet等，本文将以此物理拓扑结构描述集合通信中常用的几组通信原语。</p>
<p></p>
<h3 id="broadcast">Broadcast</h3>
<p><font color=red>Broadcast属于1对多的通信原语</font>，一个数据发送者，多个数据接收者，可以在集群内把一个节点自身的数据广播到其他节点上。如下图所示，圈圈表示集群中的训练加速卡节点，相同的颜色的小方块则代表相同的数据。当主节点 0 执行Broadcast时，数据即从主节点0被广播至其他节点。</p>
<p></p>
<p>Broadcast是数据的1对多的同步，它将一张XPU卡上的数据同步到其他所有的XPU卡上，其应用场景有：</p>
<p>1）数据并行的参数初始化，确保每张卡上的初始参数是一致的；</p>
<p>2）allReduce里的 broadcast + reduce组合里的broadcast操作；</p>
<p>3）分布式训练parameter server 参数服务器结构里的 master节点 broadcast 数据到worker节点，再从worker节点reduce数据回master节点里的broadcast操作；</p>
<h3 id="scatter">Scatter</h3>
<p>同Broadcast一样，<font color=red>Scatter也是一个1对多的通信原语</font>，也是一个数据发送者，多个数据接收者，可以在集群内把一个节点自身的数据发散到其他节点上。与Broadcast不同的是Broadcast把主节点0的数据发送给所有节点，而Scatter则是将数据的进行切片再分发给集群内所有的节点，如下图所示，不相同的颜色的小方块代表不相同的数据，主节点 0 将数据分为四份分发到了节点0-3。</p>
<p></p>
<p>Scatter是数据的1对多的分发，它将一张XPU卡上的数据进行分片再分发到其他所有的XPU卡上，他的反向操作对应Gather，其应用场景有:
1）ReduceScatter组合里的 Scatter操作；
2）模型并行里初始化时将模型scatter到不同的XPU上；</p>
<h3 id="gather">Gather</h3>
<p><font color=red>Gather操作属于多对1的通信原语</font>，具有多个数据发送者，一个数据接收者，可以在集群内把多个节点的数据收集到一个节点上，如下图所示，不相同的颜色的小方块代表不相同的数据。</p>
<p></p>
<p>Gather是数据的多对1的收集，它将多张XPU卡上的数据收集到1张XPU卡上，他的反向操作对应Scatter，其应用场景有：</p>
<p>1）ReduceScatter组合里的 Scatter操作；</p>
<h3 id="allgather">AllGather</h3>
<p><font color=red>AllGather属于多对多的通信原语</font>，具有多个数据发送者，多个数据接收者，可以在集群内把多个节点的数据收集到一个主节点上（Gather），再把这个收集到的数据分发到其他节点上（broadcast），即收集集群内所有的数据到所有的节点上。</p>
<p></p>
<p>AllGather是数据的多对多的同步全收集，它将多张XPU卡上的数据收集到多张XPU卡上，可以看做Gather + Broadcast的操作组合，它的反向操作对应ReduceScatter，其最应用场景有：</p>
<p>1） AllGather可应用于模型并行；</p>
<p>2）模型并行里前向计算里的参数全同步，需要用allgather把模型并行里将切分到不同的XPU上的参数全同步到一张XPU上才能进行前向计算。</p>
<h3 id="reduce">Reduce</h3>
<p><font color=red>Reduce属于多对1的通信原语</font>，具有多个数据发送者，一个数据接收者，可以在集群内把多个节点的数据规约运算到一个主节点上，常用的规约操作符有：求累加和SUM、求累乘积PROD、求最大值MAX、求最小值MIN、逻辑与 LAND、按位与BAND、逻辑或LOR、按位或BOR、逻辑异或LXOR、按位异或BOXR、求最大值和最小大的位置MAXLOC、求最小值和最小值的位置MINLOC等，这些规约运算也需要加速卡支持对应的算子才能生效。</p>
<p>Reuduce操作从集群内每个节点上获取一个输入数据，通过规约运算操作后，得到精简数据，如下图的SUM求累加和：节点0数值 5、节点1数值6、节点2数值7、节点3数值8，经过SUM运算后 累积和为 26，即得到更为精简的数值，在reduce原语里回会去调用 reduce SUM算子来完成这个求和累加。</p>
<p></p>
<p>Reduce是数据的多对1的规约运算，它将所有张XPU卡上的数据规约（比如SUM求和）到1张XPU卡上，其应用场景有：</p>
<p>1）AllReduce里的 broadcast + reduce组合里的reduce操作；</p>
<p>2）ReduceScatter组合里的 reduce操作；</p>
<p>3）分布式训练parameter server 参数服务器结构里的 master节点 broadcast 数据到worker节点，再从worker节点reduce数据回master节点里的reduce操作；</p>
<h3 id="reducescatter">ReduceScatter</h3>
<p>ReduceScatter属于多对多的通信原语，具有多个数据发送者，多个数据接收者，其在集群内的所有节点上都按维度执行相同的Reduce规约运算，再将结果发散到集群内所有的节点上，Reduce-scatter等价于节点个数次的reduce规约运算操作，再后面执行节点个数的scatter次操作，其反向操作是AllGather。</p>
<p>如下图所示，先reduce操作 XPU 0-3的数据reduce为 A(A0+A1+A2+A3) + B(B0 + B1 +B2 + B3) + C(C0 + C1 + C2 + C3) + D(D0 + D1 + D2 + D3 ) 到一张XPU上，再进行分片scatter到集群内所有的XPU卡上。</p>
<p></p>
<p>ReduceScatter是数据的多对多的reduce + scatter运算，它将所有的XPU卡上的数据先规约（比如SUM求和）到1张XPU卡上，再进行scatter，其应用场景有：</p>
<p>1）ReduceScatter即可应用于数据并行也可应用于模型并行；</p>
<p>2）数据并行allReduce里的 ReduceScatter+ Allgather组合里的ReduceScatter操作；</p>
<p>3）模型并行里在前向allgather后的反向计算里的ReduceScatter；</p>
<h3 id="allreduce">AllReduce</h3>
<p>AllReduce属于多对多的通信原语，具有多个数据发送者，多个数据接收者，其在集群内的所有节点上都执行相同的Reduce操作，可以将集群内所有节点的数据规约运算得到的结果发送到所有的节点上。AllReduce操作可通过在主节点上执行Reduce + Broadcast或ReduceScatter + AllGather实现，如下图所示：先在主节点上执行reduce得到规约累加和26，再把这个累加和26 broadcast到其他的节点，这样整个集群内，每个节点的数值就都保持一致。</p>
<p></p>
<p>AllReduce是数据的多对多的规约运算，它将所有的XPU卡上的数据规约（比如SUM求和）到集群内每张XPU卡上，其应用场景有：</p>
<p>1） AllReduce应用于数据并行；</p>
<p>2）数据并行各种通信拓扑结构比如Ring allReduce、Tree allReduce里的 allReduce操作；</p>
<h3 id="all-to-all">All-To-All</h3>
<p>All-To-All操作每一个节点的数据会scatter到集群内所有节点上，同时每一个节点也会Gather集群内所有节点的数据。ALLTOALL是对ALLGATHER的扩展，区别是ALLGATHER 操作中，不同节点向某一节点收集到的数据是相同的，而在ALLTOALL中，不同的节点向某一节点收集到的数据是不同的，如下图所示:</p>
<p></p>
<p>AllToAll是数据的多对多的转置，它将所有张XPU卡上的数据转置到所有的XPU卡上，其主要应用场景有：</p>
<p>1） AllToAll应用于模型并行；</p>
<p>2）模型并行里的矩阵转置；</p>
<p>3）数据并行到模型并行的矩阵转置；</p>
<h3 id="send-与-receive">Send 与 Receive</h3>
<p>数据或参数在不同XPU之间的发送与接收。</p>
<h3 id="barrier">Barrier</h3>
<p>BARRIER同步操作会阻塞所有的调用者直到所有的组内成员都调用了它， 用于一个集合通信子中所有进程的同步，调用函数时进程将处于等待状态，直到通信子中所有进程 都调用了该函数后才继续执行。</p>
<h3 id="signal与wait">Signal与Wait</h3>
<p>Signal与Wait属于记录型信号量机制： wait(s)，signal(s)可用于解决进程间的同步问题，在通信原语里从一个节点发送一个数据到另外一个节点时，会同时signal一个event值到对端，对端的wait操作接收到这个event时会返回一个确认给signal，这样保证在节点的进程间进行数据的同步操作。</p>
<h2 id="小结">小结</h2>
<p>在分布式训练过程中，深度学习训练框架不会去直接操作底层的通信网络，而是通过使用网络通信库来完成数据的集合通信，各家AI芯片加速卡厂家都会提供私有的网络通信库比如：xxx-AWARE OpenMPI或xCCL来完成这个底层通信硬件的屏蔽与抽象。在分布式训练集群里网络通信硬件连接样式多种多样，可以是Ethernet、InfiniBand 、RoCE v2/v1 等也可以是CXL、NVLINK等私有协议，这就要求在通信的后端层根据各个厂家的自己的SDK开发库接口，根据实际情况实现 各自的网络通信库，比如cuda-aware MPI、NCCL、NVSHMEM，以及根据实际的网络拓扑组合完成对应的最有效的网络拓扑算法。</p>
<p>本文讲述了分布式训练里的集合通信原语，这些原语是集合通信拓扑算法的基本组成单元，后续的文章里会讲述如何组合这些通信原语以完成合适的通信拓扑算法。</p>
]]></description></item><item><title>分布式训练 – 第2章 - 训练与系统评价指标</title><link>https://lruihao.cn/posts/distributedtraining_2/</link><pubDate>Thu, 13 Jul 2023 08:35:37 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/distributedtraining_2/</guid><description><![CDATA[<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/492667659"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/492667659<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="前言">前言</h2>
<p>不同于教科书里讲的深度学习的评价指标，这里主要讲述生产训练中常用的评价指标。通常在分布式训练中对训练的过程与结果会进行评价，比如选择一个评价指标：准确率，即表明模型求解给定问题的准确度。而本文提到的评价指标主要分为两大类，即<font color=red>训练结果评价</font>与<font color=red>训练系统评价</font>。</p>
<h2 id="训练指标">训练指标</h2>
<p>教科书里经常提到的深度学习的评价指标有准确率、精确率、召回率、F1值等，如下：</p>
<ul>
<li>准确率（Accuracy），所有的预测正确（正类负类）的占总的比重；</li>
<li>精确率（Precision），查准率，即正确预测为正的占全部预测为正的比例；</li>
<li>召回率（Recall），查全率，即正确预测为正的占全部实际为正的比例；</li>
<li>F1值（H-mean值），F1值为算数平均数除以几何平均数，且越大越好；</li>
</ul>
<p>实际上这些指标在真正的生产过程中用的不多，在实际的分布式训练过程中，比较关心的训练评价指标有：</p>
<ul>
<li>加速比（speedup），即多卡训练下的单卡吞吐量平均指标除以单卡训练下的吞吐量平均指标，比如，大规模训练下的 ResNet-50 v1.5的单卡FPS指标是600，而单卡训练的FPS指标是800，那么加速比即 600/800 = 0.75，加速比体现的是训练集群的效率与可扩展性，越高的加速比表明训练集群的资源利用率越高，但是越高的加速比要求对训练集群的技术要求也越高。比如 一个 1000张卡的训练集群，要求 加速比 0.9以上，那么对于主机间的网络、主机内的网络、全栈软件、训练卡内部的硬件架构、集合通信拓扑算法、训练算法的优化等的要求都极高，这就涉及到整个分布式训练系统的问题，而不是单个点能彻底解决的；</li>
<li>吞吐量，sequence/sec 或 FPS, 即每秒能处理的图片数或数据量；</li>
<li>收敛时间（Time）与训练次数（epoch），生产过程中对训练所有的时间是有要求的，假设给定一个模型的训练次数(epoch)为100，如果要把这个100次都训练完需要 好几天，甚至好几个星期，那么可以认为生产不适用，基本上可以定义 训练一个模型到收敛需要 24小时以上，都可以看做是生产不适用，需要扩大训练集群的规模，使之训练时间控制在24小时之内；</li>
<li>平均准确率(eval Accuracy)，平均准确率是训练是否收敛的重要评判标准之一，比如定义一个 Resnet50 v1.5 的训练模型的准确率为 76%，如果训练结束的平均准确率能达到这个值就认为训练是收敛的；</li>
<li>可收敛，训练的最终结果可以达到 平均准确率的要求，即认为可收敛，否者即任务训练失败；</li>
<li>学习率(Learning rate)与损失率(Loss)，学习率大模型训练学习速度快，但是易导致损失率爆炸, 学习率小模型训练学习速度慢，而且容易过拟合，收敛速度慢；</li>
<li>曲线拟合(Curve Fitting)，这是一个非常重要的评价手段，在XPU训练的场景下，通常先用一个已有的之前训练好模型为基础或先用GPU训练出一个基础模型，然后把XPU训练的结果指标跟GPU训练模型的指标进行比较，曲线拟合即认为XPU的训练结果达标，这也是调试XPU训练结果的一个重要手段。这里埋一个问题，按照曲线拟合的说法，假设有一个2000张XPU卡的集群，怎样评价这个集群训练的结果是正确的？以GPU训练的结果做比较，那么找一个这么大规模的GPU集群进行训练然后得到想要的模型做基础匹配也是不大现实的，那么需要采用什么技术方案才能解决这个问题？</li>
</ul>
<p>以TensorBoard为例，说明模型的评价指标，在下面的命令行列输入一个baseline:/log_path_2：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">tensorboard --logdir<span class="o">=</span>training_model:/log_path_1, baseline:/log_path_2</span></span></code></pre></td></tr></table>
</div>
</div><p>这个baseline 的模型已经确定是精度达标，生产可用的。然后 XPU训练的模型的 <code>training_model:/log_path_1</code> 与这个GPU训练处的baseline进行比，在tensorboard里可以表现如下图：</p>
<p></p>
<p>在上图里，新的模型的eval_accuracy值与baseline的值最终是一样的，这说明训练结果是收敛且精度达标，eval_accuracy中间的线有点差异是由于按不同的训练次数进行tensorboard指标保存所造成。新模型的Loss线与Learning_rate 线也与基础线吻合，这说明XPU训练的模型质量可生产适用。eval_accuracy、Loss、Learning_rate是三个最重要的度量指标，只要这样三个指标达标，那么大概率即可判断这个在XPU下新训练的模型具备生产可用能力。</p>
<h2 id="系统指标">系统指标</h2>
<p>分布式训练系统其本身也是一个分布式系统，因此除了训练领域相关的度量指标，也有与分布式系统质量有关的一套度量指标，其中比较重要的几项内容如下：</p>
<ul>
<li>可用性(Availability)，可用性指的是分布式训练系统长时间可对外提供服务的能力，通常采用小数点后的9的个数作为度量指标，按照这种约定“五个九”等于0.99999（或99.999％）的可用性，默认企业级达标的可用性为6个9。但是当前从时间维度来度量可用性已经没有太大的意义，因为设计得好的系统可以在系统出现故障得情况下也能保证对外提供得服务不中断，因此，当前更合适得可用性度量指标 是请求失败率;</li>
<li>可靠性(Reliability)，可靠性一般指系统在一定时间内、在一定条件下可以无故障地执行指定功能的能力或可能性， 也是采用小数点后的9的个数作为度量指标，通常5个9的可靠性就可以满足企业级达标；</li>
<li>可伸缩性(Scalability)，是指通过向系统添加资源来处理越来越多的工作并且维持高质量服务的能力，其受可用性以及可靠性的制约，集群规模越大出故障的概率越高从而降低可用性、可靠性，为了保证可用性以及可靠性达标，需要适配合理的可伸缩性指标；</li>
<li>韧性(resilience)，通常也叫容错性（fault-tolerant），也就是健壮和强壮的意思，指的是系统的对故障与异常的处理能力，比如在软件故障、硬件故障、认为故障这样的场景下，系统还能保持正常工作的能力，分布式训练系统的容错能力是一个非常重要的指标。</li>
</ul>
<h2 id="小结">小结</h2>
<p>本文从实践的角度讲述了分布式训练的训练结果评价指标与系统评价指标，这些指标是度量一个分布式训练系统与训练的模型是否生产可用的重要参考。日拱一卒，功不唐捐，分享是最好的学习，与其跟随不如创新，希望这个知识点对大家有用。</p>
]]></description></item><item><title>分布式训练 – 第1章 - 什么是分布式训练</title><link>https://lruihao.cn/posts/distributedtraining_1/</link><pubDate>Thu, 13 Jul 2023 08:35:27 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/distributedtraining_1/</guid><description><![CDATA[<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/487945343"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/487945343<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="前言">前言</h2>
<p>深度学习软件工程具有一体两面性：单卡的功能完备性、质量、用户体验以及多卡大规模。多卡大规模的出现是为了解决这样一个主要矛盾，即：“日益增长的数据、模型训练的需求与当前单卡计算能力无法满足这个需求之间的矛盾”，而分布式训练可以通过扩展卡子的规模解决这个矛盾，因此，这就是分布式训练的价值。</p>
<p>然而，正如懂得很多道理，仍旧过不好这一生一样，懂得很多分布式训练的理论与知识，也不一定就能做好一个分布式训练系统。把这么多机器连接跑起来、跟跑好也是两回事，分布式训练是一门实践的软件工程，只有你PK过设计方案，调试过一个个Bug，手把手的敲过一行行的代码，为了性能指标能达标无所不用其极的去验证各种性能优化方案，才能知道细节在哪里，难点在哪里，痛点、挑战点在哪里。因此，宏观处着眼，微观处着手，才能完全理解分布式训练的道理。</p>
<p>一个知识领域里的 “道 法 术 器” 这四个境界需要从 微观、中观以及宏观 三个角度来把握，微观是实践，中观讲方法论，宏观靠领悟。本系列文章我把它命名为《分布式训练》，从工程实战的角度拆解分布式训练里最重要的套路，也是从“微观实践、中观方法论、宏观领悟”这三个维度系统性的探讨分布式训练技术，本文讲述第一篇，也是最难讲清楚的一篇（后续保持迭代更新），即本质的一问：<strong>&ldquo;什么是分布式训练</strong>&quot;。</p>
<h2 id="什么是分布式训练">什么是分布式训练</h2>
<p>简单来说，<strong>分布式训练 = 分布式训练系统 = 分布式系统 + 训练系统</strong>，因此，要解答什么是分布式训练就需要解答什么是分布式系统以及什么是训练系统，而“系统 = 要素x连接 + 目的 + 边界”，因此进一步的就是需要分析分布式系统的要素、连接、目的与边界以及训练系统的要素、连接、目的与边界。</p>
<h3 id="分布式系统">分布式系统</h3>
<p>在AI训练过程中采用单卡总会遇到一些问题，比如原始的数据样本太大无法加载进训练卡，或者模型太大无法训练，那么这就需要用到分布式技术把大量的数据分割成小块由多个训练卡分别进行计算，在更新运算结果后，再将结果统一合并得出最终的可用模型。百科上对分布式系统的定义有：</p>
<blockquote>
<p>A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal.</p>
</blockquote>
<p>即：</p>
<blockquote>
<p>分布式系统是指其组件位于不同的网络计算机上的系统，这些组件通过相互传递消息来进行通信和协调其动作，且彼此相互交互以完成一个共同的任务目标。</p>
</blockquote>
<p>从这句话可以得出三个结论：</p>
<ul>
<li>分布式系统的组件是位于不同的网络计算机上的；</li>
<li>分布式系统的组件通过传递消息进行通信与协调的；</li>
<li>分布式系统的组件是通过相互交互以完成一个共同的任务目标，同时是有边界的；</li>
</ul>
<p>因此基于此定义，拆解分布式系统的概念，从中可以看到分布式系统里的要素即为组件，连接即网络，目的是共同的任务目标。其中的位于不同的网络计算机上的“组件”是分布式系统的要素，即各种计算单元，比如Ai训练加速卡，“网络”是分布式系统的连接，即神经网与数据网，“共同的任务目标”是分布式系统的目的，即训练，至此，再进一步抽象，可以推导出分布式系统的公理化定义，也是分布式系统的本质理论定义：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">分布式系统 <span class="o">=</span> 计算 x 网络 + 功能 + 边界</span></span></code></pre></td></tr></table>
</div>
</div><p>在这个公式里，计算即计算单元，是各种AI训练加速卡，比如GPU, TPU, DPU, DTU。网络即网络连接单元，在单个训练卡内为计算用的神经网，主机内的多个卡子之间是PCIE 以及PCIE Switch，以及各种高带宽通信网，比如GenZ,CXL,NVLINK,OpenCAPI,CCIX等，在主机之间是各种通信网络，比如RDMA网络、InfiniBand网络、普通的TCP网络以及对应的各种交换机，另外从磁盘 + 主机内存 + 训练卡的HBM这个IO路径我们认为属于IO网络，而这里的目的即训练，同时这个系统是有边界的，其专注于解决Ai训练过程中的难题，不是什么功能都能往里塞都能解决的。</p>
<h3 id="训练系统">训练系统</h3>
<p>以数据并行随机梯度下降( SGD )技术为例，神经网络训练的过程如下:</p>
<p></p>
<p>1，首先需要通过在第一个step进行Broadcast操作将参数同步到集群内的所有的训练卡上;</p>
<p>2，将数据样本切片分发到整个集群的每张训练卡上并且通过data Loader技术将数据样本加载进训练卡的高速内存空间内，作为输入X;</p>
<p>3，每个训练卡在其数据样本上运行前向传播，计算出误差LOSSi；</p>
<p>4，对计算出的LOSSi进行反向传播，得到梯度GRADi；</p>
<p>5，所有的训练卡在主机内及主机之间进行集合通信并进行梯度归约(AllReduce)；</p>
<p>6，最后再进行参数更新以获得新的梯度参数。</p>
<p>本质上分布式训练是<strong>数据加载</strong>、<strong>前向传播</strong>、<strong>反向传播</strong>、<strong>集合通信</strong>以及<strong>参数更新</strong>这5个步骤的逻辑组合，因此，基于以上步骤，这里可以推导出训练系统的公式定义如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">训练系统 <span class="o">=</span> 数据加载 + （前向传播 + 反向传播） + 集合通信 + 参数更新</span></span></code></pre></td></tr></table>
</div>
</div><p>从上面的步骤可知分布式训练是在固定的步骤迭代中进行的，并且需要系统内的所有的训练卡都完成它们的迭代步骤，才能进行最后的参数更新，这相当于在单个训练卡上执行梯度下降技术，但是通过在系统内所有的训练卡之间分发数据样本并同时执行计算来获得训练的加速。</p>
<h3 id="举例说明">举例说明</h3>
<p>以TensorFlow为例说明模型的训练过程，TensorFlow 是用数据流图做计算的，如下图所示:</p>
<p></p>
<p>图中显示了 TensorFlow 的训练过程，其包含输入（input）、塑形（reshape）、Relu 层（Relu layer）、Logit 层（Logit layer）、Softmax、交叉熵（cross entropy）、梯度（gradient）、SGD 训练（SGD Trainer）等部分。</p>
<p>它的训练过程是，首先从数据分片输入开始，经过Reshape数据清洗后，进行前向传播运算，通过Relu 层后得到LOSS值，然后进入 Logit 层，再进行反向传播并且用 Cross Entropy、softmax等 来计算梯度，接着进行梯度归约(Allreduce)，这一步在分布式场景就涉及集合通信的过程，最后进行参数更新SGD Trainer，如此迭代循环直到获得收敛指标达标的结果为止。</p>
<h2 id="小结">小结</h2>
<p>采用分布式训练的目的往往也是因为数据量或模型太大，一个加速卡的高速内存放不下，因此对数据或者模型进行切分，分发到多卡上进行计算与归约。本文很概况性的讲述了什么是分布式训练，简单来说分布式训练就是分布式计算的一种，通过对数据样本的计算，得出最后可用的模型再用于数据推理。本系列文章的后续内将展开讲述分布式训练的基础理论、训练过程、质量保证、集合通信、系统工程、产品化等，同样分布式训练系统除了解决训练所带来的各种故障也还需要解决分布式所带来的各种故障。</p>
]]></description></item><item><title>Horovod and Openmpi</title><link>https://lruihao.cn/posts/horovod_and_openmpi/</link><pubDate>Thu, 13 Jul 2023 08:18:52 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/horovod_and_openmpi/</guid><description><![CDATA[<h2 id="horovod-介绍">Horovod 介绍</h2>
<p>Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; 与百度 &ldquo;Ring Allreduce&rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不同深度学习框架的支持，使用起来对开发人员比较的友好，算是分布式训练方向的标杆项目了。</p>
<h2 id="集合通信库">集合通信库</h2>
<p>集合通信库，这个词可能听起来会比较的陌生，不过如果我再提几个关键字，可能大家多少都会有所耳闻。资历比较老的是 MPI (<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Message_Passing_Interface"target="_blank" rel="external nofollow noopener noreferrer">Message Passing Interface<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 及其实现 <a href="https://link.zhihu.com/?target=https%3A//www.open-mpi.org/"target="_blank" rel="external nofollow noopener noreferrer">OpenMPI<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 和 <a href="https://link.zhihu.com/?target=https%3A//www.mpich.org/"target="_blank" rel="external nofollow noopener noreferrer">MPICH<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，年轻一点的会是 Nvidia 针对其显卡开源的 NCCL，或者是 facebook 开源的 gloo，或者是像华为针对其高性能硬件提供的HCCL，大体上都可以归入到<strong>集合通信库</strong>的类别。他们相同的地方是大体上会遵照 MPI 提供的接口规定，实现了包括<font color=red><em>点对点通信</em></font>（SEND,RECV等），<font color=red><em>集合通信</em></font>（ REDUCE，BROADCAST，ALLREDUCE等）等相关接口，然后根据自己硬件或者是系统的需要，在底层实现上进行了相应的改动，保证接口的稳定和性能。</p>
<h3 id="点对点通信-point-to-point-communication">点对点通信: Point-to-Point Communication</h3>
<p><strong>Send/Recv:</strong></p>
<p></p>
<h3 id="集合通信">集合通信</h3>
<p><strong>Scatter/Gather</strong></p>
<p></p>
<p><strong>reduce/allreduce</strong></p>
<p></p>
<p><strong>boradcast/all-gather</strong></p>
<p></p>
<p>这里在机器学习训练中使用比较多的是 <strong>all-reduce</strong>，场景类似在不同的 node 上跑不同 batch 的数据，然后更新梯度需要从各个汇总之后平均再回传到各自的 node 中。而这部分，有很多种实现的方式，比较直观和简单的是把所有的梯度都汇总到的某一个 node 上（如下图 node d 所示），然后再把汇总的信息重新分发到不同的 node 上 ，这样可以计算通信量，如下：对于 P 个节点，每个节点消息大小为 M，node d 节点的通信量为 2*(P-1)M，这里假设节点之间互联互通，带宽为B。</p>
<p></p>
<p>不过这种情况下，很容易导致 <strong>node d</strong> 会成为性能瓶颈，因为 <strong>node d</strong> 需要跟其他所有 <strong>node</strong> 通信所以它的通信量是其他节点的 <strong>P</strong> 倍。假设节点间的带宽还是一样，<strong>node d</strong> 完成所有通信所需要的时间是 <em><em>2</em>(P-1)M/B</em>*。所以现在很多的集合通信框架不会使用这种方式，更多的是<strong>通过树状或者是环状(ring) 去实现 all-reduce</strong>。</p>
<p>如果只是做成树状的可以做成如下图所示，虽然传递的步数增多了，不过消除了node d 的通信瓶颈，完成所有的通信的时间大概是 <em><em>2log_2N</em>(M/B)</em>*，随着节点数目 P 的增加，树形结构的效果会越来越明显。</p>
<p></p>
<p>业界用得最多一种优化的方式是，每次只传一部分，这部分是百度提出的 ring-allreduce 的方案，具体的介绍可以参考这篇博客<a href="https://link.zhihu.com/?target=https%3A//andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"target="_blank" rel="external nofollow noopener noreferrer">Bringing HPC Techniques to Deep Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，这边就不赘述了。整体上就是每次不会像上面这样整份数据传递，而是一部分一部分传，优化后，所有节点需要传输的数据量的传输 <strong>2(N−1)M/N</strong> 比较平均，所需要的时间可以大概是 <strong>2(N−1)M/(NB)</strong>，horovod 也是基于这种 all-reduce 的形式实现的。</p>
<h3 id="实践">实践:</h3>
<h4 id="pytorchdistributed">pytorch.distributed</h4>
<p>尝试使用 pytorch 自带的分布式工具包 <a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/distributed.html"target="_blank" rel="external nofollow noopener noreferrer">torch.distributed<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，进行一些概念性的尝试。</p>
<p>为了方便尝试，我这里提供了一个简单的 demo，大家如果安装了 gpu 版本的 pytorch &gt;= 1.3，应该都可以尝试下面的例子尝试使用多进程模拟分布式（单机上可以跑）。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">argparse</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;PyTorch MNIST Example&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-m&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--mode&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;one_device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute mode, distributed/one_device&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--function&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;p2p&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;function to run (p2p/all_reduce/gpu_all_reduce)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-b&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--backend&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s2">&#34;nccl&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute backend (gloo/nccl)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Initialize the distributed environment. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data before send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Send the tensor to process 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Receive tensor from process 0</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data after send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">backend</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">backend</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;distributed&#34;</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;RANK&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in distribute mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;RANK&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in one device mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>可以简单地运行上面的例子：</p>
<p><strong>send/recv:</strong></p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上面是演示的是通过 pytorch 的 multiprocessing 包，模拟一次分布式的 send/recv 过程，这里是 rank0 的进程往 rank1 的进程发送一个 tensor，可以看到 rank 1 tensor 初始化为 0，是接收到 rank 0 的tensor 后变为 1 的。（注意：这里特别设置了 backend 为 gloo 是因为 nccl 不支持 point2point 的传输，具体不同 backend 支持什么形式的原语，参考文档backend部分 ）</p>
<p><strong>all_reduce</strong></p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对应函数</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># use rank 0 and rank 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里也很浅白，主要就是对两个进程上的 tensor 进行一次 allreduce，可以看到两个 rank 上的结果都为 2了。</p>
<p><strong>gpu_all_reduce</strong></p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="c1">#in one device mode</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;), tensor([1.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;), tensor([1.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:2&#39;), tensor([4.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:0&#39;), tensor([4.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  0  has data tensor[0]: tensor([8.], device=&#39;cuda:0&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:1&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  1  has data tensor[0]: tensor([8.], device=&#39;cuda:2&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:3&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对应函数</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<blockquote>
<p>all_reduce_multigpu: 相当于将多个gpu内的多进程的值进行相加;
all_reduce: 相当于单个gpu内的多进程的值相加</p>
</blockquote>
</blockquote>
<p>这里演示的是尝试对不同进程下多个 gpu (这里是 4 个) 进行 reduce，具体逻辑就是：</p>
<pre><code>- 对不同的进程分别把 tensor 初始化在不同的 gpu 上，rank0 初始化在 0，1 gpu 上，rank 1 在 2，3上。
- 进行一次 all_reduce_multigpu （这个函数跟 all_reduce 不同，是把不同的 node 上不同的gpu 上的tensor 都放到一个 list 中，进行reduce），这时所有 gpu 上的值都是4，作为对比，我们对 tensor_list[0] 的tensor 做一次all_reduce，得到的结果在 gpu 0,2 上的 tensor 进行了all_reduce 结果是 8，在 gpu 1,3 的 tensor 没有任何变化。
</code></pre>
<p><strong>多terminal尝试</strong></p>
<p>在验证分布式逻辑的时候，其实我们不一定需要多台机子才可以，对一些不涉及网络性能的验证，可以尝试在一台机子上开多个 terminal 进行验证。可以使用上面的例子，在多个 terminal 下跑以下命令。</p>
<p><em>terminal0:</em></p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">0</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p><em>terminal1:</em></p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">1</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里是通过本地机子上的回送地址进行模拟，结果是分别在不同的 terminal 呈现，当然可以用上面的demo，在多台机子上跑，不过需要修改一下 init_process 函数中的 os.environ[&lsquo;MASTER_ADDR&rsquo;] = &lsquo;127.0.0.1&rsquo; 为 rank 0 机子的 IP，这里就不演示了。具体 pytorch distributed 工具相关的内容可以参考<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/intermediate/dist_tuto.html"target="_blank" rel="external nofollow noopener noreferrer">官方博客<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>练习： 如果大概理解了上面的一些集合通信的原语，可以尝试着用上面 pytorch 提供的 send/recv 尝试去实现一下上面的树状 allreduce。</p>
<h3 id="mpi">MPI</h3>
<p>更深入的尝试，可以尝试了解一下 mpi 的知识，这个<a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/"target="_blank" rel="external nofollow noopener noreferrer">mpi<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>教程 算是写得比较系统的，大家可以参考一下来练习，特别是对底层不是很了解的同学，可以多看看 <a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/running-an-mpi-cluster-within-a-lan/"target="_blank" rel="external nofollow noopener noreferrer">Running an MPI cluster within a LAN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 的部分，实操一下通过 ssh 跑起一个分布式的 demo。集合通信库的基础大概先到这里，如果要深入的可以再去看看 <a href="https://link.zhihu.com/?target=https%3A//github.com/open-mpi/ompi/blob/98afc838aa53da88cba339f6dcbab256806a5745/ompi/mca/coll/tuned/coll_tuned_allreduce_decision.c"target="_blank" rel="external nofollow noopener noreferrer">openMPI<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，和 <a href="https://github.com/NVIDIA/nccl"target="_blank" rel="external nofollow noopener noreferrer">nccl<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 的实现。</p>
<h2 id="horovod流程分析">Horovod流程分析</h2>
<p>下面我会以一个简单的 pytorch horovod 的 demo 尝试去理解一下 horovod 的工作机理，demo 如下（省略了一些不关键的代码段）。为了准确起见，我们是根据 horovod v0.20.3 的版本进行阅读的，如果是其他版本，可能会跟这里的内容有一些出入。</p>
<h3 id="pytorch-demo">pytorch demo</h3>
<p>一般的 horovod 训练程序都会包含以下几个关键步骤：</p>
<pre><code>1. hvd.init: 对 horovod
2. 初始化。初始化模型，数据集，优化器，初始化不同 node 的模型权重。
3. 使用 hvd.DistributedOptimizer 包装优化器。
4. 进入训练流程，进行优化迭代。
</code></pre>
<p>我们会着重介绍第 1 和 4 步，因为主要也是1，4步会跟 c++ 后端进行信息交换。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">timeit</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">...</span> <span class="c1"># some argparse</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">lr_scaler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl"><span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: wrap optimizer with DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">named_parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">op</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">Adasum</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_adasum</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Average</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: broadcast parameters &amp; optimizer state.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up fixed fake data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">()</span> <span class="o">%</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#... some log configuration</span>
</span></span><span class="line"><span class="cl"><span class="n">img_secs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="n">benchmark_step</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span> <span class="o">/</span> <span class="n">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_secs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img_sec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Results</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span></span></span></code></pre></td></tr></table>
</div>
</div><p>然后下图是我对 horovod 整体流程的梳理，把一些不是很关键的部分隐藏了，可能有一些细节的地方和实现有出入，不过我待会会有详细的说明。这里先解释一下，下面几个大的部分:</p>
<ul>
<li>main.py： 表示训练脚本，一般是 使用 horovod 提供的函数跟特定的训练框架相互合作完成分布式训练（下文称前端）</li>
<li>C++ interface：是指 horovod python 函数调用 C++ 的接口</li>
<li>GlobalState：在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问，在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）后面会提到，主要会在下图 backgroundThreadLoop 中完成 globalstate 不同元素初始化，比较重要的有 controller 管理总体通信控制流，tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等）。</li>
<li>BackgroundThreadLoop：是训练过程中的后台线程，主要负责跟其他节点的通信，和处理前端过来的通信需求（request），会轮询调用 RunLoopOnce，不断查看 tensor_queue 中有没有需要通信的tensor，如果有跟其他节点同步更新，然后执行通信操作。</li>
</ul>
<p></p>
<h3 id="流程分析">流程分析</h3>
<p>下面使用 mpi_controller 进行 allreduce 操作进行分析。</p>
<p><strong>1.hvd.init()-&gt;InitializeHorovodOnce</strong></p>
<p>首先，hvd.init() 会通过一系列的调用和配置最终调用 horovod/common/http://operations.cc 下的 InitializeHorovodOnce 函数，这个函数会根据加载的<strong>集合通讯库</strong>（<em>mpi</em> 或者 <em>gloo</em>）为 globalstate 创建对应的 controller，然后使用 BackgroundThreadLoop 启动一个后台线程。</p>
<p>horovod/common/http://operations.cc #628</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">InitializeHorovodOnce</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ... some envParse
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Enable mpi is it&#39;s used either i[n cpu data transfer or controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="p">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建一个 MPIController 对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Reset initialization flag
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 启动后台线程
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>2.BackgroundThreadLoop</strong></p>
<p>BackgroundThreadLoop 会为 GlobalState 初始化一系列包括初始化 mpi_context， controller的元素，然后轮询调用 RunLoopOnce，还有一些对 RunLoopOnce 结束后的后处理。</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">BackgroundThreadLoop</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize mpi context
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">mpi_ctx_manager</span> <span class="o">=</span> <span class="n">MPIContextManager</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// mpi_context 会根据前端和环境变量传过来的信息，创建 mpi 线程，和一些 mpiOps
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">mpi_context</span><span class="p">.</span><span class="n">Initialize</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRanks</span><span class="p">(),</span> <span class="n">mpi_ctx_manager</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 会同步不同 node 的 global_size, local_size, rank, is_coordinator 等信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">Initialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set background thread affinity
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">parse_and_set_affinity</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">getenv</span><span class="p">(</span><span class="n">HOROVOD_THREAD_AFFINITY</span><span class="p">),</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GPU
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="p">...</span> <span class="c1">// 设置 gpu_context 的 stream 数目等初始化动作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// 下面是设置 parameter_manager 这里为了节省篇幅直接给出，设置的语句，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 原来这里会读取对应的环境变量的，去设置 parameter_manager。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 后面也会有篇幅介绍 parameter_manager，这里先不展开。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetTensorFusionThresholdBytes</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCycleTimeMs</span><span class="p">(</span><span class="mi">5</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCacheEnabled</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">response_cache</span><span class="p">.</span><span class="n">set_capacity</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">CacheEnabled</span><span class="p">()</span> <span class="o">*</span> <span class="n">state</span><span class="p">.</span><span class="n">cache_capacity</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetHierarchicalAllgather</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetAutoTuning</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 其他一些初始化设置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 设置op_manager，这里主要是注册不同的集合通信库的 ops
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//（ 如：NCCLAllreduce, MPI_GPUAllgather 等）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_manager</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">CreateOperationManager</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 初始化完成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate until shutdown.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="p">(</span><span class="n">RunLoopOnce</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Horovod background loop uncaught exception: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// 其他一些后处理函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>3.Optimizer.step()-&gt;DoAllReduce</strong>
这里我们先不急着看 RunLoopOnce 函数，先回到 InitializeHorovodOnce ，因为上面的 initialization_done = True，所以 InitializeHorovodOnce 可以退出了，就是前端的 hvd.init() 可以进行下一步了。这里 main.py 走完前向 loss = model(data,target)，后向逻辑 loss.backward()，调用 optimizer.step() 进行梯度同步。optimizer.step() 会通过一系列的调用和处理（如：compression 等操作）最终会调用 C++ interface 的 DoAllReduce 函数。</p>
<p><em><strong>DoAllReduce</strong></em> 函数会调用 EnqueueTensorAllreduce 函数会把需要 reduce 的 tensor 组装成一个Request 往 GlobalState 的 tensor_queue 里面塞。这里注意每个 tensor 会创建对应 TensorTableEntry，用于保存tensor 的权重，message 主要是一些 元信息 metadata。然后就等后台线程去读取这些allreduce 的请求了。</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="nf">EnqueueTensorAllreduce</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">OpContext</span><span class="o">&gt;</span> <span class="n">context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyEvent</span><span class="o">&gt;</span> <span class="n">ready_event</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">StatusCallback</span> <span class="n">callback</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">ReduceOp</span> <span class="n">reduce_op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">postscale_factor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span> <span class="c1">// some config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Request</span> <span class="n">message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_request_rank</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_name</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_type</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_prescale_factor</span><span class="p">(</span><span class="n">prescale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_postscale_factor</span><span class="p">(</span><span class="n">postscale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">reduce_op</span> <span class="o">==</span> <span class="n">ReduceOp</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ALLREDUCE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dims</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">add_tensor_shape</span><span class="p">((</span><span class="kt">int64_t</span><span class="p">)</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dim_size</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorTableEntry</span> <span class="n">e</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">ready_event</span> <span class="o">=</span> <span class="n">ready_event</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">shut_down</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">SHUT_DOWN_ERROR</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">status</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">AddToTensorQueue</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">TRACE</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Enqueued &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>4.RunLoopOnce</strong></p>
<p>回到后台线程 BackgroundThreadLoop，后面会轮询调用 RunLoopOnce。 RunLoopOnce会首先调用 ComputeResponseList 函数，其主要工作是同步不同 worker 之间的需要 allreduce 的 tensors，为后面 allreduce 的执行做好准备。</p>
<p>？？？为什么会在执行 tensor 的 allreduce 之前执行这样一步工作呢？而不是直接执行 allreduce 呢？我自己的猜测是，因为分布式训练是运行在不同的机子上的，因为 <u>horovod 没有引入类似参数服务器（parameter server）的节点，而是采取 master-worker</u> 的形式 进行 allreduce的。所以 allreduce 的时候必须确保所有的节点都是走到了同一句 allreduce 上，然后传输的 tensors 也要求是一致的，否则传输的 tensors 有可能没有匹配起来就执行allreduce，导致一些不可预知的错误。另外这部分引入了一些提高性能的 tricks，如对之前 reduce 过的 tensor 通过一个 bitmap 进行缓存，每次调用看一下是不是都是之前的 tensor，如果不是再 update 一下，不需要每次都全量更新。？？？（不是很确定）</p>
<p><strong>ComputeResponseList</strong>具体的流程是(可以对照上面流程图看):</p>
<ul>
<li>从自己进程的 GlobalState 读取 tensor_queue 的信息，如果有新的元素，会通过图中 popMessagesFromQueue pop 出来，然后经过一系列处理缓存到 message_queue_tmp 中。</li>
<li>当 worker 到达了前端 all_reduce 这句的时候，会用 message_queue_tmp 整理成一个 message_list通过流程图中的 SendReadyTensors 函数往主节点( coordinator ) 发送一个请求表明我打算reduce，然后会把准备 reduce 的 tensor 信息通过 message_list 迭代地送过去，最后有一个 Done 的请求</li>
<li>coordinator 会接收通过图中 RecvReadyTensors 这些 requests，然后保存在 ready_to_reduce 中，coordinator 会持续接收这些信息，直到获取的 Done 的数目等于 global_size。</li>
<li>coordinator 会找到所有准备好 reduce 的 tensors，通过 SendFinalTensors 返回一个 response 给所有的 worker，如果信息有误会返回一个 error，发送完成也会发送一个 Done。</li>
<li>worker 会通过 RecvFinalTensors 监听 response 的信息，整理出需要 reduce 的 tensor，当收到 Done，会尝试调用 performation 去进行 reduce 。</li>
<li>coordinator 和 worker 都会把同步的信息整理成一个 responses 的数组给到后面的 PerformOperation 操作。</li>
</ul>
<p>这里说一下mpi是怎么实现的，就是<u>对应的 coordinator 和 worker 会阻塞地到同一条指令</u>：</p>
<p>SendReadyTensors 和 RecvReadyTensors 阻塞到 MPI_Gather，SendFinalTensors 和 RecvFinalTensors 到 MPI_Bcast ，可以这样分辨：<font color=red><em>如果是 coordinator 发送的就是 MPI_Bcast，如果是worker 发送的是 MPI_Gather</font></em>。通信都是先同步需要通信message的大小 length，再同步message，代码如下：</p>
<p>horovod/common/mpi/http://mpi_controller.cc</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">SendReadyTensors</span><span class="p">(</span><span class="n">RequestList</span><span class="o">&amp;</span> <span class="n">message_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">encoded_message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">RequestList</span><span class="o">::</span><span class="n">SerializeToString</span><span class="p">(</span><span class="n">message_list</span><span class="p">,</span> <span class="n">encoded_message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">encoded_message_length</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 先 gather 这个 message 的大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gather</span><span class="p">(</span><span class="o">&amp;</span><span class="n">encoded_message_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Gather failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 再 gather 这个 message
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gatherv</span><span class="p">((</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">encoded_message_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvReadyTensors</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span> <span class="n">ready_to_reduce</span><span class="p">,</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">RequestList</span><span class="o">&gt;&amp;</span> <span class="n">ready_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gather</span><span class="p">(</span><span class="n">MPI_IN_PLACE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gatherv</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="n">displcmnts</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvFinalTensors</span><span class="p">(</span><span class="n">ResponseList</span><span class="o">&amp;</span> <span class="n">response_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">msg_length</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="o">&amp;</span><span class="n">msg_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;MPI_Broadcast failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">buffer</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">uint8_t</span><span class="p">[</span><span class="n">msg_length</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">  <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">msg_length</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>5.PerformOperation</strong></p>
<p>从 ComputeResponseList 继续跑 RunLoopOnce， 不同 node 下面会根据前面 ComputeResponseList 返回的 response_list 对每个 response 轮询调用 PerformOperation 完成对应的 reduce 工作。</p>
<p>PerformOperation 流程：</p>
<p><code>horovod/common/http://operations.cc</code></p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">PerformOperation</span><span class="p">(</span><span class="n">Response</span> <span class="n">response</span><span class="p">,</span> <span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;</span> <span class="n">entries</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">timeline</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">response_type</span><span class="p">()</span> <span class="o">!=</span> <span class="n">Response</span><span class="o">::</span><span class="n">JOIN</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">GetTensorEntriesFromResponse</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">entries</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                             <span class="n">state</span><span class="p">.</span><span class="n">joined</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 对数据预处理和 buffer 初始化
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 执行 all_reduce 等操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">op_manager</span><span class="o">-&gt;</span><span class="n">ExecuteOperation</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">response</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">Status</span><span class="o">::</span><span class="n">UnknownError</span><span class="p">(</span><span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 调用 callback 函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>PerformOperation 会从 horovod_global.tensor_queue 通过函数 <code>GetTensorEntriesFromResponse</code> 取出对应的 TensorEntry</li>
<li>如果还没初始化buffer，调用 horovod_global.fusion_buffer.InitializeBuffer 初始化</li>
<li>然后 status = op_manager-&gt;ExecuteOperation(entries, response) 会调用不同的 op-&gt;Execute(entries, response) 执行reduce 运算</li>
</ul>
<p>下面以 <strong>MPIAllreduce::Execute</strong> 为例：
<code>horovod/common/ops/http://mpi_operations.cc</code></p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="n">MPIAllreduce</span><span class="o">::</span><span class="n">Execute</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;&amp;</span> <span class="n">entries</span><span class="p">,</span> <span class="k">const</span> <span class="n">Response</span><span class="o">&amp;</span> <span class="n">response</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 一些变量声明
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 把 tensor copy 到 buffer 中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_IN_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyInFusionBuffer</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">fused_input_data</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span> <span class="n">buffer_len</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">fused_input_data</span> <span class="o">=</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_data</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">size_t</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Do allreduce
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">sendbuf</span> <span class="o">=</span> <span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">fused_input_data</span> <span class="o">==</span> <span class="n">buffer_data</span>
</span></span><span class="line"><span class="cl">                        <span class="o">?</span> <span class="nl">MPI_IN_PLACE</span> <span class="p">:</span> <span class="n">fused_input_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">op</span> <span class="o">=</span> <span class="n">MPI_Allreduce</span><span class="p">(</span><span class="n">sendbuf</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">num_elements</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPIDataType</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPISumOp</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPICommunicator</span><span class="p">(</span><span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Allreduce failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Copy memory out of the fusion buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 把 allreduce 后的 tensor copy 会 entries
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_OUT_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyOutFusionBuffer</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">,</span> <span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>然后调用不同 entries 的 callback，这里 callback 一般是给前端作相应的。</li>
</ul>
<p><strong>6.parameter_manager.update</strong></p>
<p>完成上述步骤之后，如果设置了 state.parameter_manager.IsAutoTuning()，RunLoopOnce 还会调用相关的逻辑，调整传输的参数，然后返回 BackgroundThreadLoop 重新调用。_重新调用时会睡一定时间再继续_上述第 3 - 5 步的工作。</p>
<h3 id="其他关键模块">其他关键模块</h3>
<p>上面只是介绍了 horovod 主流程工作原理，不过 horovod 还有其他一些模块协同主流程工作的，下面会对其中的一些我认为可以值得一说的模块说一下。</p>
<p><strong>Parameter_manager:</strong> Parameter_manager 主要是 GlobalState 的一个用于管理一些调节 horovod 性能的参数的管理器，在 BackgroundThreadLoop 中跟其他的 GlobalState 的元素一同初始化，然后会读取下面这些对应的环境变量，然后进行设置。</p>
<p><strong>HOROVOD_FUSION_THRESHOLD</strong>：指传输数据切片的大小，默认是64M，如果切片太大，传输的时候就不能很好地 pipeline 传输，如果太小，一个 tensor 需要传输多次，增加 IO 的 overhead。</p>
<p><strong>HOROVOD_CYCLE_TIME</strong>：指 <u>RunLoopOnce 的睡眠时长</u>，默认是 <strong>5ms</strong>，我自己的猜测（还没进行验证）比较理想的睡眠时间应该是 RunLoopOnce 其余逻辑处理的时间 + HOROVOD_CYCLE_TIME 刚好等于一次前向传播和后向传播所用的时间，因为睡太久前端会在等 RunLoopOnce 睡醒；如果睡太短，不断地跑一次 RunLoopOnce，tensor_queue 也不会有新的元素，只是白跑。</p>
<p><strong>HOROVOD_CACHE_CAPACITY</strong>：指 cache 的大小，这个可能跟 model 层数参数量相关了。</p>
<p><strong>HOROVOD_HIERARCHICAL_ALLGATHER</strong>：是否使用分层的allgather的方式等</p>
<p>Parameter_manager也提供了对这些参数自动调节的功能。通过Parameter_manager.SetAutoTuning进行设置，设置后会在初始的几个batch尝试不同的参数组合进行通信，后面会收敛到一组最优的参数值。</p>
<h3 id="mpicontext">MPIContext</h3>
<p>mpi_context 是在加载 C++ 的代码时候就已经创建了，同时创建的还有其他 context（ nccl_context, gpu_context），主要是维护一些节点上 mpi 通信的必要环境信息和设置，如：</p>
<ul>
<li>3 个 MPI communicator，mpi_comm，local_comm，cross_comm 分别负责 horovod mpi 传输，节点内传输，和节点间分层传输（主要用于 hierarchical allreduce）。</li>
<li>mpi_float16_t: horovod 主要以 float16 传输。</li>
<li>mpi_float16_sum: float16 对应的sum 操作。</li>
</ul>
<p>在 horovod 使用 mpi 的时候，都会使用上面的 communicator 进行数据传输。</p>
<h3 id="tensorflow2">Tensorflow2</h3>
<p>TensorFlow2 前端对 horovod 的调用跟 pytorch 类似，只是因为 tensorflow 2 是通过 tape 等级制记录梯度的, 所以会有一些不同。</p>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">applications</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">999</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tf.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl">    <span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: use DistributedGradientTape</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: add Horovod Distributed GradientTape.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tape</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>with tf.GradientTape() as tape</code>这一句会调用 <code>horovod/tensorflow/__init__.py</code> 中<code>_DistributedGradientTape</code> 下 <strong>init</strong> 函数注册 allreduce 的句柄（handle）</li>
<li>然后调用 <code>gradients = tape.gradient(loss, model.trainable_variables)</code> 会调用一系列的跳转最后会调用 <code>tensorflow/mpi_ops.py</code> 下的 _allreduce ，进而调用 `MPI_LIB.horovod_allreduce</li>
<li><code>MPI_LIB.horovod_allreduce</code> 在 <code>horovod/tensorflow/http://mpi_ops.cc</code> 中被 <code>HorovodAllreduceOp</code> 所注册，根据 TensorFlow 的 ops流程，会调用 <code>ops.ComputeAsync</code>，到这里会跟 pytorch 类似会调用 <code>EnqueueTensorAllreduce</code> 把对应的 tensor 和 ops 送到 GlobalState 的 tensor_queue 中。</li>
</ul>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodAllreduceOp</span> <span class="p">:</span> <span class="n">public</span> <span class="n">AsyncOpKernel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="n">public</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">explicit</span> <span class="n">HorovodAllreduceOp</span><span class="p">(</span><span class="n">OpKernelConstruction</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">:</span> <span class="n">AsyncOpKernel</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;reduce_op&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">reduce_op_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;prescale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">prescale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;postscale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">postscale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;ignore_name_scope&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ignore_name_scope_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">void</span> <span class="n">ComputeAsync</span><span class="p">(</span><span class="n">OpKernelContext</span><span class="o">*</span> <span class="n">context</span><span class="p">,</span> <span class="n">DoneCallback</span> <span class="n">done</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">common</span><span class="p">::</span><span class="n">CheckInitialized</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span> <span class="o">//</span> <span class="n">一些变量验证</span><span class="err">，</span><span class="n">初始化</span>
</span></span><span class="line"><span class="cl">    <span class="n">auto</span> <span class="n">enqueue_result</span> <span class="o">=</span> <span class="n">EnqueueTensorAllreduce</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd_context</span><span class="p">,</span> <span class="n">hvd_tensor</span><span class="p">,</span> <span class="n">hvd_output</span><span class="p">,</span> <span class="n">ready_event</span><span class="p">,</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">done</span><span class="p">](</span><span class="n">const</span> <span class="n">common</span><span class="p">::</span><span class="n">Status</span><span class="o">&amp;</span> <span class="n">status</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">context</span><span class="o">-&gt;</span><span class="n">SetStatus</span><span class="p">(</span><span class="n">ConvertStatus</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">          <span class="n">done</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">prescale_factor_</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">postscale_factor_</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">enqueue_result</span><span class="p">),</span> <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">private</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="nb">int</span> <span class="n">reduce_op_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Using</span> <span class="nb">float</span> <span class="n">since</span> <span class="n">TF</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">support</span> <span class="n">double</span> <span class="n">OP</span> <span class="n">attributes</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">prescale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">postscale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">bool</span> <span class="n">ignore_name_scope_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="总结">总结</h2>
<p>horovod 的流程分析大概就是这样，没有特别复杂，代码的阅读体验也是比较好的，在主流程的关键函数都有比较清晰的注释。对于第三方开发者来说，horovod 本身已经用了很多提高性能的 tricks，可以 custom 优化的地方不多，一些可以动的参数，也已经提供了autotuning，直接使用就可以得到很好的性能。如果尝试优化，可能要从传输上着手，如 BytePS 会尝试使用不同的网络拓扑引入一些 PS 节点提高带宽等，如果有时间我也会聊一下这个。另外上面的分析也有很多是我自己阅读代码时候的一些思考可能不一定准确，如果有不准确或者模糊的地方，也希望大家可以多多斧正。</p>
<p>References:
[1]. <a href="https://zhuanlan.zhihu.com/p/332825987"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/332825987<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://zhuanlan.zhihu.com/p/158584571"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/158584571<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3]. <a href="https://zhuanlan.zhihu.com/p/79030485"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/79030485<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[4]. <a href="https://github.com/zjykzj/pytorch-distributed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zjykzj/pytorch-distributed<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[5]. <a href="https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/"target="_blank" rel="external nofollow noopener noreferrer">MPI教程<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/qq_47058489/article/details/125980505"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_47058489/article/details/125980505<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&utm_relevant_index=1<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[5.] <a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1"target="_blank" rel="external nofollow noopener noreferrer">ubuntu20.04 + docker + horovod<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h1 id="horovod-and-distributed-training">Horovod and Distributed Training</h1>
]]></description></item><item><title>深度学习分布式训练框架 horovod[4] -- 网络基础 &amp; Driver</title><link>https://lruihao.cn/posts/2022-10-08_horovod_4/</link><pubDate>Mon, 10 Jul 2023 07:53:48 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/2022-10-08_horovod_4/</guid><description><![CDATA[<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。本文是系列第四篇，看看如何获取 host 之间的路由等网络信息。</p>
<h2 id="1-引子">1 引子</h2>
<p>在 horovod/runner/launch.py 文件中，_run_static 函数中使用 <code>driver_service.get_common_interfaces</code> 来获取路由信息等。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_static</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">get_common_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">all_host_names</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">remote_host_names</span><span class="p">,</span> <span class="n">fn_cache</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>因为这部分比较复杂（ Driver 的概念很类似 Spark 之中 Driver 的概念），所以本文我们单独来分析。</p>
<p>本文的分析问题点是：</p>
<ul>
<li>为什么要知道路由信息？</li>
<li>当有多个host时候，horovod如何处理？</li>
<li>如何找到路由信息？</li>
<li>怎么互相交互？</li>
<li>（后文会详细分析）SparkDriverService，SparkTaskService，ElasticDriver, Worker 都有什么区别和联系？</li>
</ul>
<p>本文重点分析 HorovodRunDriverService 和 HorovodRunTaskService 相关。</p>
<p>先给出一个图例，大家可以有些概念。</p>
<p></p>
<h2 id="2-总体架构">2 总体架构</h2>
<p>从注释可知，get_common_interfaces 完成了获得路由信息（所有host之间的共有路由接口集合）的功能，主要是调用 _driver_fn 来完成相关工作。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_common_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">all_host_names</span><span class="p">,</span> <span class="n">remote_host_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fn_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Find the set of common and routed interfaces on all the hosts.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 得到远端host地址</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">remote_host_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">remote_host_names</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">filter_local_addresses</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">remote_host_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">:</span> <span class="c1"># 如果参数有设定网络接口，就使用</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># If args.nics is provided, we will use those interfaces. All the workers</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># must have at least one of those interfaces available.</span>
</span></span><span class="line"><span class="cl">            <span class="n">nics</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Find the set of common, routed interfaces on all the hosts (remote</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># and local) and specify it in the args to be used by NCCL. It is</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># expected that the following function will find at least one interface</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># otherwise, it will raise an exception.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">local_host_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">remote_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 获取其他host的网络接口</span>
</span></span><span class="line"><span class="cl">            <span class="n">nics</span> <span class="o">=</span> <span class="n">_driver_fn</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">fn_cache</span><span class="o">=</span><span class="n">fn_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span> <span class="o">=</span> <span class="n">get_local_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span> <span class="c1"># 获取本地的网络接口</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nics</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="21-get_local_interfaces">2.1 get_local_interfaces</h3>
<p>此函数比较简单，目的是<strong>获取本地的网络接口</strong>。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_local_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># If all the given hosts are local, find the interfaces with address</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 127.0.0.1</span>
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">iface</span><span class="p">,</span> <span class="n">addrs</span> <span class="ow">in</span> <span class="n">net_if_addrs</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span> <span class="ow">and</span> <span class="n">iface</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">addrs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">addr</span><span class="o">.</span><span class="n">family</span> <span class="o">==</span> <span class="n">AF_INET</span> <span class="ow">and</span> <span class="n">addr</span><span class="o">.</span><span class="n">address</span> <span class="o">==</span> <span class="s1">&#39;127.0.0.1&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">nics</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">iface</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nics</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="22-_driver_fn">2.2 _driver_fn</h3>
<p>这是本文重点，获取其他host 的网络接口，_driver_fn 的作用是：</p>
<ul>
<li>启动 service 服务；</li>
<li>使用 driver.addresses() 获取 Driver 服务的地址（使用<code>self._addresses = self._get_local_addresses()</code>完成）；</li>
<li>使用 _launch_task_servers（利用 Driver 服务的地址）在每个 worker 之中启动 task 服务，然后 task 服务会在 service 服务中注册；</li>
<li>因为是一个环形，每个 worker 会探测 worker index + 1 的所有网络接口；</li>
<li>最后 _run_probe 返回一个所有 workers 上的所有路由接口的交集；</li>
</ul>
<p>代码如下：</p>
<p>这里需要注意的一点是：@cache.use_cache() 的使用：当第一次使用过之后，会把结果放入缓存。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@cache.use_cache</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_driver_fn</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    launches the service service, launches the task service on each worker and
</span></span></span><span class="line"><span class="cl"><span class="s2">    have them register with the service service. Each worker probes all the
</span></span></span><span class="line"><span class="cl"><span class="s2">    interfaces of the worker index + 1 (in a ring manner) and only keeps the
</span></span></span><span class="line"><span class="cl"><span class="s2">    routed interfaces. Function returns the intersection of the set of all the
</span></span></span><span class="line"><span class="cl"><span class="s2">    routed interfaces on all the workers.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param all_host_names: list of addresses. for example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;worker-0&#39;,&#39;worker-1&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;10.11.11.11&#39;, &#39;10.11.11.12&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type all_host_names: list(string)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param local_host_names: host names that resolve into a local addresses.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type local_host_names: set
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param settings: the object that contains the setting for running horovod
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type settings: horovod.runner.common.util.settings.Settings
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return: example: [&#39;eth0&#39;, &#39;eth1&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype: list[string]
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Launch a TCP server called service service on the host running horovod</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 启动 service 服务</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_hosts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">driver</span> <span class="o">=</span> <span class="n">HorovodRunDriverService</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Have all the workers register themselves with the service service.</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#（利用 Driver 服务的地址）在每个worker之中启动 task 服务，然后task服务会在 service 服务中注册</span>
</span></span><span class="line"><span class="cl">    <span class="n">_launch_task_servers</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">driver</span><span class="o">.</span><span class="n">addresses</span><span class="p">(),</span> <span class="n">settings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 返回一个所有 workers 上的所有路由接口的交集</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_run_probe</span><span class="p">(</span><span class="n">driver</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="23-获取路由接口">2.3 获取路由接口</h3>
<p>我们对 _run_probe 函数做进一步分析。</p>
<h4 id="231-probe逻辑">2.3.1 probe逻辑</h4>
<p>_run_probe 函数就是当<u>所有 task 都启动，注册，probe 环中下一个worker 邻居完成</u> 之后，得到 接口集合。</p>
<ul>
<li>利用 wait_for_initial_registration 等待所有 task 完成注册；</li>
<li>对于所有 task，完成 task.notify_initial_registration_complete 通知；</li>
<li>利用 driver.wait_for_task_to_task_address_updates 等待 每一个 worker probe 完成；</li>
<li>利用 nics.intersection_update 得到接口集合；</li>
</ul>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_probe</span><span class="p">(</span><span class="n">driver</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">       <span class="c1"># wait for all the hosts to register with the service service.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">driver</span><span class="o">.</span><span class="n">wait_for_initial_registration</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_driver</span><span class="p">(</span><span class="n">index</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_hosts</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Notify all the drivers that the initial registration is complete.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">task</span> <span class="ow">in</span> <span class="n">tasks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">notify_initial_registration_complete</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Each worker should probe the interfaces of the next worker in a ring</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># manner and filter only the routed ones -- it should filter out</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># interfaces that are not really connected to any external networks</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># such as lo0 with address 127.0.0.1.</span>
</span></span><span class="line"><span class="cl">    <span class="n">driver</span><span class="o">.</span><span class="n">wait_for_task_to_task_address_updates</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Determine a set of common interfaces for task-to-task communication.</span>
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_tasks</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span><span class="o">.</span><span class="n">intersection_update</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_tasks</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nics</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="232-等待函数">2.3.2 等待函数</h4>
<p>probe 利用 wait_for_initial_registration 等待所有 task 完成注册，具体等待函数如下：</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">wait_for_initial_registration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">timeout</span><span class="o">.</span><span class="n">remaining</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="n">timeout</span><span class="o">.</span><span class="n">check_time_out_for</span><span class="p">(</span><span class="s1">&#39;tasks to start&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">wait_for_task_to_task_address_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">timeout</span><span class="o">.</span><span class="n">remaining</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="n">timeout</span><span class="o">.</span><span class="n">check_time_out_for</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;tasks to update task-to-task addresses&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-基础网络服务">3 基础网络服务</h2>
<p>前面提到，Horovod Driver 的概念很类似 Spark 之中 Driver 的概念。Spark应用程序运行时主要分为 Driver 和 Executor，Driver负责总体调度及UI展示，Executor负责Task运行。用户的Spark应用程序运行在Driver上（某种程度上说，用户的程序就是Spark Driver程序），经过Spark调度封装成一个个Task，再将这些Task信息发给Executor执行，Task信息包括代码逻辑以及数据信息，Executor不直接运行用户的代码。</p>
<p>对于 Horovod 来说：</p>
<ul>
<li>HorovodRunDriverService 就是 Driver 的实现类。</li>
<li>HorovodRunTaskService 提供了 Task 部分服务功能，这些 task 需要注册到 HorovodRunDriverService 之中。</li>
<li>这套 driver &amp; task 机制的底层由 &ldquo;基础网络服务&rdquo; 支撑。</li>
</ul>
<p>所以我们就仔细分析下基础网络服务。</p>
<h3 id="31-继承关系">3.1 继承关系</h3>
<p>首先给出继承关系，我们下面讲解的 Driver 服务由 HorovodRunDriverService 提供，Task 服务由HorovodRunTaskService 提供。</p>
<p>这两个类最终都继承了 network.BasicService。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">                            <span class="n">network</span><span class="o">.</span><span class="n">BasicService</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                                  <span class="o">^</span>    <span class="o">^</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+-------------------+</span>    <span class="o">+-------------+</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverService</span>       <span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskService</span>
</span></span><span class="line"><span class="cl">              <span class="o">^</span>                                      <span class="o">^</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl">    <span class="n">HorovodRunDriverService</span>                <span class="n">HorovodRunTaskService</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-networkbasicservice">3.2 network.BasicService</h3>
<p>BasicService 提供了一个网络服务器功能。即通过find_port函数构建了一个<code>ThreadingTCPServer</code>对外提供服务。</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BasicService</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">service_name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_service_name</span> <span class="o">=</span> <span class="n">service_name</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wire</span> <span class="o">=</span> <span class="n">Wire</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_nics</span> <span class="o">=</span> <span class="n">nics</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">find_port</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="k">lambda</span> <span class="n">addr</span><span class="p">:</span> <span class="n">socketserver</span><span class="o">.</span><span class="n">ThreadingTCPServer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">addr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_handler</span><span class="p">()))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">_block_on_close</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_port</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">socket</span><span class="o">.</span><span class="n">getsockname</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_addresses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_local_addresses</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">serve_forever</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="321-创建server">3.2.1 创建Server</h4>
<p>创建服务器代码如下，这里是搜索一个随机端口，然后设置：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">find_port</span><span class="p">(</span><span class="n">server_factory</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_port</span> <span class="o">=</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_port</span> <span class="o">=</span> <span class="mi">65536</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_ports</span> <span class="o">=</span> <span class="n">max_port</span> <span class="o">-</span> <span class="n">min_port</span>
</span></span><span class="line"><span class="cl">    <span class="n">start_port</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_ports</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">port_offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_ports</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">port</span> <span class="o">=</span> <span class="n">min_port</span> <span class="o">+</span> <span class="p">(</span><span class="n">start_port</span> <span class="o">+</span> <span class="n">port_offset</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_ports</span>
</span></span><span class="line"><span class="cl">            <span class="n">addr</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">server</span> <span class="o">=</span> <span class="n">server_factory</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">server</span><span class="p">,</span> <span class="n">port</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Unable to find a port to bind to.&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="322-server功能">3.2.2 Server功能</h4>
<p>服务器就是基本的功能，比如获取本server地址，处理 ping，网络交互等。</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_make_handler</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">server</span> <span class="o">=</span> <span class="bp">self</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">class</span> <span class="nc">_Handler</span><span class="p">(</span><span class="n">socketserver</span><span class="o">.</span><span class="n">StreamRequestHandler</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">req</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">resp</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">_handle</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">client_address</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># A tuple is the usual response object followed by a utf8 text stream</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">resp</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span> <span class="o">=</span> <span class="n">resp</span>
</span></span><span class="line"><span class="cl">                    <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">except</span> <span class="p">(</span><span class="ne">EOFError</span><span class="p">,</span> <span class="ne">BrokenPipeError</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Happens when client is abruptly terminated, don&#39;t want to pollute the logs.</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">_Handler</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">client_address</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">PingRequest</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">PingResponse</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_service_name</span><span class="p">,</span> <span class="n">client_address</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_get_local_addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">intf</span><span class="p">,</span> <span class="n">intf_addresses</span> <span class="ow">in</span> <span class="n">psutil</span><span class="o">.</span><span class="n">net_if_addrs</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nics</span> <span class="ow">and</span> <span class="n">intf</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nics</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">intf_addresses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">addr</span><span class="o">.</span><span class="n">family</span> <span class="o">==</span> <span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">intf</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">                <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">addr</span><span class="o">.</span><span class="n">address</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_port</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_addresses</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">server_close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_port</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_port</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="33-networkbasicclient">3.3 network.BasicClient</h3>
<p>HorovodRunDriverClient 和 HorovodRunTaskClient 这两个类都继承了network.BasicClient。</p>
<p>network.BasicClient 的作用就是连接 network.BasicService，与其交互。即 network.BasicClient 是一个操作接口。</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             <span class="n">network</span><span class="o">.</span><span class="n">BasicClient</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                                <span class="o">^</span>            <span class="o">^</span>
</span></span><span class="line"><span class="cl">                                <span class="o">|</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">+------------------+</span>            <span class="o">+---------------+</span>
</span></span><span class="line"><span class="cl">             <span class="o">|</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">+</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                             <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverClient</span>               <span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskClient</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">             <span class="o">^</span>                                               <span class="o">^</span>
</span></span><span class="line"><span class="cl">             <span class="o">|</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">|</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">+</span>                                               <span class="o">+</span>
</span></span><span class="line"><span class="cl">   <span class="n">HorovodRunDriverClient</span>                           <span class="n">HorovodRunTaskClient</span></span></span></code></pre></td></tr></table>
</div>
</div><p>两个主要 API 如下：</p>
<h4 id="331-_probe">3.3.1 _probe</h4>
<p>_probe 获取 server 的网络接口。</p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_probe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">addresses</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">result_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">intf</span><span class="p">,</span> <span class="n">intf_addresses</span> <span class="ow">in</span> <span class="n">addresses</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">intf_addresses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_probe_one</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">intf</span><span class="p">,</span> <span class="n">addr</span><span class="p">,</span> <span class="n">result_queue</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">intf</span><span class="p">,</span> <span class="n">addr</span> <span class="o">=</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">intf</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="332-发送消息">3.3.2 发送消息</h4>
<p>_send 的作用是给server发送消息。</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_send</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Sends the request and returns the response object.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Streaming data response is transferred to the optional stream parameter.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Since all the addresses were vetted, use the first one.</span>
</span></span><span class="line"><span class="cl">    <span class="n">addr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_addresses</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_one</span><span class="p">(</span><span class="n">addr</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="34-总结">3.4 总结</h3>
<p>我们可以看到，network.BasicService 会提供了一个server，这个 Service 都是通过 network.BasicClient 来访问。基于此，Horovod 的HorovodRunDriverService 和 HorovodRunTaskService 这两个类就可以互相交互，进行沟通。</p>
<h2 id="4-driver-服务">4 Driver 服务</h2>
<p>Driver 服务由 HorovodRunDriverService 提供，其功能主要是维护维护各种 task 地址以及相应关系。具体各种 task 地址 就是 Task 服务 来注册的。</p>
<p>需要注意的是：HorovodRunDriverService 和 HorovodRunTaskService 都最终继承了 network.BasicService，他们之间可以是异地运行交互。</p>
<h3 id="41-horovodrundriverservice">4.1 HorovodRunDriverService</h3>
<p>HorovodRunDriverService 是对 BasicDriverService 的封装。</p>
<p>HorovodRunDriverClient 是 其 访问接口。</p>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunDriverService</span><span class="p">(</span><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">NAME</span> <span class="o">=</span> <span class="s1">&#39;horovod driver service&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunDriverService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                      <span class="n">HorovodRunDriverService</span><span class="o">.</span><span class="n">NAME</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                      <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunDriverClient</span><span class="p">(</span><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverClient</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">match_intf</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunDriverClient</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">HorovodRunDriverService</span><span class="o">.</span><span class="n">NAME</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver_addresses</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">verbose</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">match_intf</span><span class="o">=</span><span class="n">match_intf</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42-basicdriverservice">4.2 BasicDriverService</h3>
<p>BasicDriverService基类 主要就是 维护各种 task 地址以及相应关系。</p>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BasicDriverService</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">BasicService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_proc</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">BasicDriverService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span> <span class="o">=</span> <span class="n">num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Condition</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里的各种 task 地址就是 Task 服务 注册到 Driver 的数值。</p>
<p>可以看到里面有各种关于地址的变量，为了让大家理解这些变量的作用，对于每一个变量我们举例如下（这里有些变量是专门为 spark 设计，都放到基类里面有点奇怪）：</p>
<h4 id="421-_all_task_addresses">4.2.1 _all_task_addresses</h4>
<p>本变量是记录了所有 task 的地址，变量举例如下：</p>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;lo&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;1.1.1.1&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.01&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;lo&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;2.2.2.2&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.02&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本变量由 task 调用 RegisterTaskRequest 来注册。</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">RegisterTaskRequest</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">req</span><span class="o">.</span><span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span><span class="p">[</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">task_addresses</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="422-_task_addresses_for_driver">4.2.2 _task_addresses_for_driver</h4>
<p>本变量是记录了所有 task 的地址，但是网卡接口有多种，这里选择与 本 driver 地址匹配的地址。</p>
<p>变量举例如下：</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.01&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.02&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本变量由 task 调用 RegisterTaskRequest 来注册。</p>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Just use source address for service for fast probing.</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span><span class="p">[</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_filter_by_ip</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">task_addresses</span><span class="p">,</span> <span class="n">client_address</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体使用举例如下：</p>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_addresses_for_driver</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>driver用这个地址来生成 其内部 task 变量。</p>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_driver</span><span class="p">(</span><span class="n">index</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_hosts</span><span class="p">)]</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-_task_addresses_for_tasks">4.2.3 _task_addresses_for_tasks</h4>
<p>该变量举例如下：</p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.01&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.02&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本变量由RegisterTaskToTaskAddressesRequest注册。</p>
<div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">RegisterTaskToTaskAddressesRequest</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">register_task_to_task_addresses</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">task_addresses</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">network</span><span class="o">.</span><span class="n">AckResponse</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">register_task_to_task_addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">task_addresses</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">task_addresses</span> <span class="c1"># 这里赋值</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">notify_all</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>该变量被 task 用来获取 某个 task 的一套网络接口，比如：</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Determine a set of common interfaces for task-to-task communication.</span>
</span></span><span class="line"><span class="cl"><span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_tasks</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="424-_task_index_host_hash">4.2.4 _task_index_host_hash</h4>
<p>每一个 task 有一个对应的 host hash，该数值被 MPI 作为 host name 来操作。</p>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-01-dfdsfdsfdsfdsf2&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-02-treterwrtqwer&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体使用如下。这个函数是 spark 相关会使用，具体是逐一通知 spark task 进入下一阶段。</p>
<div class="highlight" id="id-26"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>或者使用如下，是为了获取某一个 host 对应的 <code>host hash name</code>。</p>
<div class="highlight" id="id-27"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_index_host_hash</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="425-_task_host_hash_indices">4.2.5 _task_host_hash_indices</h4>
<p>该变量举例如下：</p>
<div class="highlight" id="id-28"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-01-dfdsfdsfdsfdsf2&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-02-treterwrtqwer&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体是在注册 RegisterTaskRequest 时候生成。</p>
<div class="highlight" id="id-29"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span><span class="p">[</span><span class="n">req</span><span class="o">.</span><span class="n">host_hash</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>使用具体代码是：</p>
<div class="highlight" id="id-30"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_host_hash_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体是被 rsh 使用。rsh 就是在某一个 host 上，让某一个 horovod rank 启动。具体逻辑是：</p>
<ul>
<li>获取某一个 host 上所有的 task indices ；</li>
<li>利用 task_host_hash_indices 取出本进程 local rank 对应的 task index；</li>
<li>取出在 driver 中 task index 对应保持的 task address；</li>
<li>最后依据这个 task addresses 生成一个 SparkTaskClient，进行后续操作。</li>
</ul>
<div class="highlight" id="id-31"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">driver_client</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">SparkDriverClient</span><span class="p">(</span><span class="n">driver_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_indices</span> <span class="o">=</span> <span class="n">driver_client</span><span class="o">.</span><span class="n">task_host_hash_indices</span><span class="p">(</span><span class="n">host_hash</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_index</span> <span class="o">=</span> <span class="n">task_indices</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">task_addresses</span> <span class="o">=</span> <span class="n">driver_client</span><span class="o">.</span><span class="n">all_task_addresses</span><span class="p">(</span><span class="n">task_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_client</span> <span class="o">=</span> <span class="n">task_service</span><span class="o">.</span><span class="n">SparkTaskClient</span><span class="p">(</span><span class="n">task_index</span><span class="p">,</span> <span class="n">task_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_client</span><span class="o">.</span><span class="n">stream_command_output</span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_client</span><span class="o">.</span><span class="n">run_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">capture_stdout</span><span class="o">=</span><span class="n">stdout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">capture_stderr</span><span class="o">=</span><span class="n">stderr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">prefix_output_with_timestamp</span><span class="o">=</span><span class="n">prefix_output_with_timestamp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-总体逻辑">4.3 总体逻辑</h3>
<p>总体逻辑如下：</p>
<div class="highlight" id="id-32"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">                               <span class="n">network</span><span class="o">.</span><span class="n">BasicService</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                                     <span class="o">^</span>    <span class="o">^</span>
</span></span><span class="line"><span class="cl">                                     <span class="o">|</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+-------------------+</span>    <span class="o">+-------------+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl">   <span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverService</span>       <span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskService</span>
</span></span><span class="line"><span class="cl">                 <span class="o">^</span>                                      <span class="o">^</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="o">+----------------+------------------+</span>         <span class="n">HorovodRunTaskService</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">HorovodRunDriverService</span>           <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>        <span class="n">_all_task_addresses</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>    <span class="n">_task_addresses_for_driver</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_addresses_for_tasks</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_index_host_hash</span>       <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>     <span class="n">_task_host_hash_indices</span>       <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----------------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-task-服务">5 Task 服务</h2>
<p>HorovodRunTaskService 提供了 Task 部分服务功能。整体逻辑是由几个函数共同完成。</p>
<h3 id="51-启动具体服务">5.1 启动具体服务</h3>
<p>_launch_task_servers 用来启动具体服务，其主要作用是：多线程运行，在每一个线程中，远程运行 <code>horovod.runner.task_fn</code>。
其中：</p>
<ul>
<li>传入参数中，all_host_names 就是程序启动时候配置的所有host，比如 [&ldquo;1.1.1.1&rdquo;, &ldquo;2.2.2.2&rdquo;]；</li>
<li>使用了我们之前提到的 safe_shell_exec.execute 完成了安全运行保证；</li>
<li>使用我们前文提到的 get_remote_command 完成了远程命令的获取，即在命令之前加上了 ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no等等配置；</li>
<li>最终每个启动的命令举例如下： ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no 1.1.1.1 python -m horovod.runner.task_fn xxxxxxx；</li>
<li>使用 execute_function_multithreaded 在每一个 host 上运行，启动 task 服务；</li>
</ul>
<p>具体代码如下：</p>
<div class="highlight" id="id-33"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_launch_task_servers</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Executes the task server and service client task for registration on the
</span></span></span><span class="line"><span class="cl"><span class="s2">    hosts.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param all_host_names: list of addresses. for example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;worker-0&#39;,&#39;worker-1&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;10.11.11.11&#39;, &#39;10.11.11.12&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type all_host_names: list(string)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param local_host_names: names that are resolved to one of the addresses
</span></span></span><span class="line"><span class="cl"><span class="s2">    of local hosts interfaces. For example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        set([&#39;localhost&#39;, &#39;127.0.0.1&#39;])
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type local_host_names: set
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param driver_addresses: map of interfaces and their address and port for
</span></span></span><span class="line"><span class="cl"><span class="s2">    the service. For example:
</span></span></span><span class="line"><span class="cl"><span class="s2">        {
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;lo&#39;: [(&#39;127.0.0.1&#39;, 34588)],
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;docker0&#39;: [(&#39;172.122.10.1&#39;, 34588)],
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;eth0&#39;: [(&#39;11.111.33.73&#39;, 34588)]
</span></span></span><span class="line"><span class="cl"><span class="s2">        }
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type driver_addresses: map
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param settings: the object that contains the setting for running horovod
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type settings: horovod.runner.common.util.settings.Settings
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype:
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_exec_command</span><span class="p">(</span><span class="n">command</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_output</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 完成了安全运行保证</span>
</span></span><span class="line"><span class="cl">            <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stdout</span><span class="o">=</span><span class="n">host_output</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stderr</span><span class="o">=</span><span class="n">host_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">host_output</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">exit_code</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">args_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_hosts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_name</span> <span class="o">=</span> <span class="n">all_host_names</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="c1"># all_host_names 就是程序启动时候配置的所有host，比如 [&#34;1.1.1.1&#34;, &#34;2.2.2.2&#34;]</span>
</span></span><span class="line"><span class="cl">        <span class="n">command</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="s1">&#39;</span><span class="si">{python}</span><span class="s1"> -m horovod.runner.task_fn </span><span class="si">{index}</span><span class="s1"> </span><span class="si">{num_hosts}</span><span class="s1"> &#39;</span> \
</span></span><span class="line"><span class="cl">            <span class="s1">&#39;</span><span class="si">{driver_addresses}</span><span class="s1"> </span><span class="si">{settings}</span><span class="s1">&#39;</span> \
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">python</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">index</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">index</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_hosts</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">driver_addresses</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">driver_addresses</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">settings</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">settings</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">host_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">local_host_names</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 完成了远程命令的获取，即在命令之前加上了 `ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no`等等配置</span>
</span></span><span class="line"><span class="cl">            <span class="n">command</span> <span class="o">=</span> <span class="n">get_remote_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">host</span><span class="o">=</span><span class="n">host_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">port</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">identity_file</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args_list</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">command</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Each thread will use ssh command to launch the server on one task. If an</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># error occurs in one thread, entire process will be terminated. Otherwise,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># threads will keep running and ssh session -- and the the task server --</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># will be bound to the thread. In case, the horovod process dies, all</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the ssh sessions and all the task servers will die as well.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 使用 execute_function_multithreaded 在每一个 host 上运行，启动 task 服务</span>
</span></span><span class="line"><span class="cl">    <span class="n">threads</span><span class="o">.</span><span class="n">execute_function_multithreaded</span><span class="p">(</span><span class="n">_exec_command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">args_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">block_until_all_done</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="52-具体服务逻辑">5.2 具体服务逻辑</h3>
<p>上段有：<code>{python} -m horovod.runner.task_fn {index} {num_hosts} {driver_addresses} {settings}</code>执行具体服务逻辑，所以我们介绍下 <code>horovod.runner.task_fn</code>。</p>
<p><code>_task_fn</code> 函数完成了</p>
<ul>
<li>生成了 HorovodRunTaskService 实例，赋值给 task；</li>
<li>使用 HorovodRunDriverClient . register_task 来向 Driver 服务注册task（自己）的地址；</li>
<li>使用 HorovodRunDriverClient . register_task_to_task_addresses 来向 Driver 服务注册自己在Ring上 下一个邻居的地址；</li>
<li>每一个 task 都做这个操作，最后就得到了在这个 ring cluster 之中的一个路由接口；</li>
</ul>
<p>具体代码如下：</p>
<div class="highlight" id="id-34"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_task_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">task</span> <span class="o">=</span> <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskService</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">HorovodRunDriverClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 向 Driver 服务注册task（自己）的地址</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">register_task</span><span class="p">(</span><span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                             <span class="n">task</span><span class="o">.</span><span class="n">addresses</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                             <span class="n">host_hash</span><span class="o">.</span><span class="n">host_hash</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">wait_for_initial_registration</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Tasks ping each other in a circular fashion to determine interfaces</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reachable within the cluster.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_hosts</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task_addresses</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">all_task_addresses</span><span class="p">(</span><span class="n">next_task_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># We request interface matching to weed out all the NAT&#39;ed interfaces.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task</span> <span class="o">=</span> <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_task_index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_task_addresses</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">match_intf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attempts</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 向 Driver 服务注册自己在Ring上 下一个邻居的地址</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">register_task_to_task_addresses</span><span class="p">(</span><span class="n">next_task_index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                               <span class="n">next_task</span><span class="o">.</span><span class="n">addresses</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Notify the next task that the address checks are completed.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task</span><span class="o">.</span><span class="n">task_to_task_address_check_completed</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Wait to get a notification from previous task that its address checks</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># are completed as well.</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">wait_for_task_to_task_address_check_finish_signal</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">index</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_hosts</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">driver_addresses</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">settings</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_task_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">settings</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="53-horovodruntaskservice">5.3 HorovodRunTaskService</h3>
<p>HorovodRunTaskService 主要的作用是提供了两个等待函数。因为具体路由操作是需要彼此通知，所以需要互相等待。</p>
<div class="highlight" id="id-35"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunTaskService</span><span class="p">(</span><span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">NAME_FORMAT</span> <span class="o">=</span> <span class="s1">&#39;horovod task service #</span><span class="si">%d</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunTaskService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">HorovodRunTaskService</span><span class="o">.</span><span class="n">NAME_FORMAT</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">index</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_to_task_address_check_completed</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">client_address</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">TaskToTaskAddressCheckFinishedSignal</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_task_to_task_address_check_completed</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">            <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">notify_all</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">TaskToTaskAddressCheckFinishedSignalResponse</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunTaskService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_handle</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">client_address</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">wait_for_task_to_task_address_check_finish_signal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_to_task_address_check_completed</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">timeout</span><span class="o">.</span><span class="n">remaining</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">                <span class="n">timeout</span><span class="o">.</span><span class="n">check_time_out_for</span><span class="p">(</span><span class="s1">&#39;Task to task address check&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunTaskClient</span><span class="p">(</span><span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskClient</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">task_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">match_intf</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">attempts</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunTaskClient</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">HorovodRunTaskService</span><span class="o">.</span><span class="n">NAME_FORMAT</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">task_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">match_intf</span><span class="o">=</span><span class="n">match_intf</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attempts</span><span class="o">=</span><span class="n">attempts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">task_to_task_address_check_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">resp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send</span><span class="p">(</span><span class="n">TaskToTaskAddressCheckFinishedSignal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">resp</span><span class="o">.</span><span class="n">index</span></span></span></code></pre></td></tr></table>
</div>
</div><p>逻辑如下：</p>
<div class="highlight" id="id-36"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">                                                         <span class="n">_driver_fn</span>
</span></span><span class="line"><span class="cl">                                                            <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                                            <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                            <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+---------------------------------------+-------------------------------------</span><span class="n">v</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                                             <span class="n">v</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                                   <span class="n">_launch_task_servers</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                                             <span class="o">+</span>
</span></span><span class="line"><span class="cl">     <span class="n">driver</span> <span class="o">=</span> <span class="n">HorovodRunDriverService</span>                                                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                                                              <span class="o">+--------------+-------------------+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                              <span class="n">v</span>                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------+---------------+</span>                                    <span class="n">horovod</span><span class="o">.</span><span class="na">runner</span><span class="o">.</span><span class="na">task_fn</span>    <span class="o">......</span>     <span class="n">horovod</span><span class="o">.</span><span class="na">runner</span><span class="o">.</span><span class="na">task_fn</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">HorovodRunDriverService</span>           <span class="o">|</span>                                              <span class="o">+</span>                                  <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>        <span class="n">_all_task_addresses</span>        <span class="o">|</span>                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                              <span class="n">v</span>                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>    <span class="n">_task_addresses_for_driver</span>     <span class="o">|</span>          <span class="n">register_task</span>           <span class="o">+-----------+---------------+</span>          <span class="o">+-------+--------------------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                  <span class="o">|</span> <span class="n">HorovodRunTaskService</span>     <span class="o">|</span>          <span class="o">|</span>  <span class="n">HorovodRunTaskService</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_addresses_for_tasks</span>   <span class="o">|</span> <span class="o">&lt;--------------------------------+</span>                           <span class="o">|</span>          <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                  <span class="o">|</span>                           <span class="o">|</span>   <span class="n">wait</span>   <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_index_host_hash</span>       <span class="o">|</span>                                  <span class="o">|</span>                           <span class="o">|</span> <span class="o">&lt;------&gt;</span> <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span> <span class="o">&lt;--------------------------------+</span>                           <span class="o">|</span>          <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>     <span class="n">_task_host_hash_indices</span>       <span class="o">|</span>  <span class="n">register_task_to_task_addresses</span> <span class="o">|</span>                           <span class="o">|</span>          <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                  <span class="o">+---------------------------+</span>          <span class="o">+----------------------------+</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----------------------------------+</span>                                                  <span class="err">`</span></span></span></code></pre></td></tr></table>
</div>
</div><p>图示:</p>
<p></p>
<h2 id="6-总结">6 总结</h2>
<p>本文总结如下：</p>
<ul>
<li>因为 Horovod 分布式训练 涉及到多个 hosts，所以如果要彼此访问，需要知道路由信息；</li>
<li>当所有 task 都启动，注册，probe 环中下一个worker 邻居完成 之后，DriverService 会得到路由信息（所有host之间的共有路由接口集合），返回给 Horovod 主体部分使用；</li>
<li>network.BasicService 提供了网络服务功能；</li>
<li>XXXService 都是通过 XXXClient作为接口才能访问；</li>
<li>HorovodRunDriverService 和 HorovodRunTaskService 都最终继承了 network.BasicService，他们之间可以是异地运行交互。</li>
<li>HorovodRunTaskService 提供了 Task 部分服务功能，这些 task 需要注册到 Driver 之中（和Spark思路类似）。</li>
<li>HorovodRunDriverService 是对 BasicDriverService 的封装。BasicDriverService 就是 维护各种 task 地址以及相应关系，比如：
<ul>
<li>_all_task_addresses ：记录了所有 task 的地址；</li>
<li>_task_addresses_for_driver ：记录了所有 task 的地址，但是因为网卡接口有多种，这里选择与 本driver 地址匹配的地址；</li>
<li>_task_addresses_for_tasks ：用来给某一个 task 分配一个地址，同时获取本 task 的一套网络接口；</li>
<li>_task_index_host_hash ：每一个 task 有一个对应的 host hash。这个函数是 spark 相关会使用，具体是逐一通知 spark task 进入下一阶段。或者是为了获取某一个 host 对应的 host hash name；</li>
<li>_task_host_hash_indices ：具体是被 rsh 使用，由 rank 得到 在 driver 中 task index 对应保持的 task address；</li>
</ul>
</li>
<li>SparkDriverService，SparkTaskService，ElasticDriver, Worker 都有什么区别和联系？
<ul>
<li>HorovodRunDriverService 这里只是用来得到路由信息，记录各种 Task 地址；</li>
<li>SparkDriverService 除了记录路由和地址之外，还提交执行任务（Command），因为具体在哪一个Spark Executor启动之后，SparkDriverService 就需要知道 对应 SparkTaskService 的地址，这样才能知道提交到哪里；</li>
<li>SparkTaskService 负责执行命令（抛弃了Spark Executor的逻辑，自己搞了一套），就是从 SparkDriverService 那里获得训练函数，然后启动 python 进程来执行；</li>
<li>ElasticDriver 做得更多，因为还有弹性，需要容错；</li>
</ul>
</li>
</ul>
<p>references:
[1]. <a href="https://www.cnblogs.com/rossiXYZ/p/14882053.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14882053.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://www.zhihu.com/column/c_1491039346714746880"target="_blank" rel="external nofollow noopener noreferrer">https://www.zhihu.com/column/c_1491039346714746880<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>深度学习分布式训练框架 horovod[3] -- Horovodrun背后做了什么</title><link>https://lruihao.cn/posts/2022-10-08_horovod_3/</link><pubDate>Mon, 10 Jul 2023 07:53:45 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/2022-10-08_horovod_3/</guid><description><![CDATA[<p>references:
[1]. <a href="https://www.cnblogs.com/rossiXYZ/p/14881812.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14881812.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。本文是系列第三篇，从 python 开始进入 Horovod 世界，看看 Horovodrun 做了什么。</p>
<p>前两篇链接如下：</p>
<p><a href="https://www.cnblogs.com/rossiXYZ/p/14856464.html"target="_blank" rel="external nofollow noopener noreferrer">深度学习分布式训练框架 Horovod (1) &mdash; 基础知识<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://www.cnblogs.com/rossiXYZ/p/14856543.html"target="_blank" rel="external nofollow noopener noreferrer">深度学习分布式训练框架 horovod (2) &mdash; 从使用者角度切入<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="1-背景知识">1 背景知识</h2>
<p>首先介绍一些相关背景知识。</p>
<h3 id="11-分布式体系">1.1 分布式体系</h3>
<p>在设计并行计算机时，<u>最直接的方式就是<mark>多个计算单元共享一个内存</mark></u>。共享内存的编程在数据交换和访问上有较大的优势，程序编写起来更加简单。<font color=red>但在扩展性上有较大的瓶颈</font>。</p>
<p>另一种方式为<font color=red><strong>分布式内存</strong></font>。即<u>每个计算单元有单独的内存，计算单元之间的数据访问通过互联网络去传输</u>。这一架构在可移植性和扩展上会强很多，但<u>消息的传递</u>会成为程序设计中的难点。</p>
<p>将这两点结合，即是<u><font color=red><strong>分布式共享内存并行计算机的架构</strong></font></u>，也是当今最常用的体系结构。</p>
<h3 id="12-并行任务通信">1.2 并行任务通信</h3>
<p>并行任务通信一般分为<font color=red><strong>P2P</strong>(Point-to-point communication)</font>和 <font color=red><strong>Collective communication</strong></font>。</p>
<ul>
<li>P2P通信这种模式只有一个sender和一个receiver，即点到点通信.</li>
<li>Collective communication含多个sender多个receive</li>
</ul>
<p>Collective communication包含一些常见的原语</p>
<ul>
<li>broadcast</li>
<li>reduce，allreduce</li>
<li>scatter，scatter reduce</li>
<li>gather，allgather</li>
<li>ring-base collectives</li>
<li>ring-allreduce</li>
</ul>
<p>传统Collective communication假设通信节点组成的topology是一颗fat tree，这样通信效率最高。但实际的通信topology可能比较复杂，并不是一个fat tree。因此一般用<mark><strong>ring-based Collective communication</strong></mark>。</p>
<h3 id="13-mpi">1.3 MPI</h3>
<p><mark>MPI(Message Passing Interface)</mark> 是一种可以支持点对点和广播的通信协议，具体实现的库有很多，使用比较流行的包括 Open Mpi， Intel MPI 等等。</p>
<p>MPI 是一种<u>消息传递编程模型</u>。消息传递指用户必须通过显式地发送和接收消息来实现处理器间的数据交换。在这种并行编程中，<font color=red>每个控制流均有自己独立的地址空间，不同的控制流之间不能直接访问彼此的地址空间，必须通过显式的消息传递来实现</font>。这种编程方式是<mark>大规模并行处理机(MPP)</mark>和<mark>机群(Cluster)</mark>采用的主要编程方式。由于消息传递程序设计要求用户很好地分解问题，组织不同控制流间的数据交换，并行计算粒度大，特别适合于大规模可扩展并行算法。</p>
<p>MPI 是<mark>基于进程的并行环境。进程拥有独立的虚拟地址空间和处理器调度，并且执行相互独立</mark>。MPI 设计为支持通过网络连接的机群系统，且通过消息传递来实现通信，消息传递是 MPI 的最基本特色。</p>
<h3 id="14-open-mpi">1.4 Open-MPI</h3>
<p>OpenMPI 是一种高性能消息传递库，最初是作为融合的技术和资源从其他几个项目（FT-MPI， LA-MPI， LAM/MPI， 以及 PACX-MPI），它是 MPI-2 标准的一个开源实现，由一些科研机构和企业一起开发和维护。因此，OpenMPI 能够从高性能社区中获得专业技术、工业技术和资源支持，来创建最好的 MPI 库。OpenMPI 提供给系统和软件供应商、程序开发者和研究人员很多便利。易于使用，并运行本身在各种各样的操作系统，网络互连，以及一批/调度系统。</p>
<h3 id="15-mpi-使用问题">1.5 MPI 使用问题</h3>
<p>因为MPI是<u>分布式内存编程</u>，在后面的开发中涉及节点间信息的传递。往往数据和程序是在多个节点上，所以需要保证执行命令时各节点之间信息的交换。</p>
<p>具体使用之中，就有两个问题:</p>
<ul>
<li>这个多台机器Open-MPI是如何发现并建立连接的呢？</li>
<li>多机多卡在训练过程中，传输环如何建立，这个也是决定了训练效率，那么Open-MPI如何去做呢？</li>
</ul>
<p>关于第一个问题：</p>
<p>设置<font color=red>SSH免密登录</font>可以免去操作中密码的输入。各节点生成私钥和公钥后需要认证，此时可以保证本机免密登录。将各个子节点的公钥文件发送给主节点，然后分别加入到主节点的认证文件中，此时可以保证主节点对各个子节点的免密登录。最后将认证文件传回到每个子节点，从而保证各个子节点对其他节点之间的免密登录。</p>
<p>在 Open-MPI 启动的时候，可以指定<code>--hostfile</code>或者<code>--host</code>去指定要运行任务的 IP 或 Hostname，这样 Open-MPI 就会试图通过 ssh 免秘钥的方式试图去链接对方机器，并执行一系列命令，主要是为了<strong>同步环境变量、当前路径以及下发启动命令</strong>。</p>
<p>当然用户也可以通过其他方式给远程机器下发命令，这个可以通过环境变量<code>OMPI_MCA_plm_rsh_agent</code>指定。</p>
<p>关于第二个问题：</p>
<p>当所有的机器建立好连接，准备开始计算，为了能够最高效的去通信，Open-MPI中集成了组件——<a href="https://github.com/open-mpi/hwloc"target="_blank" rel="external nofollow noopener noreferrer">hwloc<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。该组件主要是<font color=red><strong>为了单机硬件资源拓扑构建，进而构建最短路径通信</strong></font>。</p>
<h2 id="2-入口点">2 入口点</h2>
<p>很多机器学习框架都会采用如下套路：shell脚本（可选），python端 和 C++端。</p>
<ul>
<li>Shell脚本是启动运行的入口，负责解析参数，确认并且调用训练程序；</li>
<li>Python是用户的接口，引入了C++库，封装了API，负责运行时和底层C++交互；</li>
<li>C++实现底层训练逻辑；</li>
</ul>
<p>以我们先看看 hordovodrun 脚本。</p>
<h3 id="21-如何运行">2.1 如何运行</h3>
<p>官方给出的 Hovorod 运行范例之一如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">horovodrun</span> <span class="o">-</span><span class="n">np</span> <span class="mi">2</span> <span class="o">-</span><span class="n">H</span> <span class="n">localhost</span><span class="p">:</span><span class="mi">4</span> <span class="o">--</span><span class="n">gloo</span> <span class="n">python</span> <span class="o">/</span><span class="n">horovod</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">tensorflow2</span><span class="o">/</span><span class="n">tensorflow2_mnist</span><span class="o">.</span><span class="n">py</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里 -np 指的是<strong>进程的数量</strong>，localhost:4<strong>表示localhost节点上4个GPU</strong>。</p>
<p>注意，如果虚拟机只有一个核。想要强行地达到并行的效果，可以使用 -np参数，它会自动帮你把一个核心切成多份处理器，每一个分布式处理就是一个slot。</p>
<p>因此，我们可以从 horovodrun 这个命令入手看看。</p>
<h3 id="22-horovodrun">2.2 horovodrun</h3>
<p>入口文件可以从 setup.py 看到，其就被映射成 horovod.runner.launch:run_commandline。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">entry_points</span><span class="o">={</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;console_scripts&#39;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;horovodrun = horovod.runner.launch:run_commandline&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所以我们看看 run_commandline</p>
<h3 id="23-run_commandline">2.3 run_commandline</h3>
<p>该命令位于：horovod-master/horovod/runner/launch.py，我们摘录重要部分。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_commandline</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">_run</span><span class="p">(</span><span class="n">args</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>于是进入到 _run 函数。可以看到，Horovod 会依据是否是弹性训练来选择不同的路径。我们在此系列中，会首先分析 非弹性训练 _run_static。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># if hosts are not specified, either parse from hostfile, or default as</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># localhost</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">hosts</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">host_discovery_script</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">hostfile</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">args</span><span class="o">.</span><span class="n">hosts</span> <span class="o">=</span> <span class="n">hosts</span><span class="o">.</span><span class="n">parse_host_files</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hostfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Set hosts to localhost if not specified</span>
</span></span><span class="line"><span class="cl">            <span class="n">args</span><span class="o">.</span><span class="n">hosts</span> <span class="o">=</span> <span class="s1">&#39;localhost:</span><span class="si">{np}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Convert nics into set</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span><span class="o">.</span><span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">nics</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">))</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">nics</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">_is_elastic</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_run_elastic</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_run_static</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="c1"># 我们先看这里</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="24-非弹性训练-_run_static">2.4 非弹性训练 _run_static()</h3>
<p>在 _run_static 之中做了如下操作：</p>
<ul>
<li>首先解析各种参数，得到 settings；</li>
<li>会调用 <code>driver_service.get_common_interfaces</code> 获取网卡以及其他host的信息，依据这些信息会进行slot分配，这部分很复杂，具体我们会有专文讲解（下一篇）。</li>
<li>这里有一个问题：为什么要得到 host, slot, rank 之间的关系信息？由于工程上的考虑，底层 C++ 世界中对于 rank 的角色做了区分：<code>rank 0</code> 是 master，<code>rank n</code> 是 worker，所以这些信息需要决定并且传递给 C++世界；</li>
<li>会根据是否在参数中传递运行函数来决定采取何种路径，一般默认没有运行参数，所以会执行_launch_job 来启动训练 job；</li>
</ul>
<p>具体代码如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_static</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">settings</span> <span class="o">=</span> <span class="n">hvd_settings</span><span class="o">.</span><span class="n">Settings</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">2</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">verbose</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ssh_port</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ssh_identity_file</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">extra_mpi_args</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">mpi_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">tcp_flag</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">tcp_flag</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">binding_args</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">binding_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">key</span><span class="o">=</span><span class="n">secret</span><span class="o">.</span><span class="n">make_secret_key</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">start_timeout</span><span class="o">=</span><span class="n">tmout</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">num_proc</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">hosts</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">output_filename</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">output_filename</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">run_func_mode</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">run_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">nics</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">nics</span><span class="p">,</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	  <span class="c1"># 首先解析各种参数，得到 settings</span>
</span></span><span class="line"><span class="cl">    <span class="n">fn_cache</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">disable_cache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="n">args</span><span class="o">.</span><span class="n">ssh_identity_file</span>
</span></span><span class="line"><span class="cl">        <span class="n">parameters_hash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">fn_cache</span> <span class="o">=</span> <span class="n">cache</span><span class="o">.</span><span class="n">Cache</span><span class="p">(</span><span class="n">CACHE_FOLDER</span><span class="p">,</span> <span class="n">CACHE_STALENESS_THRESHOLD_MINUTES</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                               <span class="n">parameters_hash</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取网卡以及其他host的信息，依据这些信息会进行slot分配</span>
</span></span><span class="line"><span class="cl">    <span class="n">all_host_names</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hosts</span><span class="o">.</span><span class="n">parse_hosts_and_slots</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">remote_host_names</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">filter_local_addresses</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">get_common_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">all_host_names</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">remote_host_names</span><span class="p">,</span> <span class="n">fn_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_func</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get the driver IPv4 address</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver_ip</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_driver_ip</span><span class="p">(</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">run_func_server</span> <span class="o">=</span> <span class="n">KVStoreServer</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span> <span class="c1"># 启动内部KV服务器</span>
</span></span><span class="line"><span class="cl">        <span class="n">run_func_server_port</span> <span class="o">=</span> <span class="n">run_func_server</span><span class="o">.</span><span class="n">start_server</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">put_data_into_kvstore</span><span class="p">(</span><span class="n">driver_ip</span><span class="p">,</span> <span class="n">run_func_server_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="s1">&#39;runfunc&#39;</span><span class="p">,</span> <span class="s1">&#39;func&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">run_func</span><span class="p">)</span> <span class="c1"># 把&#39;func&#39;, args.run_func存储成KV</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">command</span> <span class="o">=</span> <span class="p">[</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span> <span class="s1">&#39;-m&#39;</span><span class="p">,</span> <span class="s1">&#39;horovod.runner.run_task&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">driver_ip</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">run_func_server_port</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">_launch_job</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">np</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_data_from_kvstore</span><span class="p">(</span><span class="n">driver_ip</span><span class="p">,</span> <span class="n">run_func_server_port</span><span class="p">,</span><span class="s1">&#39;runfunc_result&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">results</span>
</span></span><span class="line"><span class="cl">        <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">run_func_server</span><span class="o">.</span><span class="n">shutdown_server</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">command</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">command</span>
</span></span><span class="line"><span class="cl">        <span class="n">_launch_job</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span> <span class="c1"># 我们重点讲解这里</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下：</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">              <span class="o">+-----------+</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span><span class="n">horovodrun</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+-----+-----+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">           <span class="o">+--------+--------+</span>
</span></span><span class="line"><span class="cl">           <span class="o">|</span> <span class="n">run_commandline</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="o">+----+------+-----+</span>
</span></span><span class="line"><span class="cl">                <span class="o">|</span>      <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">+---------+</span>      <span class="o">+--------+</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="n">v</span>                         <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----+--------+</span>           <span class="o">+----+--------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">_run_elastic</span> <span class="o">|</span>           <span class="o">|</span> <span class="n">_run_static</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>              <span class="o">|</span>           <span class="o">|</span>             <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+--------------+</span>           <span class="o">+-------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>至此，我们已经分析完成 horovod 的入口，下面会分析具体如何启动 Job。</p>
<h2 id="3-运行训练-job">3 运行训练 Job</h2>
<h3 id="31-_launch_job">3.1 _launch_job</h3>
<p>_launch_job 会根据配置或者安装情况来进行具体调用。我们看到有三种可能：gloo, mpi, js。</p>
<p>jsrun的资料很难找，所以我们重点看看 gloo, mpi 这两种。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_launch_job</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">command</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">config_parser</span><span class="o">.</span><span class="n">set_env_from_args</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">gloo_run_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver_ip</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_driver_ip</span><span class="p">(</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">gloo_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">driver_ip</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">mpi_run_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">js_run_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">js_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">run_controller</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">use_gloo</span><span class="p">,</span> <span class="n">gloo_run_fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">args</span><span class="o">.</span><span class="n">use_mpi</span><span class="p">,</span> <span class="n">mpi_run_fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">args</span><span class="o">.</span><span class="n">use_jsrun</span><span class="p">,</span> <span class="n">js_run_fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">args</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-run_controller">3.2 run_controller</h3>
<p>run_controller 依然是一个中介函数，具体导入 gloo 或者 mpi。</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_controller</span><span class="p">(</span><span class="n">use_gloo</span><span class="p">,</span> <span class="n">gloo_run</span><span class="p">,</span> <span class="n">use_mpi</span><span class="p">,</span> <span class="n">mpi_run</span><span class="p">,</span> <span class="n">use_jsrun</span><span class="p">,</span> <span class="n">js_run</span><span class="p">,</span> <span class="n">verbosity</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">use_gloo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">gloo_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">use_mpi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">use_jsrun</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">js_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mpi_built</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">lsf</span><span class="o">.</span><span class="n">LSFUtils</span><span class="o">.</span><span class="n">using_lsf</span><span class="p">()</span> <span class="ow">and</span> <span class="n">is_jsrun_installed</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="n">js_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">gloo_built</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">gloo_run</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">              <span class="o">+-----------+</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span><span class="n">horovodrun</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+-----+-----+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">           <span class="o">+--------+--------+</span>
</span></span><span class="line"><span class="cl">           <span class="o">|</span> <span class="n">run_commandline</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="o">+----+------+-----+</span>
</span></span><span class="line"><span class="cl">                <span class="o">|</span>      <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">+---------+</span>      <span class="o">+--------+</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="n">v</span>                         <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----+--------+</span>           <span class="o">+----+--------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">_run_elastic</span> <span class="o">|</span>           <span class="o">|</span> <span class="n">_run_static</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>              <span class="o">|</span>           <span class="o">|</span>             <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+--------------+</span>           <span class="o">+------+------+</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl">                           <span class="o">+------+------+</span>
</span></span><span class="line"><span class="cl">                           <span class="o">|</span> <span class="n">_launch_job</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">                           <span class="o">|</span>             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                           <span class="o">+------+------+</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl">                        <span class="o">+---------+--------+</span>
</span></span><span class="line"><span class="cl">                        <span class="o">|</span>  <span class="n">run_controller</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                        <span class="o">|</span>                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                        <span class="o">+----+----+-----+--+</span>
</span></span><span class="line"><span class="cl">                             <span class="o">|</span>    <span class="o">|</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">               <span class="o">+-------------+</span>    <span class="o">|</span>     <span class="o">+--------+</span>
</span></span><span class="line"><span class="cl">               <span class="o">|</span>                  <span class="o">|</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl">               <span class="o">|</span>                  <span class="o">|</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl">               <span class="n">v</span>                  <span class="n">v</span>              <span class="n">v</span>
</span></span><span class="line"><span class="cl">        <span class="o">+------+---+</span>       <span class="o">+------+----+</span>     <span class="o">+---+-----+</span>
</span></span><span class="line"><span class="cl">        <span class="o">|</span> <span class="n">gloo_run</span> <span class="o">|</span>       <span class="o">|</span>   <span class="n">mpi_run</span> <span class="o">|</span>     <span class="o">|</span> <span class="n">js_run</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">        <span class="o">|</span>          <span class="o">|</span>       <span class="o">|</span>           <span class="o">|</span>     <span class="o">|</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">        <span class="o">+----------+</span>       <span class="o">+-----------+</span>     <span class="o">+---------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>于是我们下面就分为两个分支介绍：gloo &amp; mpi。</p>
<h2 id="4-gloo-实现">4 Gloo 实现</h2>
<h3 id="41-gloo-简介">4.1 Gloo 简介</h3>
<p>Gloo 是 facebook出品的一个类似MPI的集合通信库（https://github.com/facebookincubator/gloo）。</p>
<p>集合通信库的主要特征是：大体上会遵照 MPI 提供的接口规定，实现了包括<strong>点对点通信</strong>（SEND,RECV等），<strong>集合通信</strong>（ REDUCE，BROADCAST，ALLREDUCE等）等相关接口，然后根据自己硬件或者是系统的需要，在底层实现上进行相应改动，保证接口的稳定和性能。</p>
<p>Gloo 为CPU和GPU提供了集合通信程序的优化实现。 它特别适用于GPU，因为它可以执行通信而无需使用GPUDirect 将数据传输到CPU的内存。 它还能够使用 NCCL 执行快速的节点内通信，并实现其自己的节点间例程计算。你不需要考虑内存数据的拷贝，只需要实现逻辑就可以。</p>
<p>Gloo 支持集合通信（collective Communication），并对其进行了优化。由于 GPU 之间可以直接进行数据交换，而无需经过 CPU 和内存，因此，在 GPU 上使用 <strong>gloo后端</strong>速度更快。</p>
<p>Horovod 为什么会选择 Gloo？个人认为除了其功能的全面性和性能之外，基于它可以二次开发是一个亮点，比如下面我们所说的 Rendezvous 功能就被 Horovod 用来实现弹性训练（我们后文有专门讲解）。</p>
<p>Gloo 和 MPI 都起到了同样类似作用：</p>
<ul>
<li>一方面Horovod内集成了基于 Gloo 的AllReduce，类似于NCCL，都是用作梯度规约；</li>
<li>另一方面，Gloo 可以用来启动多个进程（Hovorod里用Rank表示），实现并行计算；</li>
</ul>
<p>具体如下：</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">   <span class="o">+-----------------------+</span>   <span class="o">+-----------------------+</span>  <span class="o">+------------------------+</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span>  <span class="n">gloo_run</span>      <span class="n">slot</span> <span class="mi">1</span> <span class="o">|</span>   <span class="o">|</span> <span class="n">gloo_run</span>     <span class="n">slot</span> <span class="mi">2</span>   <span class="o">|</span>  <span class="o">|</span>  <span class="n">gloo_run</span>  <span class="n">slot</span> <span class="mi">3</span>      <span class="o">|</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span>                       <span class="o">|</span>   <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span> <span class="o">+-------------------+</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">+------------------+</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">+------------------+</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span> <span class="o">|</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>   <span class="o">|</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">|</span>  <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span> <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">|</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>  <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+----+</span>                   <span class="o">+&lt;------+</span>                  <span class="o">+&lt;------+</span>                  <span class="o">+&lt;------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">|</span> <span class="o">|</span>                   <span class="o">|</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">|</span>                  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">|</span>                  <span class="o">|</span>   <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">|</span> <span class="o">+-------------------+</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">+------------------+</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">+------------------+</span>   <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">|</span>                       <span class="o">|</span>   <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>                        <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">+-----------------------+</span>   <span class="o">+-----------------------+</span>  <span class="o">+------------------------+</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                                                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                                                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                                                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">v</span><span class="o">--------------------------------------------------------------------------------------&gt;</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">Ring</span> <span class="n">Allreduce</span> <span class="n">on</span> <span class="n">Gloo</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42-rendezvous-功能">4.2 Rendezvous 功能</h3>
<h4 id="421-rendezvous-概念">4.2.1 Rendezvous 概念</h4>
<p>在 Gloo 的文档中，如此说:</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">The rendezvous process needs to happen exactly once per Gloo context.
</span></span><span class="line"><span class="cl">It makes participating Gloo processes exchange details for setting up their communication channels. For example, when the TCP transport is used, processes exchange IP address and port number details of listening sockets.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Rendezvous can be executed by accessing a key/value store that is accessible by all participating processes. Every process is responsible for setting a number of keys and will wait until their peers have set their keys. The values stored against these keys hold
</span></span><span class="line"><span class="cl">the information that is passed to the transport layer.</span></span></code></pre></td></tr></table>
</div>
</div><p>大致意思是：</p>
<p>Gloo 在每一个 Gloo context 之中有一个 rendezvous process，Gloo 利用它来<strong>交换通讯需要的细节</strong>。</p>
<p>Rendezvous 具体实现是可以依靠访问一个 KVstore 来完成。具体细节就是通过 KVstore 来进行交互。</p>
<p>以 Horovod 为例：</p>
<ul>
<li>Horovod 在进行容错 AllReduce 训练时，除了启动 <strong>worker 进程</strong>外，还会启动一个<strong>driver 进程</strong>。这个 driver 进程用于帮助 worker 调用 gloo 构造 AllReduce 通信环。</li>
<li>driver 进程中会创建一个带有 KVStore 的 RendezvousServer，driver 会将参与通信的 worker 的 ip 等信息存入 KVstore 中。</li>
<li>然后 worker 就可以调用 gloo 来访问 RendezvousServer 构造通信环了。</li>
</ul>
<h4 id="422-rendezvousserver">4.2.2 RendezvousServer</h4>
<p>具体代码如下，可以看到是启动了RendezvousHTTPServer(就是继承拓展了 HTTPServer):</p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RendezvousServer</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_listen_thread</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span> <span class="o">=</span> <span class="n">verbose</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Rendezvous function finds a available port, create http socket,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># and start listening loop to handle request</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># self.httpd.init needs to be called after server start</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handler_cls</span><span class="o">=</span><span class="n">RendezvousHandler</span><span class="p">):</span> <span class="c1"># 下面马上介绍</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">find_port</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="k">lambda</span> <span class="n">addr</span><span class="p">:</span> <span class="n">RendezvousHTTPServer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">addr</span><span class="p">,</span> <span class="n">handler_cls</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># start the listening loop</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_listen_thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="o">.</span><span class="n">serve_forever</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">port</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_alloc_plan</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">host_alloc_plan</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_listen_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-kvstore">4.2.3 KVStore</h4>
<p>KVStore 是由 KVStoreHandler 来体现，RendezvousHandler 继承了 KVStoreHandler，进而被 RendezvousServer 作为 handler 使用。</p>
<p>KVStoreHandler 精简版代码如下：</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KVStoreHandler</span><span class="p">(</span><span class="n">SimpleHTTPRequestHandler</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Override PUT handler</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">do_PUT</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">paths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">paths</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Get body length</span>
</span></span><span class="line"><span class="cl">        <span class="n">content_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="p">[</span><span class="s1">&#39;Content-Length&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">content_length</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_put_value</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">send_status_code</span><span class="p">(</span><span class="n">OK</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_put_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">server</span><span class="o">.</span><span class="n">cache_lock</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scope_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">server</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="p">{})</span>
</span></span><span class="line"><span class="cl">            <span class="n">scope_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="424-底层使用">4.2.4 底层使用</h4>
<p>Rendezvous 具体如何使用？简要的说：</p>
<ul>
<li>Python世界构建了一个 RendezvousServer，其地址配置在环境变量（或者其他方式）中。</li>
<li>在 C++ 世界中，比如 horovod/common/gloo/gloo_context.h，horovod/common/gloo/gloo_context.cc 之中有使用。即得到 Python 配置的 RendezvousServer 的地址端口等，然后构建 gloo 所需的 context。</li>
</ul>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#define HOROVOD_HOSTNAME &#34;HOROVOD_HOSTNAME&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_RANK &#34;HOROVOD_RANK&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_SIZE &#34;HOROVOD_SIZE&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_LOCAL_RANK &#34;HOROVOD_LOCAL_RANK&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_LOCAL_SIZE &#34;HOROVOD_LOCAL_SIZE&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_CROSS_RANK &#34;HOROVOD_CROSS_RANK&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_CROSS_SIZE &#34;HOROVOD_CROSS_SIZE&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_ELASTIC &#34;HOROVOD_ELASTIC&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">  <span class="n">ctx</span> <span class="o">=</span> <span class="n">Rendezvous</span><span class="p">(</span><span class="n">HOROVOD_GLOO_GLOBAL_PREFIX</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">rendezvous_addr_env</span><span class="p">,</span> <span class="n">rendezvous_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">timeout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">local_ctx</span> <span class="o">=</span> <span class="n">Rendezvous</span><span class="p">(</span><span class="n">HOROVOD_GLOO_LOCAL_PREFIX</span> <span class="o">+</span> <span class="n">hostname</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">rendezvous_addr_env</span><span class="p">,</span> <span class="n">rendezvous_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">local_rank</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">timeout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">cross_ctx</span> <span class="o">=</span> <span class="n">Rendezvous</span><span class="p">(</span><span class="n">HOROVOD_GLOO_CROSS_PREFIX</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">local_rank</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">rendezvous_addr_env</span><span class="p">,</span> <span class="n">rendezvous_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">cross_rank</span><span class="p">,</span> <span class="n">cross_size</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">timeout</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><p>逻辑如下，C++世界会从python世界的获取到RendezvousServer的 IP，port：</p>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">          <span class="o">+---------------------&gt;</span>  <span class="n">System</span> <span class="n">Env</span>  <span class="o">+------------------+</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>  <span class="n">addr</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="o">...</span>                     <span class="n">addr</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="o">...</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">+</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>    <span class="n">Python</span>                  <span class="o">|</span>              <span class="n">C</span><span class="o">++</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+---------+---------------+</span>            <span class="o">|</span>             <span class="o">+------------+--------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">RendezvousServer</span>        <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span><span class="n">GlooContext</span>          <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>    <span class="n">RendezvousHandler</span>    <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>      <span class="n">Rendezvous</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------------+</span>            <span class="o">|</span>             <span class="o">+---------------------+</span>
</span></span><span class="line"><span class="cl">                                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                       <span class="o">+</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-horovd-的-gloo-入口">4.3 Horovd 的 gloo 入口</h3>
<p>gloo_run 是 horovod 之中，gloo 模块的 相关入口。</p>
<p>注释说的很清楚：每一个 thread 将使用 ssh 命令在远程host之上启动训练job。</p>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gloo_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">,</span> <span class="n">command</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Each thread will use ssh command to launch the job on each remote host. If an</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># error occurs in one thread, entire process will be terminated. Otherwise,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># threads will keep running and ssh session.</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec_command</span> <span class="o">=</span> <span class="n">_exec_command_fn</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">launch_gloo</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">exec_command</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>就是用 launch_gloo 来运行 exec_command。</p>
<p>此时 command 参数类似 <code>&quot;['python', 'train.py']&quot;</code>。</p>
<h3 id="44-构建可执行环境">4.4 构建可执行环境</h3>
<p>gloo_run 的第一部分是 <code>exec_command = _exec_command_fn(settings)</code>，就是基于各种配置来生成可以执行命令环境。如果是远程，就得生成相关远程可运行命令环境（包括切换目录，远程执行等等）。</p>
<h4 id="441-_exec_command_fn">4.4.1 _exec_command_fn</h4>
<p>具体又可以分为两部分：</p>
<ul>
<li>利用 get_remote_command 来生成相关远程可运行环境，比如在训练脚本前面加上 <code>'ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no'</code>；</li>
<li>调整输入输出，利用 safe_shell_exec.execute 来实现安全执行能力；</li>
</ul>
<p>具体如下：</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_exec_command_fn</span><span class="p">(</span><span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    executes the jobs defined by run command on hosts.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param hosts_alloc: list of dict indicating the allocating info.
</span></span></span><span class="line"><span class="cl"><span class="s2">    For example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        [{&#39;Hostname&#39;:&#39;worker-0&#39;, &#39;Rank&#39;: 0, &#39;Local_rank&#39;: 0, &#39;Cross_rank&#39;:0,
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;Size&#39;:2, &#39;Local_size&#39;:1, &#39;Cross_size&#39;:2},
</span></span></span><span class="line"><span class="cl"><span class="s2">        {&#39;Hostname&#39;:&#39;worker-1&#39;, &#39;Rank&#39;: 1, &#39;Local_rank&#39;: 0, &#39;Cross_rank&#39;:1,
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;Size&#39;:2, &#39;Local_size&#39;:1, &#39;Cross_size&#39;:2}
</span></span></span><span class="line"><span class="cl"><span class="s2">        ]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type hosts_alloc: list(dict)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param remote_host_names: names that are resolved to one of the addresses
</span></span></span><span class="line"><span class="cl"><span class="s2">    of remote hosts interfaces.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param _run_command: command to execute
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_exec_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">slot_info</span><span class="p">,</span> <span class="n">events</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span> <span class="o">=</span> <span class="n">slot_info</span><span class="o">.</span><span class="n">rank</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_name</span> <span class="o">=</span> <span class="n">slot_info</span><span class="o">.</span><span class="n">hostname</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_address</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">resolve_host_address</span><span class="p">(</span><span class="n">host_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_addresses</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_local_host_addresses</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 需要构建远程命令</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">host_address</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">local_addresses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_command</span> <span class="o">=</span> <span class="n">quote</span><span class="p">(</span><span class="s1">&#39;cd </span><span class="si">{pwd}</span><span class="s1"> &gt; /dev/null 2&gt;&amp;1 ; </span><span class="si">{command}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">command</span><span class="o">=</span><span class="n">command</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">command</span> <span class="o">=</span> <span class="n">get_remote_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">host</span><span class="o">=</span><span class="n">host_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">port</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">identity_file</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Redirect output if requested</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 调整输入输出，利用 safe_shell_exec.execute 来实现安全执行能力</span>
</span></span><span class="line"><span class="cl">        <span class="n">stdout</span> <span class="o">=</span> <span class="n">stderr</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">stdout_file</span> <span class="o">=</span> <span class="n">stderr_file</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_rank</span> <span class="o">=</span> <span class="n">_pad_rank</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_dir_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">,</span> <span class="s1">&#39;rank.</span><span class="si">{rank}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">padded_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">stdout_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">,</span> <span class="s1">&#39;stdout&#39;</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">stderr_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">,</span> <span class="s1">&#39;stderr&#39;</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">stdout</span> <span class="o">=</span> <span class="n">MultiFile</span><span class="p">([</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stdout_file</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">stderr</span> <span class="o">=</span> <span class="n">MultiFile</span><span class="p">([</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">,</span> <span class="n">stderr_file</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 实现安全执行能力</span>
</span></span><span class="line"><span class="cl">            <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stdout</span><span class="o">=</span><span class="n">stdout</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stderr</span><span class="o">=</span><span class="n">stderr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">events</span><span class="o">=</span><span class="n">events</span><span class="p">,</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">exit_code</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">_exec_command</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="442-get_remote_command">4.4.2 get_remote_command</h4>
<p>本函数是针对远程 host，获取如何在其上运行的方式。这个函数是比较新加入的，具体和 kubeflow mpi operator 也相关，以后有机会再分析。</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">SSH_COMMAND_PREFIX</span> <span class="o">=</span> <span class="s1">&#39;ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_ssh_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">identity_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timeout_s</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">port_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-p </span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">port</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">identity_file_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-i </span><span class="si">{</span><span class="n">identity_file</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">identity_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeout_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-o ConnectTimeout=</span><span class="si">{</span><span class="n">timeout_s</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">timeout_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">SSH_COMMAND_PREFIX</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">host</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">port_arg</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">identity_file_arg</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">timeout_arg</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">local_command</span><span class="si">}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_remote_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">identity_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timeout_s</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">env_util</span><span class="o">.</span><span class="n">KUBEFLOW_MPI_EXEC</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">host</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">local_command</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">env_util</span><span class="o">.</span><span class="n">is_kubeflow_mpi</span><span class="p">()</span> \
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="n">get_ssh_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">identity_file</span><span class="p">,</span> <span class="n">timeout_s</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大致逻辑如下：</p>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">command</span>  <span class="o">:</span>  <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>  <span class="n">get_remote_command</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">ssh</span> <span class="o">-</span><span class="n">o</span> <span class="o">...</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">            <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+--------------+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span><span class="n">safe_shell_exec</span><span class="o">.</span><span class="na">execute</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="45-使用-gloo-执行命令">4.5 使用 gloo 执行命令</h3>
<p>获取到了可执行环境 exec_command 与 执行命令 command 之后，就可以使用 gloo 来执行命令了。</p>
<p>每个 command 都是被 exec_command 来执行。</p>
<p>launch_gloo 来获取命令，各种配置信息，网卡信息（nics，比如 {&rsquo;lo&rsquo;}），host信息等，然后开始运行，就是开始运行我们的训练代码了，具体是：</p>
<ul>
<li>建立 RendezvousServer，这个会被底层 Gloo C++ 环境使用到;</li>
<li>host_alloc_plan = get_host_assignments 来根据host进行分配slot，就是horovod的哪个rank应该在哪个host上的哪个slot之上运行；</li>
<li>get_run_command 获取到可执行命令；</li>
<li>slot_info_to_command_fn 来得到在slot之上可执行的 slot command；</li>
<li>依据 slot_info_to_command_fn 构建 args_list，这个 list 之中，每一个arg就是一个 slot command；</li>
<li>多线程执行，在每一个 exec_command 之上执行每一个 arg（slot command）；</li>
</ul>
<p>代码如下：</p>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">launch_gloo</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">exec_command</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Launches the given command multiple times using gloo.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Each command is launched via exec_command.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param command: command to launch
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param exec_command: means to execute a single command
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param settings: settings for the distribution
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param nics: common interfaces
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param env: environment to use
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param server_ip: ip to use for rendezvous server
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Make the output directory if it does not exist</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">_mkdir_p</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># start global rendezvous server and get port that it is listening on</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 建立 RendezvousServer，这个会被底层 Gloo C++ 环境使用到</span>
</span></span><span class="line"><span class="cl">    <span class="n">rendezvous</span> <span class="o">=</span> <span class="n">RendezvousServer</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># allocate processes into slots</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 来根据host进行分配slot，就是horovod的哪个rank应该在哪个host上的哪个slot之上运行</span>
</span></span><span class="line"><span class="cl">    <span class="n">hosts</span> <span class="o">=</span> <span class="n">parse_hosts</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">host_alloc_plan</span> <span class="o">=</span> <span class="n">get_host_assignments</span><span class="p">(</span><span class="n">hosts</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># start global rendezvous server and get port that it is listening on</span>
</span></span><span class="line"><span class="cl">    <span class="n">global_rendezv_port</span> <span class="o">=</span> <span class="n">rendezvous</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">rendezvous</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">host_alloc_plan</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取到可执行命令</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_command</span> <span class="o">=</span> <span class="n">get_run_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">global_rendezv_port</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 得到在slot之上可执行的 slot command</span>
</span></span><span class="line"><span class="cl">    <span class="n">slot_info_to_command</span> <span class="o">=</span> <span class="n">_slot_info_to_command_fn</span><span class="p">(</span><span class="n">run_command</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">event</span> <span class="o">=</span> <span class="n">register_shutdown_event</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 依据 slot_info_to_command_fn 构建 args_list，这个 list 之中，每一个arg就是一个 slot command</span>
</span></span><span class="line"><span class="cl">    <span class="n">args_list</span> <span class="o">=</span> <span class="p">[[</span><span class="n">slot_info_to_command</span><span class="p">(</span><span class="n">slot_info</span><span class="p">),</span> <span class="n">slot_info</span><span class="p">,</span> <span class="p">[</span><span class="n">event</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">                 <span class="k">for</span> <span class="n">slot_info</span> <span class="ow">in</span> <span class="n">host_alloc_plan</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># If an error occurs in one thread, entire process will be terminated.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Otherwise, threads will keep running.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 多线程执行，在每一个 exec_command 之上执行每一个 arg（slot command）</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">=</span> <span class="n">threads</span><span class="o">.</span><span class="n">execute_function_multithreaded</span><span class="p">(</span><span class="n">exec_command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                 <span class="n">args_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                 <span class="n">block_until_all_done</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">exit_code</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">=</span> <span class="n">value</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体 HostInfo.from_string 信息如下：</p>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HostInfo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hostname</span><span class="p">,</span> <span class="n">slots</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hostname</span> <span class="o">=</span> <span class="n">hostname</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">slots</span> <span class="o">=</span> <span class="n">slots</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">from_string</span><span class="p">(</span><span class="n">host_string</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">hostname</span><span class="p">,</span> <span class="n">slots</span> <span class="o">=</span> <span class="n">host_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">HostInfo</span><span class="p">(</span><span class="n">hostname</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">slots</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><h5 id="4512-分配方案">4.5.1.2 分配方案</h5>
<p>get_host_assignments 会依据 host 和 process capacities (slots) 来给 Horovod 之中的进程分配，即给出一个 horovod rank 和 slot 的对应关系。设置了几个 np，就有几个 slot。</p>
<p>给出的分配方案类似如下，这样就知道了哪个rank对应于哪个host上的哪个slot：</p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">[
</span></span><span class="line"><span class="cl">  SlotInfo(hostname=&#39;h1&#39;, rank=0, local_rank=0, cross_rank=0, size=2, local_size=2, coress_size=1),
</span></span><span class="line"><span class="cl">	SlotInfo(hostname=&#39;h2&#39;, rank=1, local_rank=0, cross_rank=0, size=2, local_size=2, coress_size=1),
</span></span><span class="line"><span class="cl">]</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_host_assignments</span><span class="p">(</span><span class="n">hosts</span><span class="p">,</span> <span class="n">min_np</span><span class="p">,</span> <span class="n">max_np</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Assign hosts with process capacities (slots) to ranks in the Horovod process.
</span></span></span><span class="line"><span class="cl"><span class="s2">    This function will try to allocate as many as possible processes on the same host to leverage local network.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param hosts: list of HostInfo objects describing host and slot capacity
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type hosts: list[HostInfo]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param min_np: minimum number of processes to be allocated
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param max_np: (optional) maximum number of processes to be allocated
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return: a list of the allocation of process on hosts in a `SlotInfo` object.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype: list[SlotInfo]
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">host_ranks</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">cross_ranks</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 依据 hosts 信息构建 rank, local rank, cross rank(hierarchical allreduce所需要)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">host_info</span> <span class="ow">in</span> <span class="n">hosts</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">local_rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">host_info</span><span class="o">.</span><span class="n">slots</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">max_np</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">cross_ranks_at_local</span> <span class="o">=</span> <span class="n">cross_ranks</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_ranks_at_local</span><span class="p">[</span><span class="n">host_info</span><span class="o">.</span><span class="n">hostname</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cross_ranks_at_local</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">host_ranks</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">host_info</span><span class="p">,</span> <span class="n">ranks</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span> <span class="o">=</span> <span class="n">rank</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 给出一个 horovod rank 和 slot 的对应关系。返回一个alloc_list，每个SlotInfo包括各种rank信息</span>
</span></span><span class="line"><span class="cl">    <span class="n">alloc_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">host_info</span><span class="p">,</span> <span class="n">ranks</span> <span class="ow">in</span> <span class="n">host_ranks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">local_rank</span><span class="p">,</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ranks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_ranks_at_local</span> <span class="o">=</span> <span class="n">cross_ranks</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_rank</span> <span class="o">=</span> <span class="n">cross_ranks_at_local</span><span class="p">[</span><span class="n">host_info</span><span class="o">.</span><span class="n">hostname</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cross_ranks_at_local</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">alloc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">SlotInfo</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">hostname</span><span class="o">=</span><span class="n">host_info</span><span class="o">.</span><span class="n">hostname</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">local_rank</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">cross_rank</span><span class="o">=</span><span class="n">cross_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">local_size</span><span class="o">=</span><span class="n">local_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">cross_size</span><span class="o">=</span><span class="n">cross_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">alloc_list</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="452-得到运行命令">4.5.2 得到运行命令</h4>
<p>get_run_command 是从环境变量中得到 Gloo 的变量，然后加到 command 之上。此步完成之后，得到类似如下命令：</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>可以把这个格式缩写为：{horovod_gloo_env} command。</p>
<p>代码为：</p>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_run_env_vars</span><span class="p">(</span><span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">elastic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 从环境变量中得到 Gloo 的变量</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_envs</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_GLOO_RENDEZVOUS_ADDR&#39;</span><span class="p">:</span> <span class="n">server_ip</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_GLOO_RENDEZVOUS_PORT&#39;</span><span class="p">:</span> <span class="n">port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_CONTROLLER&#39;</span><span class="p">:</span> <span class="s2">&#34;gloo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_CPU_OPERATIONS&#39;</span><span class="p">:</span> <span class="s2">&#34;gloo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_GLOO_IFACE&#39;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">nics</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>   <span class="c1"># TODO: add multiple ifaces in future</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;NCCL_SOCKET_IFNAME&#39;</span><span class="p">:</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nics</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">elastic</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">run_envs</span><span class="p">[</span><span class="s2">&#34;HOROVOD_ELASTIC&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;1&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">run_envs</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_run_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">elastic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">env_vars</span> <span class="o">=</span> <span class="n">create_run_env_vars</span><span class="p">(</span><span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">elastic</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">env_string</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">env_vars</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_command</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{env_string}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{command}</span><span class="s1">&#39;</span>  <span class="c1"># expect a lot of environment variables</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_string</span><span class="o">=</span><span class="n">env_string</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">command</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">quote</span><span class="p">(</span><span class="n">par</span><span class="p">)</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">command</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">run_command</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="453-得到slot运行命令">4.5.3 得到slot运行命令</h4>
<p>得到运行命令之后，这里会结合 horovod env 和 env，以及slot 分配情况 进一步修改为适合 gloo 运行的方式。就是可以在具体每一个slot上运行的命令。</p>
<p>可以把这个格式缩写为：{horovod_gloo_env} {horovod_rendez_env} {env} run_command。</p>
<p>此步完成之后，得到类似如下：</p>
<div class="highlight" id="id-26"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">HOROVOD_HOSTNAME</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_RANK</span><span class="o">=</span><span class="m">1</span> <span class="nv">HOROVOD_SIZE</span><span class="o">=</span><span class="m">2</span> <span class="nv">HOROVOD_LOCAL_RANK</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SHELL</span><span class="o">=</span>/bin/bash <span class="nv">PATH</span><span class="o">=</span>XXXX <span class="nv">USER</span><span class="o">=</span>xxx <span class="nv">PWD</span><span class="o">=</span>xxx <span class="nv">SSH_CONNECTION</span><span class="o">=</span><span class="s2">&#34;1.1.1.1 11 2.2.2.2 22&#34;</span> <span class="nv">HOME</span><span class="o">=</span>xxx <span class="nv">SSH_CLIENZT</span><span class="o">=</span>xxxx
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>lo
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>具体代码如下：</p>
<div class="highlight" id="id-27"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_slot_info_to_command_fn</span><span class="p">(</span><span class="n">run_command</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Workaround for over-buffered outputs. Investigate how mpirun avoids this problem.</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>  <span class="c1"># copy env so we do not leak env modifications</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span><span class="p">[</span><span class="s1">&#39;PYTHONUNBUFFERED&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">slot_info_to_command</span><span class="p">(</span><span class="n">slot_info</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Given a slot_info, creates a command used by gloo to launch a single job.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param slot_info: host and slot to execute the run command on
</span></span></span><span class="line"><span class="cl"><span class="s2">        :return:
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">env_vars</span> <span class="o">=</span> <span class="n">create_slot_env_vars</span><span class="p">(</span><span class="n">slot_info</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_rendez_env</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">env_vars</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{horovod_env}</span><span class="s1"> </span><span class="si">{env}</span><span class="s1"> </span><span class="si">{run_command}</span><span class="s1">&#39;</span> <span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">horovod_env</span><span class="o">=</span><span class="n">horovod_rendez_env</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">quote</span><span class="p">(</span><span class="n">value</span><span class="p">))</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                          <span class="k">if</span> <span class="n">env_util</span><span class="o">.</span><span class="n">is_exportable</span><span class="p">(</span><span class="n">key</span><span class="p">)]),</span>
</span></span><span class="line"><span class="cl">            <span class="n">run_command</span><span class="o">=</span><span class="n">run_command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">slot_info_to_command</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="454-多线程调用命令">4.5.4 多线程调用命令</h4>
<p>这就是启动了多线程进行调用。gloo_run 的注释说的很清楚：在调用 execute_function_multithreaded 时，每一个thread将使用 ssh 命令在远程host之上启动训练job。</p>
<p>回忆下之前我们在“构建可执行环境” 中提到：利用 get_remote_command 来生成相关远程可运行环境，比如在训练脚本前面加上 &lsquo;ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no&rsquo;。大家就理解了如何在远端执行。</p>
<p>在本地运行，则命令大致为：</p>
<div class="highlight" id="id-28"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> /code directory &gt; /dev/null <span class="m">2</span> &gt;<span class="p">&amp;</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_HOSTNAME</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_RANK</span><span class="o">=</span><span class="m">1</span> <span class="nv">HOROVOD_SIZE</span><span class="o">=</span><span class="m">2</span> <span class="nv">HOROVOD_LOCAL_RANK</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SHELL</span><span class="o">=</span>/bin/bash <span class="nv">PATH</span><span class="o">=</span>XXXX <span class="nv">USER</span><span class="o">=</span>xxx <span class="nv">PWD</span><span class="o">=</span>xxx <span class="nv">SSH_CONNECTION</span><span class="o">=</span><span class="s2">&#34;1.1.1.1 11 2.2.2.2 22&#34;</span> <span class="nv">HOME</span><span class="o">=</span>xxx <span class="nv">SSH_CLIENZT</span><span class="o">=</span>xxxx
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>lo
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>在远端运行，命令就需要加上 ssh 信息，大致为：</p>
<div class="highlight" id="id-29"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ssh -o <span class="nv">PasswordAuthentication</span><span class="o">=</span>no -o <span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no 1.1.1.1
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /code directory &gt; /dev/null <span class="m">2</span> &gt;<span class="p">&amp;</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_HOSTNAME</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_RANK</span><span class="o">=</span><span class="m">1</span> <span class="nv">HOROVOD_SIZE</span><span class="o">=</span><span class="m">2</span> <span class="nv">HOROVOD_LOCAL_RANK</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SHELL</span><span class="o">=</span>/bin/bash <span class="nv">PATH</span><span class="o">=</span>XXXX <span class="nv">USER</span><span class="o">=</span>xxx <span class="nv">PWD</span><span class="o">=</span>xxx <span class="nv">SSH_CONNECTION</span><span class="o">=</span><span class="s2">&#34;1.1.1.1 11 2.2.2.2 22&#34;</span> <span class="nv">HOME</span><span class="o">=</span>xxx <span class="nv">SSH_CLIENZT</span><span class="o">=</span>xxxx
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>lo
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>execute_function_multithreaded 具体代码如下，其中：</p>
<ul>
<li><code>fn</code> 就是前面提到的程序运行环境（能力）<code>exec_command</code>。</li>
<li><code>fn(*arg[:-1])</code> 就是在 <code>exec_command</code> 之中运行<code>slot_info_to_command</code>。</li>
</ul>
<div class="highlight" id="id-30"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">execute_function_multithreaded</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">args_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">block_until_all_done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">max_concurrent_executions</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Executes fn in multiple threads each with one set of the args in the
</span></span></span><span class="line"><span class="cl"><span class="s2">    args_list.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param fn: function to be executed
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type fn:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param args_list:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type args_list: list(list)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param block_until_all_done: if is True, function will block until all the
</span></span></span><span class="line"><span class="cl"><span class="s2">    threads are done and will return the results of each thread&#39;s execution.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type block_until_all_done: bool
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param max_concurrent_executions:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type max_concurrent_executions: int
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return:
</span></span></span><span class="line"><span class="cl"><span class="s2">    If block_until_all_done is False, returns None. If block_until_all_done is
</span></span></span><span class="line"><span class="cl"><span class="s2">    True, function returns the dict of results.
</span></span></span><span class="line"><span class="cl"><span class="s2">        {
</span></span></span><span class="line"><span class="cl"><span class="s2">            index: execution result of fn with args_list[index]
</span></span></span><span class="line"><span class="cl"><span class="s2">        }
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype: dict
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">result_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">worker_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">arg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">worker_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fn_execute</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">arg</span> <span class="o">=</span> <span class="n">worker_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span>
</span></span><span class="line"><span class="cl">            <span class="n">exec_index</span> <span class="o">=</span> <span class="n">arg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># fn 就是前面提到的程序运行环境（能力）exec_command</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># fn(*arg[:-1])是在 exec_command 之中运行 slot_info_to_command</span>
</span></span><span class="line"><span class="cl">            <span class="n">res</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">arg</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">result_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">exec_index</span><span class="p">,</span> <span class="n">res</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">number_of_threads</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_concurrent_executions</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">args_list</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 在多线程中执行 fn_execute</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_threads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">fn_execute</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="ow">not</span> <span class="n">block_until_all_done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Returns the results only if block_until_all_done is set.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果有设置，则 block 等待</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">block_until_all_done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Because join() cannot be interrupted by signal, a single join()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># needs to be separated into join()s with timeout in a while loop.</span>
</span></span><span class="line"><span class="cl">        <span class="n">have_alive_child</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">have_alive_child</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">have_alive_child</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">have_alive_child</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="ow">not</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">item</span> <span class="o">=</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span><span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">results</span></span></span></code></pre></td></tr></table>
</div>
</div><p>python train.py 就会进入到我们的训练代码。</p>
<p>大致逻辑如下图，可以看到，结合了各种信息之后，构建了一个可以执行的结果，然后多host执行：</p>
<ul>
<li>图左面，是从 参数中获取 host 等信息，然后解析出 slot 信息；</li>
<li>图右边，是从 python train.py 这个待运行的命令，基于各种配置来生成可以执行命令环境。如果是远程，就得生成 相关远程可运行命令环境（包括切换目录，远程执行等等）；</li>
<li>图中间，是从 python train.py 这个待运行的命令，经过添加 env 信息，gloo 信息。然后结合 左面的 slot 信息 和 右面 的可以执行命令环境 之后，得到了可以在多线程上运行，从而在 多slot 运行的命令。</li>
</ul>
<div class="highlight" id="id-31"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">args</span> <span class="o">:</span> <span class="err">&#39;</span><span class="mf">10.11.11.11</span><span class="o">:</span><span class="mi">4</span><span class="o">,</span><span class="mf">10.11.11.12</span><span class="o">:</span><span class="mi">4</span><span class="err">&#39;</span>            <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>                  <span class="n">command</span>  <span class="o">:</span>  <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                 <span class="o">+</span>                                     <span class="o">+</span>                                     <span class="o">+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="n">v</span>                                     <span class="n">v</span>                                     <span class="n">v</span>
</span></span><span class="line"><span class="cl">      <span class="o">+----------+--------+</span>                 <span class="o">+----------+----------+</span>                <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>    <span class="n">parse_hosts</span>    <span class="o">|</span>                 <span class="o">|</span>   <span class="n">get_run_command</span>   <span class="o">|</span>                <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">+----------+--------+</span>                 <span class="o">|</span>                     <span class="o">|</span>                <span class="o">|</span>  <span class="n">get_remote_command</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                          <span class="o">+----------+----------+</span>                <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                           <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">                 <span class="n">v</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">    <span class="o">+------------+-----------+</span>                         <span class="n">v</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">    <span class="o">|</span>  <span class="n">get_host_assignments</span>  <span class="o">|</span>                                                               <span class="n">v</span>
</span></span><span class="line"><span class="cl">    <span class="o">|</span>                        <span class="o">|</span>               <span class="n">gloo</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">    <span class="o">+------------+-----------+</span>                         <span class="o">+</span>                          <span class="n">ssh</span> <span class="o">-</span><span class="n">o</span> <span class="o">...</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="n">v</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">    <span class="n">SlotInfo</span><span class="o">(</span><span class="n">hostname</span><span class="o">=</span><span class="err">&#39;</span><span class="n">h2</span><span class="err">&#39;</span><span class="o">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="o">)</span>                    <span class="n">v</span>                                     <span class="n">v</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+</span>                         <span class="o">+-----------+---------------+</span>           <span class="o">+---------+--------------+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                         <span class="o">|</span> <span class="n">_slot_info_to_command_fn</span>  <span class="o">|</span>           <span class="o">|</span><span class="n">safe_shell_exec</span><span class="o">.</span><span class="na">execute</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+-----------------------&gt;</span> <span class="o">|</span>                           <span class="o">|</span>           <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                           <span class="o">+-----------+---------------+</span>           <span class="o">+---------+--------------+</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="n">v</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                          <span class="n">HOROVOD_CONTROLLER</span><span class="o">=</span><span class="n">gloo</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">+</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="n">v</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">+-------------+-------------------+</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">|</span>                                 <span class="o">|</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">|</span> <span class="n">execute_function_multithreaded</span>  <span class="o">|</span> <span class="o">&lt;---------------+</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">|</span>                                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">+---------------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>图示如下：</p>
<p></p>
<h3 id="46-c举例">4.6 C++举例</h3>
<p>我们给出一个底层代码，大家就进一步了解 Gloo 可以起到什么作用。</p>
<p>这个就是 Horovod 之中，rank 0 最终给其他 rank 发送构建好的 Tensor。</p>
<div class="highlight" id="id-32"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GlooController</span><span class="o">::</span><span class="n">SendFinalTensors</span><span class="p">(</span><span class="n">ResponseList</span><span class="o">&amp;</span> <span class="n">response_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Notify all nodes which tensors we&#39;d like to reduce at this step.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">encoded_response</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">ResponseList</span><span class="o">::</span><span class="n">SerializeToString</span><span class="p">(</span><span class="n">response_list</span><span class="p">,</span> <span class="n">encoded_response</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Boardcast the response length
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int</span> <span class="n">encoded_response_length</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">encoded_response</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">BroadcastOptions</span> <span class="n">opts</span><span class="p">(</span><span class="n">gloo_context_</span><span class="p">.</span><span class="n">ctx</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setOutput</span><span class="p">(</span><span class="o">&amp;</span><span class="n">encoded_response_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setRoot</span><span class="p">(</span><span class="n">RANK_ZERO</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">broadcast</span><span class="p">(</span><span class="n">opts</span><span class="p">);</span> <span class="c1">// 广播给其他rank
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Boardcast the response
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">BroadcastOptions</span> <span class="n">opts</span><span class="p">(</span><span class="n">gloo_context_</span><span class="p">.</span><span class="n">ctx</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setOutput</span><span class="p">((</span><span class="kt">uint8_t</span><span class="o">*</span><span class="p">)(</span><span class="n">encoded_response</span><span class="p">.</span><span class="n">c_str</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                   <span class="n">encoded_response_length</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setRoot</span><span class="p">(</span><span class="n">RANK_ZERO</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">broadcast</span><span class="p">(</span><span class="n">opts</span><span class="p">);</span> <span class="c1">// 广播给其他rank
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-mpi-实现">5 Mpi 实现</h2>
<h3 id="51-openmpi-库">5.1 openmpi 库</h3>
<p>horovod 这里主要依赖 openmpi。</p>
<ul>
<li>MPI：英文全称是Message Passing Interface，MPI是一个跨语言的通讯协议，用于编写并行计算机。支持点对点和广播。MPI是一个信息传递应用程序接口，包括协议和和语义说明，他们指明其如何在各种实现中发挥其特性。MPI的目标是高性能，大规模性，和可移植性。</li>
<li>openMPI：英文全称是open Message Passing Interface。openMPI是MPI的一种实现，一种库项目。</li>
</ul>
<p>MPI在Hovorod的角色比较特殊：</p>
<ul>
<li>
<p>一方面Horovod内集成了<strong>基于MPI的AllReduce</strong>，类似于NCCL，都是用作梯度规约；</p>
</li>
<li>
<p>另一方面，MPI可以用来在所有机器上<strong>启动多个进程(Hovorod里用Rank表示)，实现并行计算</strong>；</p>
</li>
</ul>
<h3 id="52-mpi_run-函数">5.2 mpi_run 函数</h3>
<p>此部分代码位于：horovod/runner/mpi_run.py。</p>
<p>首先摘录其关键代码如下，可以看出来其核心是运行 mpirun 命令。</p>
<div class="highlight" id="id-33"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 我是下面大段代码中的关键代码！</span>
</span></span><span class="line"><span class="cl"><span class="n">mpirun_command</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;mpirun </span><span class="si">{basic_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;-np </span><span class="si">{num_proc}{ppn_arg}{hosts_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{binding_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_ssh_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{tcp_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{nccl_socket_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{output_filename_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{env}</span><span class="s1"> </span><span class="si">{extra_mpi_args}</span><span class="s1"> </span><span class="si">{command}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">basic_args</span><span class="o">=</span><span class="n">basic_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_proc</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">ppn_arg</span><span class="o">=</span><span class="n">ppn_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">hosts_arg</span><span class="o">=</span><span class="n">hosts_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">binding_args</span><span class="o">=</span><span class="n">binding_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_args</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mpi_impl_flags</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">tcp_intf_arg</span><span class="o">=</span><span class="n">tcp_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">nccl_socket_intf_arg</span><span class="o">=</span><span class="n">nccl_socket_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_ssh_args</span><span class="o">=</span><span class="n">mpi_ssh_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">output_filename_arg</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">env</span><span class="o">=</span><span class="n">env_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_mpi_args</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">command</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">quote</span><span class="p">(</span><span class="n">par</span><span class="p">)</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">command</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Execute the mpirun command.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">run_func_mode</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">mpirun_command</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">stderr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">execve</span><span class="p">(</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="s1">&#39;-c&#39;</span><span class="p">,</span> <span class="n">mpirun_command</span><span class="p">],</span> <span class="n">env</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>就是依据各种配置以及参数来构建 mpirun 命令的所有参数，比如 ssh 的参数，mpi 参数，nccl 参数等等。</p>
<p>最后得到的 mpirun 命令举例如下：</p>
<div class="highlight" id="id-34"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">mpirun --allow-run-as-root --np <span class="m">2</span> -bind-to none -map-by slot <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    -x <span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO -x LD_LIBRARY_PATH -x PATH <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    -mca pml ob1 -mca btl ^openib <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>具体代码如下，具体是：</p>
<div class="highlight" id="id-35"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 上面代码是我之中的片段</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mpi_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">command</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Runs mpi_run.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        settings: Settings for running MPI.
</span></span></span><span class="line"><span class="cl"><span class="s2">                  Note: settings.num_proc and settings.hosts must not be None.
</span></span></span><span class="line"><span class="cl"><span class="s2">        nics: Interfaces to include by MPI.
</span></span></span><span class="line"><span class="cl"><span class="s2">        env: Environment dictionary to use for running command.
</span></span></span><span class="line"><span class="cl"><span class="s2">        command: Command and arguments to run as a list of string.
</span></span></span><span class="line"><span class="cl"><span class="s2">        stdout: Stdout of the mpi process.
</span></span></span><span class="line"><span class="cl"><span class="s2">                Only used when settings.run_func_mode is True.
</span></span></span><span class="line"><span class="cl"><span class="s2">        stderr: Stderr of the mpi process.
</span></span></span><span class="line"><span class="cl"><span class="s2">                Only used when settings.run_func_mode is True.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 得到各种配置</span>
</span></span><span class="line"><span class="cl">    <span class="n">mpi_impl_flags</span><span class="p">,</span> <span class="n">impl_binding_args</span><span class="p">,</span> <span class="n">mpi</span> <span class="o">=</span> <span class="n">_get_mpi_implementation_flags</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">tcp_flag</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">impi</span> <span class="o">=</span> <span class="n">_IMPI_IMPL</span> <span class="o">==</span> <span class="n">mpi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理ssh参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">ssh_args</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ssh_args</span> <span class="o">+=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;-p </span><span class="si">{</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ssh_args</span> <span class="o">+=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;-i </span><span class="si">{</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mpi_ssh_args</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">ssh_args</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">joined_ssh_args</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ssh_args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_ssh_args</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-bootstrap=ssh -bootstrap-exec-args </span><span class="se">\&#34;</span><span class="si">{</span><span class="n">joined_ssh_args</span><span class="si">}</span><span class="se">\&#34;</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="sa">f</span><span class="s1">&#39;-mca plm_rsh_args </span><span class="se">\&#34;</span><span class="si">{</span><span class="n">joined_ssh_args</span><span class="si">}</span><span class="se">\&#34;</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理网络配置，网卡信息等</span>
</span></span><span class="line"><span class="cl">    <span class="n">tcp_intf_arg</span> <span class="o">=</span> <span class="s1">&#39;-mca btl_tcp_if_include </span><span class="si">{nics}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nics</span><span class="p">))</span> <span class="k">if</span> <span class="n">nics</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">nccl_socket_intf_arg</span> <span class="o">=</span> <span class="s1">&#39;-</span><span class="si">{opt}</span><span class="s1"> NCCL_SOCKET_IFNAME=</span><span class="si">{nics}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">opt</span><span class="o">=</span><span class="s1">&#39;genv&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nics</span><span class="p">))</span> <span class="k">if</span> <span class="n">nics</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理host信息</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># On large cluster runs (e.g. Summit), we need extra settings to work around OpenMPI issues</span>
</span></span><span class="line"><span class="cl">    <span class="n">host_names</span><span class="p">,</span> <span class="n">host_to_slots</span> <span class="o">=</span> <span class="n">hosts</span><span class="o">.</span><span class="n">parse_hosts_and_slots</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">impi</span> <span class="ow">and</span> <span class="n">host_names</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">host_names</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">_LARGE_CLUSTER_THRESHOLD</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_impl_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;-mca plm_rsh_no_tree_spawn true&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_impl_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;-mca plm_rsh_num_concurrent </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">host_names</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># if user does not specify any hosts, mpirun by default uses local host.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># There is no need to specify localhost.</span>
</span></span><span class="line"><span class="cl">    <span class="n">hosts_arg</span> <span class="o">=</span> <span class="s1">&#39;-</span><span class="si">{opt}</span><span class="s1"> </span><span class="si">{hosts}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s1">&#39;hosts&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">hosts</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">host_names</span><span class="p">)</span> <span class="k">if</span> <span class="n">host_names</span> <span class="ow">and</span> <span class="n">impi</span> <span class="k">else</span> <span class="n">settings</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理ppn配置</span>
</span></span><span class="line"><span class="cl">    <span class="n">ppn_arg</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">host_to_slots</span> <span class="ow">and</span> <span class="n">impi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ppn</span> <span class="o">=</span> <span class="n">host_to_slots</span><span class="p">[</span><span class="n">host_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">h_name</span> <span class="ow">in</span> <span class="n">host_names</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ppn_arg</span> <span class="o">=</span> <span class="s1">&#39; -ppn </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ppn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理超时配置</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">prefix_output_with_timestamp</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">impi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_impl_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;--timestamp-output&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">binding_args</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">binding_args</span> <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">binding_args</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">impl_binding_args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 配置需要root身份运行</span>
</span></span><span class="line"><span class="cl">    <span class="n">basic_args</span> <span class="o">=</span> <span class="s1">&#39;-l&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;--allow-run-as-root --tag-output&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;-outfile-pattern&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;--output-filename&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 构建环境信息列表</span>
</span></span><span class="line"><span class="cl">    <span class="n">env_list</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;-x </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">env_util</span><span class="o">.</span><span class="n">is_exportable</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 构建最终的 MPI 命令</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Pass all the env variables to the mpirun command.</span>
</span></span><span class="line"><span class="cl">    <span class="n">mpirun_command</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;mpirun </span><span class="si">{basic_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;-np </span><span class="si">{num_proc}{ppn_arg}{hosts_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{binding_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_ssh_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{tcp_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{nccl_socket_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{output_filename_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{env}</span><span class="s1"> </span><span class="si">{extra_mpi_args}</span><span class="s1"> </span><span class="si">{command}</span><span class="s1">&#39;</span>  <span class="c1"># expect a lot of environment variables</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">basic_args</span><span class="o">=</span><span class="n">basic_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_proc</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">ppn_arg</span><span class="o">=</span><span class="n">ppn_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">hosts_arg</span><span class="o">=</span><span class="n">hosts_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">binding_args</span><span class="o">=</span><span class="n">binding_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_args</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mpi_impl_flags</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">tcp_intf_arg</span><span class="o">=</span><span class="n">tcp_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">nccl_socket_intf_arg</span><span class="o">=</span><span class="n">nccl_socket_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_ssh_args</span><span class="o">=</span><span class="n">mpi_ssh_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">output_filename_arg</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">env</span><span class="o">=</span><span class="n">env_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_mpi_args</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">command</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">quote</span><span class="p">(</span><span class="n">par</span><span class="p">)</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">command</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># we need the driver&#39;s PATH and PYTHONPATH in env to run mpirun,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># env for mpirun is different to env encoded in mpirun_command</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">,</span> <span class="s1">&#39;PYTHONPATH&#39;</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">var</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">env</span> <span class="ow">and</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># copy env so we do not leak env modifications</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># copy var over from os.environ</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">var</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Execute the mpirun command.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">run_func_mode</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">mpirun_command</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">stderr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">execve</span><span class="p">(</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="s1">&#39;-c&#39;</span><span class="p">,</span> <span class="n">mpirun_command</span><span class="p">],</span> <span class="n">env</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="53-mpirun命令">5.3 mpirun命令</h3>
<p>因为 mpi_run 使用的是 mpirun 命令来运行，所以我们介绍一下。</p>
<p>mpirun是MPI程序的启动脚本，它简化了并行进程的启动过程，尽可能屏蔽了底层的实现细节，从而为用户提供了一个通用的MPI并行机制。</p>
<p>在用mpirun命令执行并行程序时，参数-np指明了需要并行运行的进程个数。mpirun首先在本地结点上启动一个进程，然后根据/usr/local/share/machines.LINUX文件中所列出的主机，为每个主机启动一个进程。若进程数比可用的并行节点数多，则多余的进程将重新按照上述规则进行。按这个机制分配好进程后，一般会给每个节点分一个固定的标号，类似于身份证了，后续在消息传递中会用到。</p>
<p>这里需要说明的是，实际运行的</p>
<p>orterun(Open MPI SPMD / MPMD启动器; mpirun / mpiexec只是它的符号链接)</p>
<p>命令举例如下：</p>
<div class="highlight" id="id-36"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="mi">4</span> \
</span></span><span class="line"><span class="cl">    <span class="o">-</span><span class="n">bind</span><span class="o">-</span><span class="n">to</span> <span class="n">none</span> <span class="o">-</span><span class="nb">map</span><span class="o">-</span><span class="n">by</span> <span class="n">slot</span> \
</span></span><span class="line"><span class="cl">    <span class="o">-</span><span class="n">x</span> <span class="n">NCCL_DEBUG</span><span class="o">=</span><span class="n">INFO</span> <span class="o">-</span><span class="n">x</span> <span class="n">LD_LIBRARY_PATH</span> <span class="o">-</span><span class="n">x</span> <span class="n">PATH</span> \
</span></span><span class="line"><span class="cl">    <span class="o">-</span><span class="n">mca</span> <span class="n">pml</span> <span class="n">ob1</span> <span class="o">-</span><span class="n">mca</span> <span class="n">btl</span> <span class="o">^</span><span class="n">openib</span> \
</span></span><span class="line"><span class="cl">    <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="6-总结">6 总结</h2>
<p>对比 gloo 和 mpi 的实现，我们还是能看出来区别。</p>
<h3 id="61-gloo">6.1 gloo</h3>
<p>gloo 只是一个库，需要 horovod 来完成命令分发功能。</p>
<p>gloo 需要 horovod 自己实现本地运行和远端运行方式，即 get_remote_command 函数 实现 <code>'ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no'</code>。</p>
<p>gloo 需要实现 RendezvousServer，底层会利用 RendezvousServer 进行通讯。</p>
<h3 id="62-mpi">6.2 mpi</h3>
<p>mpi 则功能强大很多，只要把命令配置成被 mpirun 包装，openmpi 就可以自行完成命令分发执行。说到底，horovod 是一个 mpirun 程序，即使运行了 tensor flow，也是一个mpi程序，可以互相交互。</p>
<p>references:
[1]. <a href="https://www.cnblogs.com/rossiXYZ/p/14881812.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14881812.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入</title><link>https://lruihao.cn/posts/2022-10-08_horovod_2/</link><pubDate>Mon, 10 Jul 2023 07:53:40 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/2022-10-08_horovod_2/</guid><description><![CDATA[<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。</p>
<p>前一篇参见如下：</p>
<p><a href="http://localhost:1313/posts/notes/2022-10-08_horovod_1/"target="_blank" rel="external nofollow noopener noreferrer">深度学习分布式训练框架 Horovod[1] &ndash; 基础知识<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="1-horovod-简介">1 Horovod 简介</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，支持TensorFlow，Keras，PyTorch和MXNet。Horovod 的名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与分布式 TensorFlow 流程使用 Horovod 互相通信的场景很像。</p>
<p>因为各个机器学习框架对于底层集合通信库（ nccl，openmpi，gloo 等等）的利用水平可能各不相同，使得他们无法充分利用这些底层集合通信库的威力。因而，hovorod 就整合这些框架，提供一个易用高效的解决方案。</p>
<p>Uber的工程师就是根据FaceBook的一篇paper：“<a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf"target="_blank" rel="external nofollow noopener noreferrer">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>”和百度的一篇“<a href="https://research.baidu.com/bringing-hpc-techniques-deep-learning/"target="_blank" rel="external nofollow noopener noreferrer">Bringing HPC Techniques to Deep Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>” 改进并发布了开源框架Horovod。</p>
<p>Horovod 相比于百度的工作，并无学术上的贡献。但是 Horovod 扎实的工程实现，使得它受到了更多的关注。它最大的优势在于对 RingAllReduce 进行了更高层次的抽象，使其支持多种不同的框架。同时引入了 Nvidia NCCL，对 GPU 更加友好。</p>
<p>Horovod依赖于Nvidia的 NCCL2 做 All Reduce，依赖于MPI做进程间通信，简化了同步多 GPU 或多节点分布式训练的开发流程。由于使用了NCCL2，Horovod也可以利用以下功能：NVLINK，RDMA，GPUDirectRDMA，自动检测通信拓扑，能够回退到 PCIe 和 TCP/IP 通信。</p>
<p>我们需要几个问题来引导分析：</p>
<ul>
<li>Hovorod 怎么进行数据分割？</li>
<li>Hovorod 怎么进行训练代码分发？</li>
<li>Hovorod 启动时候，python 和 C++ 都做了什么？</li>
<li>如何确保 Hovorod 启动时候步骤一致；</li>
</ul>
<h2 id="2-hovorod-机制概述">2 Hovorod 机制概述</h2>
<h3 id="21-horovod-机制">2.1 Horovod 机制</h3>
<p>Horovod使用<strong>数据并行化</strong>策略在GPU上分配训练。</p>
<p>在数据并行化中，作业中的每个GPU都会接收其自己的数据批处理的独立切片，即它的“批处理切片”。 每个GPU都使用自己分配到的数据来独立计算，进行梯度更新。</p>
<p>假如使用两个GPU，批处理大小为32，则第一个GPU将处理前16条记录的正向传播和向后传播，以及第二个GPU处理后16条记录的正向传播和向后传播。然后，这些梯度更新将在GPU之间平均在一起，最后应用于模型。</p>
<p>每一个迭代的操作方法如下：</p>
<ol>
<li>
<p>每个 worker 将维护自己的模型权重副本和自己的数据集副本。</p>
</li>
<li>
<p>收到执行信号后，每个工作进程都会从数据集中提取一个不相交的批次，并计算该批次的梯度。</p>
</li>
<li>
<p>Workers 使用ring all-reduce算法来同步彼此的梯度，从而在本地所有节点上计算同样的平均梯度。</p>
<ol>
<li>
<p>将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片，后续每一次通信都将给下一个邻居发送一个自己的分片（同时从上一个邻居接受一个新分片）。</p>
</li>
<li>
<p>ScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和，即每个 device 将有一个块，其中包含所有device 中该块中所有值的总和；具体如下：</p>
</li>
</ol>
<p></p>
<ol start="3">
<li>AllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和 广播到其他 device；最终所有节点都拥有所有tensor分片和。具体如下：
</li>
<li>在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度；</li>
</ol>
</li>
<li>
<p>每个 worker 将 梯度更新 应用于其模型的本地副本。</p>
</li>
<li>
<p>执行下一个batch。</p>
</li>
</ol>
<h2 id="3-示例代码">3 示例代码</h2>
<h3 id="31--摘要代码">3.1  摘要代码</h3>
<p>我们此处给出官网示例代码部分摘要，具体分析参见下面代码中的注释。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: initialize Horovod.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="c1"># 初始化 Horovod，启动相关线程和MPI线程</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 依据 local rank 为不同的进程分配不同的GPU</span>
</span></span><span class="line"><span class="cl"><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">mnist_images</span><span class="p">,</span> <span class="n">mnist_labels</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;mnist-</span><span class="si">%d</span><span class="s1">.npz&#39;</span> <span class="o">%</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 切分数据</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_images</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="o">......</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: adjust learning rate based on number of GPUs.</span>
</span></span><span class="line"><span class="cl"><span class="n">scaled_lr</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1"># 根据Worker的数量增加学习率的大小</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">scaled_lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 把常规TensorFlow Optimizer通过Horovod包装起来，进而使用 ring-allreduce 来得到平均梯度</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
</span></span><span class="line"><span class="cl"><span class="c1"># uses hvd.DistributedOptimizer() to compute gradients.</span>
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">experimental_run_tf_function</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 广播初始化，将模型的参数从第一个设备传向其他设备，以保证初始化模型参数的一致性</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">MetricAverageCallback</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateWarmupCallback</span><span class="p">(</span><span class="n">initial_lr</span><span class="o">=</span><span class="n">scaled_lr</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them. # 只有设备0需要保存模型参数作为checkpoint</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;./checkpoint-</span><span class="si">{epoch}</span><span class="s1">.h5&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: write logs on worker 0.</span>
</span></span><span class="line"><span class="cl"><span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the model.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: adjust number of steps based on number of GPUs.</span>
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">500</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-horovodrun">3.2 horovodrun</h3>
<p>Horovod训练脚本未作为Python脚本启动。 例如，您不能使用<code>python train.py</code>运行此脚本。 需要采用特殊的CLI命令 <code>horovodrun</code> 来启动（训练代码 train.py 需要手动拷贝到各个节点上，且目录相同）：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ horovodrun -np 4 -H localhost:4 python train.py</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="4-运行逻辑">4 运行逻辑</h2>
<p>我们按照顺序梳理，看看在程序初始化过程背后都做了什么。</p>
<h3 id="41-引入python文件">4.1 引入python文件</h3>
<p>如下代码会引入各种相关python文件。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42--初始化-in-python">4.2  初始化 in python</h3>
<p>python 世界的初始化位于 <code>horovod-master/horovod/mxnet/mpi_ops.py</code></p>
<h4 id="421-引入so库">4.2.1 引入SO库</h4>
<h5 id="4211-so库">4.2.1.1 SO库</h5>
<p><code>horovod/tensorflow/mpi_ops.py</code> 之中会引入SO库。
比如 <code>dist-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so</code>。</p>
<p>SO库 就是 horovod 中 C++ 代码编译出来的结果。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_load_library</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Loads a .so file containing the specified operators.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">filename</span> <span class="o">=</span> <span class="n">resource_loader</span><span class="o">.</span><span class="n">get_path_to_datafile</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">library</span> <span class="o">=</span> <span class="n">load_library</span><span class="o">.</span><span class="n">load_op_library</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">library</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check possible symbol not found error from tensorflow version mismatch</span>
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">MPI_LIB</span> <span class="o">=</span> <span class="n">_load_library</span><span class="p">(</span><span class="s1">&#39;mpi_lib&#39;</span> <span class="o">+</span> <span class="n">get_ext_suffix</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">check_installed_version</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">check_installed_version</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h5 id="4222-so作用">4.2.2.2 SO作用</h5>
<p>引入库的作用是获取到 C++ 的函数，并且用 python 封装一下，这样就可以在 python 世界使用 C++代码了。</p>
<p>由下文可以看出来，python 的 _allreduce 函数就会把功能转发给 C++，由 <code>MPI_LIB.horovod_allreduce</code> 完成。</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">Sum</span><span class="p">,</span> <span class="n">prescale_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">postscale_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">               <span class="n">ignore_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;HorovodAllreduce_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">_normalize_name</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">MPI_LIB</span><span class="o">.</span><span class="n">horovod_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">reduce_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">prescale_factor</span><span class="o">=</span><span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">postscale_factor</span><span class="o">=</span><span class="n">postscale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ignore_name_scope</span><span class="o">=</span><span class="n">ignore_name_scope</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="422-初始化配置">4.2.2 初始化配置</h4>
<p>我们摘录了主要部分，就是初始化 _HorovodBasics，然后从 _HorovodBasics 内获取各种函数，变量和配置，比如是否编译了mpi，gloo等等.</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">horovod.common.basics</span> <span class="kn">import</span> <span class="n">HorovodBasics</span> <span class="k">as</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">_basics</span> <span class="o">=</span> <span class="n">_HorovodBasics</span><span class="p">(</span><span class="vm">__file__</span><span class="p">,</span> <span class="s1">&#39;mpi_lib&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># import basic methods</span>
</span></span><span class="line"><span class="cl"><span class="n">init</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">init</span>
</span></span><span class="line"><span class="cl"><span class="n">size</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl"><span class="n">local_size</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">local_size</span>
</span></span><span class="line"><span class="cl"><span class="n">rank</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">rank</span>
</span></span><span class="line"><span class="cl"><span class="n">local_rank</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">local_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">mpi_built</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">mpi_built</span>
</span></span><span class="line"><span class="cl"><span class="n">gloo_enabled</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">gloo_enabled</span>
</span></span><span class="line"><span class="cl"><span class="o">......</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-hvdinit-初始化">4.2.3 hvd.init() 初始化</h4>
<p>首先需要用 <code>hvd.init()</code> 来初始化，horovod 管理的所有状态都会传到 hvd 对象中。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: initialize Horovod.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处调用的是 HorovodBasics 中的函数，我们看看做了什么。</p>
<p>可以看到，这部分会一直深入到 C++世界，调用了大量的 MPI_LIB_CTYPES 函数，所以我们接下来就要进入到 C++的世界看看。</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">comm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;A function that initializes Horovod.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_built</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_mpi_built</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">MPI</span><span class="o">.</span><span class="n">_sizeof</span><span class="p">(</span><span class="n">MPI</span><span class="o">.</span><span class="n">Comm</span><span class="p">)</span> <span class="o">==</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">sizeof</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">MPI_Comm</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">MPI_Comm</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init_comm</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">MPI_Comm</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">comm_obj</span> <span class="o">=</span> <span class="n">MPI_Comm</span><span class="o">.</span><span class="n">from_address</span><span class="p">(</span><span class="n">MPI</span><span class="o">.</span><span class="n">_addressof</span><span class="p">(</span><span class="n">comm</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init_comm</span><span class="p">(</span><span class="n">comm_obj</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">comm_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">comm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span> <span class="o">*</span> <span class="n">comm_size</span><span class="p">)(</span><span class="o">*</span><span class="n">comm</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">(</span><span class="n">comm_size</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下图：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-JAVA" data-lang="JAVA"><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">python</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">C</span><span class="o">++</span> <span class="n">SO</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Create</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="na">init</span><span class="o">()</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">Python</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+------------------------------------------+</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span><span class="o">++</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-初始化-in-c">4.3 初始化 in C++</h3>
<h4 id="431-horovod_init_comm">4.3.1 horovod_init_comm</h4>
<p>在初始化的时候，Horovod 会：</p>
<ul>
<li>调用 <code>MPI_Comm_dup</code> 获取一个 Communicator，这样就有了和 MPI 协调的基础。</li>
<li>然后调用 <code>InitializeHorovodOnce</code>。</li>
</ul>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">void</span> <span class="n">horovod_init_comm</span><span class="p">(</span><span class="n">MPI_Comm</span> <span class="n">comm</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Comm_dup</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mpi_context</span><span class="o">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="432-initializehorovodonce">4.3.2 InitializeHorovodOnce</h4>
<p>InitializeHorovodOnce 是初始化的主要工作，主要是：</p>
<ul>
<li>依据是否编译了 mpi 或者 gloo，对各自的 context 进行处理，为 globalstate 创建对应的 controller；</li>
<li>启动了后台线程 BackgroundThreadLoop 用来在各个worker之间协调；</li>
</ul>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">void</span> <span class="n">horovod_init</span><span class="p">(</span><span class="n">const</span> <span class="nb">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="nb">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">void</span> <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">const</span> <span class="nb">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="nb">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Ensure</span> <span class="n">background</span> <span class="n">thread</span> <span class="ow">is</span> <span class="n">only</span> <span class="n">started</span> <span class="n">once</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">initialize_flag</span><span class="o">.</span><span class="n">test_and_set</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">=</span> <span class="n">ParseControllerOpsFromEnv</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">=</span> <span class="n">ParseCPUOpsFromEnv</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#if HAVE_MPI // 依据是否编译了MPI进行处理</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Enable</span> <span class="n">mpi</span> <span class="ow">is</span> <span class="n">it</span><span class="s1">&#39;s used either in cpu data transfer or controller</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="o">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="o">//</span> <span class="n">创建一个</span> <span class="n">MPIController</span> <span class="n">对象</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">group_table</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#endif</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#if HAVE_GLOO // 依据是否编译了 GLOO 进行处理</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Enable</span> <span class="n">gloo</span> <span class="ow">is</span> <span class="n">it</span><span class="s1">&#39;s used either in cpu data transfer or controller</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">gloo_context</span><span class="o">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">new</span> <span class="n">GlooController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">group_table</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">gloo_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#endif</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Reset</span> <span class="n">initialization</span> <span class="n">flag</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">启动后台线程</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="n">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="p">::</span><span class="n">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Wait</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">background</span> <span class="n">thread</span> <span class="n">has</span> <span class="n">finished</span> <span class="n">initializing</span> <span class="n">MPI</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="err">!</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="p">::</span><span class="n">this_thread</span><span class="p">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">chrono</span><span class="p">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="433-horovodglobalstate">4.3.3 HorovodGlobalState</h4>
<p>在 C++ 世界，HorovodGlobalState 起到了<font color=red>集中管理各种全局变量</font>的作用。</p>
<p>HorovodGlobalState 在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问。HorovodGlobalState 在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）。</p>
<p>Horovod 主要会在backgroundThreadLoop 中完成 HorovodGlobalState 不同元素初始化，比较重要的有：</p>
<ul>
<li>controller 管理总体通信控制流；</li>
<li>tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等)；</li>
</ul>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// All the Horovod state that must be stored globally per-process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">HorovodGlobalState</span> <span class="n">horovod_global</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">MPIContext</span> <span class="n">mpi_context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">GlooContext</span> <span class="n">gloo_context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="p">....</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">OperationManager</span><span class="o">&gt;</span> <span class="n">op_manager</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div><p>HorovodGlobalState 摘要如下：</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">HorovodGlobalState</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Background thread running MPI communication.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">background_thread</span><span class="p">;</span> <span class="c1">// 后台线程，用来在各个worker之间协调
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">ParameterManager</span> <span class="n">parameter_manager</span><span class="p">;</span> <span class="c1">// 维护后台总体参数配置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Encapsulates the fusion buffers, handles resizing and auto-tuning of buffer
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// size.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">FusionBufferManager</span> <span class="n">fusion_buffer</span><span class="p">;</span> <span class="c1">// 融合tensor，以便缩减通信开销
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Controller</span><span class="o">&gt;</span> <span class="n">controller</span><span class="p">;</span> <span class="c1">//管理总体通信控制流
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorQueue</span> <span class="n">tensor_queue</span><span class="p">;</span> <span class="c1">//处理从前端过来的通信需求（allreduce，broadcast 等）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Pointer to shared buffer for allgather
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">void</span><span class="o">*</span> <span class="n">shared_buffer</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// LRU cache of Responses
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ResponseCache</span> <span class="n">response_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Information on registered groups.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">GroupTable</span> <span class="n">group_table</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">HorovodGlobalState</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Make sure that the destructor of the background thread is safe to
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// call. If a thread is still joinable (not detached or complete) its
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// destructor cannot be called.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">background_thread</span><span class="p">.</span><span class="n">joinable</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">shut_down</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">background_thread</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前具体逻辑如下：</p>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">python</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">C</span><span class="o">++</span> <span class="n">SO</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Create</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="na">init</span><span class="o">()</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">Python</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------------------------------------------------------------------------------------------------+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">++</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                          <span class="o">+-----------------------------+</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">|</span>  <span class="n">HorovodGlobalState</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="n">horovod_init_comm</span>                                                <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                             <span class="o">+------------------+</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span> <span class="n">horovod_global</span> <span class="o">+---------&gt;</span> <span class="o">|</span>        <span class="n">TensorQueue</span>          <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">background_thread</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                  <span class="o">|</span> <span class="n">mpi_context</span>      <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="n">InitializeHorovodOnce</span>   <span class="o">+------------&gt;</span> <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">ParameterManager</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span> <span class="n">gloo_context</span>     <span class="o">|</span>         <span class="o">|</span>        <span class="n">FusionBufferManager</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">Controller</span>           <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                             <span class="o">|</span> <span class="n">op_manager</span>       <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="n">background_threa</span>                     <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">ResponseCache</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                  <span class="o">+------------------+</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">|</span>        <span class="n">shared_buffer</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">+-----------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>如图：</p>
<p></p>
<p>至此，horovod 已经初始化完成，用户代码可以使用了。</p>
<h3 id="43-hvd-概念">4.3 hvd 概念</h3>
<p>在用户代码中，接下来是rank概念。</p>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们介绍下几个相关概念：</p>
<ul>
<li>Horovod为设备上的每个GPU启动了该训练脚本的一个副本。<strong>local rank</strong>就是分配给某一台计算机上每个执行训练的唯一编号（也可以认为是进程号或者GPU设备的ID号），范围是 0 到 n-1，其中 n 是该计算机上GPU设备的数量。</li>
<li>rank 可以认为是代表分布式任务里的一个执行训练的唯一全局编号（<font color=red>用于进程间通讯</font>）。Rank 0 在Horovod中通常具有特殊的意义：<strong>它是负责此同步的设备</strong>。
<ul>
<li>在百度的实现中，不同 Rank 的角色是不一样的，Rank 0 会充当 coordinator 的角色。它会协调来自其他 Rank 的 MPI 请求，是一个工程上的考量。这一设计也被后来的 Horovod 采用。</li>
<li>Rank 0 也用来把参数广播到其他进程 &amp; 存储 checkpoint。</li>
<li>world_size：进程总数量，会等到所有world_size个进程就绪之后才会开始训练。</li>
</ul>
</li>
</ul>
<p>hvd.init 这部分的目的就是让<strong>并行进程</strong>们可以知道自己被分配的 rank / local rank 等信息，于是后续可以根据 local rank（所在节点上的第几张 GPU 卡） 来设置所需的显存分配。</p>
<h3 id="45--数据处理">4.5  数据处理</h3>
<p>接下来是数据处理。</p>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_images</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里有几点需要说明：</p>
<ul>
<li>
<p>首先，训练的数据需要放置在任何节点都能访问的地方。</p>
</li>
<li>
<p>其次，Horovod 需要对数据进行分片处理，需要在不同机器上按Rank进行切分，以保证每个GPU进程训练的数据集是不一样的。</p>
</li>
<li>
<p>数据集本体需要出于数据并行性的需求而被拆分为多个分片，Horovod的不同工作节点都将分别读取自己的数据集分片。</p>
</li>
</ul>
<p>从 PyTorch 示例脚本看得更加清楚。</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: use DistributedSampler to partition the training data.</span>
</span></span><span class="line"><span class="cl"><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">rank</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><code>DataLoader</code>的采样器组件从要绘制的数据集中返回可迭代的索引。 PyTorch中的默认采样器是顺序的，返回序列<code>0, 1, 2, …, n</code> 。 Horovod使用其<code>DistributedSampler</code>覆盖了此行为，该DistributedSampler处理跨计算机的数据集分区。 <code>DistributedSampler</code>本身接受两个参数作为输入： <code>hvd.size()</code> (GPU的总数，例如16)和<code>hvd.rank()</code> (从总体列表中分配给该设备的ID，例如0…15)。</p>
</li>
<li>
<p>Pytorch使用的是<strong>数据分布式训练</strong>，每个进程实际上是独立加载数据的，所以需要加载相同数据集后用一定的规则根据rank来顺序切割获取不同的数据子集，DistributedSampler就是用来确保dataloader只会load到整个数据集的一个特定子集的做法(实际上不用Pytorch提供的DistributedSampler工具，自己做加载数据后切分word_size个子集按rank顺序拿到子集效果也是一样)。</p>
</li>
<li>
<p>同时为了能够按顺序划分数据子集，拿到不同部分数据，所以数据集不能够进行随机打散，所以用了参数 <code>'shuffle': False</code>。</p>
</li>
</ul>
<h3 id="46-广播初始化变量">4.6 广播初始化变量</h3>
<p>以下代码完成广播初始化的功能。</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这句代码保证的是 rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</p>
<p>下面就介绍下 Horvod 之中广播的使用。</p>
<h4 id="461-广播定义">4.6.1 广播定义</h4>
<p>广播的具体实现是：</p>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BroadcastGlobalVariablesCallbackImpl</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">BroadcastGlobalVariablesCallbackImpl</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span> <span class="o">=</span> <span class="n">root_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">_executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;variables&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># TensorFlow 2.0 or TensorFlow eager</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">root_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">root_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">bcast_op</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_global_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">bcast_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span> <span class="o">=</span> <span class="kc">True</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="462-broadcast_variables">4.6.2 broadcast_variables</h4>
<p>broadcast_variables 调用了 _make_broadcast_group_fn 完成功能，可以看到对于 执行图 的每个变量，调用了 broadcast。</p>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">broadcast_variables</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Broadcasts variables from root rank to all other processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">        variables: variables for broadcast
</span></span></span><span class="line"><span class="cl"><span class="s2">        root_rank: rank of the process from which global variables will be broadcasted
</span></span></span><span class="line"><span class="cl"><span class="s2">                   to all other processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">broadcast_group</span> <span class="o">=</span> <span class="n">_make_broadcast_group_fn</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>以及</p>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@_cache</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_make_broadcast_group_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Eager mode will parallelize independent control flow</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_make_subgraph</span><span class="p">(</span><span class="n">broadcast_group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Graph mode requires an Op</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                              <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">broadcast_group</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="463-调用-mpi">4.6.3 调用 MPI</h4>
<p>broadcast 就是调用了 MPI 函数真正完成了功能。</p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;An op which broadcasts the input tensor on root rank to the same input tensor
</span></span></span><span class="line"><span class="cl"><span class="s2">    on all other Horovod processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The broadcast operation is keyed by the name of the op. The tensor type and
</span></span></span><span class="line"><span class="cl"><span class="s2">    shape must be the same on all Horovod processes for a given name. The broadcast
</span></span></span><span class="line"><span class="cl"><span class="s2">    will not start until all processes are ready to send and receive the tensor.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">      A tensor of the same shape and type as `tensor`, with the value broadcasted
</span></span></span><span class="line"><span class="cl"><span class="s2">      from root rank.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;HorovodBroadcast_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">_normalize_name</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">MPI_LIB</span><span class="o">.</span><span class="n">horovod_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="n">root_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ignore_name_scope</span><span class="o">=</span><span class="n">ignore_name_scope</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="464-同步参数">4.6.4 同步参数</h4>
<p>在后台进程中，会<strong>根据情况定期同步参数</strong>。</p>
<div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">bool</span> <span class="nf">RunLoopOnce</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// 业务逻辑功能
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">IsAutoTuning</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">should_sync</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">Update</span><span class="p">(</span><span class="n">tensor_names</span><span class="p">,</span> <span class="n">total_tensor_size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 看看是否需要同步，如果需要，就同步。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">should_sync</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SynchronizeParameters</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">......</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>同步参数代码也是调用了 Bcast 功能完成。</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Controller</span><span class="o">::</span><span class="n">SynchronizeParameters</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ParameterManager</span><span class="o">::</span><span class="n">Params</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">is_coordinator_</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// rank 0 执行操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">param</span> <span class="o">=</span> <span class="n">parameter_manager_</span><span class="p">.</span><span class="n">GetParams</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">void</span><span class="o">*</span> <span class="n">buffer</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">size_t</span> <span class="n">param_size</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">param_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_coordinator_</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// worker 执行操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">parameter_manager_</span><span class="p">.</span><span class="n">SetParams</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="47-distributedoptimizer">4.7 DistributedOptimizer</h3>
<p>最后需要配置DistributedOptimizer，这就是关键点之一。</p>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>TF Optimizer 是模型训练的关键API，可以获取到每个OP的梯度并用来更新权重。HVD 在原始 TF Optimizer的基础上包装了hvd.DistributedOptimizer。</p>
<p>DistributedOptimizer包装器将原始优化器作为输入，将梯度计算委托给它。 即DistributedOptimizer会调用原始优化器进行梯度计算。这样，在集群中每台机器都会用原始优化器得到自己的梯度（Local Gradient）。</p>
<p><code>Horovod DistributedOptimizer</code>接下来会使用all-reduce或all-gather来完成全局梯度归并，然后将这些平均梯度应用于所有设备。</p>
<p>我们梳理下其中的调用关系：</p>
<ul>
<li>hvd.DistributedOptimizer继承 keras Optimizer，在计算时候，依然由传入的原始优化器做计算。</li>
<li>在得到计算的梯度之后，调用 hvd.allreduce 或者 hvd.allgather 来计算。</li>
<li>最后实施这些平均之后的梯度。从而实现整个集群的梯度归并操作。</li>
</ul>
<p>具体后文会详细介绍。</p>
<h3 id="48-未来可能">4.8 未来可能</h3>
<p>Horovod 目前架构的基础是：机器学习的模型参数在一张 GPU 上可以存下。</p>
<p><strong>未来是否可以把模型分片结合进来，是一个很大的看点。</strong></p>
<p>另外，如果模型的全连接层较多，则全连接层的强耦合性结合 allreduce 类似 bsp 的同步机制，还是会让网络通信时间成为瓶颈。因此，在 ring-allreduce 环境下，同步协议的改造，比如利用 SSP 来替换 BSP，或者利用梯度压缩来加快 allreduce 进程也是值得探索的方向。</p>
<h2 id="5-总结">5 总结</h2>
<p>针对文初提出的几个问题，我们现在回答如下：</p>
<ul>
<li>Hovorod 怎么进行数据分割？
答案：有的框架可以自动做数据分割。如果框架不提供，则需要用户自己进行数据分割，以保证每个GPU进程训练的数据集是不一样的。</li>
<li>Hovorod 怎么进行模型分发？
用户需要手动拷贝训练代码到各个节点上。</li>
<li>Hovorod 启动时候，python 和 C++ 都做了什么？
答案：python 会引入 C++库，初始化各种变量和配置。C++部分会对 MPI，GLOO上下文进行初始化，启动后台进程处理内部通信。</li>
<li>如何确保 Hovorod 启动时候步骤一致；
答案： rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</li>
</ul>
<p>下一篇文章将深入到python世界看看。</p>
<p>reference:
[1].https://www.cnblogs.com/rossiXYZ/p/14856543.html</p>
]]></description></item><item><title>深度学习分布式训练框架 Horovod[1] -- 基础知识</title><link>https://lruihao.cn/posts/2022-10-08_horovod_1/</link><pubDate>Mon, 10 Jul 2023 07:45:42 +0800</pubDate><author>Jian YE</author><guid>https://lruihao.cn/posts/2022-10-08_horovod_1/</guid><description><![CDATA[<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。</p>
<h2 id="1-分布式并行训练">1 分布式并行训练</h2>
<p>我们首先要介绍下<strong>分布式并行训练</strong>。</p>
<h3 id="11-分布式并行训练的必要">1.1 分布式并行训练的必要</h3>
<p>传统的模型训练中，<font color=red><strong>迭代计算</strong></font>只能利用当前进程所在主机上的所有硬件资源，可是<u>单机扩展性始终有限</u>。而目前的机器学习有如下特点：</p>
<ul>
<li><strong>样本数量大</strong> 目前训练数据越来越多，在大型互联网场景下，每天的样本量可以达到百亿级别。</li>
<li><strong>特征维度多</strong> 因为巨大样本量导致机器学习模型参数越来越多，特征维度可以达到千亿或者万亿级别。</li>
<li><strong>训练性能要求高</strong> 虽然样本量和模型参数巨大，但是业务需要我们在短期内训练出一个优秀的模型来验证。</li>
<li><strong>模型实时上线</strong> 对于推荐资讯类应用，往往要求根据用户最新行为及时调整模型进行预测。</li>
</ul>
<p>因此，单机面对海量数据和巨大模型时是无能为力的，有必要把数据或者模型分割成为多份，在多个机器上借助不同主机上的硬件资源进行训练加速。</p>
<h3 id="12-分布式训练">1.2 分布式训练</h3>
<p>本文所说的训练，指的是<font color=red>利用训练数据通过计算梯度下降的方式迭代地去优化神经网络参数，并最终输出网络模型的过程</font>。在单次模型训练迭代中，会有如下操作：</p>
<ul>
<li>首先利用数据对模型进行前向的计算。所谓的前向计算，就是将模型上一层的输出作为下一层的输入，并计算下一层的输出，从输入层一直算到输出层为止。</li>
<li>其次会根据目标函数，我们将反向计算模型中每个参数的导数，并且结合学习率来更新模型的参数。</li>
</ul>
<p>而并行梯度下降的基本思想便是：<font color=red>多个处理器分别利用自己的数据来计算梯度，最后通过聚合或其他方式来实现并行计算梯度下降以加速模型训练过程</font>。 比如两个处理器分别处理一半数据计算梯度 g_1, g_2，然后把两个梯度结果进行聚合更新，这样就实现了并行梯度下降。</p>
<h3 id="13--训练并行机制">1.3  训练并行机制</h3>
<h4 id="131-三种机制">1.3.1 三种机制</h4>
<p>由于使用小批量算法，可以把宽度$(∝W)$和深度$(∝D)$的前向传播和反向传播分发到并行的处理器上，这样深度训练的并行机制主要有三种：</p>
<ul>
<li>第一个是<font color=red><strong>模型并行机制</strong></font>（按照网络结构分区）。
<ul>
<li>通常是针对一个节点无法存下整个模型的情况下，去对图进行拆分。</li>
<li>将模型参数进行分布式存储。<strong><u>计算机上每个计算可以建模为一个有向无环图（DAG），顶点是计算指令，边是数据依赖（数据流）。</u></strong>&ldquo;基于图去拆分&rdquo; 会根据每一层中的神经元（即四维张量中的C、H或W维）来把一张大的图拆分成很多部分，每个部分都会在很多设备上去计算。</li>
<li>或者可以这么理解：深度学习的计算主要是矩阵运算，有时候矩阵非常大无法放到显存中，就只能把超大矩阵拆分了放到不同卡上计算。</li>
<li>模型较后部分的计算必须等前面计算完成，因此不同节点间的计算实际是串行的。但每个部分计算互不妨碍，更像是流水线结构。</li>
</ul>
</li>
<li>第二个是<font color=red><strong>数据并行机制</strong></font>（按照输入样本分区）。
<ul>
<li>更多场景下我们模型规模不大，在一张 GPU 可以容纳，但是训练数据量会比较大，这时候就采用数据并行机制。</li>
<li>具体就是在多节点上并行分割数据和训练。</li>
</ul>
</li>
<li>第三种不常用的并行机制是 <font color=red><strong>流水线机制</strong></font>（按层分区）。
<ul>
<li>在深度学习中，流水线可以是指重叠的计算，即在一层和下一层之间（当数据准备就绪时）连续计算；或者根据深度划分DNN，将层分配给特定处理器。</li>
<li>流水线可以看作是数据并行的一种形式，因为元素（样本）是通过网络并行处理的，但也可以看作是模型并行，因为流水线的长度是由DNN结构决定的。</li>
</ul>
</li>
</ul>
<p>具体可见下图:
</p>
<h4 id="132-如何使用">1.3.2 如何使用</h4>
<p><u>数据的并行往往意味着<strong>计算性能</strong>的可扩展，而模型的并行往往意味着<strong>内存使用</strong>的可扩展。</u></p>
<p>需要注意的是：<font color=green>数据并行和模型并行也并不冲突，两者可以同时存在，而流水线机制也可以和模型并行一起混用。</font>比如，DistBelief分布式深度学习系统结合了三种并行策略。训练在同时复制的多个模型上训练，每个模型副本在不同的样本上训练（数据并行），每个副本上，依据同一层的神经元（模型并行性）和不同层（流水线）上划分任务，进行分布训练。</p>
<p>另外也需要根据具体问题具体分析，比如现代卷积神经网络主要由两种层构成，他们具有不一样的属性和性能。</p>
<ul>
<li><strong>卷积层</strong>，占据了90% ~ 95% 的计算量，5% 的参数，但是对结果具有很大的表达能力。</li>
<li><strong>全连接层</strong>，占据了 5% ~ 10% 的计算量， 95% 的参数，但是对于结果具有相对较小的表达的能力。</li>
</ul>
<p>综上：卷积层计算量大，所需参数系数 W 少，全连接层计算量小，所需参数系数 W 多。因此对于卷积层适合使用数据并行，对于全连接层适合使用模型并行。</p>
<p></p>
<h3 id="14-数据并行训练">1.4 数据并行训练</h3>
<p>我们本系列主要讨论数据并行训练（其中的一种架构）.</p>
<p>数据并行训练只是一种逻辑架构。我们从沐神的书里面摘录：</p>
<blockquote>
<p>假设机器上有k个GPU。给定要训练的模型，每个GPU将独立地维护一组完整的模型参数，尽管GPU上的参数值是相同且同步的。例如，下图演示了在k=2时使用数据并行的训练。</p>
</blockquote>
<blockquote>
<p></p>
</blockquote>
<blockquote>
<p>一般来说，训练过程如下：</p>
<ul>
<li>在训练的任何迭代中，给定一个随机的小批量，我们将该小批量中的样本分成k个部分，并将它们均匀地分在多个GPU上。</li>
<li>每个GPU根据分配给它的小批量子集计算模型参数的损失和梯度。</li>
<li>将k个GPU中每个GPU的局部梯度聚合以获得当前的小批量随机梯度。</li>
<li>聚合梯度被重新分配到每个GPU。</li>
<li>每个GPU使用这个小批量随机梯度来更新它维护的完整的模型参数集。</li>
</ul>
</blockquote>
<h2 id="2-通信和架构">2 通信和架构</h2>
<p>前面提到并行梯度下降的例子：两个处理器分别处理一般数据计算梯度 $g_1$, $g_2$，然后把两个梯度结果进行聚合，最后再把最新参数发给各个分布计算单元，这种训练算法叫<strong>模型一致性方法</strong>（consistent model methods）。<font color=red>这就涉及到了通信问题，即如何做聚合</font>。</p>
<h3 id="21-方法和架构">2.1 方法和架构</h3>
<p>一般有两种通信方法：<strong>Share memory</strong> 和 <strong>Message passing</strong>。</p>
<ul>
<li><strong>Share memory</strong> 就是<u>所有处理器共享同一块内存</u>，这样通信很容易，<u>但是同一个节点内的处理器之间才可以共享内存，不同节点处理器之间无法共享内存</u>。</li>
</ul>
<p></p>
<ul>
<li><strong>Message passing</strong> 就是<u>不同节点之间用消息</u>（比如基于 TCP/IP 或者 RDMA）进行传递/通信，这样容易扩展，可以进行大规模训练。</li>
</ul>
<p></p>
<p>因此我们知道，Message passing 才是解决方案，于是带来了问题：<font color=red>如何协调这些节点之间的通讯</font>。</p>
<p>有两种架构：</p>
<ul>
<li><font color=red>Client-Server</font>架构: 一个 server 节点协调其他节点工作，其他节点是用来执行计算任务的 worker。</li>
<li><font color=red>Peer-to-Peer</font>架构：每个节点都有邻居，邻居之间可以互相通信。</li>
</ul>
<h3 id="22-异步-vs-同步">2.2 异步 vs 同步</h3>
<p>异步 vs 同步 是通信的另外一个侧面。</p>
<p>在数据并行训练之中，各个计算设备分别根据各自获得的batch，前向计算获得损失，进而反向传播计算梯度。计算好梯度后，就涉及到一个<font color=red><strong>梯度同步的问题</strong></font>：每个计算设备 都有根据自己的数据计算的梯度，如何在不同GPU之间维护模型的不同副本之间的一致性？ 如果不同的模型以某种方式最终获得不同的权重，则权重更新将变得不一致，并且模型训练将有所不同。</p>
<blockquote>
<blockquote>
<p><font color=red><strong>怎么做这个同步就是设计分布式机器学习系统的一个核心问题</strong></font>。</p>
</blockquote>
</blockquote>
<p>分布式训练的梯度同步策略可分为<strong>异步（asynchronous）梯度更新</strong> 和 <strong>同步（synchronous）梯度更新</strong>机制。</p>
<ul>
<li>
<p><font color=red><strong>同步</strong></font>指的是所有的设备都是采用相同的模型参数来训练，<u>等待所有设备的mini-batch训练完成后，收集它们的梯度然后取均值，然后执行模型的一次参数更新</u>。</p>
<ul>
<li>同步训练相当于通过聚合很多设备上的mini-batch形成一个很大的batch来训练模型，Facebook就是这样做的，但是他们发现当batch大小增加时，同时线性增加学习速率会取得不错的效果。</li>
<li>同步训练看起来很不错，但是实际上需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡。</li>
<li>因为每一轮结束时算得快的节点都需等待算得慢的节点算完，再进行下一轮迭代。类似于木桶效应，一个拖油瓶会严重拖慢训练进度，所以同步训练方式相对来说训练速度会慢一些。这个拖油瓶一般就叫做 straggler。(缺点)</li>
</ul>
</li>
<li>
<p><font color=red><strong>异步</strong></font>训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数，这样总体会训练速度会快很多</p>
<ul>
<li>异步训练的一个很严重的问题是<strong>梯度失效问题</strong>（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数其实已经被其它设备更新过了，此时这个梯度就过期了，因为现在的模型参数和训练前采用的参数是不一样的。由于梯度失效问题，异步训练虽然速度快，但是可能陷入次优解（sub-optimal training performance）。</li>
</ul>
</li>
</ul>
<p>具体如图所示:</p>
<p>
</p>
<p>这两种更新方式各有优缺点：</p>
<ul>
<li>异步更新可能会更快速地完成整个梯度计算。</li>
<li>同步更新 可以更快地进行一个收敛。</li>
</ul>
<p>选择哪种方式取决于实际的应用场景。</p>
<h2 id="3-具体架构">3 具体架构</h2>
<p>接下来，我们看看几种具体架构实现，先给出一个总体说明：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>通信</th>
<th>架构</th>
<th>并行性</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce</td>
<td>消息传递</td>
<td>client-server</td>
<td>批同步</td>
</tr>
<tr>
<td>Parameter Server</td>
<td>消息传递</td>
<td>client-server</td>
<td>异步</td>
</tr>
<tr>
<td>Decentralized Network</td>
<td>消息传递</td>
<td>P2P(Peer to Peer)</td>
<td>同步或异步</td>
</tr>
</tbody>
</table>
<h3 id="31-mapreduce">3.1 MapReduce</h3>
<p>MapReduce是Client-Server架构。以 Spark 为例看看是如何进行并行化：</p>
<ul>
<li>Spark Driver 就是 Server，Spark Executor 就是 Worker 节点，每一个梯度下降过程包含一个<font color=red>广播</font>、<font color=red>map</font>和一个 <font color=red>reduce</font> 操作。</li>
<li>Server 定义了 map操作（就是具体的训练），也可以把信息广播到worker节点。</li>
<li>Worker 会执行 map 操作进行训练，在此过程中，数据被分给 worker 进行计算。</li>
<li>计算结束后，worker把计算结果传回 driver 处理，这个叫做reduce。</li>
<li>在 reduce 过程中，Server 节点对 worker 传来的计算结果进行聚合之后，把聚合结果广播到各个worker节点，进行下一次迭代。</li>
</ul>
<h3 id="32-parameter-server-参数服务器">3.2 Parameter Server 参数服务器</h3>
<p>Parameter server 也是一种client-server架构。<u>和MapReduce不同在于 Parameter server 可以是异步的</u>，MapReduce只有等所有map都完成了才能做reduce操作。</p>
<p>参数服务器架构中，计算设备被划分为参数服务器（PS）和worker。</p>
<ul>
<li><strong>参数服务器（server</strong>）。是中心化的组件，主要是负责模型参数的存储，平均梯度和交换更新。参数服务器可以按照不同比例的参数服务器和工作线程进行配置，每个参数服务器都有着不同的配置数据。</li>
<li><strong>工作节点（worker）</strong>。每个工作节点会负责它领域内的数据分片所对应模型参数的更新计算（比如前向和反向传播这类计算密集的运算），同时它们又会向参数服务器去传递它所计算的梯度，由参数服务器来汇总所有的梯度，再进一步反馈到所有节点。</li>
</ul>
<p>具体步骤如下：</p>
<ul>
<li>所有的参数都存储在参数服务器中，而 工作节点（worker） 是万年打工仔。</li>
<li>工作节点 们只负责计算梯度，待所有计算设备完成梯度计算之后，把计算好的梯度发送给参数服务器，这样参数服务器收到梯度之后，执行一定的计算（梯度平均等）之后，就更新其维护的参数，做到了在节点之间对梯度进行平均，利用平均梯度对模型进行更新。</li>
<li>然后参数服务器再把更新好的新参数返回给所有的工作节点，以对每个节点中的模型副本应用一致化更新。</li>
<li>打工仔们会再进行下一轮的前后向计算。</li>
</ul>
<p>逻辑如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">     <span class="o">+----------------------------------------------+</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>  <span class="n">Parameter</span> <span class="n">Server</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>   <span class="n">Compute</span> <span class="p">:</span> <span class="n">New</span> <span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">+</span> <span class="n">Sum</span><span class="p">(</span><span class="n">Delta</span> <span class="n">P</span> <span class="o">...</span><span class="p">)</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>   <span class="n">Parameter</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Parameter</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Parameter</span> <span class="mi">3</span> <span class="o">...</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">+--+----+----------+--+----------------+--+----+</span>
</span></span><span class="line"><span class="cl">        <span class="o">^</span>    <span class="o">|</span>          <span class="o">^</span>  <span class="o">|</span>                <span class="o">^</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">        <span class="o">|</span>    <span class="o">|</span>          <span class="o">|</span>  <span class="o">|</span>                <span class="o">|</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">Delta</span> <span class="n">P</span> <span class="o">|</span>    <span class="o">|</span>   <span class="n">Delta</span> <span class="n">P</span><span class="o">|</span>  <span class="o">|</span>         <span class="n">Delta</span> <span class="n">P</span><span class="o">|</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+-----+</span>    <span class="o">|</span>          <span class="o">|</span>  <span class="o">|</span>                <span class="o">|</span>  <span class="o">+------+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="o">+-----+</span>          <span class="o">|</span>  <span class="o">|</span>                <span class="o">|</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="o">|</span> <span class="n">New</span> <span class="n">P</span>          <span class="o">|</span>  <span class="o">|</span> <span class="n">New</span> <span class="n">P</span>          <span class="o">+------+</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="o">|</span>                <span class="o">|</span>  <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>  <span class="n">New</span> <span class="n">P</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="n">v</span>                <span class="o">|</span>  <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                     <span class="o">|</span>  <span class="n">v</span>                       <span class="o">|</span>  <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-+-----------+</span>   <span class="o">+-----+--+---+</span>             <span class="o">+-----+--+---+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">Worker</span>      <span class="o">|</span>   <span class="o">|</span> <span class="n">Worker</span>     <span class="o">|</span>             <span class="o">|</span> <span class="n">Worker</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>             <span class="o">|</span>   <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>             <span class="o">|</span>   <span class="o">|</span>            <span class="o">|</span>   <span class="o">......</span>    <span class="o">|</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">Model</span> <span class="o">|</span>   <span class="o">|</span>     <span class="n">Model</span>  <span class="o">|</span>             <span class="o">|</span>     <span class="n">Model</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+------+------+</span>   <span class="o">+------+-----+</span>             <span class="o">+----+-------+</span>
</span></span><span class="line"><span class="cl">       <span class="o">^</span>                 <span class="o">^</span>                        <span class="o">^</span>
</span></span><span class="line"><span class="cl">       <span class="o">|</span>                 <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">       <span class="o">|</span>                 <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+----+----+</span>       <span class="o">+----+-----+</span>               <span class="o">+--+-----+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span> <span class="n">Data</span> <span class="mi">1</span>  <span class="o">|</span>       <span class="o">|</span>  <span class="n">Data</span> <span class="mi">2</span>  <span class="o">|</span>               <span class="o">|</span> <span class="n">Data</span> <span class="mi">3</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+</span>       <span class="o">+----------+</span>               <span class="o">+--------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>如图:
</p>
<p>参数服务器既可以用在数据并行上，也可以被用到模型并行训练上。比如可以将模型切分为多个部分，存储在不同的PS Server节点上，并提供方便的访问服务，这是参数服务器的本质。</p>
<h3 id="33--decentralized-network">3.3  Decentralized Network</h3>
<p>Decentralized Network 就是去中心化网络，其特点如下：</p>
<ul>
<li>去中心化网络没有一个中心节点，属于 Peer-to-Peer 架构。</li>
<li>采用 message passing 进行通信，且节点只和邻居通信。</li>
<li>并行方式可以采用异步或者同步。</li>
<li>去中心化网络的收敛情况取决于网络连接情况：
<ul>
<li>连接越紧密，收敛性越快，当强连接时候，模型可以很快收敛；</li>
<li>如果不是强连接，它可能不收敛；</li>
</ul>
</li>
</ul>
<h2 id="4-allreduce">4 AllReduce</h2>
<p>因为本系列是 Horovod，所以我们要先说说参数服务器的劣势，下一个系列我们再说参数服务器优势。</p>
<h3 id="41-参数服务器劣势">4.1 参数服务器劣势</h3>
<p>尽管参数服务器可以提升表现，但仍然面临几个问题：</p>
<ul>
<li><font color=red>确定工作者与参数服务器的正确比例</font>：如果使用一个参数服务器，它可能会成为网络或计算瓶颈。 如果使用多个参数服务器，则通信模式变为“All-to-All”，这可能使网络饱和。</li>
<li><font color=red>处理程序复杂性</font>：参数服务器的概念较多，这通常导致陡峭的学习曲线和大量的代码重构，压缩了实际建模的时间。</li>
<li><font color=red>硬件成本</font> : 参数服务器的引入也增加了系统的硬件成本。</li>
</ul>
<p>人们发现，MPI_AllReduce 语义也可以很好地满足数据并行训练这一需要。</p>
<p>需要注意的是：AllReduce <strong>既可以是去中心化，也可以是主从式的。</strong></p>
<h3 id="42-并行任务通信分类">4.2 并行任务通信分类</h3>
<p>并行任务的通信一般可以分为 <font color=red><strong>Point-to-point communication</strong></font>和 <font color=red><strong>Collective communication</strong></font>。</p>
<ul>
<li>P2P 这种模式只有一个sender和一个receiver，实现起来比较简单，比如NV GPU Direct P2P技术服务于单机多卡的单机卡间数据通信 。</li>
<li>Collective communication包含多个sender和多个receiver，一般的通信原理包括 broadcast，gather,all-gather，scatter，reduce，all-reduce，reduce-scatter，all-to-all等。</li>
</ul>
<h3 id="43-mpi_allreduce">4.3 MPI_AllReduce</h3>
<p>AllReduce<font color=red>(对m个独立参数进行规约，并将规约结果返回给所有进程)</font>, 其实是最显然和直接的<strong>分布式机器学习抽象</strong>，因为大部分算法的结构都是分布数据。<u>在每个子集上面算出一些局部统计量，然后整合出全局统计量，并且再分配给各个节点去进行下一轮的迭代，这样一个过程就是AllReduce</u>。</p>
<ul>
<li>
<p>可以把每个 Worker 看作是 MPI 概念中的一个进程，比如可以用 4 个 Worker 组成了一个组，该组由 4 个进程组成。我们在这四个进程中对梯度进行一次 MPI_AllReduce。</p>
</li>
<li>
<p>根据 MPI_AllReduce 的语义，所有参与计算的进程都有结果，所以梯度就完成了分发。只要在初始化的时候，我们可以保证每个 Worker 的参数是一致的，那在后续的迭代计算中，参数会一直保持一致，因为梯度信息是一致的。</p>
</li>
<li>
<p>AllReduce 跟 MapReduce 有类似，但后者采用的是<u>面向通用任务处理的多阶段执行任务的方式</u>，而AllReduce则让一个程序在必要的时候占领一台机器，并且在所有迭代的时候一直跑到底，来防止重新分配资源的开销，这更加适合于机器学习的任务处理。</p>
</li>
</ul>
<p>所以，MPI_AllReduce 的语义可以很好地解决深度学习中梯度同步的问题。但是到底能不能使用它，还是要看下层的实现对这一场景是否足够友好。</p>
<h2 id="5--ring-allreduce">5  ring-allreduce</h2>
<p>百度提出使用新算法来平均梯度，取消 Reducer，并让这些梯度在所有节点之间交流，这被称为 ring-allreduce，他们使用 TensorFlow 也实现了这种算法（https://github.com/baidu-research/tensorflow-allreduce）。</p>
<h3 id="51-特点">5.1 特点</h3>
<p><strong>Ring-Allreduce</strong>特点如下：</p>
<ul>
<li>Ring Allreduce 算法使用定义良好的成对消息传递步骤序列在一组进程之间同步状态（在这种情况下为张量）。</li>
<li>Ring-Allreduce 的命名中 Ring 意味着设备之间的拓扑结构为一个逻辑环形，每个设备都应该有一个左邻和一个右邻居，且本设备只会<strong>向它右邻居发送数据，并且从它的左邻居接受数据</strong>。</li>
<li>Ring-Allreduce 的命名中的 Allreduce 则代表着没有中心节点，架构中的每个节点都是梯度的汇总计算节点。</li>
<li>此种算法各个节点之间只与相邻的两个节点通信，并不需要参数服务器。因此，所有节点都参与计算也参与存储，也避免产生中心化的通信瓶颈。</li>
<li>相比PS架构，Ring-Allreduce 架构是<strong>带宽优化</strong>的，因为集群中每个节点的带宽都被充分利用。
<ul>
<li>在 ring-allreduce 算法中，每个 N 节点与其他两个节点进行 2 * (N-1) 次通信。在这个通信过程中，一个节点发送并接收数据缓冲区传来的块。<strong>在第一个N-1迭代中，接收的值被添加到节点缓冲区中的值</strong>。<strong>在第二个N-1迭代中，接收的值代替节点缓冲区中保存的值</strong>。百度的文章证明了这种算法是带宽上最优的，这意味着如果缓冲区足够大，它将最大化地利用可用的网络。</li>
</ul>
</li>
<li>在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于后面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。</li>
<li>Ring架构下的同步算法将参数在通信环中依次传递，往往需要多步才能完成一次参数同步。在大规模训练时会引入很大的通信开销，并且对小尺寸张量（tensor）不够友好。对于小尺寸张量，可以采用批量操作（batch）的方法来减小通信开销。</li>
</ul>
<p>综上所述，Ring-based AllReduce 架构的网络通讯量如果处理适当，不会随着机器增加而增加，而仅仅和模型 &amp; 网络带宽有关，这针对参数服务器是个巨大的提升。</p>
<h3 id="52-策略">5.2 策略</h3>
<p>Ring-based AllReduce 策略包括 <font color=red>Scatter-Reduce</font> 和 <font color=red>AllGather</font> 两个阶段。</p>
<ul>
<li>
<p>首先是scatter-reduce，scatter-reduce 会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分，是最终结果的一个块。</p>
<ul>
<li>假设环中有 N 个 worker，每个 worker 有长度相同的数组，需要将 worker 的数组进行求和。在 Scatter-Reduce 阶段，每个 worker 会将数组分成 N 份数据块，然后 worker 之间进行 N 次数据交换。在第 k 次数据交换时，第 i 个 worker 会将自己的 (i - k) % N 份数据块发送给下一个 worker。接收到上一个 worker 的数据块后，worker 会将其与自己对应的数据块求和。</li>
</ul>
</li>
<li>
<p>然后是allgather。<u>GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的最终融合梯度</u>。</p>
<ul>
<li>在执行完 Scatter-Reduce 后，每个 worker 的数组里都有某个数据块是最终求和的结果，现在需要将各数据块的最后求和结果发送到每个 worker 上。和 Scatter-Reduce 一样，也需要 N 次循环。在第 k 次循环时，第 i 个 worker 会将其第 (i+1-k)%N 个数据块发送给下一个 worker 。接收到前一个 worker 的数据块后，worker 会用接收的数据快覆盖自己对应的数据块。进行 N 次循环后，每个 worker 就拥有了数组各数据块的最终求和结果了。</li>
</ul>
</li>
</ul>
<p>以下部分来自 <a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"target="_blank" rel="external nofollow noopener noreferrer">https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，这是我能找到最优秀的解读。</p>
<h4 id="521-结构">5.2.1 结构</h4>
<p>环形结构如下，每个 GPU 应该有一个左邻居和一个右邻居；它只会向其右侧邻居发送数据，并从其左侧邻居接收数据。</p>
<p></p>
<h4 id="522-scatter-reduce">5.2.2 scatter reduce</h4>
<p>scatter-reduce：会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分。</p>
<p>为简单起见，我们假设目标是按元素对单个大型浮点数数组的所有元素求和；系统中有 N 个 GPU，每个 GPU 都有一个相同大小的数组，在 allreduce 的最后环节，每个 GPU 都应该有一个相同大小的数组，其中包含原始数组中数字的总和。</p>
<h5 id="5221-分块">5.2.2.1 分块</h5>
<p>首先，GPU 将阵列划分为 N 个较小的块（其中 N 是环中的 GPU 数量）。</p>
<p></p>
<p>接下来，GPU 将进行 N-1 次 scatter-reduce 迭代。</p>
<p>在每次迭代中，GPU 会将其一个块发送到其右邻居，并将从其左邻居接收一个块并累积到该块中。每个 GPU 发送和接收的数据块每次迭代都不同。第 n 个 GPU 通过发送块 n 和接收块 n – 1 开始，然后逐步向后进行，每次迭代发送它在前一次迭代中接收到的块。</p>
<h5 id="5222-第一次迭代">5.2.2.2 第一次迭代</h5>
<p>在第一次迭代中，上图中的五个 GPU 将发送和接收以下块：</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>发送</th>
<th>接收</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>块0</td>
<td>块4</td>
</tr>
<tr>
<td>1</td>
<td>块1</td>
<td>块0</td>
</tr>
<tr>
<td>2</td>
<td>块2</td>
<td>块1</td>
</tr>
<tr>
<td>3</td>
<td>块3</td>
<td>块2</td>
</tr>
<tr>
<td>4</td>
<td>块4</td>
<td>块3</td>
</tr>
</tbody>
</table>
<p>scatter-reduce 的第一次迭代中的数据传输如下：</p>
<p></p>
<p>第一次发送和接收完成后，每个 GPU 都会有一个块，该块由两个不同 GPU 上相同块的总和组成。例如，第二个 GPU 上的第一个块将是该块中来自第二个 GPU 和第一个 GPU 的值的总和。</p>
<p></p>
<h5 id="5222-全部迭代">5.2.2.2 全部迭代</h5>
<p>在后续迭代中，该过程继续直到最后。最终每个 GPU 将有一个块，这个块包含所有 GPU 中该块中所有值的总和。</p>
<p>下面系列图展示了所有数据传输和中间结果，从第一次迭代开始，一直持续到scatter-reduce完成。</p>
<p>iter 1：</p>
<p></p>
<p>iter2：</p>
<p></p>
<p>iter3：</p>
<p></p>
<p>iter4：</p>
<p></p>
<p>所有 scatter-reduce 传输后的最终状态</p>
<p></p>
<h4 id="523-allgather">5.2.3 Allgather</h4>
<p>在 scatter-reduce 步骤完成后，在每个 GPU 的数组中都有某一些值（每个 GPU 有一个块）是最终值，其中包括来自所有 GPU 的贡献。为了完成 allreduce，GPU 必须接下来交换这些块，以便所有 GPU 都具有最终所需的值。</p>
<p>ring allgather 与 scatter-reduce 进行相同的处理（发送和接收的 N-1 次迭代），但是他们这次不是累积 GPU 接收的值，而只是简单地覆盖块。第 n 个 GPU 开始发送第 n+1 个块并接收第 n 个块，然后在以后的迭代中始终发送它刚刚接收到的块。</p>
<h5 id="5231-第一次迭代">5.2.3.1 第一次迭代</h5>
<p>例如，在我们的 5-GPU 设置的第一次迭代中，GPU 将发送和接收以下块：</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>发送</th>
<th>接收</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>块1</td>
<td>块0</td>
</tr>
<tr>
<td>1</td>
<td>块2</td>
<td>块1</td>
</tr>
<tr>
<td>2</td>
<td>块3</td>
<td>块2</td>
</tr>
<tr>
<td>3</td>
<td>块4</td>
<td>块3</td>
</tr>
<tr>
<td>4</td>
<td>块0</td>
<td>块4</td>
</tr>
</tbody>
</table>
<p>allgather 的第一次迭代中的数据传输如下。</p>
<p></p>
<p>第一次迭代完成后，每个 GPU 都会有最终数组的两个块。在接下来的迭代中，该过程继续一直到最后，最终每个 GPU 将拥有整个数组的完全累加值。</p>
<h5 id="5232-全部迭代">5.2.3.2 全部迭代</h5>
<p>下面系列图展示了所有数据传输和中间结果，从第一次迭代开始，一直持续到全部收集完成。</p>
<p>Allgather 数据传输（迭代 1）
</p>
<p>Allgather 数据传输（迭代 2）如下：
</p>
<p>Allgather 数据传输（迭代 3）：</p>
<p></p>
<p>Allgather 数据传输（迭代 4）：</p>
<p></p>
<p>所有全部转移后的最终状态。</p>
<p></p>
<h4 id="524-horovod-架构图">5.2.4 Horovod 架构图</h4>
<p>工作原理也可以借助<a href="https://www.uber.com/blog/manifold-open-source/"target="_blank" rel="external nofollow noopener noreferrer">Horovod<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的发布帖子 来看看。</p>
<p></p>
<h4 id="525-百度思路">5.2.5 百度思路</h4>
<p>或者我们从百度的源码中也可以直接看到思路，现在摘录给大家。</p>
<p>具体代码参见 <a href="https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cm">/* Perform a ring allreduce on the data. Allocate the necessary output tensor and
</span></span></span><span class="line"><span class="cl"><span class="cm"> * store it in the output parameter.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Assumes that all MPI processes are doing an allreduce of the same tensor,
</span></span></span><span class="line"><span class="cl"><span class="cm"> * with the same dimensions.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * A ring allreduce is a bandwidth-optimal way to do an allreduce. To do the allreduce,
</span></span></span><span class="line"><span class="cl"><span class="cm"> * the nodes involved are arranged in a ring:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                   .--0--.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                  /       \
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                 3         1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                  \       /
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                   *--2--*
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Each node always sends to the next clockwise node in the ring, and receives
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  from the previous one.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  The allreduce is done in two parts: a scatter-reduce and an allgather. In
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  the scatter reduce, a reduction is done, so that each node ends up with a
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  chunk of the final output tensor which has contributions from all other
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  nodes.  In the allgather, those chunks are distributed among all the nodes,
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  so that all nodes have the entire output tensor.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Both of these operations are done by dividing the input tensor into N
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  evenly sized chunks (where N is the number of nodes in the ring).
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  The scatter-reduce is done in N-1 steps. In the ith step, node j will send
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  the (j - i)th chunk and receive the (j - i - 1)th chunk, adding it in to
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  its existing data for that chunk. For example, in the first iteration with
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  the ring depicted above, you will have the following transfers:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  In the second iteration, you&#39;ll have the following transfers:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  After this iteration, Node 2 has 3 of the four contributions to Segment 0.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  The last iteration has the following transfers:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  After this iteration, Node 3 has the fully accumulated Segment 0; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  has the fully accumulated Segment 1; and so on. The scatter-reduce is complete.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Next, the allgather distributes these fully accumululated chunks across all nodes.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Communication proceeds in the same ring, once again in N-1 steps. At the ith step,
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  node j will send chunk (j - i + 1) and receive chunk (j - i). For example, at the
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  first iteration, the following transfers will occur:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * After the first iteration, Node 0 will have a fully accumulated Segment 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> * (from Node 3) and Segment 1. In the next iteration, Node 0 will send its
</span></span></span><span class="line"><span class="cl"><span class="cm"> * just-received Segment 0 onward to Node 1, and receive Segment 3 from Node 3.
</span></span></span><span class="line"><span class="cl"><span class="cm"> * After this has continued for N - 1 iterations, all nodes will have a the fully
</span></span></span><span class="line"><span class="cl"><span class="cm"> * accumulated tensor.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Each node will do (N-1) sends for the scatter-reduce and (N-1) sends for the allgather.
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Each send will contain K / N bytes, if there are K bytes in the original tensor on every node.
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Thus, each node sends and receives 2K(N - 1)/N bytes of data, and the performance of the allreduce
</span></span></span><span class="line"><span class="cl"><span class="cm"> * (assuming no latency in connections) is constrained by the slowest interconnect between the nodes.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="53-区别">5.3 区别</h3>
<p><strong>在中等规模模型情况下，all-reduce 更适合。当规模巨大时候则应该使用参数服务器</strong>。</p>
<p>参数服务器 适合的是高维稀疏模型训练，它利用的是维度稀疏的特点，每次 pull or push 只更新有效的值。但是深度学习模型是典型的dense场景，embedding做的就是把稀疏变成稠密。所以这种 pull or push 的不太适合。而 网络通信上更优化的 all-reduce 适合中等规模的深度学习。</p>
<p>又比如由于推荐搜索领域模型的 Embedding 层规模庞大以及训练数据样本长度不固定等原因，导致容易出现显存不足和卡间同步时间耗费等问题，所以 all-reduce 架构很少被用于搜索推荐领域。</p>
<p>至此，背景知识已经介绍完毕，下一篇我们开始介绍 Horovod 的使用。</p>
<p>reference:
[1] <a href="https://www.cnblogs.com/rossiXYZ/p/14856464.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14856464.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>