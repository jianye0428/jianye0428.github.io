<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>DL - 分类 - yejian's blog</title><link>https://jianye0428.github.io/categories/dl/</link><description>DL - 分类 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Fri, 26 Apr 2024 17:15:15 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/categories/dl/" rel="self" type="application/rss+xml"/><item><title>Index</title><link>https://jianye0428.github.io/posts/dl_basics_one/</link><pubDate>Fri, 26 Apr 2024 17:15:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/dl_basics_one/</guid><description><![CDATA[<h3 id="1softmax如何防止指数上溢">1、⭐softmax如何防止指数上溢</h3>
<ul>
<li>原softmax公式：</li>
</ul>
<center>

</center>
<ul>
<li>工程化实现，防止指数上溢：</li>
</ul>
<center>

</center>
，使a等于x中最大值。
<h3 id="2transformer中的positional-encoding">2、⭐Transformer中的positional encoding</h3>
<ul>
<li>为什么需要PE: 因为transfomer是同时处理所有输入的，失去了位置信息。</li>
<li>编码应该满足的条件：a、对于每个位置词语，编码是唯一的 b、词语之间的间隔对于不同长度句子是一致的 c、能适应任意长度句子</li>
<li>公式：每个词语位置编码为不同频率的余弦函数，从1到1/10000。如下将每个词语位置编码为d维向量&ndash;&gt;</li>
</ul>
<center>

</center>
<center>

</center>
<p>可理解为一种二进制编码，二进制的不同位变化频率不一样，PE的不同位置变化频率也不一样</p>
<center>

</center>
<ul>
<li>如何获取相对位置关系：两个位置编码进行点积。</li>
</ul>
<center>

</center>
<h3 id="3求似然函数步骤">3、⭐求似然函数步骤</h3>
<ul>
<li>定义：概率是给定参数，求某个事件发生概率；似然则是给定已发生的事件，估计参数。</li>
</ul>
<ol>
<li>写出似然函数</li>
<li>对似然函数取对数并整理</li>
<li>求导数，导数为0处为最佳参数</li>
<li>解似然方程</li>
</ol>
<h3 id="4hmm和crf区别">4、⭐HMM和CRF区别</h3>
<ul>
<li>CRF是判别模型，对问题的条件概率分布建模；HMM是生成模型，对联合概率分布建模</li>
<li>HMM是概率有向图，CRF是概率无向图</li>
<li>HMM求解过程可能是局部最优，CRF是全局最优</li>
</ul>
<h3 id="5空洞卷积实现">5、⭐空洞卷积实现</h3>
<p>相比于常规卷积多了dilation rate超参数，例如dilation rate=2代表相邻两个卷积点距离为2，如图(b)。</p>
<center>

</center>
<ul>
<li>存在问题：gridding effect, 由于卷积的像素本质上是采样得到的，所以图像的局部相关性丢失了，同时远距离卷积得到的信息也没有相关性。</li>
</ul>
<h3 id="6汉明距离">6、⭐汉明距离</h3>
<p>两个字符串对应位置的不同字符的个数。</p>
<h3 id="7训练过程中发现loss快速增大应该从哪些方面考虑">7、⭐训练过程中发现loss快速增大应该从哪些方面考虑?</h3>
<ul>
<li>学习率过大</li>
<li>训练样本中有坏数据</li>
</ul>
<h3 id="8pytorch和tensorflow区别">8、⭐Pytorch和TensorFlow区别</h3>
<ul>
<li>图生成：pytorch动态图，tensorflow静态图</li>
<li>设备管理：pytorch cuda，tensorflow 自动化</li>
</ul>
<h3 id="9modeleval-vs和torchno_grad区别">9、⭐model.eval vs和torch.no_grad区别</h3>
<ul>
<li>model.eval: 依然计算梯度，但是不反传；dropout层保留概率为1；batchnorm层使用全局的mean和var</li>
<li>with torch.no_grad: 不计算梯度</li>
</ul>
<h3 id="10每个卷积层的flops计算">10、⭐每个卷积层的FLOPS计算</h3>
<center>

</center>
即计算feature map每个点需要的乘法和加法运算量，定义一个乘法和加法为一次flop，则FLOPS计算如下：
<center>

</center>
<h3 id="11pca主成分分析">11、⭐PCA(主成分分析)</h3>
<ul>
<li>⭐PCA是一种降维方法，用数据里面最主要的方面来代替原始数据，例如将$m$个$n$维数据降维到$n&rsquo;$维，希望这$m$个$n&rsquo;$维数据尽可能地代表原数据。</li>
<li>⭐两个原则：最近重构性&ndash;&gt;样本点到这个超平面的距离足够近；最大可分性&ndash;&gt;样本点在这个超平面的投影尽可能的分开。</li>
<li>⭐流程：基于特征值分解协方差矩阵和基于奇异值分解SVD分解协方差矩阵。</li>
</ul>
<p>（1）对所有样本进行中心化</p>
<p>（2）计算样本的协方差矩阵$XX^T$</p>
<p>（3）对协方差矩阵进行特征值分解</p>
<p>（4）取出最大的$n&rsquo;$个特征值对应的特征向量，将所有特征向量标准化，组成特征向量矩阵W</p>
<p>（5）对样本集中的每一个样本$x^(i)$转化为新的样本$z^(i)=W^T x^(i)$，即将每个原数据样本投影到特征向量组成的空间上</p>
<p>（6）得到输出的样本集$z^(1)、z^(2)&hellip;$</p>
<ul>
<li>⭐意义：a、使得结果容易理解 b、数据降维，降低算法计算开销 c、去除噪声</li>
</ul>
<h3 id="12k-means如何改进">12、⭐k-means如何改进？</h3>
<ul>
<li>⭐缺点：1、k个初始化的质心对最后的聚类效果有很大影响 2、对离群点和孤立点敏感 3、K值人为设定</li>
<li>⭐改进：
<ul>
<li>K-means++：从数据集随机选择一个点作为第一个聚类中心，对于数据集中每一个点计算和该中心的距离，选择下一个聚类中心，优先选择和上一个聚类中心距离较大的点。重复上述过程，得到k个聚类中心。</li>
<li>K-medoids：计算质心时，质心一定是某个样本值的点。距离度量：每个样本和所有其他样本的曼哈顿距离$((x,y), |x|+|y|)$。</li>
<li>ISODATA，又称为迭代自组织数据分析法，是为了解决K值需要人为设定的问题。核心思想：当属于某个类别的样本数过少时或者类别之间靠得非常近就将该类别去除；当属于某个类别的样本数过多时，把这个类别分为两个子类别。</li>
</ul>
</li>
<li>⭐和分类问题的区别：分类的类别已知，且需要监督；k-means是聚类问题，类别未知，不需要监督。</li>
<li>⭐终止条件：a、相邻两轮迭代过程中，非质心点所属簇发生改变的比例小于某个阈值 b、所有簇的质心均未改变 c、达到最大迭代次数</li>
<li>⭐时间复杂度：$O(迭代次数 \ast 数据个数 \ast k \ast 数据维度)$，k为k个类别中心</li>
<li>⭐空间复杂度：$O(数据个数 \ast 数据维度+k \ast 数据维度)$</li>
</ul>
<h3 id="13dropout实现">13、⭐Dropout实现</h3>
<center>

</center>
以p的概率使神经元失效，即使其激活函数输出值为0：
<center>

</center>
<p>为了使训练和测试阶段输出值期望相同，需要在训练时将输出值乘以1/(1-p)或者在测试时将权重值乘以(1-p)。</p>
<ul>
<li><strong>Dropout和Batch norm能否一起使用？</strong></li>
</ul>
<p>可以，但是只能将Dropout放在Batch norm之后使用。因为Dropout训练时会改变输入X的方差，从而影响Batch norm训练过程中统计的滑动方差值；而测试时没有Dropout，输入X的方差和训练时不一致，这就导致Batch norm测试时期望的方差和训练时统计的有偏差。</p>
<h3 id="14梯度消失和梯度爆炸">14、⭐梯度消失和梯度爆炸</h3>
<p><strong>梯度消失的原因和解决办法</strong></p>
<p>（1）隐藏层的层数过多</p>
<p>反向传播求梯度时的链式求导法则，某部分梯度小于1，则多层连乘后出现梯度消失</p>
<p>（2）采用了不合适的激活函数</p>
<p>如sigmoid函数的最大梯度为1/4，这意味着隐藏层每一层的梯度均小于1（权值小于1时），出现梯度消失。</p>
<p>解决方法：1、relu激活函数，使导数衡为1 2、batch norm 3、残差结构</p>
<p><strong>梯度爆炸的原因和解决办法</strong></p>
<p>（1）隐藏层的层数过多，某部分梯度大于1，则多层连乘后，梯度呈指数增长，产生梯度爆炸。</p>
<p>（2）权重初始值太大，求导时会乘上权重</p>
<p>解决方法：1、梯度裁剪 2、权重L1/L2正则化 3、残差结构 4、batch norm</p>
<h3 id="15yolov1-yolov4改进">15、⭐YOLOV1-YOLOV4改进</h3>
<p>⭐<strong>YOLOV1</strong>:</p>
<ul>
<li>one-stage开山之作，将图像分成S*S的单元格，根据物体中心是否落入某个单元格来决定哪个单元格来负责预测该物体，每个单元格预测两个框的坐标、存在物体的概率（和gt的IoU）、各类别条件概率。</li>
<li>损失函数：均采用均方误差。</li>
<li>优点：速度快。</li>
<li>缺点：1、每个单元格预测两个框，并且只取和gt IoU最大的框，相当于每个单元格只能预测一个物体，<strong>无法处理密集物体场景</strong>。2、输出层为<strong>全连接层</strong>，只能输入固定分辨率图片 3、<strong>计算IoU损失时，将大小物体同等对待</strong>，但同样的小误差，对大物体来说是微不足道的，而对小物体来说是很严重的，这会导致定位不准的问题。4、没有密集锚框、没有RPN，导致召回率低</li>
</ul>
<p><strong>⭐YOLOV2:</strong></p>
<ul>
<li>改进点：</li>
</ul>
<p>(1)、<strong>Batch normalization</strong>替代dropout，防止过拟合</p>
<p>(2)、<strong>去掉全连接层，使用类似RPN的全卷积层</strong></p>
<p>(3)、<strong>引入Anchor</strong>，并使用k-means聚类确定anchor大小、比例，提高了recall</p>
<p>(4)、高分辨率预训练backbone</p>
<p>(5)、<strong>限定预测框的中心点只能在cell内，具体通过预测相对于cell左上角点的偏移实现</strong>，这样网络收敛更稳定</p>
<p>(6)、添加passthrough层，相当于多尺度特征融合，$1 \ast 1$卷积将$26 \ast 26 \ast 512$ feature map降维成$26 \ast 26 \ast 64$, 然后将特征重组，拆分为4份$13 \ast 13 \ast 64$，concate得到$13 \ast 13 \ast 256$ feature map，和低分辨率的$13  \ast 13 \ast 1024$ feature map进行concate</p>
<p>(7)、提出Darknet进行特征提取，参数更少，速度更快</p>
<p>(8)、提出<strong>YOLO9000</strong>，建立层级分类的World Tree结构，可以进行细粒度分类</p>
<p><strong>⭐YOLOV3:</strong></p>
<p>(1)、<strong>使用sigmoid分类器替代softmax分类器</strong>，可以处理多标签分类问题</p>
<p>(2)、<strong>引入残差结构，进一步加深网络深度</strong></p>
<p>(3)、<strong>多尺度预测</strong>，每个尺度预测3个bbox</p>
<p><strong>⭐YOLOV4:</strong></p>
<p>(1)、<strong>Mosaic data augmentation</strong>：四张图片拼接成一张图片</p>
<p>(2)、<strong>DropBlock</strong>：drop out只丢弃单个像素，而因为二维图像中相邻特征强相关，所以丢弃后网络依然可以推断出丢失区域信息，导致过拟合；所以dropblock选择丢弃一块连续区域。</p>
<p>(3)、label smoothing</p>
<center>

</center>
<p>(4)、CIoU loss
CIoU = IoU + bbox中心距离/对角线距离+长宽比例之差</p>
<center>

</center>
<center>

</center>
<p>-1&lt;=CIoU&lt;=1</p>
<p>(5)、YOLO with SPP：就是用不同大小的卷积核对特征图进行卷积，得到不同感受野的特征，然后concate到一起。</p>
<h3 id="16ap计算">16、⭐AP计算</h3>
<p>⭐AP是对每一类先计算AP，再将所有类平均得到最终AP。
以COCO中AP计算为例。先选定用来判断TP和FP的IoU阈值，如0.5，则代表计算的是AP0.5，然后对每类做计算，例如对于class1:</p>
<div class="center">
<table>
<thead>
<tr>
<th style="text-align:left"><div style="width:300px"></th>
<th style="text-align:left">class1 <div style="width:300px"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">box1</td>
<td style="text-align:left">score1</td>
</tr>
<tr>
<td style="text-align:left">box2</td>
<td style="text-align:left">score2</td>
</tr>
<tr>
<td style="text-align:left">box3</td>
<td style="text-align:left">score3</td>
</tr>
</tbody>
</table>
</div>
<p>若box1与某gt的IoU大于指定阈值（如0.5)，记为Positive；若有多个bbox与gt的IoU大于指定阈值，选择其中score最大的记为Positive，其它记为Negative。可理解为确定box1对应的label。box2、box3同理。</p>
<p>而后要得到PR曲线，需要先对box1,2,3按score从高到低排序，假设排序结果如下：</p>
<div class="center">
<table>
<thead>
<tr>
<th style="text-align:left"><div style="width:200px"></th>
<th style="text-align:left">True class <div style="width:200px"></th>
<th style="text-align:left">class1 <div style="width:200px"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">box2</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score2</td>
</tr>
<tr>
<td style="text-align:left">box1</td>
<td style="text-align:left">Negative</td>
<td style="text-align:left">score1</td>
</tr>
<tr>
<td style="text-align:left">box3</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score3</td>
</tr>
</tbody>
</table>
</div>
<p>然后逐行计算score阈值为score2、score1、score3的Precision和Recall，score大于阈值的box才算做模型预测的Positive（TP+FP)。假设共有三个gt box，则计算结果如下：</p>
<div class="center">
<table>
<thead>
<tr>
<th style="text-align:left"><div style="width:60px"></th>
<th style="text-align:left">True class <div style="width:110px"></th>
<th style="text-align:left">class1 <div style="width:110px"></th>
<th style="text-align:left">Precision=TP/(TP+FP)</th>
<th style="text-align:left">Recall <div style="width:110px"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">box2</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score2</td>
<td style="text-align:left">1/1</td>
<td style="text-align:left">1/3</td>
</tr>
<tr>
<td style="text-align:left">box1</td>
<td style="text-align:left">Negative</td>
<td style="text-align:left">score1</td>
<td style="text-align:left">1/2</td>
<td style="text-align:left">1/3</td>
</tr>
<tr>
<td style="text-align:left">box3</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score3</td>
<td style="text-align:left">2/3</td>
<td style="text-align:left">2/3</td>
</tr>
</tbody>
</table>
</div>
<p>这样得到一个个PR曲线上的点，然后利用插值法计算PR曲线的面积，得到class1的AP。
具体插值方法：COCO中是均匀取101个recall值即0,0.01,0.02,&hellip;,1，对于每个recall值r，precision值取所有recall&gt;=r中的最大值$p_{interp(r)}$。</p>
<center>

</center>
<center>

</center>
<center>

</center>
<p>然后每个recall值区间（0-0.01，0.01-0.02，&hellip;）都对应一个矩形，将所有矩形面积加起来即为PR曲线面积，得到一个类别的AP，如下图所示。对所有类别（如COCO中80类）、所有IoU阈值（例如0.5:0.95）的AP取均值即得到最终AP。</p>
<center>

</center>
<p><strong>AR计算</strong>
计算过程同AP，也是在所有IoU阈值和类别上平均。
每给定一个IoU阈值和类别，得到一个P_R曲线，当P不为0时最大的Recall作为当前Recall。</p>
<h3 id="17iou变种合集">17、⭐IoU变种合集</h3>
<p><strong>IoU</strong></p>
<center>

</center>
<p><strong>GIoU</strong></p>
<center>

</center>
<p>$Ac$为bbox A和bbox B的最小凸集面积，U为A U B的面积，即第二项为不属于A和B的区域占最小闭包的比例。
-1&lt;=GIoU&lt;=1，当A和B不重合，仍可以计算损失，因此可作为损失函数。</p>
<center>

</center>
<p>✒️优点：在不重叠的情况下，能让预测框向gt框接近。</p>
<p>✒️缺点：遇到A框被B框包含的情况下，GIoU相同。</p>
<center>

</center>
<p><strong>DIoU</strong></p>
<center>

</center>
<p>其中， $b$和$b^{gt}$分别代表了预测框和真实框的中心点，且$ρ$代表的是计算两个中心点间的欧式距离。$c$代表的是能够同时包含预测框和真实框的最小闭包区域的对角线距离。</p>
<center>

</center>
<p>$-1 &lt; DIoU \leq 1$, 将目标与anchor之间的距离，重叠率以及尺度都考虑进去，使得目标框回归变得更加稳定</p>
<p>✒️优点：直接优化距离，解决GIoU包含情况。</p>
<center>

</center>
<p><strong>CIoU</strong></p>
<p>在DIoU的基础上考虑bbox的长宽比：</p>
<center>

</center>
<center>

</center>
<p>$-1 \leq CIoU \leq 1$</p>
<p>✒️优点：考虑bbox长宽比</p>
<h3 id="18depth-wise-separable-conv-深度可分离卷积">18、⭐depth-wise separable conv (深度可分离卷积)</h3>
<p>例如$5 \ast 5 \ast 3$图片要编码为$5 \ast 5 \ast 4$ feature map，则depth-wise conv分为两步：
先是depth wise卷积，利用3个$3 \ast 3$ conv对每个通道单独进行卷积，这一步只能改变feature map长宽，不能改变通道数：</p>
<center>

</center>
<p>参数量：</p>
<center>

</center>
<p>计算量：</p>
<center>

</center>
<p>然后是point wise卷积，利用4个$1 \ast 1 \ast 3$ conv对depth wise生成的feature map进行卷积，这一步只能改变通道数，不能改变feature map长宽：</p>
<center>

</center>
<p>参数量：</p>
<center>

</center>
<p>计算量：</p>
<center>

</center>
<p>所以与一般卷积对比，总参数量和计算量：</p>
<ul>
<li>总参数量：常规卷积&ndash;&gt;</li>
</ul>
<center>

</center>
<p>， 深度可分离卷积&ndash;&gt;</p>
<center>

</center>
<ul>
<li>总计算量：常规卷积&ndash;&gt;</li>
</ul>
<center>

</center>
<p>, 深度可分离卷积&ndash;&gt;</p>
<center>

</center>
<h3 id="19检测模型里为啥用smoothl1去回归bbox">19、⭐检测模型里为啥用smoothL1去回归bbox</h3>
<p>首先，对于L2 loss，其导数包含了(f(x)-Y)，所以当预测值与gt差异过大时，容易梯度爆炸；
而对于L1 loss，即使训练后期预测值和gt差异较小，梯度依然为常数，损失函数将在稳定值附近波动，难以收敛到更高精度。</p>
<center>

</center>
<p>所以SmoothL1 loss结合了两者的优势，当预测值和gt差异较大时使用L1 loss；差异较小时使用L2 loss：</p>
<center>

</center>
<h3 id="20deformable-conv如何实现梯度可微">20、⭐Deformable conv如何实现梯度可微？</h3>
<p>指的是对offset可微，因为offset后卷积核所在位置可能是小数，即不在feature map整数点上，所以无法求导；Deformable conv通过双线性插值实现了offset梯度可微。
用如下表达式表达常规CNN:</p>
<center>

</center>
<center>

</center>
<p>则Deformable conv可表达为：</p>
<center>

</center>
<p>$x(p_0+p_n+\Delta p_n)$可能不是整数，需要进行插值，任意点p（周围四个整数点为q）的双线性插值可表达为下式：</p>
<center>

</center>
<center>

</center>
<p>其中$g(a, b) = max(0, 1 − |a − b|)$。</p>
<p>则offest delta_pn求导公式为：</p>
<center>

</center>
<p>从而实现对offset的梯度可微。</p>
<h3 id="21swin-transformer">21、⭐Swin Transformer</h3>
<p>(1)⭐<strong>motivation</strong>
高分辨率图像作为序列输入计算量过大问题；和nlp不同，cv里每个物体的尺度不同，而nlp里每个物体的单词长度都差不多。</p>
<p>(2)⭐<strong>idea</strong>
问题一：一张图分成多个窗口，每个窗口分成多个patch，每个窗口内的多个patch相互计算自注意力；问题二：模仿CNN pooling操作，将浅层尺度patch进行path merging得到一个小的patch，实现降采样，从而得到多尺度特征。</p>
<p>(3)⭐<strong>method</strong>
整体结构很像CNN，卷积被窗口自注意力代替，pooling被patch merging代替。</p>
<center>

</center>
<ul>
<li>✒️method 1: shifted window</li>
</ul>
<center>

</center>
<p>目的是让不重叠窗口区域也能有交互，操作本质是将所有窗口往右下角移动2个patch。</p>
<p>窗口数从4个变成了9个，计算量增大，为减小计算量，使用cyclic shift，将窗口拼接成4个窗口，另外因为拼接后A、B、C相较于原图发生了相对位置变化，所以A、B、C和其他区域是不可以进行交互的，因此引入了Mask操作。</p>
<center>

</center>
<p>Mask操作：</p>
<p>以3、6所在窗口的自注意力计算为例，将7*7=49个像素展平得到长度为49的一维向量，做矩阵乘法即Q*K。</p>
<center>

</center>
<p>又因为3和6是不可以交互的，所以矩阵左下角和右上角应该被mask掉，Swin采用的方法是加上左下角和右上角为-100，其余位置为0的模板，这样得到的attention矩阵在softmax后就变成0了。</p>
<center>

</center>
<ul>
<li>✒️method2: patch merging
就是间隔采样，然后在通道维度上拼接</li>
</ul>
<center>

</center>
<p>(4)⭐<strong>SwinTransformer位置编码实现</strong></p>
<p><strong>👉核心思想就是建了一个相对位置编码embedding字典，使得相同的相对位置编码成相同的embedding。👈</strong></p>
<p>例如2*2的patch之间的相对位置关系矩阵为2*2*2，相对位置范围为[-1,1]：</p>
<center>

</center>
<p>则x，y相对位置关系可用3*3 (-1,0,1三种相对位置)的table存储所有可能的相对位置关系，并用3*3*embed_n表示所有相对位置对应的embedding。<strong>为了使得索引为整数</strong>，需要将所有相对位置变为正值：</p>
<center>

</center>
<p>可以通过简单的x,y相对位置相加将相对位置映射为一维，但是会出现(0,2)和(2,0)无法区分的问题，所以需要使得x,y相对位置编码不同：</p>
<center>

</center>
<p>然后将x和y相对位置相加：</p>
<center>

</center>
<p>从而每个相对位置对应一个一维的值，作为相对位置embedding table的索引，获取对应位置的embedding。</p>
<h3 id="22yolox核心改进">22、⭐YOLOX核心改进：</h3>
<center>

</center>
<p>(1)✒️Decoupled head：就是anchor free方法里最常用的cls head和reg head</p>
<p>(2)✒️Anchor-free: 即类似FCOS，不同的是预测的是中心点相对于grid左上角坐标的offset值，以及bbox的长宽，将物体中心的某个区域内的点定义为正样本，并且每个尺度预测不同大小物体。</p>
<p>(3)✒️Label assignment(SimOTA): 将prediction和gt的匹配过程建模为运输问题，使得cost最小。</p>
<ul>
<li>cost表示：$pred_i$和$gt_j$的cls和reg loss。</li>
<li>对每个gt，选择落在指定中心区域的top-k least cost predictions当作正样本，每个gt的k值不同。</li>
<li>最佳正锚点数量估计：某个gt的适当正锚点数量应该与该gt回归良好的锚点数量正相关，所以对于每个gt，我们根据IoU值选择前q个预测。这些IoU值相加以表示此gt的正锚点估计数量。</li>
</ul>
<h3 id="23l1l2正则化的区别">23、⭐L1、L2正则化的区别</h3>
<center>

</center>
<p>✒️L1正则化容易得到稀疏解，即稀疏权值矩阵，L2正则化容易得到平滑解（权值很小）。</p>
<p>✒️原因：a、解空间角度：二维情况，L1正则化：||w1||+||w2||，则函数图像为图中方框，显然方框的角点容易是最优解，而这些最优解都在坐标轴上，权值为0.</p>
<center>

</center>
<p>b、梯度下降角度
添加正则项 $\lambda \theta^2_j$，则L对$\theta_j$的导数为$2\lambda \theta_j$，梯度更新时$\theta_j=\theta_j-2 \lambda \theta_j=(1-2 \lambda) \theta_j$，相当于每次都会乘以一个小于1的数，使得$\theta_j$越来越小。</p>
<h3 id="24深度学习花式归一化之batchlayerinstancegroup-normalization">24、⭐深度学习花式归一化之Batch/Layer/Instance/Group Normalization</h3>
<p><strong>✒️Batch Normalization</strong></p>
<ul>
<li>⭐<strong>核心过程</strong>：顾名思义，对一个batch内的数据计算均值和方差，将数据归一化为均值为0、方差为1的正态分布数据，最后用对数据进行缩放和偏移来还原数据本身的分布，如下图所示。</li>
</ul>
<center>

</center>
<ul>
<li><strong>Batch Norm 1d</strong>
输入是b*c, 输出是b*c，即在每个维度上进行normalization。</li>
<li><strong>Batch Norm 2d</strong>
例如输入是b*c*h*w，则计算normlization时是对每个通道，求b<em>h</em>w内的像素求均值和方差，输出是1*c*1*1。</li>
</ul>
<center>

</center>
<center>

</center>
<ul>
<li><strong>BN测试时和训练时不同，测试时使用的是全局训练数据的滑动平均的均值和方差。</strong></li>
<li>作用：a、防止过拟合 b、加速网络的收敛，internal covariate shift导致上层网络需要不断适应底层网络带来的分布变化 c、缓解梯度爆炸和梯度消失</li>
<li>局限：依赖于batch size，适用于batch size较大的情况</li>
</ul>
<p><strong>✒️改进：</strong></p>
<ul>
<li>Layer normalization: 对每个样本的所有特征进行归一化，如N*C*H*W，对每个C*H*W进行归一化，得到N个均值和方差。</li>
<li>Instance normalization: 对每个样本的每个通道特征进行归一化，如N*C*H*W，对每个H*W进行归一化，得到N*C个均值和方差。</li>
<li>Group normalization：每个样本按通道分组进行归一化，如N*C*H*W，对每个C*H*W，在C维度上分成g组，则共有N*g个均值和方差。</li>
</ul>
<center>

</center>
<h3 id="25深度学习常用优化器介绍">25、⭐深度学习常用优化器介绍</h3>
<p>参考https://zhuanlan.zhihu.com/p/261695487，修正了其中的一些错误。</p>
<p>(1)<strong>⭐SGD</strong></p>
<p>a. ✒️<strong>公式</strong></p>
<center>

</center>
<p>其中$\alpha$是学习率，$g_t$是当前参数的梯度。</p>
<p>b. ✒️<strong>优点</strong>：每次只用一个样本更新模型参数，训练速度快。</p>
<p>c. ✒️<strong>缺点</strong>：容易陷入局部最优；沿陡峭方向振荡，而沿平缓维度进展缓慢，难以迅速收敛</p>
<p>(2) ⭐<strong>SGD with momentum</strong></p>
<p>a. ✒️<strong>公式</strong></p>
<center>

</center>
<p>其中$m_t$为动量。</p>
<p>b. ✒️<strong>优点</strong>：可借助动量跳出局部最优点。</p>
<p>c. ✒️<strong>缺点</strong>：容易在局部最优点里来回振荡。</p>
<p>(3) ⭐<strong>AdaGrad</strong>：经常更新的参数已经收敛得比较好了，应该减少对它的关注，即降低其学习率；对于不常更新的参数，模型学习的信息过少，应该增加对它的关注，即增大其学习率。</p>
<p>a. ✒️<strong>公式</strong>
$$w_{t+1}=w_t-\alpha \cdot g_t / \sqrt{V_t}=w_t-\alpha \cdot g_t / \sqrt{\sum_{\tau=1}^t g_\tau^2}$$
其中Vt是二阶动量，为累计梯度值的平方和，与参数更新频繁程度成正比。</p>
<p>b. ✒️<strong>优点</strong>：稀疏数据场景下表现好；自适应调节学习率。</p>
<p>c. ✒️<strong>缺点</strong>：Vt单调递增，使得学习率单调递减为0，从而使得训练过程过早结束。</p>
<p>(4) ⭐<strong>RMSProp</strong>：AdaGrad的改进版，不累计所有历史梯度，而是过去一段时间窗口内的历史梯度。</p>
<p>a. ✒️<strong>公式</strong>
$$\begin{aligned} w_{t+1} &amp; =w_t-\alpha \cdot g_t / \sqrt{V_t} \ &amp; =w_t-\alpha \cdot g_t / \sqrt{\beta_2 \cdot V_{t-1}+\left(1-\beta_2\right) g_t^2}\end{aligned}$$
即把Vt换成指数移动平均。</p>
<p>b. ✒️<strong>优点</strong>：避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p>
<p>(5) ⭐<strong>Adam</strong>：=Adaptive + momentum，即结合了momentum一阶动量+RMSProp二阶动量。</p>
<p>a. ✒️<strong>公式</strong>
$$\begin{aligned} w_{t+1} &amp; =w_t-\alpha \cdot m_t / \sqrt{V_t} \ &amp; =w_t-\alpha \cdot\left(\beta_1 \cdot m_{t-1}+\left(1-\beta_1\right) \cdot g_t\right) / \sqrt{\beta_2 \cdot V_{t-1}+\left(1-\beta_2\right) g_t^2}\end{aligned}$$</p>
<p>b. ✒️<strong>优点</strong>：通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和在鞍点的静止。</p>
<p>c. ✒️<strong>缺点</strong>：二阶动量是固定历史时间窗口的累积，窗口的变化可能导致Vt振荡，而不是单调变化，从而导致训练后期学习率的振荡，模型不收敛，可通过</p>
<center>

</center>
来修正，保证学习率单调递减；自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果，从而错过全局最优解。
<hr>
<p>整理这篇文章不易，喜欢的话可以关注我&ndash;&gt;<strong>无名氏，某乎和小红薯同名，WX：无名氏的胡言乱语。</strong> 定期分享算法笔试、面试干货。</p>
]]></description></item><item><title>长短期记忆网络 -- LSTM</title><link>https://jianye0428.github.io/posts/lstm/</link><pubDate>Thu, 28 Dec 2023 21:50:25 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/lstm/</guid><description><![CDATA[<h2 id="一传统的bp网络和cnn网络">一、传统的BP网络和CNN网络</h2>
<p>BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。
<br></p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9;
  display:  inline-block;
  color:  #999;
  padding:  2px;">BP Network</div>
</center>
<br>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9;
  display:  inline-block;
  color:  #999;
  padding:  2px;">CNN Network</div>
</center>
<br>
<p>图中的隐含层、卷积层、池化层、全连接层等，都是实际存在的，一层层前后叠加，在空间上很好理解，因此在写代码的时候，基本就是看图写代码，比如用keras就是:</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 示例代码，没有实际意义</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>  <span class="c1"># 添加卷积层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>         <span class="c1"># 添加池化层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>                          <span class="c1"># 添加dropout层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>  <span class="c1"># 添加卷积层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>         <span class="c1"># 添加池化层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>                          <span class="c1"># 添加dropout层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">....</span>   <span class="c1"># 添加其他卷积操作</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>                            <span class="c1"># 拉平三维数组为2维数组</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>        <span class="n">添加普通的全连接层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">....</span>  <span class="c1"># 训练网络</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="二lstm网络">二、LSTM网络</h2>
<p>RNN网络的机构图如下图所示:</p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9; display:  inline-block; color:  #999; padding:  2px;">2.1 RNN Architecture Overview</div>
</center>
<br>
<p><strong><font color=purple>RNN 面临的问题:短时记忆和梯度消失/梯度爆炸</font></strong></p>
<ul>
<li>
<p><font color=red>短时记忆</font>
<strong>问题描述</strong>：RNN在处理长序列时，由于信息的传递是通过隐藏状态进行的，随着时间的推移，较早时间步的信息可能会在传递到后面的时间步时逐渐消失或被覆盖。</p>
<p><strong>影响</strong>：这导致RNN难以捕捉和利用序列中的长期依赖关系，从而限制了其在处理复杂任务时的性能。</p>
</li>
<li>
<p><font color=red>梯度消失/梯度爆炸</font>
<strong>问题描述</strong>：在RNN的反向传播过程中，梯度会随着时间步的推移而逐渐消失（变得非常小）或爆炸（变得非常大）。</p>
<p><strong>影响</strong>：梯度消失使得RNN在训练时难以学习到长期依赖关系，因为较早时间步的梯度信息在反向传播到初始层时几乎为零。梯度爆炸则可能导致训练过程不稳定，权重更新过大，甚至导致数值溢出。</p>
</li>
</ul>
<p><strong><font color=blue>LSTM解决问题:</font></strong>
大脑和LSTM在处理信息时都选择性地保留重要信息，忽略不相关细节，并据此进行后续处理。这种机制使它们能够高效地处理和输出关键信息，解决了RNN（递归神经网络）在处理长序列时面临的问题。</p>
<table><body text=red><tr><td style="text-align:center;font-weight:bold" bgcolor=yellow><font size="3" color="red">大脑记忆机制</font></td></tr></body></table>
<ul>
<li>
<p>大脑记忆机制：当浏览评论时，大脑倾向于记住重要的关键词。无关紧要的词汇和内容容易被忽略。回忆时，大脑提取并表达主要观点，忽略细节。</p>
</li>
<li>
<p>LSTM门控机制：LSTM通过输入门、遗忘门和输出门选择性地保留或忘记信息，使用保留的相关信息来进行预测，类似于大脑提取并表达主要观点。</p>
</li>
</ul>
<p>图2.1是RNN循环神经网络经典的结构图，LSTM只是对隐含层节点A做了改进，整体结构不变，因此本文讨论的也是这个结构的可视化问题。</p>
<p>中间的A节点隐含层，左边是表示只有一层隐含层的LSTM网络，所谓LSTM循环神经网络就是在时间轴上的循环利用，在时间轴上展开后得到右图。</p>
<p><strong>上图右边，我们看Xt表示序列，下标t是时间轴，所以，A的数量表示的是时间轴的长度，是同一个神经元在不同时刻的状态(Ht)，不是隐含层神经元个数。</strong></p>
<p>我们知道，LSTM网络在训练时会使用上一时刻的信息，加上本次时刻的输入信息来共同训练。</p>
<p>举个简单的例子: 在第一天我生病了(初始状态H0)，然后吃药(利用输入信息X1训练网络)，第二天好转但是没有完全好(H1)，再吃药(X2),病情得到好转(H2),如此循环往复知道病情好转。因此，输入Xt是吃药，时间轴T是吃多天的药，隐含层状态是病情状况。因此我还是我，只是不同状态的我。</p>
<p>实际上，LSTM的网络是这样的:</p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9; display:  inline-block; color:  #999; padding:  2px;">LSTM Network</div>
</center>
<p>上面的图表示包含2个隐含层的LSTM网络，在T=1时刻看，它是一个普通的BP网络，在T=2时刻看也是一个普通的BP网络，只是沿时间轴展开后，T=1训练的隐含层信息H,C会被传递到下一个时刻T=2，如下图所示。上图中向右的五个常常的箭头，所指的也是隐含层状态在时间轴上的传递。</p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9; display:  inline-block; color:  #999; padding:  2px;">LSTM Architecture Overview</div>
</center>
<p>注意，图中H表示隐藏层状态，C是遗忘门，后面会讲解它们的维度。</p>
<h3 id="21-lstm的原理">2.1 LSTM的原理</h3>
<table><body text=red><tr><td style="text-align:left;font-weight:bold" bgcolor=yellow><font size="3" color="red">RNN工作原理:第一个词被转换成了机器可读的向量，然后 RNN 逐个处理向量序列。</font></td></tr></body></table>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">RNN 工作原理</div>
</center>
<br>
<ul>
<li>
<p><strong><font color=purple>隐藏状态的传递</font></strong>
过程描述：在处理序列数据时，RNN将前一时间步的隐藏状态传递给下一个时间步。</p>
<p>作用：隐藏状态充当了神经网络的“记忆”，它包含了网络之前所见过的数据的相关信息。</p>
<p>重要性：这种传递机制使得RNN能够捕捉序列中的时序依赖关系。</p>
</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">将隐藏状态传递给下一个时间步</div>
</center>
<br>
<ul>
<li>
<p><strong><font color=purple>隐藏状态的计算</font></strong>
细胞结构：RNN的一个细胞接收当前时间步的输入和前一时间步的隐藏状态。</p>
<p>组合方式：当前输入和先前隐藏状态被组合成一个向量，这个向量融合了当前和先前的信息。</p>
<p>激活函数：组合后的向量经过一个tanh激活函数的处理，输出新的隐藏状态。这个新的隐藏状态既包含了当前输入的信息，也包含了之前所有输入的历史信息。</p>
</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>**<font color=red>输出:</font>**新的隐藏状态被输出，并被传递给下一个时间步，继续参与序列的处理过程。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">RNN的细胞结构和运算</div>
</center>
<br>
<table><body text=red><tr><td style="text-align:center;font-weight:bold" bgcolor=yellow><font size="3" color="fuchsia">LSTM工作原理：</font></td></tr></body></table>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">LSTM的细胞结构和运算</div>
</center>
<br>
<ul>
<li>
<p><strong><font color=red>输入门:</font></strong>
作用：决定哪些新信息应该被添加到记忆单元中。</p>
<p>组成：输入门由一个sigmoid激活函数和一个tanh激活函数组成。sigmoid函数决定哪些信息是重要的，而tanh函数则生成新的候选信息。</p>
<p>运算：输入门的输出与候选信息相乘，得到的结果将在记忆单元更新时被考虑。</p>
</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">输入门（sigmoid激活函数 + tanh激活函数）</div>
</center>
<br>
<ul>
<li>
<p><strong><font color=red>遗忘门:</font></strong>
作用：决定哪些旧信息应该从记忆单元中遗忘或移除。</p>
<p>组成：遗忘门仅由一个sigmoid激活函数组成。</p>
</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">sigmoid激活函数（区间0～1）</div>
</center>
<br>
<p>运算：sigmoid函数的输出直接与记忆单元的当前状态相乘，用于决定哪些信息应该被保留，哪些应该被遗忘。输出值越接近1的信息将被保留，而输出值越接近0的信息将被遗忘。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">遗忘门（sigmoid激活函数）</div>
</center>
<br>
<ul>
<li>
<p><strong><font color=red>输出门:</font></strong></p>
<p>作用：决定记忆单元中的哪些信息应该被输出到当前时间步的隐藏状态中。</p>
<p>组成：输出门同样由一个sigmoid激活函数和一个tanh激活函数组成。sigmoid函数决定哪些信息应该被输出，而tanh函数则处理记忆单元的状态以准备输出。</p>
<p>运算：sigmoid函数的输出与经过tanh函数处理的记忆单元状态相乘，得到的结果即为当前时间步的隐藏状态。</p>
</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">输出门(sigmoid激活函数 + tanh激活函数)</div>
</center>
<br>
<h2 id="三lstm的输入结构">三、LSTM的输入结构</h2>
<p>为了更好理解LSTM结构，还必须理解LSTM的数据输入情况。仿照3通道图像的样子，再加上时间轴后的多样本的多特征的不同时刻的数据立方体如下图所示:</p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9; display:  inline-block; color:  #999; padding:  2px;">Input Structure of LSTM Network</div>
</center>
<p>右边的图是我们常见模型的输入，比如XGBOOST，lightGBM，决策树等模型，输入的数据格式都是这种(N<em>F)的矩阵，而左边是加上时间轴后的数据立方体，也就是时间轴上的切片，它的维度是(N</em>T*F),第一维度是样本数，第二维度是时间，第三维度是特征数，如下图所示:</p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9; display:  inline-block; color:  #999; padding:  2px;">天气数据立方体</div>
</center>
<p>这样的数据立方体很多，比如天气预报数据，把样本理解成城市，时间轴是日期，特征是天气相关的降雨风速PM2.5等，这个数据立方体就很好理解了。在NLP里面，一句话会被embedding成一个矩阵，词与词的顺序是时间轴T，索引多个句子的embedding三维矩阵如下图所示:</p>
<center>
  
  <br>
  <div style="color: orange; border-bottom:  1px solid #d9d9d9; display:  inline-block; color:  #999; padding:  2px;">NLP Embedding Matrix</div>
</center>
<h2 id="四pytorch中的lstm">四、Pytorch中的LSTM</h2>
<h3 id="41-pytorch中定义的lstm模型">4.1 Pytorch中定义的LSTM模型</h3>
<p>pytorch中定义的LSTM模型的参数如下</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">参数有</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">input_size</span><span class="p">:</span> <span class="n">x的特征维度</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="n">隐藏层的特征维度</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">num_layers</span><span class="p">:</span> <span class="n">lstm隐层的层数</span><span class="err">，</span><span class="n">默认为1</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">bias</span><span class="p">:</span> <span class="n">False则bihbih</span><span class="o">=</span><span class="mi">0</span><span class="n">和bhhbhh</span><span class="o">=</span><span class="mf">0.</span> <span class="n">默认为True</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">batch_first</span><span class="p">:</span> <span class="n">True则输入输出的数据格式为</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">dropout</span><span class="p">:</span> <span class="n">除最后一层</span><span class="err">，</span><span class="n">每一层的输出都进行dropout</span><span class="err">，</span><span class="n">默认为</span><span class="p">:</span>  <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">bidirectional</span><span class="p">:</span> <span class="n">True则为双向lstm默认为False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>结合前面的图形，我们一个个看。</p>
<p>(1)input_size: x的特征维度，就是数据立方体中的F，在NLP中就是一个词被embedding后的向量长度，如下图所示:</p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">LSTM Feature Matrix</div>
</center>
<p>(2)hidden_size: 隐藏层的特征维度(隐藏层神经元个数)，如下图所示，我们有两个隐含层，每个隐藏层的特征维度都是5。注意，<strong>非双向LSTM的输出维度等于隐藏层的特征维度</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">隐藏层特征维度</div>
</center>
<br>
<p>(3)num_layers: lstm隐层的层数，上面的图我们定义了2个隐藏层。
(4)batch_first: 用于定义输入输出维度，后面再讲。
(5)bidirectional: 是否是双向循环神经网络，如下图是一个双向循环神经网络，因此在使用双向LSTM的时候我需要特别注意，正向传播的时候有(Ht, Ct),反向传播也有(Ht&rsquo;, Ct&rsquo;),前面我们说了非双向LSTM的输出维度等于隐藏层的特征维度，而<strong>双向LSTM的输出维度是隐含层特征数<em>2，而且H,C的维度是时间轴长度</em>2</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Bidirectional RNN</div>
</center>
<br>
<h3 id="42-喂给lstm的数据格式">4.2 喂给LSTM的数据格式</h3>
<p>pytorch中LSTM的输入数据格式默认如下:</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">参数有</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">seq_len</span><span class="p">:</span> <span class="n">序列长度</span><span class="err">，</span><span class="n">在NLP中就是句子长度</span><span class="err">，</span><span class="n">一般都会用pad_sequence补齐长度</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">batch</span><span class="p">:</span> <span class="n">每次喂给网络的数据条数</span><span class="err">，</span><span class="n">在NLP中就是一次喂给网络多少个句子</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">input_size</span><span class="p">:</span> <span class="n">特征维度</span><span class="err">，</span><span class="n">和前面定义网络结构的input_size一致</span><span class="err">。</span></span></span></code></pre></td></tr></table>
</div>
</div><p>前面也说到，如果LSTM的参数 batch_first=True，则要求输入的格式是:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>刚好调换前面两个参数的位置。其实这是比较好理解的数据形式，下面以NLP中的embedding向量说明如何构造LSTM的输入。</p>
<p>之前我们的embedding矩阵如下图:
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Embedding Matrix</div>
</center>
<br>
<p>如果把batch放在第一位，则三维矩阵的形式如下:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Batch First</div>
</center>
<br>
<p>其转换过程如下图所示:
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">将三维矩阵转换成二维形式</div>
</center>
<br>
<p>看懂了吗，这就是输入数据的格式，是不是很简单。
LSTM的另外两个输入是 h0 和 c0，可以理解成网络的初始化参数，用随机数生成即可。</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">h0</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c0</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">参数</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">num_layers</span><span class="p">:</span> <span class="n">隐藏层数</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">num_directions</span><span class="p">:</span> <span class="n">如果是单向循环网络</span><span class="err">，</span><span class="n">则num_directions</span><span class="o">=</span><span class="mi">1</span><span class="err">，</span><span class="n">双向则num_directions</span><span class="o">=</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">batch</span><span class="p">:</span> <span class="n">输入数据的batch</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="n">隐藏层神经元个数</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意，如果我们定义的input格式是:</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>则H和C的格式也是要变的:</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">h0</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c0</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-lstm的output格式">4.3 LSTM的output格式</h3>
<p>LSTM的输出是一个tuple，如下:</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output</span><span class="p">,(</span><span class="n">ht</span><span class="p">,</span> <span class="n">ct</span><span class="p">)</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">output</span><span class="p">:</span>  <span class="n">最后一个状态的隐藏层的神经元输出</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">ht</span><span class="p">:</span> <span class="n">最后一个状态的隐含层的状态值</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">ct</span><span class="p">:</span> <span class="n">最后一个状态的隐含层的遗忘门值</span></span></span></code></pre></td></tr></table>
</div>
</div><p>output的默认维度是:</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ht</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ct</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和input的情况类似，如果我们前面定义的input格式是:</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>则ht和ct的格式也是要变的:</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ht</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ct</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>说了这么多，我们回过头来看看ht和ct在哪里，请看下图:
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">LSTM Network</div>
</center>
<br>
<p>output在哪里？请看下图:
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">输出层</div>
</center>
<br>
<h2 id="五lstm和其他网络组合">五、LSTM和其他网络组合</h2>
<p>还记得吗，output的维度等于隐藏层神经元的个数，即hidden_size，在一些时间序列的预测中，会在output后，接上一个全连接层，全连接层的输入维度等于LSTM的hidden_size，之后的网络处理就和BP网络相同了，如下图:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">在LSTM网络后接上全连接层</div>
</center>
<br>
<p>用pytorch实现上面的结构:</p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RegLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">RegLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 定义LSTM</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_num_layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 定义回归层网络，输入的特征维度等于LSTM的输出，输出维度为1</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">ht</span><span class="p">,</span><span class="n">ct</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span></span></span></code></pre></td></tr></table>
</div>
</div><p>当然，有些模型则是将输出当做另一个LSTM的输入，或者使用隐藏层ht,ct的信息进行建模，不一而足。
好了，以上就是我对LSTM的一些学习心得，看完记得关注点赞。</p>
<p>REF:
[[1]. 漂亮，LSTM模型结构的可视化](https: //mp.weixin.qq.com/s?__biz=MzU1OTYzNjg5OQ==&amp;mid=2247545117&amp;idx=1&amp;sn=670ba155d94b229d39c5bf0bf20239d5&amp;chksm=fc1639d1cb61b0c72434a00454b2af8f9022e7ac3030a4186cda22ef5594ef5994620dc5fd52&amp;mpshare=1&amp;scene=1&amp;srcid=0617kfSozC3sKY1lRjYg1f0u&amp;sharer_shareinfo=6833fdea9df7fee2c423a9474c0928be&amp;sharer_shareinfo_first=6833fdea9df7fee2c423a9474c0928be#rd)
[2].https: //zhuanlan.zhihu.com/p/94757947
[3].https: //zhuanlan.zhihu.com/p/59862381
[4].https: //zhuanlan.zhihu.com/p/36455374
[5].https: //www.zhihu.com/question/41949741/answer/318771336
[6].https: //blog.csdn.net/android_ruben/article/details/80206792
to be added: <br>
[7].https: //www.analyticsvidhya.com/blog/2021/01/understanding-architecture-of-lstm/
[8]. <a href="https://mp.weixin.qq.com/s/p1jmj__DQIwDMDTCExHs5Q"target="_blank" rel="external nofollow noopener noreferrer">神经网络算法 - 一文搞懂LSTM(长短期记忆网络)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>