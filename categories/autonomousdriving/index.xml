<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>AutonomousDriving - 分类 - yejian's blog</title><link>https://jianye0428.github.io/categories/autonomousdriving/</link><description>AutonomousDriving - 分类 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Fri, 05 Jan 2024 17:34:44 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/categories/autonomousdriving/" rel="self" type="application/rss+xml"/><item><title>Argoverse 2 数据集</title><link>https://jianye0428.github.io/posts/argoverse2/</link><pubDate>Fri, 05 Jan 2024 17:34:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/argoverse2/</guid><description><![CDATA[<h2 id="一简介">一、简介</h2>
<p>Argoverse数据集是由Argo AI、卡内基梅隆大学、佐治亚理工学院发布的用于支持自动驾驶汽车3D Tracking和Motion Forecasting研究的数据集。
数据集包括:</p>
<ul>
<li><strong>带标注的传感器数据集:</strong> 含1000个多模态数据序列，包括来自七个环视摄像机和两个双目摄像机的高分辨率图像，以及激光雷达点云和6自由度地图配准位姿。序列包含26个目标类别的三维长方体标注，所有这些标注都是充分采样的，以支持训练和三维感知模型的评估。</li>
<li><strong>激光雷达数据集:</strong> 包含20,000个未标记的激光雷达点云序列和地图配准位姿。该数据集是有史以来最大的激光雷达传感器数据集合，支持自监督学习和新兴的点云预测任务。</li>
<li><strong>运动预测数据集:</strong> 包含250,000个场景，挖掘每个场景中自车与其他参与者之间有趣和具有挑战性的交互。模型的任务是预测每个场景中scored actors的未来运动，并提供跟踪历史，捕捉目标的位置、航向、速度和类别。</li>
</ul>
<p>在所有三个数据集中，每个场景都包含自己的高精地图，带有3D车道和人行横道几何形状&ndash;来自六个不同城市的数据。所有数据集都是在CC BY-NC-SA 4.0许可下发布的。</p>
<h2 id="二argoverse-2-datasets">二、Argoverse 2 Datasets</h2>
<h3 id="21-sensor-dataset-传感器数据集">2.1 Sensor Dataset 传感器数据集</h3>
<p>Argoverse 2传感器数据集是Argoverse 1 3D跟踪数据集的后续。AV2更大，有1000个场景，高于Argoverse 1中的113个，但每个AV2场景也更丰富&ndash;AV2中有23倍的非车辆、非行人长方体。作者手工选择Argoverse 2传感器数据集中的30s组成场景，以包含拥挤的场景，其中包含未被表示的对象、值得注意的天气和有趣的行为，如插队和乱穿马路。每个场景的持续时间为15秒。表1将AV2传感器数据集与自动驾驶数据集进行了比较。图1、2和3显示了AV2的场景在标注范围、目标多样性、目标密度和场景动态性方面如何优于其他数据集。</p>
<p>与本文最相似的传感器数据集是非常有影响力的nuScenes[4]&ndash;这两个数据集都有1000个场景和高清地图，尽管Argoverse在拥有地面高度地图方面是独一无二的。nuScenes包含毫米波雷达数据，而AV2包含双目图像。nuScenes有一个很大的分类学&ndash;23个目标类别，其中10个有适合训练和评估（evaluation）的数据。本文的数据集包含30个目标类别，其中26个被很好地采样，足以用于训练和评估。nuScenes横跨两个城市，而本文的提出的数据集横跨六个城市。</p>
<p><strong>传感器套件。</strong> 激光雷达扫描收集在10赫兹，以及20 fps图像从7个摄像头定位，以提供一个完整的全景视野。此外，还提供了全局坐标系下的摄像机内参、外参和6自由度 ego-vehicle 姿态。激光雷达回波由两个32波束激光雷达捕获，激光雷达在同一方向以10赫兹旋转，但在方向上相隔180°。摄像机触发与两个激光雷达同步，导致20赫兹的帧率。七个全局快门摄像机与激光雷达同步，使它们的曝光集中在激光雷达上，扫描它们的视野。在附录中，本文提供了一个示意图，说明了汽车传感器套件及其坐标框架。</p>
<p><strong>激光雷达同步精度。</strong> 在AV2中，本文改进了摄像机和激光雷达的同步比Argoverse 1明显。本文的同步精度在[-1.39,1.39]ms，与Waymo开放数据集[-6,7]ms[45]相比较。</p>
<p><strong>标注。</strong> AV2传感器数据集包含本文30个类分类法中的对象的10 Hz 3D长方体标注（图1）。长方体的轨道标识符随着时间的推移对于相同的目标实例是一致的。如果对象在“感兴趣区域”(ROI)内&ndash;在映射的“可驾驶”区域的五米内，则对其进行标注。</p>
<p><strong>隐私。</strong> 为了保护隐私，所有的脸和车牌，无论是在车辆内还是在可驾驶区域外，都被广泛模糊。</p>
<p>传感器数据集分割。 本文随机地将数据集划分为700、150和150个场景的训练、验证和测试拆分。</p>
<h3 id="22-lidar-dataset-激光雷达数据集">2.2 Lidar Dataset 激光雷达数据集</h3>
<p>Argoverse 2 激光雷达数据集旨在支持激光雷达域中的自监督学习研究以及点云预测[48,49]。由于激光雷达数据比完整的传感器套件更紧凑，本文可以包括两倍长度的场景（30秒而不是15秒），和更多 （20,000 而不是 1,000），相当于大约40倍的驾驶小时，空间预算是5倍。AV2激光雷达数据集的挖掘标准与预测数据集（第3.3.2节）相同，以确保每个场景都是有趣的。虽然激光雷达数据集没有3D目标标注，但每个场景都带有一张高清地图，其中包含关于场景的丰富的3D信息。</p>
<p>本文的数据集是迄今为止最大的此类集合，有20,000个30秒序列。唯一一个类似的数据集，是同时发布的ONCE[36]，包含1M激光雷达帧，而本文的是6M激光雷达帧。本文的数据集以10 Hz采样，而不是像ONCE[36]中那样以2 Hz采样，使本文的数据集更适合于点云预测或自监督任务，这些任务点云随时间的演变是重要的。</p>
<p><strong>激光雷达数据集分割。</strong> 本文用分别为16,000个、2000个和2000个场景的train、validation和test拆分 随机划分数据集。</p>
<h3 id="23-motion-forecasting-dataset-运动预测数据集">2.3 Motion Forecasting Dataset 运动预测数据集</h3>
<p>运动预测解决了预测局部环境中动态行为者的未来状态（或占用图）的问题。自动驾驶相关行为者的一些例子包括：车辆（停车和移动）、行人、骑自行车的人、滑板车和宠物。由预测系统生成的预测未来被用作运动规划的主要输入，运动规划根据这种预测条件进行轨迹选择。生成这些预测提出了一个复杂的、多模态的问题，涉及许多不同的、部分观察的和社会交互的主体。然而，通过利用观察到的ground truth futures 来“自我标记”数据的能力，运动预测成为机器学习应用的理想领域（ideal domain）。</p>
<p>在Argoverse 1成功的基础上，Argoverse 2运动预测数据集提供了从自动驾驶车队收集的一组更新的预测场景。下面列举的设计决策总结了本文从内部研究/开发中吸取的集体经验教训，以及来自3个竞赛中近260个独特团队提交的2700多份submissions的反馈意见[43]:</p>
<ol>
<li>
<p>运动预测是长尾域中的一个安全关键系统。 因此，本文的数据集偏向于包含不同类型focal agent的不同和有趣的场景（见第3.3.2节）。本文的目标是鼓励开发确保尾部事件（tail events）期间安全的方法，而不是优化“轻松里程”上的预期性能。</p>
</li>
<li>
<p>There is a “Goldilocks zone” of task difficulty. Argoverse1测试集的性能已经开始稳定下来，如附录的图10所示。Argoverse 2的设计是为了增加预测的难度，在未来几年刺激富有成效的重点研究。这些变化旨在激励在扩展预测范围(3s→6s)上表现良好的方法，处理多种类型的动态对象(1→5)，并确保长尾场景的安全性。未来的Argoverse releases可能会通过减少观测窗口和增加预测层位来继续增加问题的难度。</p>
</li>
<li>
<p>可用性很重要。 Argoverse 1受益于一个庞大而活跃的研究社区&ndash;在很大程度上是由于设置和使用的简单性。因此，本文注意确保现有的Argoverse模型可以很容易地移植到Argoverse 2上运行。特别是，本文优先考虑对地图元素的直观访问，鼓励使用车道图作为强优先级的方法。为了提高训练和泛化，所有姿态也被插值和重新采样在精确的10赫兹（Argoverse 1是近似的）。新的数据集包括更少，但更长和更复杂的场景；这确保总的数据集大小保持足够大，可以训练复杂的模型，但足够小，可以方便地访问。</p>
</li>
</ol>
<h3 id="24-hd-maps-高精地图">2.4 HD Maps 高精地图</h3>
<p>上述三个数据集中的每个场景共享相同的HD地图表示。每个场景都带有自己的本地地图区域，类似于Waymo Open Motion[12]数据集。这与最初的Argoverse数据集不同，在最初的数据集中，所有场景都被本地化到两张城市地图上&ndash;一张是匹兹堡的，一张是迈阿密的。在附录中，本文提供了例子。每个场景映射的优点包括更高效的查询和处理映射更改的能力。在本文的数据集中，一个特定的十字路口可能会被观察多次，在此期间车道、人行横道甚至地面高度都可能发生变化。</p>
<p><strong>车道图。</strong> HD地图的核心特征是车道图，由图组成，其中是单个车道段。在附录中，本文列举并定义了本文为每个车道段提供的属性。与Argoverse 1不同，本文提供了实际的3D车道边界，而不仅仅是中心线。但是，本文的API提供了代码，可以在任何期望的采样分辨率下快速推断中心线。折线被量化到1cm分辨率。本文的表示比nuScenes更丰富，它只在2D中提供车道几何，而不是3D。</p>
<p><strong>可驾驶区域。</strong> 而不是像在Argoverse 1中所做的那样，以光栅化格式提供可驾驶区域分割，本文以矢量格式释放它，即作为3D多边形。这提供了多种优势，主要是在压缩方面，允许本文为成千上万的场景存储单独的地图，然而光栅格式仍然很容易衍生。将多边形顶点量化到1cm分辨率。</p>
<p>**地表高度。**只有传感器数据集包括密集的地表高度图（尽管其他数据集仍然有关于折线的稀疏的三维高度信息）。地地面高度为可行驶区域边界5m等值线内的区域提供，本文将其定义为感兴趣区域(ROI)[6]。本文这样做是因为对于建筑物内部和建筑密集的城市街区内部，地面车辆由于遮挡而无法观察的区域，地表高度的概念定义不清(ill-defined)。光栅栅格被量化到30cm分辨率，比Argoverse 1中的1m分辨率更高。</p>
<p>**本地地图的面积。**每个场景的局部地图都包括在ego-vehicle轨迹的l2范数中100米膨胀范围内找到的所有实体。</p>
<h2 id="三argoverse-2-api-简介">三、Argoverse 2 API 简介</h2>
<p>轨迹预测常用的有场景数据<code>ArgoverseScenario</code>和地图<code>ArgoverseStaticMap</code></p>
<p>轨迹序列读取的API为<code>scenario_serialization</code></p>
<p>可视化的API为<code>visualize_scenario</code></p>
<p>argoverse2和argoverse1不一样的地方是，每一段轨迹序列（Scenario）内有自己的json地图文件（虽然说都是同一幅HD map，但是对应HD map中的不同的位置），而argoverse1是所有轨迹序列共享一个地图文件</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 存放轨迹序列的类</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.datasets.motion_forecasting.data_schema</span> <span class="kn">import</span> <span class="n">ArgoverseScenario</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 用于读取轨迹序列的API</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.datasets.motion_forecasting</span> <span class="kn">import</span> <span class="n">scenario_serialization</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 用于可视化的API</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.datasets.motion_forecasting.viz.scenario_visualization</span> <span class="kn">import</span> <span class="n">visualize_scenario</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 用于读取地图的API</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.map.map_api</span> <span class="kn">import</span> <span class="n">ArgoverseStaticMap</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>ArgoverseScenario</strong>
每个scenario有11s长的序列，包含actor的历史轨迹集合，就是这里面的tracks，对于每一个scenario，提供了以下的顶层属性:</p>
<ul>
<li><code>scenario_id</code>: 该scenario的特有ID</li>
<li><code>timestamps_ns</code>: 该scenario的所有时间戳</li>
<li><code>tracks</code>: 该scenario的所有轨迹序列</li>
<li><code>focal_track_id</code>: 该scenario的焦点agent(focal agent)的track ID</li>
<li><code>city_name</code>: 该scenario对应的城市名</li>
</ul>
<p>每个<code>track</code>包含以下属性:</p>
<ul>
<li><code>track_id</code>: 该track的特有ID</li>
<li><code>object_states</code>: 该轨迹序列对应的object在这11s内的有效观测的状态，以timestep表示时间步，一般来说最多有110步，因为采样频率为10Hz，一步对应0.1s</li>
<li><code>object_type</code>: 该轨迹序列对应的object的类型，如vehicle等</li>
<li><code>category</code>: 给轨迹序列分配种类，用于给轨迹预测的数据质量提供参考，一般来说，有四种：SCORED_TRACK，UNSCORED_TRACK，FOCAL_TRACK，TRACK_FRAGMENT。其中FOCAL_TRACK和SCORED_TRACK数据质量较好，UNSCORED_TRACK用于当作上下文输入，数据质量一般，而TRACK_FRAGMENT的时间长度不定，数据质量较差
<ul>
<li><code>TRACK_FRAGMENT</code>: Lower quality track that may only contain a few timestamps of observations. 在数据中以整数0表示</li>
<li><code>UNSCORED_TRACK</code>: Unscored track used for contextual input. 在数据中以整数1表示</li>
<li><code>SCORED_TRACK</code>: High-quality tracks relevant to the AV - scored in the multi-agent prediction challenge. 在数据中以整数2表示</li>
<li><code>FOCAL_TRACK</code>: The primary track of interest in a given scenario - scored in the single-agent prediction challenge. 在数据中以整数3表示</li>
</ul>
</li>
</ul>
<p>每个<code>object_states</code>包含以下属性，对应某一actor在某一时间点的所有信息：</p>
<ul>
<li><code>observed</code>: Boolean 指示这个object state是否在该scenario的观测区间内(observed segment)</li>
<li><code>timestep</code>: 时间步，范围是[0, num_scenario_timesteps) Time step corresponding to this object state [0, num_scenario_timesteps).</li>
<li><code>position</code>: (x, y) Coordinates of center of object bounding box. object bounding box的xy坐标</li>
<li><code>heading</code>: Heading associated with object bounding box (in radians, defined w.r.t the map coordinate frame). object bounding box的航向角，单位是弧度，是在地图坐标系下的</li>
<li><code>velocity</code>: (x, y) Instantaneous velocity associated with the object (in m/s). object的xy方向的速度</li>
</ul>
<p>每个track有以下10种label:</p>
<ul>
<li><code>Dynamic</code>
<ul>
<li><code>VEHICLE</code></li>
<li><code>PEDESTRIAN</code></li>
<li><code>MOTORCYCLIST</code></li>
<li><code>CYCLIST</code></li>
<li><code>BUS</code></li>
</ul>
</li>
<li><code>Static</code>
<ul>
<li><code>STATIC</code></li>
<li><code>BACKGROUND</code></li>
<li><code>CONSTRUCTION</code></li>
<li><code>RIDERLESS_BICYCLE</code></li>
</ul>
</li>
<li><code>UNKNOWN</code></li>
</ul>
<p><strong>ArgoverseStaticMap</strong></p>
<ul>
<li>
<p>Vector Map: Lane Graph and Lane Segments</p>
<blockquote>
<p>The core feature of the HD map is the lane graph, consisting of a graph G = (V, E), where V are individual lane segments.</p>
</blockquote>
<p>Argoverse2 提供了3D的道路边界线，而不是仅仅有centerlines，也提供了快速获取特定采样分辨率的centerlines的API，在release中多边形的分辨率被设置为1cm</p>
<p>地图以json文件的形式提供, 可以通过以下方式读取：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.map.map_api</span> <span class="kn">import</span> <span class="n">ArgoverseStaticMap</span>
</span></span><span class="line"><span class="cl"><span class="n">log_map_dirpath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&#34;av2&#34;</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&#34;00a6ffc1-6ce9-3bc3-a060-6006e9893a1a&#34;</span> <span class="o">/</span> <span class="s2">&#34;map&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">avm</span> <span class="o">=</span> <span class="n">ArgoverseStaticMap</span><span class="o">.</span><span class="n">from_map_dir</span><span class="p">(</span><span class="n">log_map_dirpath</span><span class="o">=</span><span class="n">log_map_dirpath</span><span class="p">,</span> <span class="n">build_raster</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>LaneSegment
LaneSegment中包含以下属性：</p>
<ul>
<li><code>id:</code> unique identifier for this lane segment (guaranteed to be unique only within this local map). 该lane segment的特有ID（仅在局部地图中保证是特有的ID）</li>
<li><code>is_intersection:</code> boolean value representing whether or not this lane segment lies within an intersection. boolean value，用来表示该lane segment是否位于一个路口内</li>
<li><code>lane_type:</code> designation of which vehicle types may legally utilize this lane for travel. 表示车道线类型</li>
<li><code>right_lane_boundary:</code> 3d polyline representing the right lane boundary. 3D线条，表示右车道边界线</li>
<li><code>left_lane_boundary:</code> 3d polyline representing the left lane boundary. 3D线条，表示左车道边界线</li>
<li><code>right_mark_type:</code> type of painted marking found along the right lane boundary . 右车道边界线的线型</li>
<li><code>left_mark_type:</code> type of painted marking found along the left lane boundary. 左车道边界线的线型</li>
<li><code>predecessors:</code> unique identifiers of lane segments that are predecessors of this object. 该lane segment的前继lane segment的unique ID</li>
<li><code>successors:</code> unique identifiers of lane segments that represent successor of this object. Note: this list will be empty if no successors exist. 该lane segment的后继lane segment的unique ID</li>
<li><code>right_neighbor_id:</code> unique identifier of the lane segment representing this object’s right neighbor. 该lane segment的右邻lane segment的unique ID</li>
<li><code>left_neighbor_id:</code> unique identifier of the lane segment representing this object’s left neighbor. 该lane segment的左邻lane segment的unique ID</li>
</ul>
</li>
<li>
<p>Vector Map: Drivable Area
多边形向量的分辨率也是1cm, 包含以下属性：</p>
<ul>
<li><code>id:</code> unique identifier. 特有ID</li>
<li><code>area_boundary:</code> 3d vertices of polygon, representing the drivable area’s boundary. 3D多边形，用来表示可行驶区域的边</li>
</ul>
</li>
<li>
<p>Vector Map: Pedestrian Crossings
代表人行道，由两条沿同一主轴的edge构成</p>
<ul>
<li><code>id:</code> unique identifier of pedestrian crossing. 人行道的特有ID</li>
<li><code>edge1:</code> 3d polyline representing one edge of the crosswalk, with 2 waypoints. 3D多边形，代表人行道的其中一边，由2个waypoints组成</li>
<li><code>edge2:</code> 3d polyline representing the other edge of the crosswalk, with 2 - waypoints. 3D多边形，代表人行道的其中一边，由2个waypoints组成</li>
</ul>
</li>
<li>
<p>Area of Local Maps
Each scenario’s local map includes all entities found within a 100 m dilation in l2-norm from the ego-vehicle trajectory.
每个scenario的局部地图包含距离ego-vehicle的轨迹100m l2距离内的所有实体</p>
</li>
</ul>
<p>常用API汇总:</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.datasets.motion_forecasting.data_schema</span> <span class="kn">import</span> <span class="n">ArgoverseScenario</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.datasets.motion_forecasting</span> <span class="kn">import</span> <span class="n">scenario_serialization</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.datasets.motion_forecasting.viz.scenario_visualization</span> <span class="kn">import</span> <span class="n">visualize_scenario</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.map.map_api</span> <span class="kn">import</span> <span class="n">ArgoverseStaticMap</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.map.map_primitives</span> <span class="kn">import</span> <span class="n">Polyline</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">av2.utils.io</span> <span class="kn">import</span> <span class="n">read_json_file</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 利用pandas读取轨迹序列parquet文件</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet文件</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_dir</span><span class="p">,</span> <span class="n">raw_file_name</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;scenario_</span><span class="si">{</span><span class="n">raw_file_name</span><span class="si">}</span><span class="s1">.parquet&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 加载地图json文件</span>
</span></span><span class="line"><span class="cl"><span class="n">map_data</span> <span class="o">=</span> <span class="n">read_json_file</span><span class="p">(</span><span class="n">map_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 从地图的json文件读取车道中心线</span>
</span></span><span class="line"><span class="cl"><span class="n">centerlines</span> <span class="o">=</span> <span class="p">{</span><span class="n">lane_segment</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]:</span> <span class="n">Polyline</span><span class="o">.</span><span class="n">from_json_data</span><span class="p">(</span><span class="n">lane_segment</span><span class="p">[</span><span class="s1">&#39;centerline&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                           <span class="k">for</span> <span class="n">lane_segment</span> <span class="ow">in</span> <span class="n">map_data</span><span class="p">[</span><span class="s1">&#39;lane_segments&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 加载地图API</span>
</span></span><span class="line"><span class="cl"><span class="n">map_api</span> <span class="o">=</span> <span class="n">ArgoverseStaticMap</span><span class="o">.</span><span class="n">from_json</span><span class="p">(</span><span class="n">map_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 以某个query中心搜索附近一定半径的的lane_segments</span>
</span></span><span class="line"><span class="cl"><span class="n">query_center</span> <span class="o">=</span> <span class="n">scenario_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;position_x&#39;</span><span class="p">,</span> <span class="s1">&#39;position_y&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl"><span class="n">search_radius_m</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl"><span class="n">nearby_lane_segments</span> <span class="o">=</span> <span class="n">map_api</span><span class="o">.</span><span class="n">get_nearby_lane_segments</span><span class="p">(</span><span class="n">query_center</span><span class="p">,</span> <span class="n">search_radius_m</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 通过map_api获取lane_segments的车道中心线</span>
</span></span><span class="line"><span class="cl"><span class="n">nearby_lane_centerlines</span> <span class="o">=</span> <span class="n">get_lane_centerlines</span><span class="p">(</span><span class="n">map_api</span><span class="p">,</span> <span class="n">nearby_lane_segments</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 获取地图内的所有pedestrian crossings（目前av2 API没有提供获取附近pedestrian crossings的API）</span>
</span></span><span class="line"><span class="cl"><span class="n">crosswalks</span> <span class="o">=</span> <span class="n">map_api</span><span class="o">.</span><span class="n">get_scenario_ped_crossings</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="四argoverse-2-数据提取">四、Argoverse 2 数据提取</h2>
<p>raw_dir 文件夹内包含<code>.parquet</code>和<code>.json</code>文件，其中文件组织形式为<code>log_map_archive_{$scenario_name}.parquet</code>和<code>scenario_{$scenario_name}.json</code>，分别代表<strong>障碍物时序信息</strong>和<strong>地图信息</strong>。</p>
<ul>
<li>设置原始数据路径:
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">raw_dir</span> <span class="o">=</span> <span class="s2">&#34;/home/yejian/yejian_personal/QCNet/train/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">raw_file_name</span> <span class="o">=</span> <span class="s2">&#34;ffffe3df-8d26-42c3-9e7a-59de044736a0&#34;</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>读取障碍物信息和地图信息
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parquet_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">raw_dir</span><span class="p">,</span> <span class="n">raw_file_name</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;scenario_</span><span class="si">{</span><span class="n">raw_file_name</span><span class="si">}</span><span class="s1">.parquet&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;parquet_file: </span><span class="si">{</span><span class="n">parquet_file</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span> <span class="c1"># 障碍物信息</span>
</span></span><span class="line"><span class="cl"><span class="n">map_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">raw_dir</span><span class="p">,</span> <span class="n">raw_file_name</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;log_map_archive_</span><span class="si">{</span><span class="n">raw_file_name</span><span class="si">}</span><span class="s1">.json&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;map_file: </span><span class="si">{</span><span class="n">map_file</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span><span class="c1"># 地图信息</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_from_parquet</span><span class="p">(</span><span class="n">parquet_file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">map_file</span> <span class="o">=</span> <span class="n">read_json_file</span><span class="p">(</span><span class="n">map_file</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>查看障碍物信息文件内容
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><code>['observed', 'track_id', 'object_type', 'object_category', 'timestep', 'position_x', 'position_y', 'heading', 'velocity_x', 'velocity_y', 'scenario_id', 'start_timestamp', 'end_timestamp', 'num_timestamps', 'focal_track_id', 'city']</code></li>
</ul>
<p>ref:
<a href="https://blog.csdn.net/Yong_Qi2015/article/details/128731798"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Yong_Qi2015/article/details/128731798<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/m0_56423263/article/details/134593815"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/m0_56423263/article/details/134593815<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>BEVFormer 论文解读</title><link>https://jianye0428.github.io/posts/bevformer/</link><pubDate>Sun, 03 Sep 2023 19:31:13 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/bevformer/</guid><description><![CDATA[<h2 id="1-背景motivation">1. 背景/Motivation</h2>
<h3 id="11-为什么视觉感知要用bev">1.1 为什么视觉感知要用BEV？</h3>
<p>相机图像描述的是一个2D像素世界，然而自动驾驶中利用相机感知结果的后续决策、路径规划都是在车辆所处的3D世界下进行。由此引入的2D和3D维度不匹配，就导致基于相机感知结果直接进行自动驾驶变得异常困难。</p>
<p>这种感知和决策规划的空间维度不匹配的矛盾，也体现在学开车的新手上。倒车泊车时，新手通过后视镜观察车辆周围，很难直观地构建车子与周围障碍物的空间联系，容易导致误操作剐蹭或需要尝试多次才能泊车成功，本质上还是新手从2D图像到3D空间的转换能力较弱。基于相机图像平面感知结果进行决策规划的自动驾驶AI，就好比缺乏空间理解力的驾驶新手，很难把车开好。</p>
<p>实际上，利用感知结果进行决策和路径规划，问题还出现在多视角融合过程中: 在每个相机上进行目标检测，然后对目标进行跨相机融合。如2021 TESLA AI Day给出的图1，带拖挂的卡车分布在多个相机感知野内，在这种场景下试图通过目标检测和融合来真实地描述卡车在真实世界中的姿态，存在非常大的挑战。</p>
<p></p>
<p>为了解决这些问题，很多公司采用硬件补充深度感知能力，如引入毫米波雷达或激光雷达与相机结合，辅助相机把图像平面感知结果转换到自车所在的3D世界，描述这个3D世界的专业术语叫做BEV map或BEV features(鸟瞰图或鸟瞰图特征)，如果忽略高度信息，就把拍扁后的自车坐标系叫做BEV坐标系(即鸟瞰俯视图坐标系)。</p>
<p>但另外一些公司则坚持不引入深度感知传感器，他们尝试从本质入手，基于视觉学习得到从图像理解空间的能力，让自动驾驶AI系统更像老司机，例如TESLA。Elon Musk认为: 人类不是超人，也不是蝙蝠侠，不能够眼放激光，也没安装雷达，但是通过眼睛捕捉到的图像，人类反复练习就可以构建出对周围世界的3D空间理解能力从而很好地掌握驾驶这项能力，那么要像人一样单纯利用眼睛(相机)进行自动驾驶就必须<font color=red>具备从2D图像平面到3D自车空间(BEV)的转换能力</font>。</p>
<p>传统获取BEV map/features的方法有局限性，它一般是利用相机外参以及地面平面假设，即IPM(Inverse Perspective Mapping)方法，将图像平面的感知结果反投影到自车BEV坐标系。Tesla以前的方案也是这样，然而当车辆周围地面不满足平面假设，且多相机视野关联受到各种复杂环境影响的时候，这类方法就难以应付。</p>
<p>针对IPM方法获取BEV遇到的困难，TESLA自动驾驶感知负责人Andrej Karparthy的团队<font color=red><strong>直接在神经网络中完成图像平面到BEV的空间变换</strong></font>，这一改变成为了2020年10月发布的FSD Beta与之前Autopilot产品最显著的差别。TESLA利用Transformer生成BEV Featrues，得到的Features通道数是256(IPM方法最多保留RGB3个channel)，这样能极大程度地保留图像信息，用于后续基于BEV Features的各种任务，如动、静态目标检测和线检测等。</p>
<h3 id="12-生成bev视角的方法有哪些为何选用transformer呢">1.2 生成BEV视角的方法有哪些？为何选用Transformer呢？</h3>
<p><strong>把相机2D平面图像转换成BEV视角的方法有两种: <u>视觉几何方法</u>和<u>神经网络方法</u>。</strong></p>
<p><strong>视觉几何方法</strong>: 基于IPM进行逐像素几何投影转换为BEV视角，再对多个视角的部分BEV图拼接形成完整BEV图。此方法有两个假设: 1.路面与世界坐标系平行，2.车辆自身的坐标系与世界坐标系平行。前者在路面非平坦的情况下并不满足，后者依赖车辆姿态参数(Pitch和Roll)实时校正，且精度要求较高，不易实现。</p>
<p><strong>神经网络方法</strong>: 用神经网络生成BEV，其中的关键要找到合适的方法实现神经网络内部Feature Map空间尺寸上的变换。</p>
<p>实现空间尺寸变换的神经网络主流操作有两种方法，如图2所示: MLP中的Fully Connected Layer和Transformer Cross Attention(图片引用自<a href="https://zhuanlan.zhihu.com/p/458076977"target="_blank" rel="external nofollow noopener noreferrer">《超长延迟的特斯拉AI Day解析: 讲明白FSD车端感知》<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p></p>
<p><strong>MLP Fully Connected Layer方法:</strong></p>
<p>$$O=Act(W_{mlp}*X+b)$$</p>
<p>忽略非线性激活函数和bias后，可以写成:</p>
<p>$$O=W_{mlp}*X$$</p>
<p><strong>Transformer Cross Attention方法:</strong></p>
<p>$$O=Softmax(Q*K^T)*V$$</p>
<p>其中，K和V是输入图像特征X经线性变换后得到的；在最终输出的BEV视角下，把自车周围空间中的索引量称作 $\boldsymbol{\Phi}$</p>
<p>$$O=Softmax(\Phi W_q*(XW_K)^T)*XW_V=W_{transformer}(X,\Phi)*XW_V$$</p>
<p>BEV变换的本质是将输入的2D图像空间特征图转换成BEV特征图。在进行BEV转换之前，先通过多层CNN(或者任一图像处理backbone网络)在图像上提取特征，得到图像空间特征图层尺寸($h * w$)，即$X$的尺寸。 BEV变换输出O所在的BEV空间尺寸，是以自车位置为原点的前后左右各若干米范围内建立的栅格空间($x * y$)。</p>
<p>MLP Fully Connect Layer和Cross Attention的<font color=red><strong>显著差别</strong></font>在于作用于输入量X的系数W: 全联接层的W，一旦训练结束后在Inference阶段是固定不变的；而<font color=green>Cross Attention的Transformer的系数W，是输入量X和索引量的函数，在Inference阶段会根据输入量X和索引量的不同发生改变</font>。从这个角度来讲，使用Cross Attention来进行空间变换可能使模型获得更强的表达能力。</p>
<p>TESLA在2021年AI Day上仅介绍了用Transformer转换BEV Features的技术思想，并未披露更多实现细节。<font color=red><strong>论文BEVFormer充分研究了TESLA的技术思想后，利用Transformer融合图像的时、空特征，得到BEV Features，与TESLA的关键方法、实现效果都非常接近</strong></font>。BEVFormer既通过论文披露了详尽方法，又在2022年6月开源了工程，接下来就围绕BEVFormer介绍如何通过Transformer获取BEV Features。</p>
<h2 id="2-methodstrategybevformer">2. Method/Strategy——BEVFormer</h2>
<h3 id="21-overall-architecture">2.1 Overall Architecture</h3>
<p>如下图3所示: BEVFormer主体部分有6层结构相同的<code>BEVFormer encoder layers</code>，每一层都是由以transformer为核心的modules(TSA+SCA)，再加上FF、Add和Norm组成。BEVFormer encoder layer结构中有3个特别的设计: <font color=red><strong>BEV Queries</strong></font>， <font color=red><strong>Spatial Cross-attention(SCA)</strong></font>和<font color=red><strong>Temporal Self-attention(TSA)</strong></font>。其中BEV Queries是栅格形可学习参数，承载着通过attention机制在multi-camera views中查询、聚合的features。SCA和TSA是以BEV Queries作为输入的注意力层，负责实施查询、聚合空间features(来自multi-camera images)和时间features(来自历史BEV)的过程。</p>
<p>下面分步骤观察BEV完整模型的前向推理过程:</p>
<ul>
<li>在t时刻，输入车上多个视角相机的图像到backbone，输出各图像的多尺度特征(multi-scale features): $x = 1$, $F_t = {{F^i_t}}^a_b$, $F_t={{{F_t^i}}}<em>{i=1}^{N</em>{view}}$，其中 $F^i_t$ 是第i个视角相机的feature，$N_{view}$ 是多个相机视角的总数。同时，还要保留t-1时刻的BEV Features ${\boldsymbol{B}}_{t-1}$。</li>
<li>在每个BEVFormer Encoder layer中，首先用BEV Queries Q通过TSA模块从 ${\boldsymbol{B}}_{t-1}$ 中查询并融合时域信息(the temporal information)，得到修正后的BEV Queries $Q^{\prime}$；</li>
<li>然后在同一个BEVFormer Encoder layer中，对TSA“修证”过的BEV Queries $Q^{\prime}$ ，通过SCA模块从multi-camera features $F_{t}$ 查询并融合空域信息(spatial information)，得到进一步修正的BEV Queries $Q^{^{\prime\prime}}$ 。</li>
<li>这一层encoder layer把经过两次修正的BEV features $Q^{^{\prime\prime}}$ (也可以叫做BEV Queries)进行FF计算，然后输出，作为下一个encoder layer的输入。</li>
<li>如此堆叠6层，即经过6轮微调，t时刻的统一BEV features $B_{t}$ 就生成了。</li>
<li>最后，以BEV Features $B_{t}$ 作为输入，3D detection head和map segmentation head预测感知结果，例如3D bounding boxes和semantic map。</li>
</ul>
<p></p>
<h3 id="22-bev-queries">2.2 BEV Queries</h3>
<p>BEVFormer采用显性定义BEV的方式，BEV Queries就是其中的显性BEV features。</p>
<p>可从3个概念循序渐进地认识/理解BEV Queries: <strong>BEV平面 → BEV 感知空间 → BEV Queries</strong>。</p>
<ul>
<li>
<p><strong>BEV 平面</strong>
BEV平面($R^{H\times W\times1}$)是以自车为中心的栅格化二维平面，H、W是BEV平面在x、y方向的栅格尺寸，此平面显性地与车辆周围横、纵向物理空间关联，一个栅格表示s米，论文中把BEV平面中的栅格叫做2D参考点(2D reference points)。例如论文中定义nuScenes数据集栅格尺寸200x200，对应[-51.2米, 51.2米]，那么s就是0.512米。</p>
</li>
<li>
<p><strong>BEV 感知空间</strong>
BEV感知空间( $R^{H\times W\times N_{ref}}$ )把BEV平面在z轴方向选取 $N_{ref}$个<strong>3D参考点</strong>进行扩展，表示车辆周围有限空间。查看BEV开源工程可知，BEV感知空间的精确表示范围: 在nuScenes数据集中，是以Lidar作为中心点，前后各51.2m、左右各51.2m、向上3m、向下5m的矩形空间。
仔细深究作者这样设置的用意，会发现蛮有意思/意义: nuScenes采集车上安装的Lidar距地面高约1.84m，[-5m, 3m]的设置让BEV Queries感知空间在自车轮胎接地点平面以上部分约4.84m，接地点平面以下部分约3.16m。前者确保不仅能感知高大目标物，如货车等，还能感知覆盖坡度9.5%(4.84m/51.2m100%)上坡场景，后者保证感知覆盖坡度6.2%(3.16m/51.2m100%)下坡场景。备注: 坡度=(高程差/水平距离)x100%，车辆前进100m的垂直上升/下降高度，我国规定城市道路最大纵坡8％，公路9％。</p>
</li>
<li>
<p><strong>BEV Queries</strong>
BEV Queries是预定义的一组栅格形(grid-shaped)可学习参数，简称 $Q\in R^{H\times W\times C}$，它是对上述BEV 感知空间的特征描述，可直白地理解为学习完成了，它就变成了BEV features。
BEV Queries的H、W与BEV平面在x、y方向的栅格尺寸定义一致，因此也继承了BEV平面显性地与车辆周围横、纵向物理空间关联的特性。但C是BEV Queries作为features时在channel维度的尺寸，并不显性地对应于BEV平面z轴的物理空间尺寸。特别指出，位于 $p=(x,y)$ 处的某个query $Q_{p}\in R^{1\times C}$，表示BEV queries中的一个栅格。在输入到BEVFormer之前，BEV Queries加上了可学习的位置编码(learnable positional embedding)。
论文中多处提及BEV Queries和BEV features，它俩什么关系？本质上描述的是同一个东西么？
先说答案: 两者本质上是同一个东西。模型中预定义参数BEV Queries 输入到BEV Encoder layer，输出的就是经过1次微调过的BEV features，它作为下一层的输入时，又被看作下一层的BEV Queries。经过所有layers的多次微调，最后一个layer输出的就是BEV features ${\mathit{B}}_{t}$ ，作为各类感知task heads的输入。</p>
</li>
</ul>
<h3 id="23-sca-spatial-cross-attention">2.3 SCA: Spatial cross-attention</h3>
<p>如上图(b)所示，作者设计了一种空间交叉注意力机制，使 BEV queries 从多个相机的image Features中提取所需信息并转换为BEV Features。</p>
<p>每个BEV栅格的query在image features的哪些范围提取信息呢？这里有3个方案:</p>
<p>一、从image features的所有点上提取信息，即global attention。</p>
<p>二、从BEV栅格在image features的投影点上提取信息。</p>
<p>三、从BEV栅格在image features的投影点及其周围提取信息，即deformable attention。</p>
<p>由于使用了多尺度图像特征和高分辨率 BEV 特征(200x200)，如果采用方案一 global attention ，会带来无法负担的计算代价(显存和计算复杂度)。但是，方案一完全<u>用不到相机内外参</u>，这算是它<strong>独有的优势</strong>。</p>
<p>方案二依赖非常精确的相机内、外参，且不能充分利用image features上的局部区域信息。</p>
<p>因此作者选用了方案三，基于deformable attention 的<strong>稀疏注意力机制</strong>，使BEV Queries中的每个Query 只与其所代表世界坐标系物理空间投影到图像上的部分区域进行交互，且解除了对相机内、外参的高精度依赖。注意，这里复数BEV Queries表示整个BEV图，而单数Query表示BEV 图中位于 $(x, y)$的一个栅格。</p>
<p>$$\mathrm{SCA}(Q_p,F_t)=\frac1{|\mathcal{V}<em>\mathrm{hit}|}\sum</em>{i\in\mathcal{V}<em>\mathrm{hit}}\sum</em>{j=1}^{N_\mathrm{ref}}\mathrm{Deform}\mathrm{Attn}(Q_p,\mathcal{P}(p,i,j),F_t^i)\quad\quad\quad\quad(2)$$</p>
<p>在DeformAttn()中，$Q_{p}$、$P(p,i,j)$ 和 $F_{t}^{i}$ 分别表示query, reference points和输入特征。</p>
<p>通俗理解公式: <font color=red><strong>在BEV Query对应的图像2D features 有效区域附近计算注意力，把图像2D features加权融合到BEV Query作为SCA的输出</strong></font>。</p>
<p>参考图(b)和上述公式2，总结Spatial cross-attention的实现方式:</p>
<ol>
<li>对于每一个位于 $(x, y)$ 位置的 BEV query $Q_{p}$ ，计算其对应真实世界的坐标 $(x^{&rsquo;},y^{&rsquo;})$。然后将 BEV query 在z方向进行 lift 操作，设置 $N_{ref}$ 个 3D参考点，即对应 $N_{ref}$ 个世界坐标系下的3D空间参考点。</li>
<li>通过相机内外参，把第j个3D参考点投影到 $v_{hit}$ 个相机图像上。受相机感知范围限制，每个3D参考点一般只在 1-2 个相机上找到有效的投影点(反过来描述，每个相机的features只与部分BEV queries构成有效投影关系)。</li>
<li>基于 Deformable Attention，把像平面上的这些投影点作为2D图像参考点，在其周围对 $F_{t}^{i}$ 进行特征采样，得到sampled features。</li>
<li>最后，对 $V_{hit}$ 、$N_{ref}$ 个sampled features进行加权求和，作为spatial cross-attention的输出来更新BEV query，从而完成 spatial 空间的特征聚合。</li>
</ol>
<p>详细介绍一下第2步中如何把3D参考点投影到相机图像上获得2D参考点:</p>
<ul>
<li>首先计算位于 $p = (x, y)$ 的query $Q_{p}$ 在以车辆为中心世界坐标 $(x^{&rsquo;},y^{&rsquo;})$
$$x^{^{\prime}}=(x-W/2)*s; \quad y^{^{\prime}}=(y-{H}/2)*s$$
其中H和W是BEV queries的尺寸，s是BEV中一个栅格所代表的物理空间尺寸;</li>
<li>计算第j个3D参考点投影到第i个相机图像上的2D参考点坐标:
$$P(p,i,j)=(x_{i,j},y_{i,j})$$
其中，
$$z_{i,j}[x_{i,j},y_{i,j},1]^T=T_i*[x^{&rsquo;},y^{&rsquo;},z^{&rsquo;},1]^T$$
$T_i$ 是第 $i$ 个相机的投影矩阵。</li>
</ul>
<h3 id="24-tsa-temporal-self-attention">2.4 TSA: Temporal self-attention</h3>
<p>从经典 RNN 网络获得启发，将 BEV 特征 $B_t$ 视为能够传递序列信息的 memory。每一时刻生成的 BEV 特征 $B_t$ 都从上一时刻的 BEV 特征  $B_{t-1}$ 获取所需的时序信息，这样能保证动态地获取所需的时序特征，而非像堆叠不同时刻 BEV 特征那样只能获取定长的时序信息。</p>
<p></p>
<p>$$\text{TSA}(Q_p,{Q,B&rsquo;<em>{t-1}})=\sum</em>{V\in{Q,B&rsquo;_{t-1}}}\text{DeformAttn}(Q_p,p,V),\quad(5)$$</p>
<p>参考图4和上述公式5，总结temporal self-attention的实现方法:</p>
<ol>
<li>给定t-1时刻的 BEV 特征 $B_{t-1}$ ，先根据 ego motion 将 $B_{t-1}$ 对齐到 $t$ 时刻，来确保 $B_{t-1}$ 和 $B_{t}$ 在相同index位置的栅格对应于现实世界的同一位置，把时间对齐后的BEV特征记作 $B_{t-1}^{&rsquo;}$ 。</li>
<li>$t$ 时刻位于 $(x, y)$ 处的 BEV query所表征的物体可能静态或者动态，它在t-1时刻会出现在 $B_{t-1}^{&rsquo;}$ 的 $(x, y)$ 周围，因此利用 deformable attention 以 $(x, y)$ 作为参考点在其周围进行特征采样。</li>
</ol>
<p>上述方法<strong>没有显式地设计遗忘门</strong>，而是通过 attention 机制中的 attention weights 来平衡历史时序特征和当前 BEV 特征的融合过程。</p>
<p><strong>关于TSA的后记</strong>: BEVFormer V2对 BEVFormer的时域融合方法TSA做了修改。</p>
<p>BEVFormer中TSA采用了继承式的时域信息融合方式: 利用attention机制在t时刻融合了t-1时刻的BEV features信息，由于t-1时刻的BEV features 也融合了更早时刻(t-2)的信息，因此t时刻BEV features间接地融合了比t-1时刻更早的信息。</p>
<p>但是这种继承式时域融合方式有遗忘的特点，即不能有效利用较长时间的历史信息。</p>
<p>BEVFormer V2把时域融合改成了: 根据ego motion，把过去多个时刻的BEV features 对齐到当前时刻，然后在channel 维度把这些对齐后的BEV features 与当前时刻BEV features串联，然后用Residual 模块降低channel数，就完成了时域融合。</p>
<p>综合2.3和2.4节，观察6个 BEVFormer Encoder Layers的完整结构会发现， BEV query 既能通过 spatial cross-attention 聚合空间特征，又能通过 temporal self-attention 聚合时序特征，这个过程会重复多次，让时空特征融合能够相互促进，最终得到更好的融合BEV features。</p>
<h3 id="25-application-of-bev-features">2.5 Application of BEV Features</h3>
<p>BEV Features $B_{t}\in R^{H\times W\times C}$ 是可用于多种自动驾驶感知任务的2D feature map， 基于2D 感知方法稍加改动就可在 $B_t$ 上开发3D目标检测和map segmentation任务。</p>
<p><strong>3D object detection</strong></p>
<p>参考2D 目标检测器Deformable DETR，论文设计了end-to-end的3D目标检测head，修改部分包括: 用single-scale 的BEV features $B_t$ 作为检测头的输入，预测输出3D b-boxes和速度而不是2D b-boxes，仅用 $L_1$ loss监督3D b-boxes的回归。继承DETR方法的优势，预测有限数目的候选目标集合(开源工程中设为300个)，这种end-to-end的检测头不需要NMS后处理。</p>
<p><strong>map segmentation</strong></p>
<p>参考2D segmentation 方法Panoptic SegFormer，论文设计了map segmentation head。因为基于BEV的map segmentation基本上与常见语义分割相同，作者利用了参考文章[22]中的mask decoder和class-fixed queries设计head来查找每个语义类别，包括car、vehicle、road(可通行区域)和车道线。</p>
<h3 id="26-implementation-details">2.6 Implementation details</h3>
<p>训练时的实施细节:</p>
<ul>
<li>对于时间t，在过去2s中随机选取3个时刻t-3、t-2和t-1；</li>
<li>在初始3个时刻，循环生成BEV features ${B_{t-3},B_{t-2},B_{t-1}}$，且在此阶段不计算梯度；</li>
<li>计算第一个时刻t-3的temporal self-attention输出时，它并没有前序BEV features，就用它自身作为前序时刻输入，那么temporal self-attention暂时退化成了self-attention。</li>
</ul>
<p>inference时的实施细节:</p>
<p>按照时间顺序计算图像序列中的每一帧，前序时刻的BEV features被保持下来并用于后一时刻，这种online的inference策略在应用中比较高效。</p>
<h2 id="3-experiments">3. Experiments</h2>
<h3 id="31-experimental-settings">3.1 experimental settings</h3>
<ol>
<li>采用两种Backbone: ResNet101-DCN，采用来自FCOS3D训练得到的参数；VoVnet-99，采用来自DD3D训练得到的参数。</li>
<li>默认情况下，使用FPN输出的多尺度 features的尺寸包含1/16，1/32，1/64(注意，代码中实际用好的1/8尺寸的features，即nuScenes中的116*200的FPN输出)，同时把dimension C设为256。</li>
<li>在nuScenes数据集试验中，BEV queries尺寸设为200x200，对应感x和y方向的知范围都是[-51.2m, 51.2m]，对应BEV Grid尺寸精度是0.512m。</li>
<li>在Waymo数据集试验中，BEV queries尺寸设为300x220，对应感x方向的感知范围是[-35.0m, 75.0m]，y方向的知范围是[-75.0m, 75.0m]，对应BEV Grid尺寸精度是0.5m。自车中心位于 BEV的(70，150)处。</li>
<li>对每个BEV query，在 spatial cross-attention模块中，设置 $N_{ref}=4$ 个3D参考点，预定义它们对应的高度范围是-5m到3m。</li>
<li>对每个2D图像参考点(3D参考点投影到2D view上的点)，在其周围选取4个采样点送入SCA。</li>
<li>默认情况下，训练24epoches，学习率设为 $12\times10^{-4}$。</li>
<li><strong>Baselines</strong>: 为了合理评估task heads的影响，公平地与其它生成BEV方法对比。选择VPN和Lift-Splat作为baselines，对它们head之前的部分替换BEVFormer，保留task heads和其它设置。
论文中，通过把temporal self-attention修改为普通的self-attention，这样就使BEVFormer变成了一个静态模型，命名为<strong>BEVFormer-S</strong>，它不使用历史BEV Features。</li>
</ol>
<h3 id="32-3d目标检测结果">3.2 3D目标检测结果</h3>
<p>BEVFormer的3D检测性能，如Table1、Table2和Table3所示，远超之前最优方法DETR3D。</p>
<p>BEVFormer引入了temporal information，因此它在估计目标速度方面效果也很好。从速度估计指标mean Average Velocity (mAVE)来看，BEVFormer误差为0.378m/s，效果远好于同类基于相机的方法，甚至逼近了基于激光的方法。</p>
<p>
</p>
<h3 id="33-multi-tasks-perception-results">3.3 multi-tasks perception results</h3>
<p>联合训练3D detection和map segmentation任务，与单独训练比较训练效果，如Table4所示: 对3D目标检测和分割中的车辆类语义感知，联合训练效果有提升；对分割中的road、lane类的语义感知，联合训练效果反而会下降。</p>
<p></p>
<h3 id="34-消融试验">3.4 消融试验</h3>
<p>Spatial Cross-attention有效性</p>
<p>为了验证SCA的有效性，利用不包含TSA的BEVFormer-S来设计消融试验，结果如Table5所示。</p>
<p>默认的SCA基于deformable attention，在对比试验中构建了基于2种不同attention机制的baselines: 1. 用global attention取代deformable attention；2. 让每个query仅与它的图像参考点交互，而不是像SCA那样query与图像参考点周围区域交互。为了扩大对比范围，把BEVFormer中的BEV生成方法替换为了VPN和Lift-Spalt中的方法。从Table5结果可见Deformable Attention方法显著优于其它方法，且在GPU Memory使用量和query兴趣区域大小之间实现了balance。</p>
<p></p>
<p><strong>Temporal Self-attention有效性</strong></p>
<p>从Table1和Table4可见，在相同的设置下，BEVFormer相比于BEVFormer-S的性能大幅提升，针对有挑战性的检测任务提升更明显。TSA主要是在以下方面影响性能提升的: 1.temporal information的引入对提高目标速度估计精度非常有益；2.利用temporal information，目标预测的location和orientations更精确；3.受益于temporal information包含过去时刻object的信息，如图4所示，严重遮挡目标的recall 更高。根据nuScenes标注的遮挡程度把验证数据集进行划分成4部分，来评估BEVFormer对各种程度遮挡的性能，针对每个数据子集都会计算average recall(匹配时把中心距离的阈值设为2m)。</p>
<p></p>
<p><strong>Model Scale and Latency</strong></p>
<p>针对不同Scale Settings的BEVFormer，对比检测性能和latency，结果如Table6所示。在3方面进行BEVFormer的Scale配置: 1. 输入到BEVFormer Encoder的features是multi-scale还是single-scale；2.BEV Queries/features的尺寸；3. encoder layer数目。</p>
<p></p>
<p>从实验结果看来: Backbone的Latency远大于BEVFormer，因此Latency优化的主要瓶颈在于Backbone而不是BEVFormer(这里指BEVFormer Encoder部分)，BEVFormer可以采用不同的Scale，具备支持灵活平衡性能和efficiency的特性。</p>
<h2 id="4-discussion">4. Discussion</h2>
<p>理论上视觉图像的数据比激光数据稠密，但基于视觉的BEV效果还是比基于激光的方法性能差，那么也说明了理论上视觉还有可提升空间。
BEV Features能用于泊车位检测么？ 可能可以用BEVFormer在环视鱼眼相机上生成BEV features，用于泊车位检测或近距离目标的精确检测。
显性地引入BEV 特征，限制了最大检测距离，在高速公路场景，检测远处目标非常重要，如何权衡BEV的大小与检测距离是一个需要考虑的问题。
如何在检测精度和grid大小之间做平衡是一个问题。
针对2、3问题的一个优化方向: 设计自适应尺寸的BEV特征。这里的自适应是指根据场景来调整BEV尺寸或精度。</p>
<h2 id="5-references">5. References</h2>
<p>论文: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.17270.pdf"target="_blank" rel="external nofollow noopener noreferrer">《BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers》<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>开源工程: <a href="https://github.com/zhiqi-li/BEVFormer"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zhiqi-li/BEVFormer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>参考中文解读</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/495819042"target="_blank" rel="external nofollow noopener noreferrer">使用Transformer融合时空信息的自动驾驶感知框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/491890969"target="_blank" rel="external nofollow noopener noreferrer">3d camera-only detection: BEVFormer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/496284695"target="_blank" rel="external nofollow noopener noreferrer">BEVFormer，通过一个时空Transformer学习 BEV表征<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ol>
<p>类似工作:
Cross-view Transformers for real-time Map-view Semantic Segmentation
论文: <a href="https://arxiv.org/pdf/2205.02833v1.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2205.02833v1.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码: <a href="https://github.com/bradyz/cross"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/bradyz/cross<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>REF:
[1]. <a href="https://zhuanlan.zhihu.com/p/538490215"target="_blank" rel="external nofollow noopener noreferrer">一文读懂BEVFormer论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://zhuanlan.zhihu.com/p/543335939"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/543335939<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://zhuanlan.zhihu.com/p/629792598"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/629792598<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>FastBEV:快速而强大的BEV感知基线</title><link>https://jianye0428.github.io/posts/fastbev/</link><pubDate>Sat, 02 Sep 2023 16:49:37 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/fastbev/</guid><description><![CDATA[<p>近年来，基于鸟瞰图(BEV)表示的感知任务越来越受到关注，BEV表示有望成为下一代自动驾驶车辆(AV)感知的基础。现有大多数的BEV解决方案要么需要大量资源来执行车载推理，要么性能不佳。本文提出了一种简单而有效的框架，称为Fast BEV，它能够在车载芯片上执行更快的BEV感知。为了实现这一目标，作者首先从经验上发现，BEV表示可以足够强大，而无需昂贵的基于transformer的变换或深度表示。Fast BEV由五个部分组成，论文新颖地提出：</p>
<p>(1)一种轻量级的、易于部署的视图转换，它将<strong>2D图像特征快速传输到3D体素空</strong>间;</br>
(2)一种利用<strong>多尺度信息</strong>获得更好性能的<strong>多尺度图像编码器</strong>;</br>
(3)一种高效的<strong>BEV编码器</strong>，它专门设计用于加快车载推理;</br>
(4)针对图像和BEV空间的强大<strong>数据增强策略</strong>以避免过度拟合;</br>
(5)利用时间信息的<strong>多帧特征融合</strong>机制。</p>
<p>其中，(1)和(3)使Fast BEV能够在车载芯片上快速推理和部署，(2)、(4)和(5)确保Fast BEV具有竞争性能。所有这些都使Fast BEV成为一种具有高性能、快速推理速度和在自动驾驶车载芯片上部署友好的解决方案。通过实验，在2080Ti平台上，R50模型可以在nuScenes验证集上以47.3%的NDS运行52.6 FPS，超过了BEVDepth-R50模型的41.3 FPS和47.5%的NDS，以及BEVDet4DR50模型的30.2 FPS和45.7%的NDS。最大的型号(R101@900x1600)在nuScenes验证集上建立了具有竞争力的53.5%NDS，论文在当前主流的车载芯片上进一步开发了具有相当高精度和效率的基准！</p>
<h2 id="领域现状">领域现状</h2>
<p>快速准确的3D感知系统对于自动驾驶至关重要。经典方法依赖于激光雷达点云提供的精确3D信息。然而，激光雷达传感器通常要花费数千美元，阻碍了它们在经济型车辆上的应用。基于纯相机的鸟瞰图(BEV)方法最近显示出其强大的3D感知能力和低成本的巨大潜力。它们基本上遵循这样的范式：将多摄像机2D图像特征转换为自我汽车坐标中的3D BEV特征，然后将特定头部应用于统一BEV表示以执行特定的3D任务，例如，3D检测、分割等。统一BEV表达可以单独处理单个任务或同时处理多个任务，这是高效和灵活的。</p>
<p>为了从2D图像特征执行3D感知，nuScenes上的现有BEV方法使用基于查询的transformation [17]，[18]或基于隐式/显式深度的transformation [13]，[15]，[26]。然而，它们很难部署在车载芯片上，并且推理速度慢：</p>
<ul>
<li>(1) 基于基于查询的transformation 方法如图1(a)所示，由于解码器需要transformer内的注意机制，这些方法通常需要专用芯片来支持。</li>
<li>(2) 基于深度变换的方法如图1(b)所示。</li>
</ul>
<p>这些方法通常需要加速不友好的体素池操作，甚至多线程CUDA内核也可能不是最佳解决方案。此外，这对于在资源受限或CUDA加速推理库不支持的芯片上运行是不方便的。此外，它们在推理上很耗时，这妨碍了它们的实际部署。本文旨在为车载芯片设计一种具有友好部署、高推理速度和竞争性能的BEV感知框架，例如Xavier、Orin、Tesla T4等。</p>
<p></p>
<p>基于这些观察结果，遵循M2BEV[16]的原理，该原理假设在图像到BEV(2D到3D)视图转换期间沿相机光线的深度分布均匀，我们提出<strong>Fast-Ray转换</strong>，如图1(c)所示，借助于“查找表”和“多视图到一个体素”操作，将BEV转换加速到一个新的水平。基于Fast Ray变换，论文进一步提出了Fast BEV，这是一种更快、更强的全卷积BEV感知框架，无需昂贵的视图transformer[17]、[18]或深度表示[15]、[23]、[26]。所提出的快速BEV包括五个部分，<font color=red>Fast-Ray变换</font>、<font color=red>多尺度图像编码器</font>、<font color=red>高效BEV编码器</font>、<font color=red>数据增强</font>和<font color=red>时间融合</font>，这些共同构成了一个框架，赋予Fast BEV快速推理速度和有竞争力的性能。</p>
<p>详细展开说，本文提出了Fast Ray变换，这是一种用于快速推理的轻量级和部署友好的视图变换，通过将多视图2D图像特征沿着相机射线的体素投影到3D来获得BEV表示。此外，还提出了两种操作，即“查找表”和“多视图到一个体素”，以优化车载平台的流程。现有工作的视图转换耗时，多尺度投影操作将具有较大的时间开销，因此难以在实践中使用。基于Fast Ray变换，本文的2D到3D投影具有极快的速度，使具有多尺度投影操作的多尺度图像编码器成为可能。具体而言，与大多数使用单尺度特征作为图像编码器输出的现有工作不同，论文在图像编码器输出部分使用了3层多尺度特征金字塔网络(FPN)结构。随后是相应的3级多尺度投影操作。对于BEV编码器，作者使用很少的原始残差网络作为基本BEV编码器。在此基础上，使用三维缩减操作来加速编码器，分别是“空间到信道”(S2C)算子、多尺度级联融合(MSCF)算子和多帧级联融合(MFCF)算子。论文还进一步为图像和BEV空间引入了强大的数据增强策略，如翻转、旋转、调整大小等。分别在图像空间和BEV中执行的数据增强不仅避免了过度拟合，而且实现了更好的性能。最后引入了时间融合[17]，[23]，它通过引入时间特征融合模块将Fast BEV从纯空间扩展到时空空间，使当前关键帧能够利用来自历史帧的信息。</p>
<p>BEV感知在学术界经常更新性能基准，如NuScenes基准，但很少在工业应用方面进行研究。本文首次在当前流行的车载芯片上开发了一个具有相当准确度和效率的基准，从延迟到不同计算能力的车载芯片之间的性能，这为BEV解决方案的实际部署提供了参考。凭借其高效率和具有竞争力的性能，Fast BEV打破了现有BEV解决方案难以在低计算芯片上部署的信念，<strong>简单</strong>和<strong>高效</strong>是其主要优势。所提出的Fast BEV表现出了出色的性能，可以轻松部署在车载平台上。在nuScenes数据集上，在2080Ti平台上，R50模型可以在nuScene验证集上运行52.6 FPS和47.3%的NDS，超过了BEVDepth-R50模型的41.3 FPS和47.5%的NDS以及BEVDet4D-R50模型的30.2 FPS和45.7%的NDS。最大的型号(R101@900x1600)在nuScenes验证集上建立了具有竞争力的53.5%NDS。</p>
<h2 id="领域的主流方案梳理">领域的主流方案梳理</h2>
<p><strong>1)基于camera的单目3D目标检测</strong></p>
<p>3D目标检测中的检测器旨在预测物体在3D空间中的位置和类别，给定由激光雷达或相机传感器生成的输入。基于LiDAR的方法，例如CenterPoint，倾向于使用3D CNN从LiDAR点提取空间特征，并进一步回归到3D属性，如目标的3D中心。与LiDAR传感器相比，仅将相机图像作为输入不仅成本更低，而且图像包含更丰富的语义信息。单目3D目标检测的一种实用方法是基于3D图像特征学习3D边界框，M3DRPN提出了3D区域建议网络和深度卷积层，以提高3D场景理解。在FCOS之后，FCOS3D[7]通过将3D目标转换为图像域，直接预测每个对象的3D边界框。PGD[9]使用对象之间的关系和概率表示来捕获深度不确定性，以便于3D对象检测的深度估计，DD3D[10]受益于深度预训练，并显著改善端到端3D检测！</p>
<p><strong>2)基于camera的环视3D目标检测</strong></p>
<p>一些大型基准的最新进展，特别是使用更多的周围视图，进一步推动了3D感知领域。在基于camera的3D目标检测中，新提出的多视图transformation 技术[11]、[12]、[13]、[14]将任务重新表述为立体匹配问题，其中周围的图像特征转换成立体表示，如BEV或3D体素。例如，LSS在预测的深度分布上投影逐像素特征，以生成相机截头体，然后转换截头体进入BEV grid。OFT[14]提出通过将预定义的体素投影到图像特征上来生成体素表示，BEVDet将LSS应用于全卷积方法，并首先验证了显式BEV的性能。M2BEV首先跟随OFT探索BEV多任务感知，BEVDepth进一步扩展了LSS[13]，具有强大的深度监督和高效的池化操作。视图转换的另一个路线图是网格状的BEV查询，DETR3D和Graph-DETR3D将每个BEV查询解码为3D参考点，以从图像中采样相关的2D特征进行细化。BEVFormer在二维到三维转换中引入了空间交叉关注，允许每个查询可以跨相机视图聚合其相关的二维特征。PETR提出3D坐标生成来感知3D位置感知特征，避免生成3D参考点。这些工作的成功激励我们有效和高效地扩展周围的多摄像机检测pipeline。作者发现在类似LSS的方法中，使用深度分布是不必要的，并且可以删除它以进一步加快整个pipeline的速度！</p>
<p><strong>3)基于camera的多视图时间融合</strong></p>
<p>最近，一些基于相机的方法试图在检测过程中引入多帧融合，这已被证明对基于LiDAR的检测器中的速度估计和box定位有效[20]，[21]，[22]。而BEV作为一种同时组合来自多个相机的视觉信息的中间特征，适合于时间对齐。[40]提出了一种动态模块，该模块使用过去的空间BEV特征来学习时空BEV表示，BEVDet4D通过对齐多帧特征并利用自我运动中的空间相关性来扩展BEVDet[15]。PETRv2基于3D位置嵌入的视角，直接实现3D空间中的时间对齐。BEVFormer设计了一种时间自关注，以递归地融合历史BEV信息，类似于RNN模型中的隐藏状态。本文的工作也受到时间对齐的启发，具体来说，我们应用该技术来进一步提高性能，同时保持高效率。</p>
<h2 id="本文的方法">本文的方法</h2>
<p>BEV感知中最重要的是如何将2D特征转移到3D空间。如图1所示，基于查询的方法通过变换器中的注意力机制获得3D BEV特征，这个过程可以表示为以下公式(1)，基于深度的方法通过计算2D特征和预测深度的外积来获得3D BEV特征，具体过程如公式(2)所示:</p>
<p>$$F_{bev}(x,y,z)=Attn(q,k,v)\quad(1)$$</p>
<p>$$F_{bev}(x,y,z)=Pool{F_{2D}(u,v)\otimes D(u,v)}_{x,y,z}\quad(2)$$</p>
<p>其中F2D(u,v)表示从图像中提取的2D特征，D(u,v)表示来自2D特征的深度预测。⊗表示out producter，Pool表示体素池操作。x、y、z是三维空间中的平均坐标，u，v在二维空间中的坐标。在CUDA多线程的支持下，这些方法大大提高了GPU平台上的推理速度，但在更大的分辨率和特征维度上会遇到计算速度瓶颈，并且在没有推理库支持的情况下转移到非GPU平台不是很友好。</p>
<p>作者提出了基于光线投影的Fast-Ray transformation方法，借助于查找表和多视图到一个体素操作，以便在GPU平台上实现极高的2D到3D推理速度。此外，由于其标量索引的高效性，当在CPU平台上运行时，它仍然具有优于现有解决方案的速度性能，这使得转移到更多平台成为可能。</p>
<p></p>
<p>M2BEV[16]是解决具有统一BEV表示的多摄像机多任务感知的第一个工作之一，因为它没有昂贵的视图转换器或深度表示，因此在车载平台上具有巨大的应用潜力。受其简单性的启发，本文提出了具有卓越速度和性能的Fast BEV，如图2所示，Fast BEV将多摄像机图像作为输入，并预测3D边界框(包括速度)作为输出。其主要框架可分为五个关键模块：Fast-Ray Transformation、Multi-Scale Image Encoder、Efficient BEV Encoder、Data Augmentation、Temporal Fusion！</p>
<p><strong>1) Fast-Ray Transformation</strong></p>
<p><font color=green><strong>视图转换</strong></font>是将特征从2D图像空间转换到3D BEV空间的关键组件，这通常在整个pipelines中花费大量时间。论文按照[16]，[34]假设沿射线的深度分布是均匀的，这种方式优点是，一旦获得了相机的内在/外在参数，就可以很容易地知道2D到3D的投影。由于这里没有使用可学习的参数，可以很容易地计算图像特征图和BEV特征图中的点之间的对应矩阵。基于这一假设，本文从两个角度进一步加速该过程：<font color=red>预计算投影索引(查找表)</font>和<font color=red>密集体素特征生成(多视图到一个体素)</font>。</p>
<p></p>
<p><font color=red><strong>查找表</strong></font>。投影索引是从2D图像空间到3D体素空间的映射索引，考虑到在构建感知系统时相机位置及其内在/外在参数是固定的，并且本文的方法既不依赖于数据相关的深度预测，也不依赖于transformer，因此每个输入的投影指数都是相同的。所以不需要为每次迭代计算相同的索引，只需预先计算固定投影索引并将其存储。在推断过程中，可以通过查询查找表来获得投影索引，这是边缘设备上的一个超级便宜的操作。此外，如果我们从单个帧扩展到多个帧，还可以很容易地预先计算内部和外部参数，并将它们与当前帧预对齐。如算法1所示，通过相机参数矩阵投影构建了具有与输出三维体素空间相同维度的查找表LUT。迭代每个体素单元，并通过投影计算对应于3D坐标的2D像素坐标。如果获得的2D像素坐标地址是合法的，可以将其填充到LUT中以建立与数据无关的索引映射！</p>
<p></p>
<p><font color=red><strong>多视图到一个体素</strong></font>。基本视图变换使用原始体素聚合操作，该操作为每个相机视图存储离散体素特征，然后<strong>聚合</strong>它们以生成最终体素特征。如图3(a)所示，这是填充的每个离散体素的鸟瞰图。因为每个相机只有有限的视角，所以每个体素特征非常稀疏，例如，只有大约17%的位置是非零的。我们发现这些体素特征的聚集是非常昂贵的，因为它们的巨大尺寸。<font color=green>本文建议生成密集体素特征以避免昂贵的体素聚集</font>，具体来说，让所有相机视图中的图像特征投影到同一个体素特征，从而在最后生成一个密集体素，名为“多视图到一个体元”。如图3(b)所示，这是填充了密集体素的鸟瞰图。如算法2所示的快速射线变换算法，其将输入的多视图2D图像特征转移到一个体素3d空间中，其中每个体素单元由预先计算的LUT填充相应的2D图像特征。对于具有重叠区域的多个视图的情况，直接采用第一个遇到的视图来提高表构建的速度。结合“查找表”和“多视图到一个体素”加速设计，视图转换操作具有极快的投影速度！</p>
<p></p>
<p><strong>2)多尺度Image Encoder</strong></p>
<p>多尺度图像编码器从多视图图像中提取多层次特征，N个图像∈$R^{H×W×3}$作为输入，F1/4、F1/8、F1/16三级特征作为输出。</p>
<p></p>
<p><strong>3)高效BEV编码器</strong></p>
<p>BEV特征是4D张量，时间融合将叠加特征，这将使BEV编码器具有大量计算量。三维缩减操作用于加快编码器的速度，即<u>“空间到信道”(S2C)算子</u>、多尺度，其中所述多帧融合(MFCF)算子分别是<font color=red><u>级联融合(MSCF)算子</u></font>和<font color=red><u>多帧凹融合(MFF)算子</u></font>。S2C算子将4D体素张量V∈RX×Y×Z×C转换为3D BEV张量V∈R X×Y×(ZC)，从而避免使用内存昂贵的3D卷积。在MFCF算子之前，值得注意的是，通过多尺度投影获得的BEV特征是不同的尺度。论文将首先对X和Y维度上的多尺度BEV特征进行上采样，使其大小相同，例如200×200。MSCF和MFCF运营商在信道维度中合并多尺度多帧特征，并将它们从较高的参数量融合到较低的参数量。此外，通过实验发现，BEV编码器和3D体素分辨率的大小对性能的影响相对较小，但占用了较大的速度消耗，因此更少的block和更小的体素分辨率也更为关键！</p>
<p><strong>4)数据增强</strong></p>
<p>数据扩充的好处已在学术界形成共识。此外，3D数据集(如NuScenes、KITTI)很难标记，且成本高昂，这导致数据集中样本数量较少，因此数据增强可以带来更显著的性能提升。<strong>本文在图像空间和BEV空间中添加了数据增强，主要遵循BEVDet</strong>。</p>
<p><strong>图像增强</strong>：由于3D场景中的图像与3D相机坐标有直接关系，因此3D目标检测中的数据增强比2D检测更具挑战性。因此，如果对图像应用数据增强，还需要改变相机固有矩阵。对于增强操作，基本上遵循常见的操作，例如翻转、裁剪和旋转，在图5的左侧部分，展示了一些图像增强的示例。</p>
<p><strong>BEV增强</strong>：类似于图像增强，类似的操作可以应用于BEV空间，例如翻转、缩放和旋转。注意，增强变换应应用于BEV特征图和3D GT框，以保持一致性。BEV增强变换可以通过相应地修改相机外部矩阵来控制，在图5的右侧部分，展示了随机旋转增强，一种BEV增强！</p>
<p></p>
<p><strong>5)时间融合</strong></p>
<p>受BEVDet4D和BEVFormer的启发，作者还将历史帧引入到当前帧中以进行时间特征融合。<strong>通过空间对齐操作和级联操作，将历史帧的特征与当前帧的对应特征融合</strong>。时间融合可以被认为是帧级的特征增强，在一定范围内较长的时间序列可以带来更多的性能增益。具体来说，用三个历史关键帧对当前帧进行采样;每个关键帧具有0.5s间隔，本文采用了BEVDet4D中的多帧特征对齐方法。如图6所示，在获得四个对齐的BEV特征后，直接将它们连接起来，并将它们馈送到BEV编码器。在训练阶段，使用图像编码器在线提取历史帧特征，在测试阶段，历史帧功能可以离线保存，并直接取出用于加速。与BEVDet4D和BEVFormer进行比较，BEVDet4D只引入了一个历史框架，我们认为这不足以利用历史信息。Fast BEV使用三个历史帧，从而显著提高了性能，通过使用两个历史帧，BEVFormer略优于BEVDet4D。然而，由于记忆问题，在训练阶段，历史特征在没有梯度的情况下被分离，这不是最佳的。此外，BEVFormer使用RNN样式来顺序融合特征，这是低效的。相比之下，Fast BEV中的所有帧都以端到端的方式进行训练，这与普通GPU相比更易于训练！</p>
<h2 id="实验">实验</h2>
<p>数据集描述：评估了nuScenes数据集上的 Fast BEV，该数据集包含1000个自动驾驶场景，每个场景20秒。数据集被分成850个场景用于训练/验证，其余150个场景用于测试。虽然nuScenes数据集提供来自不同传感器的数据，但我们只使用相机数据。相机有六个视图：左前、前、右前、左后、后、右后。</p>
<p>评估指标。为了全面评估检测任务，使用平均精度(mAP)和nuScenes检测分数(NDS)的标准评估指标进行3D目标检测评估。此外，为了计算相应方面的精度(例如，平移、缩放、方向、速度和属性)，使用平均平移误差(mATE)、平均缩放误差(mASE)、平均方向误差(mAOE)、平均速度误差(mAVE)，以及平均属性误差(mAAE)作为度量。</p>
<p>和主流方法的Latency进行比较：</p>
<p></p>
<p>在可比性能下，Fast BEV、BEVDet4D和BEVDepth方案的端到端延迟比较。表的上部是三种方案的详细设置，包括每个组件的具体配置。表的下半部分是在可比性能下三种方案的每个部分的延迟和总延迟的比较。2D到3D部分包括CPU和CUDA两个平台的延迟，MSO表示多尺度输出。</p>
<p></p>
<p>nuScenes集的比较。“L”表示激光雷达，“C”表示摄像机，“D”表示深度/激光雷达监控。MS表示图像和BEV编码器中的多尺度。“¶”表示我们使用MS、scale NMS和测试时间数据增强的方法。</p>
<p></p>
<p>更多消融实验对比：</p>
<p></p>
<p></p>
<p>高效型号系列：为了满足不同计算能力平台的部署需求，本文设计了一系列从M0到M5的高效模型，如表10所示。设置了不同的图像编码器(从ResNet18到ResNet50)，图像分辨率(从256×704到900×1600)、体素分辨率(从200×200×4到250×250×6)和BEV编码器(从2b-192c到6b-256c)来设计模型尺寸。从表10可以看出，从M0到M5，随着图像编码器、图像分辨率、体素分辨率和BEV编码器逐渐变大，模型性能逐渐提高。</p>
<p>在流行设备上部署：除了注重性能，还将M系列车型部署在不同的车载平台(Xavier、Orin、T4)上，并使用CUDA-TensorRT-INT8加速。具体而言，AGX Xavier在没有使用CUDA11.4-TRTT8.4.0-INT8部署DLA加速的情况下的计算能力为22TOPS，AGX Orin 64G在没有使用CUDA11.4-TTRT8.4.0-INT8部署DLB加速的情况下的计算能力是170TOPS，T4在使用CUDA11.1-TRT7.2.1-INT8部署时的计算能力则是130TOPS。如表11所示，评估了这些车载设备上M系列模型的延迟，并将延迟分解为2D/2D到3D/3D三个部分。</p>
<p>从表11中可以看出：(1)随着M系列型号逐渐变大，性能逐渐提高，同一计算平台上的延迟也基本上逐渐增加，2D和3D部分的延迟也分别增加。(2) 从左到右，随着这三个设备的实际计算能力1逐渐增加，M系列的每个模型的延迟逐渐减少，2D和3D部分的延迟分别减少。(3) 结合表11中M系列的性能，可以发现，仅考虑延迟时，M0模型在Xavier等低计算平台上可以达到19.7FPS，这可以实现实时推理速度。考虑到性能，M2模型在性能和延迟之间具有最合理的权衡。在与表2中R50系列模型的性能相当的前提下，它在Orin平台上可以达到43.3 FPS，这可以实现实际的实时推理要求。</p>
<p>目前，BEV感知解决方案越来越多，但它们主要追求学术领域的性能，很少考虑如何更好地将其部署在车载芯片上，尤其是低计算芯片上。不能否认，BEVDet和BEVDepth等工作是当前考虑的首批可能解决方案，因为它们在部署车载芯片时非常方便。但Fast BEV在以下情况下提供了应用的可能性：</p>
<p>低计算能力芯片：尽管自动驾驶芯片的计算能力在逐渐增加，但一些计算能力较低的芯片，如英伟达Xavier，仍被用于经济型车辆。Fast BEV可以以更快的速度在低计算能力芯片上表现得更好。</p>
<p>非GPU部署：DEVEDepth和BEVDet的成功部署和应用主要依赖于CUDA多线程支持的高效体素池操作。然而，没有CUDA库的非GPU芯片，例如以DSP为计算单元的德州仪器芯片，很难开发DSP多线程处理器，CPU速度不够快，这使得它们的解决方案在此类芯片上失去了优势。Fast BEV以其快速的CPU速度提供了在非GPU芯片上部署的便利。</p>
<p>实践中可扩展：随着技术的发展，许多自动驾驶制造商已经开始放弃激光雷达，只使用纯摄像头进行感知。结果，在真实车辆收集的大量数据中没有深度信息。在实际开发中，模型放大或数据放大通常基于从真实车辆收集的数据，以利用数据潜力提高性能。在这种情况下，基于深度监控的解决方案遇到瓶颈，而Fast BEV不引入任何深度信息，可以更好地应用！</p>
<p>ref:</br>
[1]. 论文：https://arxiv.org/abs/2301.12511</br>
[2]. 代码：https://github.com/Sense-GVT/Fast-BEV</br>
[3]. <a href="https://mp.weixin.qq.com/s/aVbaw2qYc6-i21zbCNhfOg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/aVbaw2qYc6-i21zbCNhfOg<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[4]. <a href="https://zhuanlan.zhihu.com/p/608929598"target="_blank" rel="external nofollow noopener noreferrer">FastBEV 代码注释<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
]]></description></item></channel></rss>