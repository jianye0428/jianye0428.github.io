<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>GPU - 分类 - yejian's blog</title><link>https://jianye0428.github.io/categories/gpu/</link><description>GPU - 分类 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Wed, 12 Jul 2023 10:48:05 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/categories/gpu/" rel="self" type="application/rss+xml"/><item><title>CUDA Introduction</title><link>https://jianye0428.github.io/posts/cuda/</link><pubDate>Wed, 12 Jul 2023 10:48:05 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/cuda/</guid><description><![CDATA[<p>[1] <a href="https://blog.csdn.net/Augusdi/article/details/12187291"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Augusdi/article/details/12187291<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="cuda编程">CUDA编程</h2>
<h3 id="1什么是cuda">1.什么是CUDA</h3>
<p>CUDA(Compute Unified Device Architecture)，统一计算架构，是NVidia推出的并行计算平台。NVidia官方对其的解释是：一个并行计算平台和简单（简洁）地使用图像处理单元（GPU）进行通用计算的编程模型。利用GPU的能力在计算性能上有惊人的提升。</p>
<p>简单地说CUDA是便于程序员利用NVidia GPU进行通用计算的开发环境及工具，目前支持C/C++语言，将来还会支持Fortran语言。</p>
<h3 id="2为什么要用到cuda">2.为什么要用到CUDA</h3>
<p>CPU主频要比GPU高2-3倍左右，但是通常情况下GPU核心的数量要比CPU多2-3个数量级以上。因此GPU的计算能力要远大于CPU，充分发挥GPU的计算能力，可以有成倍的性能提升。</p>
<p>早期利用GPU的计算能力是使用着色器和着色语言（GLSL等）。目前广泛使用的是CUDA和OpenCL。CUDA是针对NVidia GPU硬件设备设计的，而 OpenCL是针对跨平台设计的。因此CUDA可充分发挥NVidia GPU的计算性能。</p>
<p>CUDA可以直接使用C/C++语言来开发GPU程序，省去了程序员重新学一种新语言的麻烦。</p>
<h3 id="3cuda环境搭建">3.CUDA环境搭建</h3>
<p>CUDA环境主要分为四点：硬件（GPU设备）、操作系统、C/C++编译器和CUDA工具包。</p>
<p>硬件（GPU设备），必须是支持CUDA的GPU。可到NVidia官网查询支持CUDA的GPU设备，具体地址为：http://www.nvidia.com/object/cuda_home_new.html 。</p>
<p>操作系统，支持Microsoft Windows、Mac OS X和Linux。</p>
<p>C/C++编译器，对不同的操作系统有不同的要求。</p>
<p>CUDA工具包，NVidia提供了不同操作系统对应的CUDA Toolkit，可从https://developer.nvidia.com/cuda-downloads 下载对应的版本。</p>
<p>本文只以Microsoft Windows为例介绍如何搭建CUDA环境。</p>
<p>准备材料：</p>
<p>·一台装有支持CUDA GPU的电脑。</p>
<p>·Microsoft Windows操作系统（Microsoft Windows XP,Vista,7,or 8 or Windows Server 2003 or 2008）。</p>
<p>·CUDA工具包（相应操作系统）。下载地址：https://developer.nvidia.com/cuda-downloads</p>
<p>·C/C++编译器：Microsoft Visual Studio 2008 或 2010，或者对应版本的Microsoft Visual C++ Express产品。</p>
<p>安装步骤：</p>
<p>·在装有支持CUDA GPU的电脑上安装Microsoft Windows操作系统（一般情况下都已经完成这步骤）。</p>
<p>·安装C/C++编译器，可只安装其中的C++编译器部分。</p>
<p>·安装CUDA工具包。（CUDA工具包中有NVidia GPU的驱动程序，尚未安装的请选择安装。）</p>
<p>安装验证：</p>
<p>Windows XP系统：进入 C:\Documents and Settings\All Users\Application Data\NVIDIA Corporation\CUDA Samples\v5.0\bin\win32\Release 目录运行deviceQuery.exe文件。</p>
<p>Windows Vista, Windows 7, Windows 8, Windows Server 2003, and Windows Server 2008系统：进入 C:\ProgramData\NVIDIA Corporation\CUDA Samples\v5.0\bin\win32\Release 目录运行deviceQuery.exe文件。</p>
<p>如果安装正确，执行deviceQuery.exe文件会得到GPU设备的相应信息。如果没有安装支持CUDA的GPU也会得出GPU的信息，其中CUDA Capability Major/Minor version number信息为9999.9999。</p>
<p>Microsoft Windows上更详细的安装信息请查看：http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/index.html 。</p>
<p>Mac OS X的安装：http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-mac-os-x/index.html 。
Linux的安装：http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/index.html 。</p>
<h3 id="4第一个cuda程序">4.第一个CUDA程序</h3>
<p>在Microsoft Windows系统上，如果成功搭建了CUDA环境，则在Microsoft Visual Studio中已经集成了CUDA的开发组件。</p>
<p>以下以Windows 7 + Microsoft Visual Studio 2008为例，创建第一个CUDA程序。</p>
<p>打开Microsoft Visual Studio 2008，依次：File-&gt;New-&gt;Project-&gt;NVIDIA-&gt;CUDA-&gt;CUDA 5.0 Runtime，输入相应的项目名称确定即可。</p>
<p>默认会生成一个kernel.cu文件，内容如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span><span class="lnt">92
</span><span class="lnt">93
</span><span class="lnt">94
</span><span class="lnt">95
</span><span class="lnt">96
</span><span class="lnt">97
</span><span class="lnt">98
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;cuda_runtime.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;device_launch_parameters.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">addWithCuda</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">addKernel</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">arraySize</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="n">arraySize</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span> <span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="n">arraySize</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span> <span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">c</span><span class="p">[</span><span class="n">arraySize</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="mi">0</span> <span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Add vectors in parallel.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">addWithCuda</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">arraySize</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">printf</span><span class="p">(</span><span class="s">&#34;{1,2,3,4,5} + {10,20,30,40,50} = {%d,%d,%d,%d,%d}</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">4</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// cudaThreadExit must be called before exiting in order for profiling and
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// tracing tools such as Nsight and Visual Profiler to show complete traces.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">cudaThreadExit</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Helper function for using CUDA to add vectors in parallel.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">addWithCuda</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="o">*</span><span class="n">dev_b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="o">*</span><span class="n">dev_c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Choose which GPU to run on, change this on a multi-GPU system.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">cudaSetDevice</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Allocate GPU buffers for three vectors (two input, one output)    .
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Copy input vectors from host memory to GPU buffers.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">cudaMemcpy</span><span class="p">(</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">cudaMemcpy</span><span class="p">(</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Launch a kernel on the GPU with one thread for each element.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">addKernel</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// cudaThreadSynchronize waits for the kernel to finish, and returns
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// any errors encountered during the launch.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">cudaThreadSynchronize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Copy output vector from GPU buffer to host memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="nf">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">cudaFree</span><span class="p">(</span><span class="n">dev_c</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">cudaFree</span><span class="p">(</span><span class="n">dev_a</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">cudaFree</span><span class="p">(</span><span class="n">dev_b</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>代码1</p>
<p>这是一个将两个一维数组相加的例子。</p>
<p>其中addKernel是内核函数，它的计算过程是在GPU上实现的，用函数类型限定符__global__限制，且函数类型为void型。</p>
<p>cuda_runtime.h头文件包括了运行时API和其参数的定义。（如果使用驱动API则使用cuda.h头文件）。</p>
<p>device_launch_parameters.h头文件包含了内核函数的5个变量threadIdx、blockDim、blockIdx、gridDim和wrapSize。</p>
<p>对其中CUDA运行时API函数的解释：</p>
<ul>
<li>
<p>cudaSetDevice()：选择设备（GPU）。（可以不使用，不使用的情况下，默认选择设备0）</p>
</li>
<li>
<p>cudaMalloc()：动态分配显存。</p>
</li>
<li>
<p>cudaMemcpy()：设备与主机之内的数据拷贝。</p>
</li>
<li>
<p>cudaThreadSynchronize()：同步所有设备上的线程，等待所有线程结束。</p>
</li>
<li>
<p>cudaFree():释放由cudaMalloc分配的显存。</p>
</li>
<li>
<p>cudaThreadExit():结束CUDA上下文环境，释放其中的资源。</p>
</li>
</ul>
<p>这些函数的具体介绍在 <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/index.html"target="_blank" rel="external nofollow noopener noreferrer">http://docs.nvidia.com/cuda/cuda-runtime-api/index.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 中。</p>
<h3 id="5-cuda编程">5. CUDA编程</h3>
<h4 id="51-基本概念">5.1. 基本概念</h4>
<p>CUDA编程中需要注意一些基本概念，分别为：主机(host)、设备(device)、运行时API、驱动API、warp、bank、函数类型限定符、变量类型限定符、thread、block、grid、计算能力、SIMT、内置变量、纹理、CUDA数组等。</p>
<p>主机(host)：可理解为CPU与内存的组合。</p>
<p>设备(device)：可理解为GPU与显存的组合。</p>
<p>运行时API：是指CUDA运行时API是在驱动API的基础上封装而成的，简化了CUDA的开发。</p>
<p>驱动API：是指CUDA驱动API，相比运行时API更接近于设备，可灵活运用设备的特性开发CUDA，可实现运行时API无法实现的功能。</p>
<p>warp：多处理器激活、管理、调度和执行并行任务的单位。计算能力2.x的设备warp为32个线程。未来的设备可能不同，可以通过内置变量warpSize查询。</p>
<p>bank：为了获得较高的存储器带宽，共享存储器被划分为多个大小相等的存储器模块，称为存储体，这些存储体就叫bank，可同步访问。</p>
<p>函数类型限定符：是CUDA C中特有的，用来修饰是主机函数，设备调用的设备函数，还是主机调用的设备函数。有__device__、<strong>global</strong>、<strong>host</strong>。</p>
<p>变量类型限定符：是用来修饰设备变量的。有__device__、<strong>constant</strong>、<strong>shared</strong>。</p>
<p>thread：设备中的线程，与主机中的线程是同一个概念。</p>
<p>block：线程块，由一组线程组成。一个线程块中的所以线程会在同一个多处理器上执行，一个多处理器上可同时执行多个线程块。</p>
<p>grid：有所有线程块组成的网格。</p>
<p>计算能力：是NVidia GPU不同架构的计算能力。</p>
<p>SIMT：单指令多线程，与单指令多数据（SIMD）类似。一条指令多个线程一同执行，实现程序的并行化。</p>
<p>内置变量：有threadIdx、blockDim、blockIdx、gridDim、warpSize。其中threadIdx指此线程在线程块中的位置；blockDim指线程块维度；blockIdx指该线程块在网格中的位置；gridDim指线程块网格维度；warpSize指一个warp多少个线程。</p>
<p>纹理：本文主要涉及到的是纹理参考、纹理绑定、纹理获取。</p>
<p>CUDA数组：区别于线性存储器，对数据进行了对齐等的处理，包括一维、二维和三维。其中的数据为：一元、二元或四元组。</p>
<p><strong>CUDA编程模型基础</strong></p>
<p>在给出CUDA的编程实例之前，这里先对CUDA编程模型中的一些概念及基础知识做个简单介绍。CUDA编程模型是一个异构模型，需要CPU和GPU协同工作。在CUDA中，host和device是两个重要的概念，我们用host指代CPU及其内存，而用device指代GPU及其内存。CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：</p>
<pre><code>分配host内存，并进行数据初始化；分配device内存，并从host将数据拷贝到device上；调用CUDA的核函数在device上完成指定的运算；将device上的运算结果拷贝到host上；释放device和host上分配的内存。
</code></pre>
<p>上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，kernel是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用__global__符号声明，在调用时需要用&laquo;&lt;grid, block&raquo;&gt;来指定kernel要执行的线程数量，在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量threadIdx来获得。</p>
<p>由于GPU实际上是异构模型，所以需要区分host和device上的代码，在CUDA中是通过函数类型限定词开区别host和device上的函数，主要的三个函数类型限定词如下：</p>
<pre><code>__global__：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。__device__：在device上执行，单仅可以从device中调用，不可以和__global__同时用。__host__：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译。
</code></pre>
<p>要深刻理解kernel，必须要对kernel的线程层次结构有一个清晰的认识。首先GPU上很多并行化的轻量级线程。kernel在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，grid是线程结构的第一层次，而网格又可以分为很多线程块（block），一个线程块里面包含很多线程，这是第二个层次。线程两层组织结构如下图所示，这是一个gird和block均为2-dim的线程组织。grid和block都是定义为dim3类型的变量，dim3可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block可以灵活地定义为1-dim，2-dim以及3-dim结构，对于图中结构（主要水平方向为x轴），定义的grid和block如下所示，kernel在调用时也必须通过执行配置&laquo;&lt;grid, block&raquo;&gt;来指定kernel所使用的线程数及结构。</p>
<p>所以，一个线程需要两个内置的坐标变量（blockIdx，threadIdx）来唯一标识，它们都是dim3类型变量，其中blockIdx指明线程所在grid中的位置，而threaIdx指明线程所在block中的位置，如图中的Thread (1,1)满足：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="mi">1</span></span></span></code></pre></td></tr></table>
</div>
</div><p>一个线程块上的线程是放在同一个流式多处理器（SM)上的，但是单个SM的资源有限，这导致线程块中的线程数是有限制的，现代GPUs的线程块可支持的线程数可达1024个。有时候，我们要知道一个线程在blcok中的全局ID，此时就必须还要知道block的组织结构，这是通过线程的内置变量blockDim来获得。它获取线程块各个维度的大小。对于一个2-dim的block ，线程 的ID值为 ，如果是3-dim的block ，线程 的ID值为</p>
<p>。另外线程还有内置变量gridDim，用于获得网格块各个维度的大小。</p>
<p>kernel的这种线程组织结构天然适合vector,matrix等运算，如我们将利用上图2-dim结构实现两个矩阵的加法，每个线程负责处理每个位置的两个元素相加，代码如下所示。线程块大小为(16, 16)，然后将N*N大小的矩阵均分为不同的线程块来执行加法运算。</p>
<p>此外这里简单介绍一下CUDA的内存模型，如下图所示。可以看到，每个线程有自己的私有本地内存（Local Memory），而每个线程块有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）。内存结构涉及到程序优化，这里不深入探讨它们。</p>
<p>还有重要一点，你需要对GPU的硬件实现有一个基本的认识。上面说到了kernel的线程组织层次，那么一个kernel实际上会启动很多线程，这些线程是逻辑上并行的，但是在物理层却并不一定。这其实和CPU的多线程有类似之处，多线程如果没有多核支持，在物理层也是无法实现并行的。但是好在GPU存在很多CUDA核心，充分利用CUDA核心可以充分发挥GPU的并行计算能力。GPU硬件的一个核心组件是SM，前面已经说过，SM是英文名是 Streaming Multiprocessor，翻译过来就是流式多处理器。SM的核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。当一个kernel被执行时，它的gird中的线程块被分配到SM上，一个线程块只能在一个SM上被调度。SM一般可以调度多个线程块，这要看SM本身的能力。那么有可能一个kernel的各个线程块被分配多个SM，所以grid只是逻辑层，而SM才是执行的物理层。SM采用的是SIMT (Single-Instruction, Multiple-Thread，单指令多线程)架构，基本的执行单元是线程束（warps)，线程束包含32个线程，这些线程同时执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。所以尽管线程束中的线程同时从同一程序地址执行，但是可能具有不同的行为，比如遇到了分支结构，一些线程可能进入这个分支，但是另外一些有可能不执行，它们只能死等，因为GPU规定线程束中所有线程在同一周期执行相同的指令，线程束分化会导致性能下降。当线程块被划分到某个SM上时，它将进一步划分为多个线程束，因为这才是SM的基本执行单元，但是一个SM同时并发的线程束数是有限的。这是因为资源限制，SM要为每个线程块分配共享内存，而也要为每个线程束中的线程分配独立的寄存器。所以SM的配置会影响其所支持的线程块和线程束并发数量。总之，就是网格和线程块只是逻辑划分，一个kernel的所有线程其实在物理层是不一定同时并发的。所以kernel的grid和block的配置不同，性能会出现差异，这点是要特别注意的。还有，由于SM的基本执行单元是包含32个线程的线程束，所以block大小一般要设置为32的倍数。</p>
<h4 id="52-线程层次结构">5.2. 线程层次结构</h4>
<p>CUDA线程的层次结构，由小到大依次为线程(thread)、线程块(block)、线程块网格(grid)。一维、二维或三维的线程组组成一个线程块，一维、二维或三维的线程块组组成一个线程块网格。</p>
<p>下图是由二维的线程块组组成的线程块网络，其中线程块是由二维的线程组组成。</p>
<p>图1 NVidia GPU的硬件结构是，一组流处理器组成一个多处理器，一个或多个多处理器组成一个GPU。其中流处理器，可以理解为处理计算的核心单元。多处理器类似于多核CPU。NVidia GPU从DX10（DirectX10）开始出现了Tesla、Fermi、Kepler架构，不同的架构多处理器中流处理器数量都有差别。</p>
<p>在进行CUDA编程前，可以先检查一下自己的GPU的硬件配置，这样才可以有的放矢，可以通过下面的程序获得GPU的配置属性：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">dev</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaDeviceProp</span> <span class="n">devProp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="nf">CHECK</span><span class="p">(</span><span class="nf">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">devProp</span><span class="p">,</span> <span class="n">dev</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;使用GPU device &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">dev</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">devProp</span><span class="p">.</span><span class="n">name</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;SM的数量：&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">devProp</span><span class="p">.</span><span class="n">multiProcessorCount</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;每个线程块的共享内存大小：&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">devProp</span><span class="p">.</span><span class="n">sharedMemPerBlock</span> <span class="o">/</span> <span class="mf">1024.0</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; KB&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;每个线程块的最大线程数：&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">devProp</span><span class="p">.</span><span class="n">maxThreadsPerBlock</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;每个EM的最大线程数：&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">devProp</span><span class="p">.</span><span class="n">maxThreadsPerMultiProcessor</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;每个SM的最大线程束数：&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">devProp</span><span class="p">.</span><span class="n">maxThreadsPerMultiProcessor</span> <span class="o">/</span> <span class="mi">32</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 输出如下
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="err">使用</span><span class="n">GPU</span> <span class="n">device</span> <span class="mi">0</span><span class="o">:</span> <span class="n">GeForce</span> <span class="n">GT</span> <span class="mi">730</span>
</span></span><span class="line"><span class="cl">    <span class="n">SM</span><span class="err">的数量：</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="err">每个线程块的共享内存大小：</span><span class="mi">48</span> <span class="n">KB</span>
</span></span><span class="line"><span class="cl">    <span class="err">每个线程块的最大线程数：</span><span class="mi">1024</span>
</span></span><span class="line"><span class="cl">    <span class="err">每个</span><span class="n">EM</span><span class="err">的最大线程数：</span><span class="mi">2048</span>
</span></span><span class="line"><span class="cl">    <span class="err">每个</span><span class="n">EM</span><span class="err">的最大线程束数：</span><span class="mi">64</span></span></span></code></pre></td></tr></table>
</div>
</div><p>ref: <a href="https://zhuanlan.zhihu.com/p/34587739"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/34587739<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="53-存储器层次结构">5.3. 存储器层次结构</h3>
<p>CUDA存储器有：寄存器(register)、共享存储器(shared memory)、常量存储器(constant memory)、本地存储器(local memory)、全局存储器(global memory)、纹理存储器等。其中寄存器和本地存储器是线程(thread)私有的，共享存储器是对线程块(block)中的所有线程可见，常量存储器、全局存储器和纹理存储器是对网格(grid)中所有线程可见。</p>
<p>下图解释了存储器的层次结构：</p>
<h4 id="54-运行时api">5.4. 运行时API</h4>
<p>运用运行时API开发CUDA程序需要了解：初始化、设备管理、存储器管理、流管理、事件管理、纹理参考管理、OpenGL互操作和Direct3D互操作。</p>
<p>运行时API文档地址为：http://docs.nvidia.com/cuda/cuda-runtime-api/index.html 。</p>
<h5 id="541-初始化">5.4.1. 初始化</h5>
<p>运行时API不存在显示初始化函数，初始化会在首次调用运行时函数时完成。虽然不需要调用初始化函数进行初始化，但是退出时需要调用退出函数cudaThreadExit()释放资源。</p>
<h5 id="542-设备管理">5.4.2. 设备管理</h5>
<p>有些电脑上可能有多块设备，因此对于不同的要求选择合适的设备。设备管理主要是获取设备信息和选择执行设备。</p>
<p>主要有三个函数：</p>
<p>·cudaGetDeviceCount()：得到电脑上设备的个数。</p>
<p>·cudaGetDeviceProperties()：获得对应设备的信息。</p>
<p>·cudaSetDevice()：设置CUDA上下文对应的设备。</p>
<p>运行__global__函数前需要提前选择设备，如果不调用cudaSetDevice()函数，则默认使用0号设备。</p>
<p>上面三个函数的具体用法请查看CUDA运行时API文档。</p>
<h5 id="543-存储器管理">5.4.3. 存储器管理</h5>
<p>共享存储器、常量存储器、线性存储器和CUDA数组的使用是存储器管理的主要部分。</p>
<h6 id="5431-共享存储器">5.4.3.1 共享存储器</h6>
<p>共享存储器，使用__shared__变量限定符修饰，可静态或动态分配共享存储器。</p>
<p>代码一：</p>
<ul>
<li>静态分配共享存储器，是在设备代码中直接分配共享存储器的大小，如下代码：</li>
</ul>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#define SHARED_MEM 16
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel</span><span class="p">(</span><span class="err">…</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">shared</span><span class="p">[</span><span class="n">SHARED_MEM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">nBlock</span><span class="p">,</span> <span class="n">nThread</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="err">…</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>代码2</p>
<ul>
<li>动态分配共享存储器，是在主机代码中使用内核函数的第三个特定参数传入分配共享存储器的大小，如下代码：
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#define SHARED_MEM 16
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">kernel</span><span class="p">(</span><span class="err">…</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">shared</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">nSharedMem</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">SHARED_MEM</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">nBlock</span><span class="p">,</span> <span class="n">nThread</span><span class="p">,</span> <span class="n">nSharedMem</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="err">…</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h6 id="5432-常量存储器">5.4.3.2. 常量存储器</h6>
<p>常量存储器，使用__constant__变量限定符修饰。使用常量存储器，是由于其在设备上有片上缓存，比全局存储器读取效率高很多。</p>
<p>使用常量存储器时会涉及的运行时API函数主要有：</p>
<p>·cudaMemcpyToSymbol()</p>
<p>·cudaMemcpyFromSymbol()</p>
<p>·cudaGetSymbolAddress()</p>
<p>·cudaGetSymbolSize()</p>
<p>主机代码中使用cudaGetSymbolAddress()获取__constant__或__device__定义的变量地址。设备代码中可通过提取__device__、__shared__或__constant__变量的指针获取变量地址。</p>
<h6 id="5433-线性存储器">5.4.3.3. 线性存储器</h6>
<p>线性存储器是使用cudaMalloc()、cudaMallocPitch()或cudaMalloc3D()分配的，使用cudaFree()释放。二维的时候建议使用cudaMallocPitch()分配，cudaMallocPitch()函数对对齐进行了调整。这三个分配函数对应cudaMemset()、cudaMemset2D()、cudaMemset3D()三个memset函数和cudaMemcpy()、cudaMemcpy2D()、cudaMemcpy3D()三个memcpy函数。</p>
<h6 id="5434-cuda数组">5.4.3.4. CUDA数组</h6>
<p>CUDA数组是使用cudaMallocArray()、cudaMalloc3DArray()分配的，使用cudaFreeArray()释放。</p>
<p>相关memcpy函数请查阅CUDA运行时API文档。</p>
<p>具体使用可查阅CUDA编程指南：http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html 。</p>
<h5 id="544-流管理">5.4.4. 流管理</h5>
<p>主机设备之间的内存拷贝与内核在设备上执行是异步的。在不使用流的情况下，是这样执行的：设备先从主机上拷贝内存，拷贝完成之后，再在设备上执行内核代码计算，最后当内核执行完毕，再把设备上的内存拷贝到主机上。当使用两个流的情况下，0号流执行内核代码的同时1号流拷贝主机内存到设备，1号流执行的同时0号流拷贝设备内存到主机（具体的实现并不一定如此，这里是为了说明流的作用简单做了假设）。两个流的情况下，部分内存拷贝和内置执行是同时进行的（异步的），比同步的内存拷贝和内核执行节省了时间。</p>
<p>与流有关的函数有：</p>
<pre><code>·cudaStreamCreate()：流的创建；

·cudaStreamDestroy()：流的销毁；

·cudaStreamSynchronize()：流同步；

·*Async：与流相关的其他函数。

内核&lt;&lt;&lt;…&gt;&gt;&gt;的第四个参数为哪个流。

CUDA编程指南中有对流具体实现的讲解。

https://blog.csdn.net/a925907195/article/details/39500915
</code></pre>
]]></description></item><item><title>CUDA_C_NOTES [5]</title><link>https://jianye0428.github.io/posts/cuda_05/</link><pubDate>Wed, 12 Jul 2023 10:40:20 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/cuda_05/</guid><description><![CDATA[<h2 id="第5章-共享内存和常量内存">第5章 共享内存和常量内存</h2>
<ul>
<li>了解数据在共享内存中是如何被安排的</li>
<li>掌握从二维共享内存到线性全局内存的索引转换</li>
<li>解决不同访问模式中存储体中的冲突</li>
<li>在共享内存中缓存数据以减少对全局内存的访问</li>
<li>使用共享内存避免非合并全局内存的访问</li>
<li>理解常量缓存和只读缓存之间的差异</li>
<li>使用线程束洗牌指令编程</li>
</ul>
<h3 id="51-cuda共享内存概述">5.1 CUDA共享内存概述</h3>
<p>GPU中有两种类型的内存:</p>
<ul>
<li>板载内存: 全局内存是较大的板载内存，具有相对较高的延迟。</li>
<li>片上内存: 共享内存是较小的片上内存，具有相对较低的延迟，并且共享内存可以提供比全局内存高得多的带宽</li>
</ul>
<p>共享内存通常的用途有:</p>
<ul>
<li>块内线程通信的通道</li>
<li>用于全局内存数据的可编程管理的缓存</li>
<li>高速暂存存储器，用于转换数据以优化全局内存访问模式</li>
</ul>
<h4 id="511-共享内存">5.1.1 共享内存</h4>
<p>共享内存（shared memory，SMEM）是GPU的一个关键部件。物理上，每个SM都有一个小的<strong>低延迟内存池</strong>，这个内存池被当前正在该SM上执行的线程块中的所有线程所共享。(共享内存就是SM上的一块低延迟内存池)</p>
<p>共享内存使同一个线程块中的线程能够互相协作，便于<strong>重用片上数据</strong>，并可以大大降低核函数所需的全局内存带宽。由于共享内存中的内容是由应用程序显式管理的，所以它通常被描述为<strong>可编程管理的缓存</strong>。</p>
<p><font color=red>当每个线程块开始执行时，会分配给它一定数量的共享内存。这个共享内存的地址空间被线程块中所有的线程共享。</font>它的内容和创建时所在的线程块具有相同生命周期。每个线程束发出共享内存访问请求。在理想的情况下，每个被线程束共享内存访问的请求在一个事务中完成。最坏的情况下，每个共享内存的请求在32个不同的事务中顺序执行。如果多个线程访问共享内存中的同一个字，一个线程读取该字后，通过多播把它发送给其他线程。</p>
<p>共享内存被SM中的所有常驻线程块划分，因此，共享内存是限制设备并行性的关键资源。一个核函数使用的共享内存越多，处于并发活跃状态的线程块就越少。</p>
<p></p>
<p><strong>可编程管理的缓存</strong></p>
<p>共享内存是一个可编程管理的缓存。当数据移动到共享内存中以及数据被释放时,我们对它有充分的控制权。由于在CUDA中允许手动管理共享内存,所以通过在数据布局上提供更多的细粒度控制和改善片上数据的移动,使得对应用程序代码进行优化变得更简单了</p>
<h4 id="512-共享内存分配">5.1.2 共享内存分配</h4>
<p>有多种方法可以用来分配或声明由应用程序请求所决定的共享内存变量。可以静态或动态地分配共享内存变量。在CUDA的源代码文件中,共享内存可以被声明为一个本地的CUDA核函数或是一个全局的CUDA核函数。CUDA支持一维、二维和三维共享内存数组的声明。</p>
<p>共享内存变量用下列修饰符进行声明: <code>__shared__</code></p>
<p>如果在核函数中进行声明,那么这个变量的作用域就局限在该内核中。如果在文件的任何核函数外进行声明,那么这个变量的作用域对所有核函数来说都是全局的。</p>
]]></description></item><item><title>CUDA_C_NOTES [4]</title><link>https://jianye0428.github.io/posts/cuda_04/</link><pubDate>Wed, 12 Jul 2023 10:40:14 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/cuda_04/</guid><description><![CDATA[<h2 id="ch04-全局内存">CH04 全局内存</h2>
<h3 id="41-cuda内存模型概述">4.1 CUDA内存模型概述</h3>
<p>在现有的硬件存储子系统下， 必须依靠内存模型获得最佳的延迟和带宽。 CUDA内存模结合了主机和设备的内存系统， 展现了完整的内存层次结构， 使你能显式地控制数据布以优化性能.s</p>
<h4 id="411-内存层次结构的优点">4.1.1 内存层次结构的优点</h4>
<p>两种不同类型的局部性:</p>
<ul>
<li>时间局部性：时间局部性认为如果一个数据位置被引用， 那么该数据在较短的间周期内很可能会再次被引用， 随着时间流逝， 该数据被引用的可能性逐渐降低</li>
<li>空间局部性：空间局部性认为如果一个内存位置被引用， 则附近的位置也可能会被引用</li>
</ul>
<p>现代计算机使用不断改进的<strong>低延迟低容量</strong>的内存层次结构来优化性能。 这种内存层次结构仅在支持局部性原则的情况下有效。 一个内存层次结构由具有不同延迟、 带宽容量的多级内存组成。 通常， 随着从处理器到内存延迟的增加， 内存的容量也在增加。</p>
<p>CPU和GPU的主存都采用的是DRAM（动态随机存取存储器），而低延迟内存（如CPU一级缓存）使用的则是SRAM（静态随机存取存储器）。内存层次结构中最大且最慢的级别通常使用磁盘或闪存驱动来实现。在这种内存层次结构中，当数据被处理器频繁使用时，该数据保存在低延迟、低容量的存储器中；而当该数据被存储起来以备后用时，数据就存储在高延迟、大容量的存储器中。这种内存层次结构符合大内存低延迟的设想。</p>
<p>GPU和CPU内存模型的主要区别是， CUDA编程模型能将内存层次结构更好地呈现给用户， 能让我们显式地控制它的行为.</p>
<h4 id="412-cuda内存模型">4.1.2 CUDA内存模型</h4>
<p>对于程序员来说， 一般有两种类型的存储器：</p>
<ul>
<li>可编程的： 你需要显式地控制哪些数据存放在可编程内存中</li>
<li>不可编程的： 你不能决定数据的存放位置， 程序将自动生成存放位置以获得好的性能</li>
</ul>
<p>在CPU内存层次结构中， 一级缓存和二级缓存都是不可编程的存储器。</p>
<p>CUDA内存模型提出了多种可编程内存的类型:</p>
<ul>
<li>寄存器 (register)</li>
<li>共享内存 (shared memory)</li>
<li>本地内存 (local memory)</li>
<li>常量内存（constant memory）</li>
<li>纹理内存 ()</li>
<li>全局内存(global memory)</li>
</ul>
<blockquote>
<p>一个核函数中的线程都有自己私有的本地内存。
一个线程块有自己的共享内存， 对同一线程块中所有线程都可见， 其内容持续线程块的整个生命周期。
所有线程都可以访问全局内存。
所有线程都能访问的<font color=red>只读内存空间</font>有： <strong>常量内存空间</strong>和<strong>纹理内存空间</strong>。 &gt; 全局内存、 常量内存和纹理内存空间有不同的用途。 纹理内存为各种数据布局提供了不同的寻址模式和滤波模式。
对于一个应用程序来说， 全局内存、 常量内存和纹理内存中的内容具有相同的生命周期.</p>
</blockquote>
<h5 id="4121-寄存器">4.1.2.1 寄存器</h5>
<p>寄存器是GPU上运行速度最快的内存空间。</p>
<p>核函数中声明的一个没有其他修饰符的自变量， 通常存储在寄存器中。  在核函数声明的数组中， 如果用于引用该数组的索引是常量且能在编译时确定， 那么该数组也存储在寄存器中。</p>
<p>寄存器变量对于每个线程来说都是私有的， 一个核函数通常使用寄存器来保存需要频
繁访问的线程私有变量。 寄存器变量与核函数的生命周期相同。 一旦核函数执行完毕， 就不能对寄存器变量进行访问了。</p>
<p>寄存器是一个在SM中由活跃线程束划分出的较少资源:</p>
<p>在Fermi架构中，每个线程最多有63个寄存器；
在Kepler架构中，每个线程最多有255个寄存器；</p>
<p>在核函数中使用较少的寄存器将使在SM上有更多的常驻线程块。 每个SM上并发线程块越多，使用率和性能就越高</p>
<p>如果一个核函数使用了超过硬件限制数量的寄存器， 则会用本地内存替代多占用的寄
存器。</p>
<h5 id="4122-本地内存local-memory">4.1.2.2 本地内存（local memory）</h5>
<p>编译器可能存放到本地内存中的变量有：</p>
<ul>
<li>在编译时使用未知索引引用的本地数组</li>
<li>可能会占用大量寄存器空间的较大本地结构体或数组</li>
<li>任何不满足核函数寄存器限定条件的变量</li>
</ul>
<p>“本地内存”这一名词是有歧义的： 溢出到本地内存中的变量本质上与全局内存在同一
块存储区域， 因此本地内存访问的特点是高延迟和低带宽， 并且如在本章后面的4.3节中所描述的那样， 本地内存访问符合高效内存访问要求.</p>
<h5 id="4123-共享内存">4.1.2.3 共享内存</h5>
<p>在核函数中使用如下修饰符修饰的变量存放在共享内存中：
<code>__shared__</code>
因为共享内存是片上内存， 所以与本地内存或全局内存相比， 它具有更高的带宽和更
低的延迟。 它的使用类似于CPU一级缓存， 但它是可编程的。</p>
<p>每一个SM都有一定数量的由线程块分配的共享内存。 因此， 必须非常小心不要过度使用共享内存， 否则将在不经意间限制活跃线程束的数量。</p>
<p>共享内存在核函数的范围内声明， 其<strong>生命周期伴随着整个线程块</strong>。 当一个线程块执行结束后， 其分配的共享内存将被释放并重新分配给其他线程块。</p>
<p>共享内存是线程之间相互通信的基本方式。 一个块内的线程通过使用共享内存中的数
据可以相互合作。 访问共享内存必须同步使用如下调用， 该命令是在之前章节中介绍过的CUDA运行时调用：
<code>void __syncthreads();</code></p>
<p>该函数设立了一个执行障碍点， 即同一个线程块中的所有线程必须在其他线程被允许
执行前达到该处。 为线程块里所有线程设立障碍点， 这样可以避免潜在的数据冲突。</p>
<p>SM中的一级缓存和共享内存都使用64KB的片上内存， 它通过静态划分， 但在运行时
可以通过如下指令进行动态配置：
<code>cudaError_t cudaFuncSetCacheConfig(const void* func, enum cadaFuncCache cacheConfig)</code></p>
<h5 id="4124-常量内存">4.1.2.4 常量内存</h5>
<p>常量内存驻留在设备内存中， 并在每个SM专用的常量缓存中缓存。 常量变量用如下
修饰符来修饰:
<code>__constant__</code></p>
<p>常量变量必须在全局空间内和所有核函数之外进行声明。 对于所有计算能力的设备，
都只可以声明64KB的常量内存。 常量内存是静态声明的， 并对同一编译单元中的所有核函数可见。</p>
<p><strong>核函数只能从常量内存中读取数据。（不能往常量内存中写数据）</strong> 因此， 常量内存必须在主机端使用下面的函数来
初始化：
<code>cudaError_t cudaMemoryToSymbol(const void* symbol, const void* src, size_t count)</code></p>
<p>这个函数将count个字节从src指向的内存复制到symbol指向的内存中， 这个变量存放在设备的全局内存或常量内存中。</p>
<p>线程束中的所有线程从相同的内存地址中读取数据时， 常量内存表现最好。</p>
<p>举个例子， 数学公式中的系数就是一个很好的使用常量内存的例子， 因为一个线程束中所有的线程使用相同的系数来对不同数据进行相同的计算。 如果线程束里每个线程都从不同的地址空间读取数据， 并且只读一次， 那么常量内存中就不是最佳选择， 因为每从一个常量内存中读取一次数据， 都会广播给线程束里的所有线程。</p>
<h5 id="4125-纹理内存">4.1.2.5 纹理内存</h5>
<p>纹理内存是一种通过<strong>指定的只读缓存访问的全局内存</strong>。 只读缓存包括硬件滤波的支持， 它可以将浮点插入作为读过程的一部分来执行。 纹理内存是对二维空间局部性的优化， 所以线程束里使用纹理内存访问二维数据的线程可以达到最优性能。</p>
<h5 id="4126-全局内存">4.1.2.6 全局内存</h5>
<p>全局内存是GPU中最大、 延迟最高并且最常使用的内存。 global指的是其作用域和生命周期。 它的声明可以在任何SM设备上被访问到， 并且贯穿应用程序的整个生命周期。</p>
<p>一个全局内存变量可以被静态声明或动态声明。 你可以使用如下修饰符在设备代码中
静态地声明一个变量：</p>
<p><code>__device__</code></p>
<p>在第2章的2.1节中， 你已经学习了如何动态分配全局内存。 在主机端使用cuda-Malloc
函数分配全局内存， 使用cudaFree函数释放全局内存。 然后指向全局内存的指针就会作为
参数传递给核函数。 全局内存分配空间存在于应用程序的整个生命周期中， 并且可以访问
所有核函数中的所有线程。 从多个线程访问全局内存时必须注意。 因为线程的执行不能跨
线程块同步， 不同线程块内的多个线程并发地修改全局内存的同一位置可能会出现问题，
这将导致一个未定义的程序行为。</p>
<p>优化内存事务对于获得最优性能来说是至关重要的。 当一个线程束执行内存加载/
存储时， 需要满足的传输数量通常取决于以下两个因素：</p>
<ul>
<li>跨线程的内存地址分布</li>
<li>每个事务内存地址的对齐方式</li>
</ul>
<p>对于一个给定的线程束内存请求， 事务数量和数据吞吐率是由设备的计算能力来确定
的。 对于计算能力为1.0和1.1的设备， 全局内存访问的要求是非常严格的。 对于计算能力高于1.1的设备， 由于内存事务被缓存， 所以要求较为宽松。 缓存的内存事务利用数据局部性来提高数据吞吐率。</p>
<h5 id="4127-gpu缓存">4.1.2.7 GPU缓存</h5>
<p>跟CPU缓存一样， GPU缓存是不可编程的内存。 在GPU上有4种缓存：</p>
<ul>
<li>一级缓存</li>
<li>二级缓存</li>
<li>只读常量缓存</li>
<li>只读纹理缓存</li>
</ul>
<p><font color=purple>每个SM都有一个一级缓存， 所有的SM共享一个二级缓存。</font> 一级和二级缓存都被用来在存储本地内存和全局内存中的数据， 也包括寄存器溢出的部分。对Fermi GPU和Kepler K40或其后发布的GPU来说， CUDA允许我们配置读操作的数据是使用一级和二级缓存，还是只使用二级缓存。</p>
<p>在GPU上只有内存加载操作可以被缓存，内存存储操作不能被缓存。
每个SM也有一个<strong>只读常量缓存</strong>和<strong>只读纹理缓存</strong>， 它们用于在设备内存中提高来自于各自内存空间内的读取性能。</p>
<h5 id="4128-cuda变量声明总结">4.1.2.8 CUDA变量声明总结</h5>
<h5 id="4129-静态全局内存">4.1.2.9 静态全局内存</h5>
<h3 id="42-内存管理">4.2 内存管理</h3>
<p>CUDA编程的内存管理与C语言的类似， 需要程序员显式地管理主机和设备之间的数
据移动。 随着CUDA版本的升级， NVIDIA正系统地实现主机和设备内存空间的统一， 但对于大多数应用程序来说， 仍需要手动移动数据。</p>
<ul>
<li>分配和释放设备内存</li>
<li>在主机和设备之间传输数据</li>
</ul>
<h4 id="421-内存分配和释放">4.2.1 内存分配和释放</h4>
<p>CUDA编程模型假设了一个包含一个主机和一个设备的异构系统， 每一个异构系统都
有自己独立的内存空间。 核函数在设备内存空间中运行， CUDA运行时提供函数以分配和释放设备内存。</p>
<p>你可以在主机上使用下列函数分配全局内存：</p>
<p><code>cudaError_t cudaMalloc(void **devPrt, size_t count);</code></p>
<p>这个函数在设备上分配了count字节的全局内存， 并用devptr指针返回该内存的地址。</p>
<p>你需要用从主机上传输的数据来填充所分配的全局内存， 或用下列函数将其初始
化:</p>
<p><code>cudaError_t cudaMemset(void *devPtr, int value, size_t count);</code></p>
<p>这个函数用存储在变量value中的值来填充从设备内存地址devPtr处开始的count字节。</p>
<p>一旦一个应用程序不再使用已分配的全局内存， 那么可以以下代码释放该内存空间：
<code>cudaError_t cudaFree(void *devPtr);</code></p>
<p>这个函数释放了devPtr指向的全局内存， 该内存必须在此前使用了一个设备分配函数
（如cudaMalloc） 来进行分配。 否则， 它将返回一个错误cudaErrorInvalidDevicePointer。
如果地址空间已经被释放， 那么cudaFree也返回一个错误。</p>
<h4 id="422-内存传输">4.2.2 内存传输</h4>
<p>一旦分配好了全局内存， 你就可以使用下列函数从主机向设备传输数据：</p>
<p><code>cudaError_t cudaMemory(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind)</code></p>
<p>这个函数从内存位置src复制了count字节到内存位置dst。 变量kind指定了复制的方向， 可以有下列取值：</p>
<ul>
<li><code>cudaMemcpyHostToHost</code></li>
<li><code>cudaMemcpyHostToDevice</code></li>
<li><code>cudaMemcpyDeviceToHost</code></li>
<li><code>cudaMemcpyDeviceToDevice</code></li>
</ul>
<p>CUDA编程的一个基本原则应是尽可能地减少主机与设备之间的传输.</p>
<h4 id="423-固定内存">4.2.3 固定内存</h4>
<p>分配的主机内存默认是pageable（可分页） ， 它的意思也就是因页面错误导致的操
作， 该操作按照操作系统的要求将主机虚拟内存上的数据移动到不同的物理位置。 虚拟内存给人一种比实际可用内存大得多的假象， 就如同一级缓存好像比实际可用的片上内存大得多一样。</p>
<p>GPU不能在可分页主机内存上安全地访问数据， 因为当主机操作系统在物理位置上移
动该数据时， 它无法控制。 当从可分页主机内存传输数据到设备内存时， CUDA驱动程序首先分配临时页面锁定的或固定的主机内存， 将主机源数据复制到固定内存中， 然后从固定内存传输数据给设备内存， 如图4-4左边部分所示</p>
<p>CUDA运行时允许你使用如下指令直接分配固定主机内存：
<code>cudaError_t cudaMallocHost(void **devPtr, size_t count);</code></p>
<p>这个函数分配了count字节的主机内存， 这些内存是页面锁定的并且对设备来说是可
访问的。 由于固定内存能被设备直接访问， 所以它能用比可分页内存高得多的带宽进行读写。 然而， 分配过多的固定内存可能会降低主机系统的性能， 因为它减少了用于存储虚拟内存数据的可分页内存的数量， 其中分页内存对主机系统是可用的。</p>
<p><strong>主机与设备间的内存传输</strong></p>
<p>与可分页内存相比， 固定内存的分配和释放成本更高， 但是它为大规模数据传输提供
了更高的传输吞吐量</p>
<h4 id="424-零拷贝内存">4.2.4 零拷贝内存</h4>
<p>通常来说， 主机不能直接访问设备变量， 同时设备也不能直接访问主机变量。 但有一个例外： 零拷贝内存。 主机和设备都可以访问零拷贝内存。</p>
<p>GPU线程可以直接访问零拷贝内存。 在CUDA核函数中使用零拷贝内存有以下几个优
势。</p>
<ul>
<li>当设备内存不足时可利用主机内存</li>
<li>避免主机和设备间的显式数据传输</li>
<li>提高PCIe传输率</li>
</ul>
<p>当使用零拷贝内存来共享主机和设备间的数据时， 你必须同步主机和设备间的内存访
问， 同时更改主机和设备的零拷贝内存中的数据将导致不可预知的后果。</p>
<p>零拷贝内存是固定（不可分页） 内存， 该内存映射到设备地址空间中。 你可以通过下列函数创建一个到固定内存的映射：</p>
<p><code>cudaError_t cudaHostAlloc(void **pHost, size_t count, unsigned int flags);</code></p>
<p>这个函数分配了count字节的主机内存， 该内存是页面锁定的且设备可访问的。 用这
个函数分配的内存必须用cudaFreeHost函数释放。 flags参数可以对已分配内存的特殊属性
进一步进行配置：</p>
<pre><code>- cudaHostAllocDefault
- cudaHostAllocPortable
- cudaHostAllocWriteCombined
- cudaHostAllocMapped
</code></pre>
<p>cudaHostAllocDefault函数使cudaHostAlloc函数的行为与cudaMallocHost函数一致。</p>
<p>设置cudaHostAllocPortable函数可以返回能被所有CUDA上下文使用的固定内存， 而不仅是执
行内存分配的那一个。</p>
<p>标志cudaHostAllocWriteCombined返回写结合内存， 该内存可以在某些系统配置上通过PCIe总线上更快地传输， 但是它在大多数主机上不能被有效地读取。因此， 写结合内存对缓冲区来说是一个很好的选择， 该内存通过设备使用映射的固定内存或主机到设备的传输。</p>
<p>零拷贝内存的最明显的标志是cudaHostAllocMapped， 该标志返回， 可以实现主机写入和设备读取被映射到设备地址空间中的主机内存。</p>
<p>你可以使用下列函数获取映射到固定内存的设备指针：</p>
<p><code>cudaError_t cudaHostGetDevicePointer(void **pDevice, void *pHost, unsigned int flags);</code></p>
<p>该函数返回了一个在pDevice中的设备指针， 该指针可以在设备上被引用以访问映射得到的固定主机内存。 如果设备不支持映射得到的固定内存， 该函数将失效。 flag将留作以后使用。 现在， 它必须被置为0。</p>
<p>在进行频繁的读写操作时， 使用零拷贝内存作为设备内存的补充将显著降低性能。 因为每一次映射到内存的传输必须经过PCIe总线。 与全局内存相比， 延迟也显著增加。</p>
<p><strong>零拷贝内存</strong></p>
<p>有两种常见的异构计算系统架构： 集成架构和离散架构。</p>
<p>在集成架构中， CPU和GPU集成在一个芯片上， 并且<strong>在物理地址上共享主存</strong>。 在这种架构中， 由于无须在PCIe总线上备份， 所以零拷贝内存在性能和可编程性方面可能更佳。</p>
<p>对于通过PCIe总线将设备连接到主机的离散系统而言， 零拷贝内存只在特殊情况下有优势。</p>
<p>因为映射的固定内存在主机和设备之间是共享的， 你必须同步内存访问来避免任何潜在的数据冲突， 这种数据冲突一般是由多线程异步访问相同的内存而引起的。</p>
<p>注意不要过度使用零拷贝内存。 由于其延迟较高， 从零拷贝内存中读取设备核函数可能很慢。</p>
<h4 id="425-统一虚拟寻址">4.2.5 统一虚拟寻址</h4>
]]></description></item><item><title>CUDA_C_NOTES [3]</title><link>https://jianye0428.github.io/posts/cuda_03/</link><pubDate>Wed, 12 Jul 2023 10:40:11 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/cuda_03/</guid><description><![CDATA[<h2 id="ch03-cuda执行模型">CH03 CUDA执行模型</h2>
<h3 id="31-cuda执行模型概述">3.1 CUDA执行模型概述</h3>
<p>CUDA执行模型能够提供有助于在指令吞吐量和内存访问方面编写高效代码的见解</p>
<h4 id="311-gpu架构概述">3.1.1 GPU架构概述</h4>
<p>GPU架构是围绕一个流式多处理器（SM） (Stream Multiprocessor)的可扩展阵列搭建的,可以通过复制这种架构的构建块来实现GPU的硬件并行</p>
<p>Fermi SM的关键组件：</p>
<ul>
<li>CUDA核心</li>
<li>共享内存/一级缓存</li>
<li>寄存器文件</li>
<li>加载/存储单元</li>
<li>特殊功能单元</li>
<li>线程束调度器</li>
</ul>
<p>GPU中的每一个SM都能支持数百个线程并发执行， 每个GPU通常有多个SM， 所以在一个GPU上并发执行数千个线程是有可能的。 当启动一个内核网格时， 它的线程块被分布在了可用的SM上来执行。 线程块一旦被调度到一个SM上， 其中的线程只会在那个指定的SM上并发执行。 多个线程块可能会被分配到同一个SM上， 而且是根据SM资源的可用性进行调度的。同一线程中的指令利用指令级并行性进行流水线化， 另外， 在CUDA中已经介绍了线程级并行。</p>
<p>CUDA采用单指令多线程（SIMT）（single instruciton multi thread） 架构来管理和执行线程， <strong>每32个线程为一组</strong>， 被称为<strong>线程束（warp）</strong> 。 <font color=red>线程束中的所有线程同时执行相同的指令</font>。 每个线程都有自己的指令地址计数器和寄存器状态， 利用自身的数据执行当前的指令。 每个SM都将分配给它的线程块划分到包含32个线程的线程束中， 然后在可用的硬件资源上调度执行。</p>
<p>SIMT架构与SIMD（单指令多数据） 架构相似。 两者都是将相同的指令广播给多个执行单元来实现并行。 一个关键的区别是SIMD要求同一个向量中的所有元素要在一个统一的同步组中一起执行， 而SIMT允许属于同一线程束的多个线程独立执行.</p>
<p>SIMT确保可以编写独立的线程级并行代码、 标量线程以及用于协调线程的数据并行代码。</p>
<p><strong>SIMT模型</strong>包含3个SIMD所不具备的关键特征。</p>
<ul>
<li>每个线程都有自己的指令地址计数器</li>
<li>每个线程都有自己的寄存器状态</li>
<li>每个线程可以有一个独立的执行路径</li>
</ul>
<p><strong>一个神奇的数字： 32</strong></p>
<p>从概念上讲， 它是SM用SIMD方式所同时处理的工作粒度。 优化工作负载以适应线程束（一组有32个线程） 的边界， 一般这样会更有效地利用GPU计算资源。</p>
<p>一个线程块只能在一个SM上被调度。 一旦线程块在一个SM上被调度， 就会保存在该SM上直到执行完成。 在同一时间， 一个SM可以容纳多个线程块.</p>
<p>在SM中， 共享内存和寄存器是非常重要的资源。 <strong>共享内存</strong>被分配在SM上的常驻线程块中， <strong>寄存器</strong>在线程中被分配。</p>
<p>尽管线程块里的所有线程都可以逻辑地并行运行， 但是并不是所有线程都可以同时在物理层面执行。 因此， 线程块里的不同线程可能会以不同的速度前进。</p>
<p>在并行线程中共享数据可能会引起竞争： 多个线程使用未定义的顺序访问同一个数据， 从而导致不可预测的程序行为。 CUDA提供了一种用来同步线程块里的线程的方法，从而保证所有线程在进一步动作之前都达到执行过程中的一个特定点。 然而， 没有提供块间同步的原语。</p>
<p>当线程束由于任何理由闲置的时候（如等待从设备内存中读取数值） ， SM可以从同一SM上的常驻线程块中调度其他可用的线程束。 在并发的线程束间切换并没有开销， 因为硬件资源已经被分配到了SM上的所有线程和块中， 所以最新被调度的线程束的状态已经存储在SM上</p>
<p><strong>SM： GPU架构的核心</strong>*</p>
<p>SM是GPU架构的核心。 寄存器和共享内存是SM中的稀缺资源。 CUDA将这些资源分配到SM中的所有常驻线程里。</p>
<p>这些有限的资源限制了在SM上活跃的线程束数量，活跃的线程束数量对应于SM上的并行量。</p>
<p>了解一些SM硬件组成的基本知识， 有助于组织线程和配置内核执行以获得最佳的性能</p>
<h4 id="312-fermi架构">3.1.2 Fermi架构</h4>
<p>Fermi的特征是多达512个加速器核心， 这被称为CUDA核心。 每个CUDA核心都有一个全流水线的整数算术逻辑单元（ALU） 和一个浮点运算单元（FPU） ， 在这里每个时钟周期执行一个整数或是浮点数指令。 CUDA核心被组织到16个SM中， 每一个SM含有32个CUDA核心。 Fermi架构有6个384位的GDDR5 DRAM存储器接口， 支持多达6GB的全局机载内存， 这是许多应用程序关键的计算资源。 主机接口通过PCIe总线将GPU与CPU相连。 GigaThread引擎（图示左侧第三部分） 是一个全局调度器， 用来分配线程块到SM线程束调度器上。</p>
<p>一个SM(Stream Multiprocessor)包含以下内容：</p>
<ul>
<li>执行单元（CUDA核心）</li>
<li>调度线程束的调度器和调度单元</li>
<li>共享内存、 寄存器文件和一级缓存</li>
</ul>
<p>每一个多处理器有16个加载/存储单元（如图3-1所示） ， 允许每个时钟周期内有16个线程（线程束的一半） 计算源地址和目的地址。 特殊功能单元（SFU） 执行固有指令， 如正弦、 余弦、 平方根和插值。 每个SFU每个时钟周期内的每个线程上执行一个固有指令</p>
<p>每个SM有两个线程束调度器和两个指令调度单元。 <strong>当一个线程块被指定给一个SM时， 线程块中的所有线程被分成了线程束。</strong> 两个线程束调度器选择两个线程束， 再把一个
指令从线程束中发送到一个组上， 组里有16个CUDA核心、 16个加载/存储单元或4个特殊功能单元（如图3-4所示） 。 Fermi架构， 计算性能2.x， 可以在每个SM上同时处理48个线程束， 即可在一个SM上同时常驻1536个线程。</p>
<h4 id="313-kepler架构">3.1.3 Kepler架构</h4>
<p>发布于2012年秋季的Kepler GPU架构是一种快速、 高效、 高性能的计算架构。 Kepler
的特点使得混合计算更容易理解。 图3-6表示了Kepler K20X芯片框图， 它包含了15个SM
和6个64位的内存控制器。 以下是Kepler架构的3个重要的创新。</p>
<ul>
<li>强化的SM</li>
<li>动态并行</li>
<li>Hyper-Q技术</li>
</ul>
<p>Kepler K20X的关键部分是有一个<strong>新的SM单元</strong>， 其包括一些结构的创新， 以提高编程效率和功率效率。 每个Kepler SM单元包含192个单精度CUDA核心， 64个双精度单元， 32个特殊功能单元（SFU） 以及32个加载/存储单元（LD/ST）</p>
<h4 id="314-配置文件驱动优化">3.1.4 配置文件驱动优化</h4>
<p>配置文件驱动的发展对于CUDA编程尤为重要， 原因主要有以下几个方面。</p>
<ul>
<li>一个单纯的内核应用一般不会产生最佳的性能。 性能分析工具能帮助你找到代码中影响性能的关键部分， 也就是性能瓶颈。</li>
<li>CUDA将SM中的计算资源当前SM中的多个常驻线程块之间进行分配。 这种分配形式导致一些资源成为了性能限制者。 性能分析工具能帮助我们理解计算资源是如何被利用的。</li>
<li>CUDA提供了一个硬件架构的抽象， 它能够让用户控制线程并发。 性能分析工具可以检测和优化， 并将优化可视化。</li>
</ul>
<h3 id="32-理解线程束执行的本质">3.2 理解线程束执行的本质</h3>
<p>本章已经提到了把32个线程划分到一个执行单元中的概念： 线程束（warp）。 现在从硬件的角度来介绍线程束执行， 并能够获得指导内核设计的方法。</p>
<h4 id="321-线程束和线程块">3.2.1 线程束和线程块</h4>
<p>线程束是SM中基本的执行单元。</p>
<p>当一个线程块的网格被启动后， 网格中的线程块分布在SM中。 一旦线程块被调度到一个SM上， 线程块中的线程会被进一步划分为线程束。
一个线程束由32个连续的线程组成， 在一个线程束中， 所有的线程按照单指令多线程（SIMT） 方式执行； 也就是说， 所有线程都执行相同的指令， 每个线程在私有数据上进
行操作。</p>
<p>一个给定的二维线程块， 在一个块中每个线程的独特标识符都可以用内置变量threadIdx和blockDim来计算：
<code>threadIdx.y * blockDim.x + threadIdx.x</code></p>
<p>对于一个三维线程块， 计算如下：</p>
<p><code>threadIdx.x * blockDim.y * block.Dim.x + threadIdx.y * blockDim.x * threadIdx.x</code></p>
<p>一个线程块的线程束的数量可以根据下式确定：</p>
<p>$$一个线程块中线程束的数量 = 向正无穷取整（\frac{一个线程块中线程的数量}{线程束大小}）$$</p>
<p>因此， 硬件总是给一个线程块分配一定数量的线程束。 线程束不会在不同的线程块之间分离。 如果线程块的大小不是线程束大小的偶数倍， 那么在最后的线程束里有些线程就不会活跃。</p>
<p>从逻辑角度来看， 线程块是线程的集合， 它们可以被组织为一维、 二维或三维布局。</p>
<p>从硬件角度来看， 线程块是一维线程束的集合。 在线程块中线程被组织成一维布局，每32个连续线程组成一个线程束。</p>
<h4 id="322-线程束分化">3.2.2 线程束分化</h4>
<p>GPU是相对简单的设备， 它没有复杂的分支预测机制。 一个线程束中的所有线程在同一周期中必须执行相同的指令， 如果一个线程执行一条指令， 那么线程束中的所有线程都必须执行该指令。 如果在同一线程束中的线程使用不同的路径通过同一个应用程序， 这可能会产生问题。</p>
<p>如果一个线程束中的线程产生分化， 线程束将连续执行每一个分支路径， 而禁用不执行这一路径的线程。 线程束分化会导致性能明显地下降。</p>
<p><strong>重要提示:</strong></p>
<ul>
<li>当一个分化的线程采取不同的代码路径时， 会产生线程束分化</li>
<li>不同的if-then-else分支会连续执行</li>
<li>尝试调整分支粒度以适应线程束大小的倍数， 避免线程束分化</li>
<li>不同的分化可以执行不同的代码且无须以牺牲性能为代价</li>
</ul>
<h4 id="323-资源分配">3.2.3 资源分配</h4>
<p>线程束的本地执行上下文主要由以下资源组成：</p>
<ul>
<li>程序计数器</li>
<li>寄存器</li>
<li>共享内存</li>
</ul>
<p>由SM处理的每个线程束的执行上下文， 在整个线程束的生存期中是保存在芯片内的。 因此， 从一个执行上下文切换到另一个执行上下文没有损失。</p>
<p>每个SM都有32位的寄存器组， 它存储在寄存器文件中， 并且可以在线程中进行分配， 同时固定数量的共享内存用来在线程块中进行分配。 对于一个给定的内核， 同时存在于同一个SM中的线程块和线程束的数量取决于在SM中可用的且内核所需的寄存器和共享内存的数量。</p>
<p>若每个线程消耗的寄存器越多， 则可以放在一个SM中的线程束就越少。 如果可以减少内核消耗寄存器的数量， 那么就可以同时处理更多的线程束。</p>
<p>若一个线程块消耗的共享内存越多， 则在一个SM中可以被同时处理的线程块就会变少。 如果每个线程块使用的共享内存数量变少， 那么可以同时处理更多的线程块。</p>
<p>当计算资源（如寄存器和共享内存） 已分配给线程块时， 线程块被称为活跃的块。 它所包含的线程束被称为活跃的线程束。 活跃的线程束可以进一步被分为以下3种类型：</p>
<ul>
<li>选定的线程束</li>
<li>阻塞的线程束</li>
<li>符合条件的线程束</li>
</ul>
<p>一个SM上的线程束调度器在每个周期都选择活跃的线程束， 然后把它们调度到执行
单元。 活跃执行的线程束被称为选定的线程束。 如果一个活跃的线程束准备执行但尚未执
行， 它是一个符合条件的线程束。 如果一个线程束没有做好执行的准备， 它是一个阻塞的
线程束。 如果同时满足以下两个条件则线程束符合执行条件。</p>
<ul>
<li>32个CUDA核心可用于执行</li>
<li>当前指令中所有的参数都已就绪</li>
</ul>
<h4 id="324-延迟隐藏">3.2.4 延迟隐藏</h4>
<p>SM依赖线程级并行， 以最大化功能单元的利用率， 因此， 利用率与常驻线程束的数量直接相关。 在指令发出和完成之间的时钟周期被定义为指令延迟。 当每个时钟周期中所有的线程调度器都有一个符合条件的线程束时， 可以达到计算资源的完全利用。 这就可以保证， 通过在其他常驻线程束中发布其他指令， 可以隐藏每个指令的延迟。</p>
<p>考虑到指令延迟， 指令可以被分为两种基本类型：</p>
<ul>
<li>算术指令: 一个算术操作从开始到它产生输出之间的时间；</li>
<li>内存指令: 指发送出的加载或存储操作和数据到达目的地之间的时间。</li>
</ul>
<p>你可能想知道如何估算隐藏延迟所需要的活跃线程束的数量。 利特尔法则（Little’s
Law） 可以提供一个合理的近似值。 它起源于队列理论中的一个定理， 它也可以应用于
GPU中：</p>
<p>$$所需线程束数量 = 延迟 \times 吞吐量$$</p>
<p><strong>吞吐量和带宽</strong></p>
<p>吞吐量和带宽都是用来度量性能的速度指标。</p>
<p>带宽通常是指理论峰值， 而吞吐量是指已达到的值</p>
<p>带宽通常是用来描述单位时间内最大可能的数据传输量， 而吞吐量是用来描述单位时
间内任何形式的信息或操作的执行速度， 例如， 每个周期完成多少个指令。</p>
<p>吞吐量由SM中每个周期内的操作数量确定， 而执行一条指令的一个线程束对应32个
操作。</p>
<p>这个简单的单位转换表明， 有两种方法可以提高并行：</p>
<ul>
<li>指令级并行（ILP） ： 一个线程中有很多独立的指令</li>
<li>线程级并行（TLP） ： 很多并发地符合条件的线程</li>
</ul>
<p>延迟隐藏取决于每个SM中活跃线程束的数量， 这一数量由执行配置和资源约束隐式
决定（一个内核中寄存器和共享内存的使用情况） 。 选择一个最优执行配置的关键是在延
迟隐藏和资源利用之间找到一种平衡。</p>
<p><strong>显示充足的并行</strong>
因为GPU在线程间分配计算资源并在并发线程束之间切换的消耗（在一个或两个周期
命令上） 很小， 所以所需的状态可以在芯片内获得。 如果有足够的并发活跃线程， 那么可
以让GPU在每个周期内的每一个流水线阶段中忙碌。 在这种情况下， 一个线程束的延迟可
以被其他线程束的执行隐藏。 因此， 向SM显示足够的并行对性能是有利的</p>
<h4 id="325-占用率">3.2.5 占用率</h4>
<p>在每个CUDA核心里指令是顺序执行的。 当一个线程束阻塞时， SM切换执行其他符
合条件的线程束。 理想情况下， 我们想要有足够的线程束占用设备的核心。 占用率是每个
SM中活跃的线程束占最大线程束数量的比值。</p>
<p>$$占用率 = \frac{活跃线程束数量}{最大线程束数量}$$</p>
<p>极端地操纵线程块会限制资源的利用：</p>
<ul>
<li>小线程块： 每个块中线程太少， 会在所有资源被充分利用之前导致硬件达到每个SM的线程束数量的限制</li>
<li>大线程块： 每个块中有太多的线程， 会导致在每个SM中每个线程可用的硬件资源较少</li>
</ul>
<p><strong>网格和线程块大小的准则</strong></p>
<p>使用这些准则可以使应用程序适用于当前和将来的设备：</p>
<ul>
<li>保持每个块中线程数量是线程束大小（32） 的倍数</li>
<li>避免块太小： 每个块至少要有128或256个线程</li>
<li>根据内核资源的需求调整块大小</li>
<li>块的数量要远远多于SM的数量， 从而在设备中可以显示有足够的并行</li>
<li>通过实验得到最佳执行配置和资源使用情况</li>
</ul>
<p>占用率唯一注重的是在每个SM中并发线程或线
程束的数量。 然而， 充分的占用率不是性能优化的唯一目标。 内核一旦达到一定级别的占
用率， 进一步增加占用率可能不会改进性能。 为了提高性能， 可以调整很多其他因素。</p>
<h4 id="326-同步">3.2.6 同步</h4>
<p>在CUDA中， 同步可以在两个级别执行：</p>
<ul>
<li>系统级： 等待主机和设备完成所有的工作</li>
<li>块级： 在设备执行过程中等待一个线程块中所有线程到达同一点</li>
</ul>
<p>对于主机来说：</p>
<ul>
<li><code>cudaError_t cudaDeviceSynchronize(void)</code>:cudaDeviceSyn-chronize函数可以用来阻塞主机应用程序， 直到所有的CUDA操作（复制、核函数等） 完成;</li>
<li><code>__device__ void __syncthreads(void);</code>:CUDA提供了一个使用块局部栅栏来同步它们的执行的功能。
<ul>
<li>当__syncthreads被调用时， 在同一个线程块中每个线程都必须等待直至该线程块中所有其他线程都已经达到这个同步点。</li>
</ul>
</li>
</ul>
<p><strong>线程块中的线程可以通过共享内存和寄存器来共享数据。</strong></p>
<p>在不同的块之间没有线程同步。 块间同步， 唯一安全的方法是在每个内核执行结束端使用全局同步点； 也就是说， 在全局同步之后， 终止当前的核函数， 开始执行新的核函数。
不同块中的线程不允许相互同步， 因此GPU可以以任意顺序执行块。 这使得CUDA程序在大规模并行GPU上是可扩展的。</p>
<h4 id="327-可扩展性">3.2.7 可扩展性</h4>
<p>对于任何并行应用程序而言， 可扩展性是一个理想的特性。 可扩展性意味着为并行应用程序提供了额外的硬件资源， 相对于增加的资源， 并行应用程序会产生加速。 例如， 若一个CUDA程序在两个SM中是可扩展的， 则与在一个SM中运行相比， 在两个SM中运行会使运行时间减半。 一个可扩展的并行程序可以高效地使用所有的计算资源以提高性能。 可扩展性意味着增加的计算核心可以提高性能。 串行代码本身是不可扩展的， 因为在成千上万的内核上运行一个串行单线程应用程序， 对性能是没有影响的。 并行代码有可扩展的潜能， 但真正的可扩展性取决于算法设计和硬件特性。</p>
<h3 id="33-并行性的表现">3.3 并行性的表现</h3>
<h3 id="36-动态并行">3.6 动态并行</h3>
<p>在本书中， 到目前为止， 所有核函数都是从主机线程中被调用的。 GPU的工作负载完
全在CPU的控制下。 CUDA的动态并行允许在GPU端直接创建和同步新的GPU内核。 在一
个核函数中在任意点动态增加GPU应用程序的并行性， 是一个令人兴奋的新功能。</p>
]]></description></item><item><title>CUDA_C_NOTES [2]</title><link>https://jianye0428.github.io/posts/cuda_02/</link><pubDate>Wed, 12 Jul 2023 10:40:04 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/cuda_02/</guid><description><![CDATA[<h2 id="ch02-cuda编程模型">CH02 CUDA编程模型</h2>
<h3 id="21-cuda编程模型概述">2.1 CUDA编程模型概述</h3>
<p>CUDA编程模型提供了一个计算机架构抽象作为应用程序和其可用硬件之间的桥梁。</p>
<p>CUDA编程模型还利用GPU架构的计算能力提供了以下几个特有功能:</p>
<ul>
<li>一种通过层次结构在GPU中<strong>组织线程的方法</strong>(2.3)</li>
<li>一种通过层次结构在GPU中<strong>访问内存的方法</strong>(4.5)</li>
</ul>
<p>程序员可以通过以下几个不同层面来看待并行计算:</p>
<ul>
<li>领域层：如何解析数据和函数，以便在并行环境中正确高效的解决问题（在并行编程中高效的使用pthreads或者OpemMP技术显式地管理线程）</li>
<li>逻辑层：如何组织并发线程</li>
<li>硬件层：理解线程如何映射到核心以帮助提高其性能</li>
</ul>
<h3 id="211-cuda编程">2.1.1 CUDA编程</h3>
<p>在一个异构环境中包含多个CPU和GPU， 每个GPU和CPU的内存都由一条PCI-Express总线分隔开。</p>
<ul>
<li>主机： CPU及其内存（主机内存）</li>
<li>设备： GPU及其内存（设备内存）</li>
</ul>
<p>“统一寻址”（Unified Memory） 的编程模型的改进， 它连接了主机内存和设备内存空间， 可使用单个指针访问CPU和GPU内存， 无须彼此之间手动拷贝数据。</p>
<blockquote>
<p>什么是“统一寻址”（Unified Memory)?
CUDA 6.0提出了统一寻址， 使用一个指针来访问CPU和GPU的内存。(详见第4章)</p>
</blockquote>
<p>内核（kernel） 是CUDA编程模型的一个重要组成部分， 其代码在GPU上运行。</p>
<p>CUDA编程模型主要是<strong>异步</strong>的， 因此在<strong>GPU上进行的运算</strong>可以与<strong>主机-设备通信</strong>重叠。 一个典型的CUDA程序包
括由并行代码互补的串行代码。</p>
<p>串行代码在cpu上执行，并行代码在GPU上执行。</p>
<p>一个典型的CUDA程序实现流程遵循以下模式：</p>
<ol>
<li>把数据从CPU内存拷贝到GPU内存；</li>
<li>调用核函数对存储在GPU内存中的数据进行操作；</li>
<li>将数据从GPU内存传送回到CPU内存。</li>
</ol>
<h4 id="212-内存管理">2.1.2 内存管理</h4>
<p>CUDA运行时负责分配与释放设备内存， 并且在主机内存和设备内存之间传输数据。</p>
<p>表2-1 主机和设备内存函数</p>
<table>
<thead>
<tr>
<th>标准c函数</th>
<th>CUDA C函数</th>
<th>标准c函数</th>
<th>CUDA C函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>malloc</td>
<td>cudaMalloc</td>
<td>memset</td>
<td>cudaMemset</td>
</tr>
<tr>
<td>memcpy</td>
<td>cudaMemcpy</td>
<td>free</td>
<td>cudaFree</td>
</tr>
</tbody>
</table>
<p><strong>cudaMalloc</strong>函数负责在GPU的内存里分配内存；
<strong>cudaMemcpy</strong>函数负责主机和设备之间的数据传输；</p>
<ul>
<li>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">cudaError_t</span> <span class="nf">cudaMemcpy</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span> <span class="n">dst</span><span class="p">,</span> <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">src</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">count</span><span class="p">,</span> <span class="n">cudaMemcpyKind</span> <span class="n">kind</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>从src指向的源存储区复制一定数量的字节到dst指向的目标存储区</li>
<li>kind有以下几种:
<ul>
<li><code>cudaMemcpyHostToHost</code></li>
<li><code>cudaMemcpyHostToDevice</code></li>
<li><code>cudaMemcpyDeviceToHost</code></li>
<li><code>cudaMemcpyDeviceToDevice</code></li>
</ul>
</li>
</ul>
<p>CUDA编程模型从GPU架构中抽象出一个<u>内存层次结构</u>：<strong>全局内存</strong>和<strong>共享内存</strong>。</p>
<p><strong>内存层次结构</strong></p>
<ul>
<li>全局内存</li>
<li>共享内存</li>
</ul>
<blockquote>
<p>为什么CPU和GPU是异步的？
当数据被转移到GPU的全局内存后， 主机端调用核函数在GPU上进行数组求和。 一旦内核被调用， 控制权立刻被传回主机， 这样的话， 当核函数在GPU上运行时， 主机可以执行其他函数。 因此， 内核与主机是异步的。</p>
</blockquote>
<p><strong>不同的存储空间</strong></p>
<h4 id="213-线程管理">2.1.3 线程管理</h4>
<p>当核函数在主机端启动时， 它的执行会移动到设备上， 此时设备中会产生大量的线程并且每个线程都执行由核函数指定的语句。</p>
<p>由一个内核启动所产生的所有线程统称为一个<strong>网格</strong>。 同一网格中的所有线程共享相同的全局内存空间。 <u>一个网格由多个线程块构成， 一个线程块包含一组线程， 同一线程块内的线程协作可以通过以下方式来实现：</u></p>
<ul>
<li>同步</li>
<li>共享内存</li>
</ul>
<blockquote>
<p>不同线程块内的线程不能协作。</p>
</blockquote>
<p>线程依靠以下两个坐标变量来区分彼此</p>
<ul>
<li>blockIdx(线程块在线程格内的索引)</li>
<li>threadIdx(块内的线程索引)</li>
</ul>
<p>些变量是核函数中需要预初始化的内置变量。 当执行一个核函数时， CUDA运行时为每个线程分配坐标变量blockIdx和threadIdx。 基于这些坐标， 你可以将部分数据分配给不同的线程。</p>
<p>该坐标变量是基于uint3定义的CUDA内置的向量类型， 是一个包含3个无符号整数的结构， 可以通过x、 y、 z三个字段来指定：</p>
<ul>
<li>blockIdx.x</li>
<li>blockIdx.y</li>
<li>blockIdx.z</li>
<li>threadIdx.x</li>
<li>threadIdx.y</li>
<li>threadIdx.z</li>
</ul>
<p>CUDA可以组织三维的网格和块.</p>
<p>网格和块的维度由下列两个内置变量指定:</p>
<ul>
<li>blockDim(线程块的维度， 用每个线程块中的线程数来表示)</li>
<li>gridDim(线程格的维度， 用每个线程格中的线程数来表示)</li>
</ul>
<p>它们是dim3类型的变量， 是基于uint3定义的整数型向量， 用来表示维度。 当定义一个dim3类型的变量时， 所有未指定的元素都被初始化为1。 dim3类型变量中的每个组件可以通过它的x、 y、 z字段获得。 如下所示:</p>
<ul>
<li>blockDim.x</li>
<li>blockDim.y</li>
<li>blockDim.z</li>
</ul>
<p><strong>网格和线程块的维度</strong></p>
<blockquote>
<blockquote>
<p>一个线程格会被组织成线程块的二维数组形式， 一个线程块会被组织成线程的三维数组形式</p>
</blockquote>
</blockquote>
<p>在CUDA程序中有两组不同的网格和块变量： 手动定义的dim3数据类型和预定义的uint3数据类型。</p>
<p>手动定义的dim3类型的网格和块变量仅在主机端可见， 而unit3类型的内置预初始化的网格和块变量仅在设备端可见。</p>
<p><strong>从主机端和设备端访问网格/块变量</strong></p>
<p>区分主机端和设备端的网格和块变量的访问是很重要的。</p>
<p>例如， 声明一个主机端的块变量， 你按如下定义它的坐标并对其进行访问：</p>
<p><code>block.x, block.y, block.z</code></p>
<p>在设备端， 你已经预定义了内置块变量的大小：</p>
<p><code>blockDim.x, blockDim.y, and blockDim.z</code></p>
<p>在启动内核之前就定义了主机端的网格和块变量， 并从主机端通过由x、 y、 z三个字段决定的矢量结构来访问它们。 当内核启动时， 可以使用内核中预初始化的内置变量。</p>
<p>总之， 在启动内核之前就定义了主机端的网格和块变量， 并从主机端通过由x、 y、 z三个字段决定的矢量结构来访问它们。 当内核启动时， 可以使用内核中预初始化的内置变量.</p>
<p>对于一个给定的数据大小， 确定网格和块尺寸的一般步骤为：</p>
<ul>
<li>确定块的大小</li>
<li>在已知数据大小和块大小的基础上计算网格维度</li>
</ul>
<p>要确定块尺寸， 通常需要考虑：</p>
<ul>
<li>内核的性能特性</li>
<li>GPU资源的限制</li>
</ul>
<p><strong>线程层次结构</strong></p>
<p>CUDA的特点之一就是通过编程模型揭示了一个两层的线程层次结构（grid-&gt;block-&gt;thread）。 由于一个内核
<strong>启动的网格和块的维数</strong>会影响性能， 这一结构为程序员优化程序提供了一个额外的途径。</p>
<h4 id="214-启动一个cuda核函数">2.1.4 启动一个CUDA核函数</h4>
<p>CUDA内核调用是对C语言函数调用语句的延伸， &laquo;&lt;&raquo;&gt;运算符内是核函数的执行配置。</p>
<p><code>kernel_name &lt;&lt;&lt;grid, block&gt;&gt;&gt;(argument list)</code></p>
<p>利用执行配置可以指定线程在GPU上调度运行的方式。 执行配置的<strong>第一个值是网格维度</strong>， 也就是启动块的数目。 <strong>第二个值是块维度</strong>， 也就是每个块中线程的数目。 通过指定网格和块的维度， 你可以进行以下
配置：</p>
<ul>
<li>内核中线程的数目</li>
<li>内核中使用的线程布局</li>
</ul>
<blockquote>
<p>同一个块(block)中的线程之间可以相互协作， 不同块内的线程不能协作。</p>
</blockquote>
<p>假设你有32个数据元素用于计算， 每8个元素一个块， 需要启动4个块：</p>
<p><code>kernel_name&lt;&lt;&lt;4, 8&gt;&gt;&gt;(argument list)</code></p>
<p>由于数据在全局内存中是线性存储的， 因此可以用变量blockIdx.x和threadId.x来进行以下操作。</p>
<ul>
<li>在网格中标识一个唯一的线程</li>
<li>建立线程和数据元素之间的映射关系</li>
</ul>
<p>如果把所有32个元素放到一个块里， 那么只会得到一个块:</p>
<p><code>kernel_name&lt;&lt;&lt;1, 32&gt;&gt;&gt;(argument list)</code></p>
<p>如果每个块只含有一个元素， 那么会有32个块：</p>
<p><code>kernel_name&lt;&lt;&lt;32, 1&gt;&gt;&gt;(argument list)</code></p>
<p>核函数的调用与主机线程是异步的。 核函数调用结束后， 控制权立刻返回给主机端.</p>
<p>你可以调用以下函数来强制主机端程序等待所有的核函数执行结束：</p>
<p><code>cudaError_t cudaDeivceSynchronize(void);</code></p>
<p>一些CUDA运行时API在主机和设备之间是<strong>隐式同步</strong>的。 当使用cudaMemcpy函数在主
机和设备之间拷贝数据时， 主机端隐式同步， 即主机端程序必须等待数据拷贝完成后才能
继续执行程序。</p>
<p><strong>异步行为</strong></p>
<p>不同于C语言的函数调用， 所有的CUDA核函数的启动都是异步的。 CUDA内核调用完成后， 控制权立刻返回给CPU。</p>
<h4 id="215-编写核函数">2.1.5 编写核函数</h4>
<p>核函数是在设备端执行的代码。</p>
<p>用__global__声明定义核函数:</p>
<p><code>__global__ void kernel_name(argument list);</code></p>
<p>核函数必须有一个void返回类型。</p>
<p>表2-2总结了CUDA C程序中的函数类型限定符</p>
<table>
<thead>
<tr>
<th>限定符</th>
<th>执行</th>
<th>调用</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>global</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>device</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>host</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>CUDA核函数的限制</strong>
以下限制适用于所有核函数:</p>
<ul>
<li>只能访问设备内存</li>
<li>必须具有void返回类型</li>
<li>不支持可变数量的参数</li>
<li>不支持静态变量</li>
<li>显示异步行为</li>
</ul>
<h3 id="23-组织并行线程-以阅读为主">2.3 组织并行线程 (以阅读为主)</h3>
<p>从前面的例子可以看出， 如果使用了合适的网格和块大小来正确地组织线程， 那么可以对内核性能产生很大的影响。</p>
<h4 id="231-使用块和线程建立矩阵索引">2.3.1 使用块和线程建立矩阵索引</h4>
<p>在一个矩阵加法核函数中，一个线程通常被分配一个数据元素来处理。首先要完成的任务是使用块和线程索引从全局内存中访问指定的数据。</p>
]]></description></item><item><title>CUDA_C_NOTES [1]</title><link>https://jianye0428.github.io/posts/cuda_01/</link><pubDate>Wed, 12 Jul 2023 10:38:32 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/cuda_01/</guid><description><![CDATA[<h2 id="ch01-基于cuda的异构并行计算">Ch01 基于CUDA的异构并行计算</h2>
<h3 id="11-并行计算">1.1 并行计算</h3>
<p>并行计算通常设计两个不同的计算机领域</p>
<ul>
<li>计算机架构(硬件)：在结构级别上支持并行性</li>
<li>并行程序设计(软件)：充分使用计算机架构的计算能力来并发地解决问题</li>
</ul>
<h4 id="111-串行编程和并行编程">1.1.1 串行编程和并行编程</h4>
<h4 id="112-并行性">1.1.2 并行性</h4>
<p>并行性方式</p>
<ul>
<li>
<p>任务并行： 当许多任务或函数可以独立地、大规模地并行执行时，这就是任务并行。任务并行的核心是在于<u>利用多核系统对<strong>任务</strong>进行分配</u>。</p>
</li>
<li>
<p>数据并行： 当可以处理许多数据的时候，就是数据并行。数据并行的重点是利用多核系统对<strong>数据</strong>进行分配。</p>
</li>
</ul>
<p>CUDA编程非常适合解决数据并行问题。</p>
<p><strong>数据划分</strong>方式：</p>
<ul>
<li>块划分：
<ul>
<li>每个线程作用于一部分数据， 通常这些数据具有相同大小。</li>
<li>一组连续数据被分到一个块内，每个数据块以任意次序被安排给一个线程，线程通常在同一时间只处理一个数据块。</li>
</ul>
</li>
<li>周期划分：
<ul>
<li>每个线程作用于数据的多部分。</li>
<li>在周期划分中，更少的数据被分到一个块内。相邻的线程处理相邻的数据块，每个线程可以处理多个数据块。为一个待处理的线程选择一个新的块，就意味着要跳过和现有线程一样多的数据块。</li>
</ul>
</li>
</ul>
<h4 id="113-计算机架构">1.1.3 计算机架构</h4>
<p>计算机机构分类(弗林分类(Flynn&rsquo;s Taxonomy))：根据指令和数据进入CPU的方式进行分类</p>
<ul>
<li>单指令单数据（SISD）
<ul>
<li>一种串行架构。 在这种计算机上只有一个核心。在任何时间点上只有一个指令流在处理一个数据流。</li>
</ul>
</li>
<li>单指令多数据（SIMD）
<ul>
<li>一种并行架构类型。在这种计算机上有多个核心。 在任何时间点上所有的核心只有一个指令流处理不同的数据流，例如向量机。</li>
<li>优势: 在CPU上编写代码时， 程序员可以继续按串行逻辑思考但对并行数据操作实现并行加速，而其他细节则由编译器来负责。</li>
</ul>
</li>
<li>多指令单数据（MISD）
<ul>
<li>比较少见, 每个核心通过使用多个指令流处理同一个数据流</li>
</ul>
</li>
<li>多指令多数据（MIMD）
<ul>
<li>一种并行架构， 在这种架构中，多个核心使用多个指令流来异步处理多个数据流，从而实现<strong>空间上的并行性</strong>。 许多MIMD架构还包括SIMD执行的子组件。</li>
</ul>
</li>
</ul>
<p>计算机架构优劣的评价指标：</p>
<ul>
<li>降低延迟
<ul>
<li>延迟是一个操作从开始到完成所需要的时间， 常用微秒来表示</li>
</ul>
</li>
<li>提高带宽
<ul>
<li>带宽是单位时间内可处理的数据量， 通常表示为MB/s或GB/s。</li>
</ul>
</li>
<li>提高吞吐量
<ul>
<li>吞吐量是单位时间内成功处理的运算数量， 通常表示为gflops（即每秒十亿次的浮点运算数量） ， 特别是在重点使用浮点计算的科学计算领域经常用到</li>
<li>延迟用来衡量完成一次操作的时间， 而吞吐量用来衡量在给定的单位时间内处理的操作量</li>
</ul>
</li>
</ul>
<p>根据内存组织方式进一步划分计算机架构:</p>
<ul>
<li>分布式内存的多节点系统
<ul>
<li>大型计算引擎是由许多网络连接的处理器构成的。 每个处理器有自己的本地内存， 而且处理器之间可以通过网络进行通信(类似于多机多卡)</li>
</ul>
</li>
<li>共享内存的多处理器系统</li>
</ul>
<p>GPU代表了一种众核架构，几乎包括了前文描述的所有并行结构： 多线程、MIMD（多指令多数据）、 SIMD（单指令多数据）， 以及指令级并行。 NVIDIA公司称这
种架构为SIMT（单指令多线程）。</p>
<p><strong>GPU核心和CPU核心</strong></p>
<p>尽管可以使用多核和众核来区分CPU和GPU的架构， 但这两种核心是完全不同的。</p>
<ul>
<li>CPU核心比较重， 用来处理非常复杂的控制逻辑， 以优化串行程序执行。</li>
<li>GPU核心较轻， 用于优化具有简单控制逻辑的数据并行任务， 注重并行程序的吞吐量。</li>
</ul>
<h3 id="12-异构计算">1.2 异构计算</h3>
<p>CPU和GPU是两个独立的处理器， 它们通过单个计算节点中的PCI-Express总线相连。
在这种典型的架构中， GPU指的是离散的设备，从同构系统到异构系统的转变是高性能计算
史上的一个里程碑。 同构计算使用的是同一架构下的一个或多个处理器来执行一个应用。
而<u>异构计算则使用一个处理器架构来执行一个应用，为任务选择适合它的架构，使其最终
对性能有所改进.</u></p>
<h4 id="121-异构架构">1.2.1 异构架构</h4>
<p>一个典型的异构计算节点包括两个多核CPU插槽和两个或更多个的众核GPU。 GPU不
是一个独立运行的平台而是CPU的协处理器。 因此， GPU必须通过PCIe总线与基于CPU的
主机相连来进行操作， 如图1-9所示。 这就是为什么CPU所在的位置被称作主机端(host)而GPU
所在的位置被称作设备端(device)。</p>
<p>一个异构应用包括两部分：</p>
<ul>
<li>主机代码：在CPU上运行</li>
<li>设备代码：在GPU上运行.</li>
</ul>
<p>描述GPU容量的两个重要特征</p>
<ul>
<li>CUDA核心数量</li>
<li>内存大小</li>
</ul>
<p>相应的， 有两种不同的指标来评估GPU的性能:</p>
<ul>
<li>峰值计算性能：用来评估计算容量的一个指标， 通常定义为每秒能处理的单精度或双精度浮点运算的数量，通常用GFlops（每秒十亿次浮点运算） 或TFlops（每秒万
亿次浮点运算） 来表示</li>
<li>内存带宽：从内存中读取或写入数据的比率。 内存带宽通常用GB/s表示</li>
</ul>
<p><strong>计算能力</strong></p>
<h4 id="122-异构计算范例">1.2.2 异构计算范例</h4>
<p>GPU与CPU结合后， 能有效提高大规模计算问题的处理速度与性能。 CPU针对<strong>动态工作</strong>负载进行了优化， 这些动态工作负载是由短序列的计算操作和不可预测的控制流程标
记的； 而GPU在其他领域内的目的是： 处理由计算任务主导的且带有简单控制流的工作负载。</p>
<p><strong>CPU线程与GPU线程</strong></p>
<p>CPU上的线程通常是重量级的实体。 操作系统必须交替线程使用启用或关闭CPU执行通道以提供多线程处理功能。 上下文的切换缓慢且开销大。</p>
<p>GPU上的线程是高度轻量级的。 在一个典型的系统中会有成千上万的线程排队等待工作。 如果GPU必须等待一组线程执行结束， 那么它只要调用另一组线程执行其他任务即可</p>
<h4 id="123-cuda-一种异构计算平台">1.2.3 CUDA： 一种异构计算平台</h4>
<p>CUDA是一种通用的并行计算平台和编程模型，它利用NVIDIA GPU中的并行计算引擎能更有效地解决复杂的计算问题。通过使用CUDA，你可以像在CPU上那样，通过GPU来进行计算。</p>
<p>CUDA提供了两层API来管理GPU设备和组织线程， 如图1-13所示。</p>
<ul>
<li>CUDA驱动API：驱动API是一种低级API， 它相对来说较难编程， 但是它对于在GPU设备使用上提供了更多的控制。</li>
<li>CUDA运行时API：运行时API是一个高级API， 它<u>在驱动API的上层实现</u>。 每个运行时API函数都被分解为更多传给驱动API的基本运算。</li>
</ul>
<p>一个CUDA程序包含了以下两个部分:</p>
<ul>
<li>在CPU上运行的主机代码</li>
<li>在GPU上运行的设备代码</li>
</ul>
<p>NVIDIA的CUDA nvcc编译器在编译过程中将设备代码从主机代码中分离出来. 主机代码是标准的C代码，使用C编译器进行编译。 设备代码，也就是核函数，
是用扩展的带有标记数据并行函数关键字的CUDA C语言编写的. 设备代码通过nvcc进行编译。 在链接阶段，在内核程序调用和显示GPU设备操作中添加CUDA运行时库。</p>
<h3 id="13-用gpu输出hello-world">1.3 用GPU输出Hello World</h3>
<ol>
<li>
<p>用专用扩展名.cu来创建一个源文件</p>
</li>
<li>
<p>使用CUDA nvcc编译器来编译程序</p>
</li>
<li>
<p>从命令行运行可执行文件， 这个文件有可在GPU上运行的内核代码。
首先， 我们编写一个C语言程序来输出“Hello World”， 如下所示</p>
</li>
</ol>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="cp">#include</span><span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nf">printf</span><span class="p">(</span><span class="s">&#34;Hello World from CPU!</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">)</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>把代码保存到hello.cu中， 然后使用nvcc编译器来编译。 CUDA nvcc编译器和gcc编译器及其他编译器有相似的语义</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">nvcc hello.cu -o hello</span></span></code></pre></td></tr></table>
</div>
</div><p>如果你运行可执行文件hello， 将会输出： <code>Hello World from CPU!</code></p>
<p>接下来， 编写一个内核函数， 命名为helloFromGPU， 用它来输出字符串“Hello World
from GPU！ ”。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">helloFromGPU</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nf">printf</span><span class="p">(</span><span class="s">&#34;Hello World from GPU!</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>修饰符__global__告诉编译器这个函数将会从CPU中调用， 然后在GPU上执行。用下面的代码启动内核函数.</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">helloFromGPU <span class="o">&lt;&lt;&lt;</span>1, 10&gt;&gt;&gt;<span class="o">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>三重尖括号意味着从主线程到设备端代码的调用。 一个内核函数通过一组线程来执行， 所有线程执行相同的代码。</p>
<p>三重尖括号里面的参数是<strong>执行配置</strong>， 用来说明<strong>使用多少线程来执行内核函数</strong>。 在这个例子中，有10个GPU线程被调用。</p>
<p><code>cudaDeviceReset()</code>用来显式地释放和清空当前进程中与当前设备有关的所有资源。</p>
<p>一个典型的<font color=red>CUDA编程结构</font>包括5个主要步骤:
1. 分配GPU内存
2. 从CPU内存中拷贝数据到GPU内存
3. 调用CUDA内核函数来完成程序指定的运算
4. 将数据从GPU拷回CPU内存
5. 释放GPU内存空间</p>
<h3 id="14-使用cuda-c编程难吗">1.4 使用CUDA C编程难吗</h3>
<p>数据局部性: 指的是数据重用， 以降低内存访问的延迟</p>
<ul>
<li>时间局部性：指在相对较短的时间段内数据或资源的重用</li>
<li>空间局部性：指在相对较接近的存储空间内数据元素的重用。</li>
</ul>
<p>CUDA中有内存层次和线程层次的概念</p>
<ul>
<li>内存层次结构</li>
<li>线程层次结构</li>
</ul>
<p>CUDA核中有3个关键抽象</p>
<ul>
<li>线程组的层次结构</li>
<li>内存的层次结构</li>
<li>障碍同步</li>
</ul>
<h3 id="15-总结">1.5 总结</h3>
<p>CPU + GPU的异构系统成为高性能计算的主流架构: 在GPU上执行数据并行工作， 在CPU上执行串行和任务并行的工作。</p>
]]></description></item></channel></rss>