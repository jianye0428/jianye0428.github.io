<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Transformer - 分类 - yejian's blog</title><link>https://jianye0428.github.io/categories/transformer/</link><description>Transformer - 分类 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sat, 15 Jul 2023 15:24:40 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/categories/transformer/" rel="self" type="application/rss+xml"/><item><title>Transformer Introduction</title><link>https://jianye0428.github.io/posts/transformerintroduction/</link><pubDate>Sat, 15 Jul 2023 15:24:40 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/transformerintroduction/</guid><description><![CDATA[<p>reference:</br>
[1]. <a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/"target="_blank" rel="external nofollow noopener noreferrer">The Transformer Family <i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://lilianweng.github.io/posts/2018-06-24-attention/"target="_blank" rel="external nofollow noopener noreferrer">Attention<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3]. <a href="https://zhuanlan.zhihu.com/p/60821628"target="_blank" rel="external nofollow noopener noreferrer">细节考究<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="transformer-family">Transformer Family</h2>
<h3 id="notations">Notations</h3>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>$d$</td>
<td>The model size / hidden state dimension / positional encoding size.</td>
</tr>
<tr>
<td>$h$</td>
<td>The number of heads in multi-head attention layer.</td>
</tr>
<tr>
<td>$L$</td>
<td>The segment length of input sequence.</td>
</tr>
<tr>
<td>$X \in \mathbb R ^ {L \times d}$</td>
<td>The input sequence where each element has been mapped into an embedding vector of shape , same as the model size.</td>
</tr>
<tr>
<td>$W^k \in \mathbb R ^ {d \times d^k}$</td>
<td>The key weight matrix.</td>
</tr>
<tr>
<td>$W^q \in \mathbb R ^ {d \times d^k}$</td>
<td>The query weight matrix.</td>
</tr>
<tr>
<td>$W^v \in \mathbb R ^ {d \times d^k}$</td>
<td>The value weight matrix.Often we have $d_k = d_v = d$.</td>
</tr>
<tr>
<td>$W^K_i, W^q_i \in \mathbb R ^ {d \times d^k / h}; W^v_i \in \mathbb R^{d x d_v / h}$</td>
<td>The weight matrices per head.</td>
</tr>
<tr>
<td>$W^o \in \mathbb d_v \times d$</td>
<td>The output weight matrix.</td>
</tr>
<tr>
<td>$Q = XW^q \in \mathbb R^{L \times d_q}$</td>
<td>The query embedding inputs.</td>
</tr>
<tr>
<td>$K = XW^k \in \mathbb R^{L \times d_k}$</td>
<td>The key embedding inputs.</td>
</tr>
<tr>
<td>$V = XW^v \in \mathbb R^{L \times d_v}$</td>
<td>The value embedding inputs.</td>
</tr>
<tr>
<td>$S_i$</td>
<td>A collection of key positions for the -th query to attend to.</td>
</tr>
<tr>
<td>$A \in \mathbb R ^ {L \times L}$</td>
<td>The self-attention matrix between a input sequence of lenght $L$ and itself. $A = softmax (Q K^T/\sqrt{(d_k)} )$</td>
</tr>
<tr>
<td>$a_ij \ in A $</td>
<td>The scalar attention score between query $q_i$ and key $k_j$.</td>
</tr>
<tr>
<td>$P \in \mathbb R ^ {L \times d}$</td>
<td>position encoding matrix, where the $i-th$ row is the positional encoding for input $x_i$.</td>
</tr>
</tbody>
</table>
<h3 id="attention-and-self-attention">Attention and Self-Attention</h3>
<p>Attention is a mechanism in the neural network that a model can learn to make predictions by <strong>selectively attending to a given set of data</strong>. The amount of attention is quantified by learned weights and thus the output is usually formed as a weighted average.</p>
<p>Self-attention is a type of attention mechanism where the model makes prediction for one part of a data sample using other parts of the observation about the same sample. Conceptually, it feels quite similar to non-local means. Also note that self-attention is permutation-invariant; in other words, it is an operation on sets.</p>
<p>There are various forms of attention / self-attention, Transformer (<a href="https://arxiv.org/abs/1706.03762"target="_blank" rel="external nofollow noopener noreferrer">Vaswani et al., 2017<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>) relies on the scaled dot-product attention: given a query matrix $Q$, a key matrix $K$ and a value matrix $V$, the output is a weighted sum of the value vectors, where the weight assigned to each value slot is determined by the dot-product of the query with the corresponding key:</p>
<p>$$\text{Attention}(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p>
<p>And for a query and a key vector $q_i, k_j \in \mathbb R ^ d$ (row vectors in query and key matrices), we have a scalar score:</p>
<p>$$a_{ij} = softmax(\frac{q_i k_j^T}{\sqrt{d_k}}) = \frac{\exp(q_i k_j^T)}{\sqrt{d_k}\sum_{r \in S_i}(q_i k_j^T)}$$</p>
<p>where $S_i$ is a collection of key positions for the $i$-th query to attend to.</p>
<p>See my old <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#a-family-of-attention-mechanisms"target="_blank" rel="external nofollow noopener noreferrer">post<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> for other types of attention if interested.</p>
<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>
<p>The multi-head self-attention module is a key component in Transformer. Rather than only computing the attention once, the multi-head mechanism splits the inputs into smaller chunks and then computes the scaled dot-product attention over each subspace in parallel. The independent attention outputs are simply concatenated and linearly transformed into expected dimensions.</p>
<p>$$\text{MulitHeadAttention}(X_q, X_k, X_v) = [\text{head}_1,;&hellip;; \text{head}_h] W^o, where \text{head}_i = \text{Attention}(X_qW_i^q, X_kW_i^k, X_vW_i^v)$$</p>
<p>where $[.;.]$ is a concatenation operation. $W_i^q, W_i^k \in \mathbb R^{d \times d_{k} / h}$, $W_i^v \in \mathbb R^{d \times d_{v} / h}$ are weight matrices to map input embeddings of size $L \times d$ into query, key and value matrices. And $W^o \in \mathbb R ^ {d_v \times d}$ is the output linear transformation. All the weights should be learned during training.</p>
<p></p>
<h3 id="transformer">Transformer</h3>
<p>The Transformer (which will be referred to as “vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762"target="_blank" rel="external nofollow noopener noreferrer">Vaswani, et al., 2017<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>) model has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation"target="_blank" rel="external nofollow noopener noreferrer">NMT<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> models. Later decoder-only Transformer was shown to achieve great performance in language modeling tasks, like in <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt"target="_blank" rel="external nofollow noopener noreferrer">GPT and BERT<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>.</p>
<p><strong>Encoder-Decoder Architecture</strong></p>
<p>The encoder generates an attention-based representation with capability to locate a specific piece of information from a large context. It consists of a stack of 6 identity modules, each containing two submodules, a multi-head self-attention layer and a point-wise fully connected feed-forward network. By point-wise, it means that it applies the same linear transformation (with same weights) to each element in the sequence. This can also be viewed as a convolutional layer with filter size 1. Each submodule has a residual connection and layer normalization. All the submodules output data of the same dimension $d$.</p>
<p>The function of Transformer decoder is to retrieve information from the encoded representation. The architecture is quite similar to the encoder, except that the decoder contains two multi-head attention submodules instead of one in each identical repeating module. The first multi-head attention submodule is masked to prevent positions from attending to the future.</p>
<p></p>
<p><strong>Positional Encoding</strong></p>
<p>Because self-attention operation is permutation invariant, it is important to use proper <strong>positional encoding</strong> to provide order information to the model. The positional encoding $P \in \mathbb R ^ {L \times d}$ has the same dimension as the input embedding, so it can be added on the input directly. The vanilla Transformer considered two types of encodings:</p>
<p>(1). Sinusoidal positional encoding is defined as follows, given the token $i = 1, &hellip;, L$ position and the dimension $\delta = 1, &hellip;, d$:</p>
<p>$$ \text{PE}(i, \delta)  = \left{
\begin{aligned}
\sin\big(\frac{i}{10000^{2\delta&rsquo;/d}}\big) , if \delta&amp;=2\delta&rsquo;\
\cos\big(\frac{i}{10000^{2\delta&rsquo;/d}}\big) , if \delta&amp;=2\delta&rsquo;+1 \
\end{aligned}
\right.$$</p>
<p>In this way each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions, from $2\pi$ to 10000 * $2\pi$.</p>
<p></p>
<p>(2). Learned positional encoding, as its name suggested, assigns each element with a learned column vector which encodes its absolute position (<a href="https://arxiv.org/abs/1705.03122"target="_blank" rel="external nofollow noopener noreferrer">Gehring, et al. 2017<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>).</p>
]]></description></item></channel></rss>