<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Perception - 分类 - yejian's blog</title><link>https://jianye0428.github.io/categories/perception/</link><description>Perception - 分类 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sun, 03 Sep 2023 19:31:13 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/categories/perception/" rel="self" type="application/rss+xml"/><item><title>BEVFormer 论文解读</title><link>https://jianye0428.github.io/posts/bevformer/</link><pubDate>Sun, 03 Sep 2023 19:31:13 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/bevformer/</guid><description><![CDATA[<h2 id="1-背景motivation">1. 背景/Motivation</h2>
<h3 id="11-为什么视觉感知要用bev">1.1 为什么视觉感知要用BEV？</h3>
<p>相机图像描述的是一个2D像素世界，然而自动驾驶中利用相机感知结果的后续决策、路径规划都是在车辆所处的3D世界下进行。由此引入的2D和3D维度不匹配，就导致基于相机感知结果直接进行自动驾驶变得异常困难。</p>
<p>这种感知和决策规划的空间维度不匹配的矛盾，也体现在学开车的新手上。倒车泊车时，新手通过后视镜观察车辆周围，很难直观地构建车子与周围障碍物的空间联系，容易导致误操作剐蹭或需要尝试多次才能泊车成功，本质上还是新手从2D图像到3D空间的转换能力较弱。基于相机图像平面感知结果进行决策规划的自动驾驶AI，就好比缺乏空间理解力的驾驶新手，很难把车开好。</p>
<p>实际上，利用感知结果进行决策和路径规划，问题还出现在多视角融合过程中: 在每个相机上进行目标检测，然后对目标进行跨相机融合。如2021 TESLA AI Day给出的图1，带拖挂的卡车分布在多个相机感知野内，在这种场景下试图通过目标检测和融合来真实地描述卡车在真实世界中的姿态，存在非常大的挑战。</p>
<p></p>
<p>为了解决这些问题，很多公司采用硬件补充深度感知能力，如引入毫米波雷达或激光雷达与相机结合，辅助相机把图像平面感知结果转换到自车所在的3D世界，描述这个3D世界的专业术语叫做BEV map或BEV features(鸟瞰图或鸟瞰图特征)，如果忽略高度信息，就把拍扁后的自车坐标系叫做BEV坐标系(即鸟瞰俯视图坐标系)。</p>
<p>但另外一些公司则坚持不引入深度感知传感器，他们尝试从本质入手，基于视觉学习得到从图像理解空间的能力，让自动驾驶AI系统更像老司机，例如TESLA。Elon Musk认为: 人类不是超人，也不是蝙蝠侠，不能够眼放激光，也没安装雷达，但是通过眼睛捕捉到的图像，人类反复练习就可以构建出对周围世界的3D空间理解能力从而很好地掌握驾驶这项能力，那么要像人一样单纯利用眼睛(相机)进行自动驾驶就必须<font color=red>具备从2D图像平面到3D自车空间(BEV)的转换能力</font>。</p>
<p>传统获取BEV map/features的方法有局限性，它一般是利用相机外参以及地面平面假设，即IPM(Inverse Perspective Mapping)方法，将图像平面的感知结果反投影到自车BEV坐标系。Tesla以前的方案也是这样，然而当车辆周围地面不满足平面假设，且多相机视野关联受到各种复杂环境影响的时候，这类方法就难以应付。</p>
<p>针对IPM方法获取BEV遇到的困难，TESLA自动驾驶感知负责人Andrej Karparthy的团队<font color=red><strong>直接在神经网络中完成图像平面到BEV的空间变换</strong></font>，这一改变成为了2020年10月发布的FSD Beta与之前Autopilot产品最显著的差别。TESLA利用Transformer生成BEV Featrues，得到的Features通道数是256(IPM方法最多保留RGB3个channel)，这样能极大程度地保留图像信息，用于后续基于BEV Features的各种任务，如动、静态目标检测和线检测等。</p>
<h3 id="12-生成bev视角的方法有哪些为何选用transformer呢">1.2 生成BEV视角的方法有哪些？为何选用Transformer呢？</h3>
<p><strong>把相机2D平面图像转换成BEV视角的方法有两种: <u>视觉几何方法</u>和<u>神经网络方法</u>。</strong></p>
<p><strong>视觉几何方法</strong>: 基于IPM进行逐像素几何投影转换为BEV视角，再对多个视角的部分BEV图拼接形成完整BEV图。此方法有两个假设: 1.路面与世界坐标系平行，2.车辆自身的坐标系与世界坐标系平行。前者在路面非平坦的情况下并不满足，后者依赖车辆姿态参数(Pitch和Roll)实时校正，且精度要求较高，不易实现。</p>
<p><strong>神经网络方法</strong>: 用神经网络生成BEV，其中的关键要找到合适的方法实现神经网络内部Feature Map空间尺寸上的变换。</p>
<p>实现空间尺寸变换的神经网络主流操作有两种方法，如图2所示: MLP中的Fully Connected Layer和Transformer Cross Attention(图片引用自<a href="https://zhuanlan.zhihu.com/p/458076977"target="_blank" rel="external nofollow noopener noreferrer">《超长延迟的特斯拉AI Day解析: 讲明白FSD车端感知》<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p></p>
<p><strong>MLP Fully Connected Layer方法:</strong></p>
<p>$$O=Act(W_{mlp}*X+b)$$</p>
<p>忽略非线性激活函数和bias后，可以写成:</p>
<p>$$O=W_{mlp}*X$$</p>
<p><strong>Transformer Cross Attention方法:</strong></p>
<p>$$O=Softmax(Q*K^T)*V$$</p>
<p>其中，K和V是输入图像特征X经线性变换后得到的；在最终输出的BEV视角下，把自车周围空间中的索引量称作 $\boldsymbol{\Phi}$</p>
<p>$$O=Softmax(\Phi W_q*(XW_K)^T)*XW_V=W_{transformer}(X,\Phi)*XW_V$$</p>
<p>BEV变换的本质是将输入的2D图像空间特征图转换成BEV特征图。在进行BEV转换之前，先通过多层CNN(或者任一图像处理backbone网络)在图像上提取特征，得到图像空间特征图层尺寸($h * w$)，即$X$的尺寸。 BEV变换输出O所在的BEV空间尺寸，是以自车位置为原点的前后左右各若干米范围内建立的栅格空间($x * y$)。</p>
<p>MLP Fully Connect Layer和Cross Attention的<font color=red><strong>显著差别</strong></font>在于作用于输入量X的系数W: 全联接层的W，一旦训练结束后在Inference阶段是固定不变的；而<font color=green>Cross Attention的Transformer的系数W，是输入量X和索引量的函数，在Inference阶段会根据输入量X和索引量的不同发生改变</font>。从这个角度来讲，使用Cross Attention来进行空间变换可能使模型获得更强的表达能力。</p>
<p>TESLA在2021年AI Day上仅介绍了用Transformer转换BEV Features的技术思想，并未披露更多实现细节。<font color=red><strong>论文BEVFormer充分研究了TESLA的技术思想后，利用Transformer融合图像的时、空特征，得到BEV Features，与TESLA的关键方法、实现效果都非常接近</strong></font>。BEVFormer既通过论文披露了详尽方法，又在2022年6月开源了工程，接下来就围绕BEVFormer介绍如何通过Transformer获取BEV Features。</p>
<h2 id="2-methodstrategybevformer">2. Method/Strategy——BEVFormer</h2>
<h3 id="21-overall-architecture">2.1 Overall Architecture</h3>
<p>如下图3所示: BEVFormer主体部分有6层结构相同的<code>BEVFormer encoder layers</code>，每一层都是由以transformer为核心的modules(TSA+SCA)，再加上FF、Add和Norm组成。BEVFormer encoder layer结构中有3个特别的设计: <font color=red><strong>BEV Queries</strong></font>， <font color=red><strong>Spatial Cross-attention(SCA)</strong></font>和<font color=red><strong>Temporal Self-attention(TSA)</strong></font>。其中BEV Queries是栅格形可学习参数，承载着通过attention机制在multi-camera views中查询、聚合的features。SCA和TSA是以BEV Queries作为输入的注意力层，负责实施查询、聚合空间features(来自multi-camera images)和时间features(来自历史BEV)的过程。</p>
<p>下面分步骤观察BEV完整模型的前向推理过程:</p>
<ul>
<li>在t时刻，输入车上多个视角相机的图像到backbone，输出各图像的多尺度特征(multi-scale features): $x = 1$, $F_t = {{F^i_t}}^a_b$, $F_t={{{F_t^i}}}<em>{i=1}^{N</em>{\text{view}}}$，其中 $F^i_t$ 是第i个视角相机的feature，$N_{\text{view}}$ 是多个相机视角的总数。同时，还要保留t-1时刻的BEV Features ${\boldsymbol{B}}_{t-1}$。</li>
<li>在每个BEVFormer Encoder layer中，首先用BEV Queries Q通过TSA模块从 ${\boldsymbol{B}}_{t-1}$ 中查询并融合时域信息(the temporal information)，得到修正后的BEV Queries $Q^{\prime}$；</li>
<li>然后在同一个BEVFormer Encoder layer中，对TSA“修证”过的BEV Queries $Q^{\prime}$ ，通过SCA模块从multi-camera features $F_{t}$ 查询并融合空域信息(spatial information)，得到进一步修正的BEV Queries $Q^{^{\prime\prime}}$ 。</li>
<li>这一层encoder layer把经过两次修正的BEV features $Q^{^{\prime\prime}}$ (也可以叫做BEV Queries)进行FF计算，然后输出，作为下一个encoder layer的输入。</li>
<li>如此堆叠6层，即经过6轮微调，t时刻的统一BEV features $B_{t}$ 就生成了。</li>
<li>最后，以BEV Features $B_{t}$ 作为输入，3D detection head和map segmentation head预测感知结果，例如3D bounding boxes和semantic map。</li>
</ul>
<p></p>
<h3 id="22-bev-queries">2.2 BEV Queries</h3>
<p>BEVFormer采用显性定义BEV的方式，BEV Queries就是其中的显性BEV features。</p>
<p>可从3个概念循序渐进地认识/理解BEV Queries: <strong>BEV平面 → BEV 感知空间 → BEV Queries</strong>。</p>
<ul>
<li>
<p><strong>BEV 平面</strong>
BEV平面($R^{H\times W\times1}$)是以自车为中心的栅格化二维平面，H、W是BEV平面在x、y方向的栅格尺寸，此平面显性地与车辆周围横、纵向物理空间关联，一个栅格表示s米，论文中把BEV平面中的栅格叫做2D参考点(2D reference points)。例如论文中定义nuScenes数据集栅格尺寸200x200，对应[-51.2米, 51.2米]，那么s就是0.512米。</p>
</li>
<li>
<p><strong>BEV 感知空间</strong>
BEV感知空间( $R^{H\times W\times N_{ref}}$ )把BEV平面在z轴方向选取 $N_{ref}$个<strong>3D参考点</strong>进行扩展，表示车辆周围有限空间。查看BEV开源工程可知，BEV感知空间的精确表示范围: 在nuScenes数据集中，是以Lidar作为中心点，前后各51.2m、左右各51.2m、向上3m、向下5m的矩形空间。
仔细深究作者这样设置的用意，会发现蛮有意思/意义: nuScenes采集车上安装的Lidar距地面高约1.84m，[-5m, 3m]的设置让BEV Queries感知空间在自车轮胎接地点平面以上部分约4.84m，接地点平面以下部分约3.16m。前者确保不仅能感知高大目标物，如货车等，还能感知覆盖坡度9.5%(4.84m/51.2m100%)上坡场景，后者保证感知覆盖坡度6.2%(3.16m/51.2m100%)下坡场景。备注: 坡度=(高程差/水平距离)x100%，车辆前进100m的垂直上升/下降高度，我国规定城市道路最大纵坡8％，公路9％。</p>
</li>
<li>
<p><strong>BEV Queries</strong>
BEV Queries是预定义的一组栅格形(grid-shaped)可学习参数，简称 $Q\in R^{H\times W\times C}$，它是对上述BEV 感知空间的特征描述，可直白地理解为学习完成了，它就变成了BEV features。
BEV Queries的H、W与BEV平面在x、y方向的栅格尺寸定义一致，因此也继承了BEV平面显性地与车辆周围横、纵向物理空间关联的特性。但C是BEV Queries作为features时在channel维度的尺寸，并不显性地对应于BEV平面z轴的物理空间尺寸。特别指出，位于 $p=(x,y)$ 处的某个query $Q_{p}\in R^{1\times C}$，表示BEV queries中的一个栅格。在输入到BEVFormer之前，BEV Queries加上了可学习的位置编码(learnable positional embedding)。
论文中多处提及BEV Queries和BEV features，它俩什么关系？本质上描述的是同一个东西么？
先说答案: 两者本质上是同一个东西。模型中预定义参数BEV Queries 输入到BEV Encoder layer，输出的就是经过1次微调过的BEV features，它作为下一层的输入时，又被看作下一层的BEV Queries。经过所有layers的多次微调，最后一个layer输出的就是BEV features ${\mathit{B}}_{t}$ ，作为各类感知task heads的输入。</p>
</li>
</ul>
<h3 id="23-sca-spatial-cross-attention">2.3 SCA: Spatial cross-attention</h3>
<p>如上图(b)所示，作者设计了一种空间交叉注意力机制，使 BEV queries 从多个相机的image Features中提取所需信息并转换为BEV Features。</p>
<p>每个BEV栅格的query在image features的哪些范围提取信息呢？这里有3个方案:</p>
<p>一、从image features的所有点上提取信息，即global attention。</p>
<p>二、从BEV栅格在image features的投影点上提取信息。</p>
<p>三、从BEV栅格在image features的投影点及其周围提取信息，即deformable attention。</p>
<p>由于使用了多尺度图像特征和高分辨率 BEV 特征(200x200)，如果采用方案一 global attention ，会带来无法负担的计算代价(显存和计算复杂度)。但是，方案一完全<u>用不到相机内外参</u>，这算是它<strong>独有的优势</strong>。</p>
<p>方案二依赖非常精确的相机内、外参，且不能充分利用image features上的局部区域信息。</p>
<p>因此作者选用了方案三，基于deformable attention 的<strong>稀疏注意力机制</strong>，使BEV Queries中的每个Query 只与其所代表世界坐标系物理空间投影到图像上的部分区域进行交互，且解除了对相机内、外参的高精度依赖。注意，这里复数BEV Queries表示整个BEV图，而单数Query表示BEV 图中位于 $(x, y)$的一个栅格。</p>
<p>$$\mathrm{SCA}(Q_p,F_t)=\frac1{|\mathcal{V}<em>\mathrm{hit}|}\sum</em>{i\in\mathcal{V}<em>\mathrm{hit}}\sum</em>{j=1}^{N_\mathrm{ref}}\mathrm{Deform}\mathrm{Attn}(Q_p,\mathcal{P}(p,i,j),F_t^i)\quad\quad\quad\quad(2)$$</p>
<p>在DeformAttn()中，$Q_{p}$、$P(p,i,j)$ 和 $F_{t}^{i}$ 分别表示query, reference points和输入特征。</p>
<p>通俗理解公式: <font color=red><strong>在BEV Query对应的图像2D features 有效区域附近计算注意力，把图像2D features加权融合到BEV Query作为SCA的输出</strong></font>。</p>
<p>参考图(b)和上述公式2，总结Spatial cross-attention的实现方式:</p>
<ol>
<li>对于每一个位于 $(x, y)$ 位置的 BEV query $Q_{p}$ ，计算其对应真实世界的坐标 $(x^{&rsquo;},y^{&rsquo;})$。然后将 BEV query 在z方向进行 lift 操作，设置 $N_{ref}$ 个 3D参考点，即对应 $N_{ref}$ 个世界坐标系下的3D空间参考点。</li>
<li>通过相机内外参，把第j个3D参考点投影到 $v_{hit}$ 个相机图像上。受相机感知范围限制，每个3D参考点一般只在 1-2 个相机上找到有效的投影点(反过来描述，每个相机的features只与部分BEV queries构成有效投影关系)。</li>
<li>基于 Deformable Attention，把像平面上的这些投影点作为2D图像参考点，在其周围对 $F_{t}^{i}$ 进行特征采样，得到sampled features。</li>
<li>最后，对 $V_{hit}$ 、$N_{ref}$ 个sampled features进行加权求和，作为spatial cross-attention的输出来更新BEV query，从而完成 spatial 空间的特征聚合。</li>
</ol>
<p>详细介绍一下第2步中如何把3D参考点投影到相机图像上获得2D参考点:</p>
<ul>
<li>首先计算位于 $p = (x, y)$ 的query $Q_{p}$ 在以车辆为中心世界坐标 $(x^{&rsquo;},y^{&rsquo;})$
$$x^{^{\prime}}=(x-W/2)*s; \quad y^{^{\prime}}=(y-{H}/2)*s$$
其中H和W是BEV queries的尺寸，s是BEV中一个栅格所代表的物理空间尺寸;</li>
<li>计算第j个3D参考点投影到第i个相机图像上的2D参考点坐标:
$$P(p,i,j)=(x_{i,j},y_{i,j})$$
其中，
$$z_{i,j}[x_{i,j},y_{i,j},1]^T=T_i*[x^{&rsquo;},y^{&rsquo;},z^{&rsquo;},1]^T$$
$T_i$ 是第 $i$ 个相机的投影矩阵。</li>
</ul>
<h3 id="24-tsa-temporal-self-attention">2.4 TSA: Temporal self-attention</h3>
<p>从经典 RNN 网络获得启发，将 BEV 特征 $B_t$ 视为能够传递序列信息的 memory。每一时刻生成的 BEV 特征 $B_t$ 都从上一时刻的 BEV 特征  $B_{t-1}$ 获取所需的时序信息，这样能保证动态地获取所需的时序特征，而非像堆叠不同时刻 BEV 特征那样只能获取定长的时序信息。</p>
<p></p>
<p>$$\text{TSA}(Q_p,{Q,B&rsquo;<em>{t-1}})=\sum</em>{V\in{Q,B&rsquo;_{t-1}}}\text{DeformAttn}(Q_p,p,V),\quad(5)$$</p>
<p>参考图4和上述公式5，总结temporal self-attention的实现方法:</p>
<ol>
<li>给定t-1时刻的 BEV 特征 $B_{t-1}$ ，先根据 ego motion 将 $B_{t-1}$ 对齐到 $t$ 时刻，来确保 $B_{t-1}$ 和 $B_{t}$ 在相同index位置的栅格对应于现实世界的同一位置，把时间对齐后的BEV特征记作 $B_{t-1}^{&rsquo;}$ 。</li>
<li>$t$ 时刻位于 $(x, y)$ 处的 BEV query所表征的物体可能静态或者动态，它在t-1时刻会出现在 $B_{t-1}^{&rsquo;}$ 的 $(x, y)$ 周围，因此利用 deformable attention 以 $(x, y)$ 作为参考点在其周围进行特征采样。</li>
</ol>
<p>上述方法<strong>没有显式地设计遗忘门</strong>，而是通过 attention 机制中的 attention weights 来平衡历史时序特征和当前 BEV 特征的融合过程。</p>
<p><strong>关于TSA的后记</strong>: BEVFormer V2对 BEVFormer的时域融合方法TSA做了修改。</p>
<p>BEVFormer中TSA采用了继承式的时域信息融合方式: 利用attention机制在t时刻融合了t-1时刻的BEV features信息，由于t-1时刻的BEV features 也融合了更早时刻(t-2)的信息，因此t时刻BEV features间接地融合了比t-1时刻更早的信息。</p>
<p>但是这种继承式时域融合方式有遗忘的特点，即不能有效利用较长时间的历史信息。</p>
<p>BEVFormer V2把时域融合改成了: 根据ego motion，把过去多个时刻的BEV features 对齐到当前时刻，然后在channel 维度把这些对齐后的BEV features 与当前时刻BEV features串联，然后用Residual 模块降低channel数，就完成了时域融合。</p>
<p>综合2.3和2.4节，观察6个 BEVFormer Encoder Layers的完整结构会发现， BEV query 既能通过 spatial cross-attention 聚合空间特征，又能通过 temporal self-attention 聚合时序特征，这个过程会重复多次，让时空特征融合能够相互促进，最终得到更好的融合BEV features。</p>
<h3 id="25-application-of-bev-features">2.5 Application of BEV Features</h3>
<p>BEV Features $B_{t}\in R^{H\times W\times C}$ 是可用于多种自动驾驶感知任务的2D feature map， 基于2D 感知方法稍加改动就可在 $B_t$ 上开发3D目标检测和map segmentation任务。</p>
<p><strong>3D object detection</strong></p>
<p>参考2D 目标检测器Deformable DETR，论文设计了end-to-end的3D目标检测head，修改部分包括: 用single-scale 的BEV features $B_t$ 作为检测头的输入，预测输出3D b-boxes和速度而不是2D b-boxes，仅用 $L_1$ loss监督3D b-boxes的回归。继承DETR方法的优势，预测有限数目的候选目标集合(开源工程中设为300个)，这种end-to-end的检测头不需要NMS后处理。</p>
<p><strong>map segmentation</strong></p>
<p>参考2D segmentation 方法Panoptic SegFormer，论文设计了map segmentation head。因为基于BEV的map segmentation基本上与常见语义分割相同，作者利用了参考文章[22]中的mask decoder和class-fixed queries设计head来查找每个语义类别，包括car、vehicle、road(可通行区域)和车道线。</p>
<h3 id="26-implementation-details">2.6 Implementation details</h3>
<p>训练时的实施细节:</p>
<ul>
<li>对于时间t，在过去2s中随机选取3个时刻t-3、t-2和t-1；</li>
<li>在初始3个时刻，循环生成BEV features ${B_{t-3},B_{t-2},B_{t-1}}$，且在此阶段不计算梯度；</li>
<li>计算第一个时刻t-3的temporal self-attention输出时，它并没有前序BEV features，就用它自身作为前序时刻输入，那么temporal self-attention暂时退化成了self-attention。</li>
</ul>
<p>inference时的实施细节:</p>
<p>按照时间顺序计算图像序列中的每一帧，前序时刻的BEV features被保持下来并用于后一时刻，这种online的inference策略在应用中比较高效。</p>
<h2 id="3-experiments">3. Experiments</h2>
<h3 id="31-experimental-settings">3.1 experimental settings</h3>
<ol>
<li>采用两种Backbone: ResNet101-DCN，采用来自FCOS3D训练得到的参数；VoVnet-99，采用来自DD3D训练得到的参数。</li>
<li>默认情况下，使用FPN输出的多尺度 features的尺寸包含1/16，1/32，1/64(注意，代码中实际用好的1/8尺寸的features，即nuScenes中的116*200的FPN输出)，同时把dimension C设为256。</li>
<li>在nuScenes数据集试验中，BEV queries尺寸设为200x200，对应感x和y方向的知范围都是[-51.2m, 51.2m]，对应BEV Grid尺寸精度是0.512m。</li>
<li>在Waymo数据集试验中，BEV queries尺寸设为300x220，对应感x方向的感知范围是[-35.0m, 75.0m]，y方向的知范围是[-75.0m, 75.0m]，对应BEV Grid尺寸精度是0.5m。自车中心位于 BEV的(70，150)处。</li>
<li>对每个BEV query，在 spatial cross-attention模块中，设置 $N_{ref}=4$ 个3D参考点，预定义它们对应的高度范围是-5m到3m。</li>
<li>对每个2D图像参考点(3D参考点投影到2D view上的点)，在其周围选取4个采样点送入SCA。</li>
<li>默认情况下，训练24epoches，学习率设为 $12\times10^{-4}$。</li>
<li><strong>Baselines</strong>: 为了合理评估task heads的影响，公平地与其它生成BEV方法对比。选择VPN和Lift-Splat作为baselines，对它们head之前的部分替换BEVFormer，保留task heads和其它设置。
论文中，通过把temporal self-attention修改为普通的self-attention，这样就使BEVFormer变成了一个静态模型，命名为<strong>BEVFormer-S</strong>，它不使用历史BEV Features。</li>
</ol>
<h3 id="32-3d目标检测结果">3.2 3D目标检测结果</h3>
<p>BEVFormer的3D检测性能，如Table1、Table2和Table3所示，远超之前最优方法DETR3D。</p>
<p>BEVFormer引入了temporal information，因此它在估计目标速度方面效果也很好。从速度估计指标mean Average Velocity (mAVE)来看，BEVFormer误差为0.378m/s，效果远好于同类基于相机的方法，甚至逼近了基于激光的方法。</p>
<p>
</p>
<h3 id="33-multi-tasks-perception-results">3.3 multi-tasks perception results</h3>
<p>联合训练3D detection和map segmentation任务，与单独训练比较训练效果，如Table4所示: 对3D目标检测和分割中的车辆类语义感知，联合训练效果有提升；对分割中的road、lane类的语义感知，联合训练效果反而会下降。</p>
<p></p>
<h3 id="34-消融试验">3.4 消融试验</h3>
<p>Spatial Cross-attention有效性</p>
<p>为了验证SCA的有效性，利用不包含TSA的BEVFormer-S来设计消融试验，结果如Table5所示。</p>
<p>默认的SCA基于deformable attention，在对比试验中构建了基于2种不同attention机制的baselines: 1. 用global attention取代deformable attention；2. 让每个query仅与它的图像参考点交互，而不是像SCA那样query与图像参考点周围区域交互。为了扩大对比范围，把BEVFormer中的BEV生成方法替换为了VPN和Lift-Spalt中的方法。从Table5结果可见Deformable Attention方法显著优于其它方法，且在GPU Memory使用量和query兴趣区域大小之间实现了balance。</p>
<p></p>
<p><strong>Temporal Self-attention有效性</strong></p>
<p>从Table1和Table4可见，在相同的设置下，BEVFormer相比于BEVFormer-S的性能大幅提升，针对有挑战性的检测任务提升更明显。TSA主要是在以下方面影响性能提升的: 1.temporal information的引入对提高目标速度估计精度非常有益；2.利用temporal information，目标预测的location和orientations更精确；3.受益于temporal information包含过去时刻object的信息，如图4所示，严重遮挡目标的recall 更高。根据nuScenes标注的遮挡程度把验证数据集进行划分成4部分，来评估BEVFormer对各种程度遮挡的性能，针对每个数据子集都会计算average recall(匹配时把中心距离的阈值设为2m)。</p>
<p></p>
<p><strong>Model Scale and Latency</strong></p>
<p>针对不同Scale Settings的BEVFormer，对比检测性能和latency，结果如Table6所示。在3方面进行BEVFormer的Scale配置: 1. 输入到BEVFormer Encoder的features是multi-scale还是single-scale；2.BEV Queries/features的尺寸；3. encoder layer数目。</p>
<p></p>
<p>从实验结果看来: Backbone的Latency远大于BEVFormer，因此Latency优化的主要瓶颈在于Backbone而不是BEVFormer(这里指BEVFormer Encoder部分)，BEVFormer可以采用不同的Scale，具备支持灵活平衡性能和efficiency的特性。</p>
<h2 id="4-discussion">4. Discussion</h2>
<p>理论上视觉图像的数据比激光数据稠密，但基于视觉的BEV效果还是比基于激光的方法性能差，那么也说明了理论上视觉还有可提升空间。
BEV Features能用于泊车位检测么？ 可能可以用BEVFormer在环视鱼眼相机上生成BEV features，用于泊车位检测或近距离目标的精确检测。
显性地引入BEV 特征，限制了最大检测距离，在高速公路场景，检测远处目标非常重要，如何权衡BEV的大小与检测距离是一个需要考虑的问题。
如何在检测精度和grid大小之间做平衡是一个问题。
针对2、3问题的一个优化方向: 设计自适应尺寸的BEV特征。这里的自适应是指根据场景来调整BEV尺寸或精度。</p>
<h2 id="5-references">5. References</h2>
<p>论文: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.17270.pdf"target="_blank" rel="external nofollow noopener noreferrer">《BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers》<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>开源工程: <a href="https://github.com/zhiqi-li/BEVFormer"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zhiqi-li/BEVFormer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>参考中文解读</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/495819042"target="_blank" rel="external nofollow noopener noreferrer">使用Transformer融合时空信息的自动驾驶感知框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/491890969"target="_blank" rel="external nofollow noopener noreferrer">3d camera-only detection: BEVFormer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/496284695"target="_blank" rel="external nofollow noopener noreferrer">BEVFormer，通过一个时空Transformer学习 BEV表征<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ol>
<p>类似工作:
Cross-view Transformers for real-time Map-view Semantic Segmentation
论文: <a href="https://arxiv.org/pdf/2205.02833v1.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2205.02833v1.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码: <a href="https://github.com/bradyz/cross"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/bradyz/cross<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>REF:
[1]. <a href="https://zhuanlan.zhihu.com/p/538490215"target="_blank" rel="external nofollow noopener noreferrer">一文读懂BEVFormer论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://zhuanlan.zhihu.com/p/543335939"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/543335939<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://zhuanlan.zhihu.com/p/629792598"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/629792598<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>FastBEV:快速而强大的BEV感知基线</title><link>https://jianye0428.github.io/posts/fastbev/</link><pubDate>Sat, 02 Sep 2023 16:49:37 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/fastbev/</guid><description><![CDATA[<p>近年来，基于鸟瞰图(BEV)表示的感知任务越来越受到关注，BEV表示有望成为下一代自动驾驶车辆(AV)感知的基础。现有大多数的BEV解决方案要么需要大量资源来执行车载推理，要么性能不佳。本文提出了一种简单而有效的框架，称为Fast BEV，它能够在车载芯片上执行更快的BEV感知。为了实现这一目标，作者首先从经验上发现，BEV表示可以足够强大，而无需昂贵的基于transformer的变换或深度表示。Fast BEV由五个部分组成，论文新颖地提出：</p>
<p>(1)一种轻量级的、易于部署的视图转换，它将<strong>2D图像特征快速传输到3D体素空</strong>间;</br>
(2)一种利用<strong>多尺度信息</strong>获得更好性能的<strong>多尺度图像编码器</strong>;</br>
(3)一种高效的<strong>BEV编码器</strong>，它专门设计用于加快车载推理;</br>
(4)针对图像和BEV空间的强大<strong>数据增强策略</strong>以避免过度拟合;</br>
(5)利用时间信息的<strong>多帧特征融合</strong>机制。</p>
<p>其中，(1)和(3)使Fast BEV能够在车载芯片上快速推理和部署，(2)、(4)和(5)确保Fast BEV具有竞争性能。所有这些都使Fast BEV成为一种具有高性能、快速推理速度和在自动驾驶车载芯片上部署友好的解决方案。通过实验，在2080Ti平台上，R50模型可以在nuScenes验证集上以47.3%的NDS运行52.6 FPS，超过了BEVDepth-R50模型的41.3 FPS和47.5%的NDS，以及BEVDet4DR50模型的30.2 FPS和45.7%的NDS。最大的型号(R101@900x1600)在nuScenes验证集上建立了具有竞争力的53.5%NDS，论文在当前主流的车载芯片上进一步开发了具有相当高精度和效率的基准！</p>
<h2 id="领域现状">领域现状</h2>
<p>快速准确的3D感知系统对于自动驾驶至关重要。经典方法依赖于激光雷达点云提供的精确3D信息。然而，激光雷达传感器通常要花费数千美元，阻碍了它们在经济型车辆上的应用。基于纯相机的鸟瞰图(BEV)方法最近显示出其强大的3D感知能力和低成本的巨大潜力。它们基本上遵循这样的范式：将多摄像机2D图像特征转换为自我汽车坐标中的3D BEV特征，然后将特定头部应用于统一BEV表示以执行特定的3D任务，例如，3D检测、分割等。统一BEV表达可以单独处理单个任务或同时处理多个任务，这是高效和灵活的。</p>
<p>为了从2D图像特征执行3D感知，nuScenes上的现有BEV方法使用基于查询的transformation [17]，[18]或基于隐式/显式深度的transformation [13]，[15]，[26]。然而，它们很难部署在车载芯片上，并且推理速度慢：</p>
<ul>
<li>(1) 基于基于查询的transformation 方法如图1(a)所示，由于解码器需要transformer内的注意机制，这些方法通常需要专用芯片来支持。</li>
<li>(2) 基于深度变换的方法如图1(b)所示。</li>
</ul>
<p>这些方法通常需要加速不友好的体素池操作，甚至多线程CUDA内核也可能不是最佳解决方案。此外，这对于在资源受限或CUDA加速推理库不支持的芯片上运行是不方便的。此外，它们在推理上很耗时，这妨碍了它们的实际部署。本文旨在为车载芯片设计一种具有友好部署、高推理速度和竞争性能的BEV感知框架，例如Xavier、Orin、Tesla T4等。</p>
<p></p>
<p>基于这些观察结果，遵循M2BEV[16]的原理，该原理假设在图像到BEV(2D到3D)视图转换期间沿相机光线的深度分布均匀，我们提出<strong>Fast-Ray转换</strong>，如图1(c)所示，借助于“查找表”和“多视图到一个体素”操作，将BEV转换加速到一个新的水平。基于Fast Ray变换，论文进一步提出了Fast BEV，这是一种更快、更强的全卷积BEV感知框架，无需昂贵的视图transformer[17]、[18]或深度表示[15]、[23]、[26]。所提出的快速BEV包括五个部分，<font color=red>Fast-Ray变换</font>、<font color=red>多尺度图像编码器</font>、<font color=red>高效BEV编码器</font>、<font color=red>数据增强</font>和<font color=red>时间融合</font>，这些共同构成了一个框架，赋予Fast BEV快速推理速度和有竞争力的性能。</p>
<p>详细展开说，本文提出了Fast Ray变换，这是一种用于快速推理的轻量级和部署友好的视图变换，通过将多视图2D图像特征沿着相机射线的体素投影到3D来获得BEV表示。此外，还提出了两种操作，即“查找表”和“多视图到一个体素”，以优化车载平台的流程。现有工作的视图转换耗时，多尺度投影操作将具有较大的时间开销，因此难以在实践中使用。基于Fast Ray变换，本文的2D到3D投影具有极快的速度，使具有多尺度投影操作的多尺度图像编码器成为可能。具体而言，与大多数使用单尺度特征作为图像编码器输出的现有工作不同，论文在图像编码器输出部分使用了3层多尺度特征金字塔网络(FPN)结构。随后是相应的3级多尺度投影操作。对于BEV编码器，作者使用很少的原始残差网络作为基本BEV编码器。在此基础上，使用三维缩减操作来加速编码器，分别是“空间到信道”(S2C)算子、多尺度级联融合(MSCF)算子和多帧级联融合(MFCF)算子。论文还进一步为图像和BEV空间引入了强大的数据增强策略，如翻转、旋转、调整大小等。分别在图像空间和BEV中执行的数据增强不仅避免了过度拟合，而且实现了更好的性能。最后引入了时间融合[17]，[23]，它通过引入时间特征融合模块将Fast BEV从纯空间扩展到时空空间，使当前关键帧能够利用来自历史帧的信息。</p>
<p>BEV感知在学术界经常更新性能基准，如NuScenes基准，但很少在工业应用方面进行研究。本文首次在当前流行的车载芯片上开发了一个具有相当准确度和效率的基准，从延迟到不同计算能力的车载芯片之间的性能，这为BEV解决方案的实际部署提供了参考。凭借其高效率和具有竞争力的性能，Fast BEV打破了现有BEV解决方案难以在低计算芯片上部署的信念，<strong>简单</strong>和<strong>高效</strong>是其主要优势。所提出的Fast BEV表现出了出色的性能，可以轻松部署在车载平台上。在nuScenes数据集上，在2080Ti平台上，R50模型可以在nuScene验证集上运行52.6 FPS和47.3%的NDS，超过了BEVDepth-R50模型的41.3 FPS和47.5%的NDS以及BEVDet4D-R50模型的30.2 FPS和45.7%的NDS。最大的型号(R101@900x1600)在nuScenes验证集上建立了具有竞争力的53.5%NDS。</p>
<h2 id="领域的主流方案梳理">领域的主流方案梳理</h2>
<p><strong>1)基于camera的单目3D目标检测</strong></p>
<p>3D目标检测中的检测器旨在预测物体在3D空间中的位置和类别，给定由激光雷达或相机传感器生成的输入。基于LiDAR的方法，例如CenterPoint，倾向于使用3D CNN从LiDAR点提取空间特征，并进一步回归到3D属性，如目标的3D中心。与LiDAR传感器相比，仅将相机图像作为输入不仅成本更低，而且图像包含更丰富的语义信息。单目3D目标检测的一种实用方法是基于3D图像特征学习3D边界框，M3DRPN提出了3D区域建议网络和深度卷积层，以提高3D场景理解。在FCOS之后，FCOS3D[7]通过将3D目标转换为图像域，直接预测每个对象的3D边界框。PGD[9]使用对象之间的关系和概率表示来捕获深度不确定性，以便于3D对象检测的深度估计，DD3D[10]受益于深度预训练，并显著改善端到端3D检测！</p>
<p><strong>2)基于camera的环视3D目标检测</strong></p>
<p>一些大型基准的最新进展，特别是使用更多的周围视图，进一步推动了3D感知领域。在基于camera的3D目标检测中，新提出的多视图transformation 技术[11]、[12]、[13]、[14]将任务重新表述为立体匹配问题，其中周围的图像特征转换成立体表示，如BEV或3D体素。例如，LSS在预测的深度分布上投影逐像素特征，以生成相机截头体，然后转换截头体进入BEV grid。OFT[14]提出通过将预定义的体素投影到图像特征上来生成体素表示，BEVDet将LSS应用于全卷积方法，并首先验证了显式BEV的性能。M2BEV首先跟随OFT探索BEV多任务感知，BEVDepth进一步扩展了LSS[13]，具有强大的深度监督和高效的池化操作。视图转换的另一个路线图是网格状的BEV查询，DETR3D和Graph-DETR3D将每个BEV查询解码为3D参考点，以从图像中采样相关的2D特征进行细化。BEVFormer在二维到三维转换中引入了空间交叉关注，允许每个查询可以跨相机视图聚合其相关的二维特征。PETR提出3D坐标生成来感知3D位置感知特征，避免生成3D参考点。这些工作的成功激励我们有效和高效地扩展周围的多摄像机检测pipeline。作者发现在类似LSS的方法中，使用深度分布是不必要的，并且可以删除它以进一步加快整个pipeline的速度！</p>
<p><strong>3)基于camera的多视图时间融合</strong></p>
<p>最近，一些基于相机的方法试图在检测过程中引入多帧融合，这已被证明对基于LiDAR的检测器中的速度估计和box定位有效[20]，[21]，[22]。而BEV作为一种同时组合来自多个相机的视觉信息的中间特征，适合于时间对齐。[40]提出了一种动态模块，该模块使用过去的空间BEV特征来学习时空BEV表示，BEVDet4D通过对齐多帧特征并利用自我运动中的空间相关性来扩展BEVDet[15]。PETRv2基于3D位置嵌入的视角，直接实现3D空间中的时间对齐。BEVFormer设计了一种时间自关注，以递归地融合历史BEV信息，类似于RNN模型中的隐藏状态。本文的工作也受到时间对齐的启发，具体来说，我们应用该技术来进一步提高性能，同时保持高效率。</p>
<h2 id="本文的方法">本文的方法</h2>
<p>BEV感知中最重要的是如何将2D特征转移到3D空间。如图1所示，基于查询的方法通过变换器中的注意力机制获得3D BEV特征，这个过程可以表示为以下公式(1)，基于深度的方法通过计算2D特征和预测深度的外积来获得3D BEV特征，具体过程如公式(2)所示:</p>
<p>$$F_{bev}(x,y,z)=Attn(q,k,v)\quad(1)$$</p>
<p>$$F_{bev}(x,y,z)=Pool{F_{2D}(u,v)\otimes D(u,v)}_{x,y,z}\quad(2)$$</p>
<p>其中F2D(u,v)表示从图像中提取的2D特征，D(u,v)表示来自2D特征的深度预测。⊗表示out producter，Pool表示体素池操作。x、y、z是三维空间中的平均坐标，u，v在二维空间中的坐标。在CUDA多线程的支持下，这些方法大大提高了GPU平台上的推理速度，但在更大的分辨率和特征维度上会遇到计算速度瓶颈，并且在没有推理库支持的情况下转移到非GPU平台不是很友好。</p>
<p>作者提出了基于光线投影的Fast-Ray transformation方法，借助于查找表和多视图到一个体素操作，以便在GPU平台上实现极高的2D到3D推理速度。此外，由于其标量索引的高效性，当在CPU平台上运行时，它仍然具有优于现有解决方案的速度性能，这使得转移到更多平台成为可能。</p>
<p></p>
<p>M2BEV[16]是解决具有统一BEV表示的多摄像机多任务感知的第一个工作之一，因为它没有昂贵的视图转换器或深度表示，因此在车载平台上具有巨大的应用潜力。受其简单性的启发，本文提出了具有卓越速度和性能的Fast BEV，如图2所示，Fast BEV将多摄像机图像作为输入，并预测3D边界框(包括速度)作为输出。其主要框架可分为五个关键模块：Fast-Ray Transformation、Multi-Scale Image Encoder、Efficient BEV Encoder、Data Augmentation、Temporal Fusion！</p>
<p><strong>1) Fast-Ray Transformation</strong></p>
<p><font color=green><strong>视图转换</strong></font>是将特征从2D图像空间转换到3D BEV空间的关键组件，这通常在整个pipelines中花费大量时间。论文按照[16]，[34]假设沿射线的深度分布是均匀的，这种方式优点是，一旦获得了相机的内在/外在参数，就可以很容易地知道2D到3D的投影。由于这里没有使用可学习的参数，可以很容易地计算图像特征图和BEV特征图中的点之间的对应矩阵。基于这一假设，本文从两个角度进一步加速该过程：<font color=red>预计算投影索引(查找表)</font>和<font color=red>密集体素特征生成(多视图到一个体素)</font>。</p>
<p></p>
<p><font color=red><strong>查找表</strong></font>。投影索引是从2D图像空间到3D体素空间的映射索引，考虑到在构建感知系统时相机位置及其内在/外在参数是固定的，并且本文的方法既不依赖于数据相关的深度预测，也不依赖于transformer，因此每个输入的投影指数都是相同的。所以不需要为每次迭代计算相同的索引，只需预先计算固定投影索引并将其存储。在推断过程中，可以通过查询查找表来获得投影索引，这是边缘设备上的一个超级便宜的操作。此外，如果我们从单个帧扩展到多个帧，还可以很容易地预先计算内部和外部参数，并将它们与当前帧预对齐。如算法1所示，通过相机参数矩阵投影构建了具有与输出三维体素空间相同维度的查找表LUT。迭代每个体素单元，并通过投影计算对应于3D坐标的2D像素坐标。如果获得的2D像素坐标地址是合法的，可以将其填充到LUT中以建立与数据无关的索引映射！</p>
<p></p>
<p><font color=red><strong>多视图到一个体素</strong></font>。基本视图变换使用原始体素聚合操作，该操作为每个相机视图存储离散体素特征，然后<strong>聚合</strong>它们以生成最终体素特征。如图3(a)所示，这是填充的每个离散体素的鸟瞰图。因为每个相机只有有限的视角，所以每个体素特征非常稀疏，例如，只有大约17%的位置是非零的。我们发现这些体素特征的聚集是非常昂贵的，因为它们的巨大尺寸。<font color=green>本文建议生成密集体素特征以避免昂贵的体素聚集</font>，具体来说，让所有相机视图中的图像特征投影到同一个体素特征，从而在最后生成一个密集体素，名为“多视图到一个体元”。如图3(b)所示，这是填充了密集体素的鸟瞰图。如算法2所示的快速射线变换算法，其将输入的多视图2D图像特征转移到一个体素3d空间中，其中每个体素单元由预先计算的LUT填充相应的2D图像特征。对于具有重叠区域的多个视图的情况，直接采用第一个遇到的视图来提高表构建的速度。结合“查找表”和“多视图到一个体素”加速设计，视图转换操作具有极快的投影速度！</p>
<p></p>
<p><strong>2)多尺度Image Encoder</strong></p>
<p>多尺度图像编码器从多视图图像中提取多层次特征，N个图像∈$R^{H×W×3}$作为输入，F1/4、F1/8、F1/16三级特征作为输出。</p>
<p></p>
<p><strong>3)高效BEV编码器</strong></p>
<p>BEV特征是4D张量，时间融合将叠加特征，这将使BEV编码器具有大量计算量。三维缩减操作用于加快编码器的速度，即<u>“空间到信道”(S2C)算子</u>、多尺度，其中所述多帧融合(MFCF)算子分别是<font color=red><u>级联融合(MSCF)算子</u></font>和<font color=red><u>多帧凹融合(MFF)算子</u></font>。S2C算子将4D体素张量V∈RX×Y×Z×C转换为3D BEV张量V∈R X×Y×(ZC)，从而避免使用内存昂贵的3D卷积。在MFCF算子之前，值得注意的是，通过多尺度投影获得的BEV特征是不同的尺度。论文将首先对X和Y维度上的多尺度BEV特征进行上采样，使其大小相同，例如200×200。MSCF和MFCF运营商在信道维度中合并多尺度多帧特征，并将它们从较高的参数量融合到较低的参数量。此外，通过实验发现，BEV编码器和3D体素分辨率的大小对性能的影响相对较小，但占用了较大的速度消耗，因此更少的block和更小的体素分辨率也更为关键！</p>
<p><strong>4)数据增强</strong></p>
<p>数据扩充的好处已在学术界形成共识。此外，3D数据集(如NuScenes、KITTI)很难标记，且成本高昂，这导致数据集中样本数量较少，因此数据增强可以带来更显著的性能提升。<strong>本文在图像空间和BEV空间中添加了数据增强，主要遵循BEVDet</strong>。</p>
<p><strong>图像增强</strong>：由于3D场景中的图像与3D相机坐标有直接关系，因此3D目标检测中的数据增强比2D检测更具挑战性。因此，如果对图像应用数据增强，还需要改变相机固有矩阵。对于增强操作，基本上遵循常见的操作，例如翻转、裁剪和旋转，在图5的左侧部分，展示了一些图像增强的示例。</p>
<p><strong>BEV增强</strong>：类似于图像增强，类似的操作可以应用于BEV空间，例如翻转、缩放和旋转。注意，增强变换应应用于BEV特征图和3D GT框，以保持一致性。BEV增强变换可以通过相应地修改相机外部矩阵来控制，在图5的右侧部分，展示了随机旋转增强，一种BEV增强！</p>
<p></p>
<p><strong>5)时间融合</strong></p>
<p>受BEVDet4D和BEVFormer的启发，作者还将历史帧引入到当前帧中以进行时间特征融合。<strong>通过空间对齐操作和级联操作，将历史帧的特征与当前帧的对应特征融合</strong>。时间融合可以被认为是帧级的特征增强，在一定范围内较长的时间序列可以带来更多的性能增益。具体来说，用三个历史关键帧对当前帧进行采样;每个关键帧具有0.5s间隔，本文采用了BEVDet4D中的多帧特征对齐方法。如图6所示，在获得四个对齐的BEV特征后，直接将它们连接起来，并将它们馈送到BEV编码器。在训练阶段，使用图像编码器在线提取历史帧特征，在测试阶段，历史帧功能可以离线保存，并直接取出用于加速。与BEVDet4D和BEVFormer进行比较，BEVDet4D只引入了一个历史框架，我们认为这不足以利用历史信息。Fast BEV使用三个历史帧，从而显著提高了性能，通过使用两个历史帧，BEVFormer略优于BEVDet4D。然而，由于记忆问题，在训练阶段，历史特征在没有梯度的情况下被分离，这不是最佳的。此外，BEVFormer使用RNN样式来顺序融合特征，这是低效的。相比之下，Fast BEV中的所有帧都以端到端的方式进行训练，这与普通GPU相比更易于训练！</p>
<h2 id="实验">实验</h2>
<p>数据集描述：评估了nuScenes数据集上的 Fast BEV，该数据集包含1000个自动驾驶场景，每个场景20秒。数据集被分成850个场景用于训练/验证，其余150个场景用于测试。虽然nuScenes数据集提供来自不同传感器的数据，但我们只使用相机数据。相机有六个视图：左前、前、右前、左后、后、右后。</p>
<p>评估指标。为了全面评估检测任务，使用平均精度(mAP)和nuScenes检测分数(NDS)的标准评估指标进行3D目标检测评估。此外，为了计算相应方面的精度(例如，平移、缩放、方向、速度和属性)，使用平均平移误差(mATE)、平均缩放误差(mASE)、平均方向误差(mAOE)、平均速度误差(mAVE)，以及平均属性误差(mAAE)作为度量。</p>
<p>和主流方法的Latency进行比较：</p>
<p></p>
<p>在可比性能下，Fast BEV、BEVDet4D和BEVDepth方案的端到端延迟比较。表的上部是三种方案的详细设置，包括每个组件的具体配置。表的下半部分是在可比性能下三种方案的每个部分的延迟和总延迟的比较。2D到3D部分包括CPU和CUDA两个平台的延迟，MSO表示多尺度输出。</p>
<p></p>
<p>nuScenes集的比较。“L”表示激光雷达，“C”表示摄像机，“D”表示深度/激光雷达监控。MS表示图像和BEV编码器中的多尺度。“¶”表示我们使用MS、scale NMS和测试时间数据增强的方法。</p>
<p></p>
<p>更多消融实验对比：</p>
<p></p>
<p></p>
<p>高效型号系列：为了满足不同计算能力平台的部署需求，本文设计了一系列从M0到M5的高效模型，如表10所示。设置了不同的图像编码器(从ResNet18到ResNet50)，图像分辨率(从256×704到900×1600)、体素分辨率(从200×200×4到250×250×6)和BEV编码器(从2b-192c到6b-256c)来设计模型尺寸。从表10可以看出，从M0到M5，随着图像编码器、图像分辨率、体素分辨率和BEV编码器逐渐变大，模型性能逐渐提高。</p>
<p>在流行设备上部署：除了注重性能，还将M系列车型部署在不同的车载平台(Xavier、Orin、T4)上，并使用CUDA-TensorRT-INT8加速。具体而言，AGX Xavier在没有使用CUDA11.4-TRTT8.4.0-INT8部署DLA加速的情况下的计算能力为22TOPS，AGX Orin 64G在没有使用CUDA11.4-TTRT8.4.0-INT8部署DLB加速的情况下的计算能力是170TOPS，T4在使用CUDA11.1-TRT7.2.1-INT8部署时的计算能力则是130TOPS。如表11所示，评估了这些车载设备上M系列模型的延迟，并将延迟分解为2D/2D到3D/3D三个部分。</p>
<p>从表11中可以看出：(1)随着M系列型号逐渐变大，性能逐渐提高，同一计算平台上的延迟也基本上逐渐增加，2D和3D部分的延迟也分别增加。(2) 从左到右，随着这三个设备的实际计算能力1逐渐增加，M系列的每个模型的延迟逐渐减少，2D和3D部分的延迟分别减少。(3) 结合表11中M系列的性能，可以发现，仅考虑延迟时，M0模型在Xavier等低计算平台上可以达到19.7FPS，这可以实现实时推理速度。考虑到性能，M2模型在性能和延迟之间具有最合理的权衡。在与表2中R50系列模型的性能相当的前提下，它在Orin平台上可以达到43.3 FPS，这可以实现实际的实时推理要求。</p>
<p>目前，BEV感知解决方案越来越多，但它们主要追求学术领域的性能，很少考虑如何更好地将其部署在车载芯片上，尤其是低计算芯片上。不能否认，BEVDet和BEVDepth等工作是当前考虑的首批可能解决方案，因为它们在部署车载芯片时非常方便。但Fast BEV在以下情况下提供了应用的可能性：</p>
<p>低计算能力芯片：尽管自动驾驶芯片的计算能力在逐渐增加，但一些计算能力较低的芯片，如英伟达Xavier，仍被用于经济型车辆。Fast BEV可以以更快的速度在低计算能力芯片上表现得更好。</p>
<p>非GPU部署：DEVEDepth和BEVDet的成功部署和应用主要依赖于CUDA多线程支持的高效体素池操作。然而，没有CUDA库的非GPU芯片，例如以DSP为计算单元的德州仪器芯片，很难开发DSP多线程处理器，CPU速度不够快，这使得它们的解决方案在此类芯片上失去了优势。Fast BEV以其快速的CPU速度提供了在非GPU芯片上部署的便利。</p>
<p>实践中可扩展：随着技术的发展，许多自动驾驶制造商已经开始放弃激光雷达，只使用纯摄像头进行感知。结果，在真实车辆收集的大量数据中没有深度信息。在实际开发中，模型放大或数据放大通常基于从真实车辆收集的数据，以利用数据潜力提高性能。在这种情况下，基于深度监控的解决方案遇到瓶颈，而Fast BEV不引入任何深度信息，可以更好地应用！</p>
<p>ref:</br>
[1]. 论文：https://arxiv.org/abs/2301.12511</br>
[2]. 代码：https://github.com/Sense-GVT/Fast-BEV</br>
[3]. <a href="https://mp.weixin.qq.com/s/aVbaw2qYc6-i21zbCNhfOg"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/aVbaw2qYc6-i21zbCNhfOg<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[4]. <a href="https://zhuanlan.zhihu.com/p/608929598"target="_blank" rel="external nofollow noopener noreferrer">FastBEV 代码注释<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
]]></description></item></channel></rss>