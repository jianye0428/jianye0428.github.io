<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习笔记 [14] | Actor-Critic - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy B"><meta name=keywords content='RL'><meta itemprop=name content="强化学习笔记 [14] | Actor-Critic"><meta itemprop=description content="0. 引言 在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy B"><meta itemprop=datePublished content="2024-02-25T15:35:58+08:00"><meta itemprop=dateModified content="2024-02-25T21:12:29+08:00"><meta itemprop=wordCount content="3888"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL,"><meta property="og:title" content="强化学习笔记 [14] | Actor-Critic"><meta property="og:description" content="0. 引言 在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy B"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_14/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-25T15:35:58+08:00"><meta property="article:modified_time" content="2024-02-25T21:12:29+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习笔记 [14] | Actor-Critic"><meta name=twitter:description content="0. 引言 在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy B"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_14/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_13/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_15/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习笔记 [14] | Actor-Critic","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_14\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":3888,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_14\/","datePublished":"2024-02-25T15:35:58+08:00","dateModified":"2024-02-25T21:12:29+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习笔记 [14] | Actor-Critic</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-25 15:35:58"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span title="更新于 2024-02-25 21:12:29"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3888 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 8 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习笔记 [14] | Actor-Critic">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><ul><li></li></ul><h1 id=0-引言>0. 引言</h1><p>在<a href=https://www.cnblogs.com/pinard/p/10137696.html target=_blank rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。</p><p>在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。</p><p>本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。</p><h1 id=1-actor-critic算法简介>1. Actor-Critic算法简介</h1><p>Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。</p><p>回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。</p><p>也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：</p><p>$$
\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)
$$</p><p>第二组是价值函数的近似，对于状态价值和动作价值函数分别是：</p><p>$$
\hat{v}(s,w)\approx v_\pi(s)
$$</p><p>$$
\hat{q}(s,a,w)\approx q_\pi(s,a)
$$</p><p>对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：</p><p>$$
\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t
$$</p><p>梯度更新部分中，$\nabla_\theta log\pi_\theta(s_t,a_t)$是我们的分值函数，不用动，要变成Actor的话改动的是$v_t$，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。</p><p>而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。</p><p>现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值$v_t$,而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$,在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{te}$</p><h1 id=2-actor-critic算法可选形式>2. Actor-Critic算法可选形式</h1><p>在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值 $v_t$实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：</p><ul><li><p>a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：</p><ul><li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)V(s,w)$$</li></ul></li><li><p>b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：</p><ul><li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)Q(s,a,w)$$</li></ul></li><li><p>c) 基于TD误差：在<a href=https://www.cnblogs.com/pinard/p/9529828.html target=_blank rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讲到了TD误差，它的表达式是 $\delta(t)=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 或者 $\delta(t)=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t)$, 这样Actor的策略函数参数更新的法公式是：</p><ul><li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)$$</li></ul></li><li><p>d) 基于优势函数：在<a href=https://www.cnblogs.com/pinard/p/9923859.html target=_blank rel="external nofollow noopener noreferrer">强化学习(十二) Dueling DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讲到过优势函数A的定义：$A(S,A,w,\beta)=Q(S,A,w,\alpha,\beta)-V(S,w,\alpha)$, 即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：</p><ul><li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,A,w,\beta)$$</li></ul></li><li><p>e) 基于 $TD(λ)$ 误差：一般都是基于后向 $TD(λ)$误差, 在<a href=https://www.cnblogs.com/pinard/p/9529828.html target=_blank rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：</p><ul><li>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)E(t)$</li></ul></li></ul><p>对于Critic本身的模型参数 $w$ ，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如 $Q(s,a,w)=ϕ(s,a)^Tw$,则Critic本身的模型参数 $w$的更新公式可以表示为：</p><p>$$\begin{gathered}
\delta=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t) \\
w=w+\beta\delta\phi(s,a)
\end{gathered}$$</p><p>通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。</p><h1 id=3-actor-critic算法流程>3. Actor-Critic算法流程</h1><p>这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数　　</p><p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，衰减因子 $γ$, 探索率 $ϵ$, Critic网络结构和Actor网络结构。</p><p>输出：Actor 网络参数 $θ$, Critic网络参数 $w$</p><ul><li>(1). 随机初始化所有的状态和动作对应的价值Q�. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。</li><li>(2). for i from 1 to T，进行迭代。<ul><li>a) 初始化 $S$ 为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li><li>b) 在Actor网络中使用 $ϕ(S)$ 作为输入，输出动作 $A$,基于动作 $A$得到新的状态 $S&rsquo;$,反馈 $R$。</li><li>c) 在Critic网络中分别使用 $ϕ(S)$，$ϕ(S&rsquo;)$ 作为输入，得到Q值输出 $V(S)$，$V(S&rsquo;)$</li><li>d) 计算TD误差 $\delta=R+\gamma V(S^{\prime})-V(S)$</li><li>e) 使用均方差损失函数 $\sum(R+\gamma V(S^{\prime})-V(S,w))^2$ 作Critic网络参数 $w$的梯度更新</li><li>f) 更新Actor网络参数 $θ$:<ul><li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(S_t,A)\delta $$</li></ul></li></ul></li></ul><p>对于Actor的分值函数 $∇_θlogπ_θ(S_t,A)$,可以选择softmax或者高斯分值函数。</p><p>上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。</p><h1 id=4-actor-critic算法实例>4. Actor-Critic算法实例</h1><p>下面我们用一个具体的例子来演示上面的Actor-Critic算法。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href=https://github.com/openai/gym/wiki/CartPole-v0 target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p><p>算法流程可以参考上面的第三节，这里的分值函数我们使用的是softmax函数，和上一片的类似。完整的代码参见Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py</p><p>代码主要分为两部分，第一部分是Actor，第二部分是Critic。对于Actor部分，大家可以和上一篇策略梯度的代码对比，改动并不大，主要区别在于梯度更新部分，策略梯度使用是蒙特卡罗法计算出的价值 $v(t)$,则我们的actor使用的是TD误差。</p><p>在策略梯度部分，对应的位置如下：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>neg_log_prob</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>tf_vt</span><span class=p>)</span>  <span class=c1># reward guided loss</span></span></span></code></pre></td></tr></table></div></div><p>而我们的Actor对应的位置的代码是：</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>exp</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>neg_log_prob</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>td_error</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>此处要注意的是，由于使用的是TD误差，而不是价值 $v(t)$,此处需要最大化<code>self.exp</code>,而不是最小化它，这点和策略梯度不同。对应的Actor代码为：</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>train_op</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>(</span><span class=n>LEARNING_RATE</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>exp</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>除此之外，Actor部分的代码和策略梯度的代码区别并不大。</p><p>对于Critic部分，我们使用了类似于DQN的三层神经网络。不过我们简化了这个网络的输出，只有一维输出值，而不是之前DQN使用的有多少个可选动作，就有多少维输出值。网络结构如下:</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_Q_network</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># network weights</span>
</span></span><span class=line><span class=cl>  <span class=n>W1q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span> <span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>b1q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>W2q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>b2q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>state_input</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>],</span> <span class=s2>&#34;state&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># hidden layers</span>
</span></span><span class=line><span class=cl>  <span class=n>h_layerq</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>,</span> <span class=n>W1q</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># Q Value layer</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>Q_value</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layerq</span><span class=p>,</span> <span class=n>W2q</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2q</span></span></span></code></pre></td></tr></table></div></div><p>和之前的DQN相比，这里还有一个区别就是我们的critic没有使用DQN的经验回放，只是使用了反馈和当前网络在下一个状态的输出来拟合当前状态。</p><p>对于算法中Actor和Critic交互的逻辑，在main函数中：</p><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>STEP</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>action</span> <span class=o>=</span> <span class=n>actor</span><span class=o>.</span><span class=n>choose_action</span><span class=p>(</span><span class=n>state</span><span class=p>)</span> <span class=c1># e-greedy action for train</span>
</span></span><span class=line><span class=cl>  <span class=n>next_state</span><span class=p>,</span><span class=n>reward</span><span class=p>,</span><span class=n>done</span><span class=p>,</span><span class=n>_</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>td_error</span> <span class=o>=</span> <span class=n>critic</span><span class=o>.</span><span class=n>train_Q_network</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>next_state</span><span class=p>)</span>  <span class=c1># gradient = grad[r + gamma * V(s_) - V(s)]</span>
</span></span><span class=line><span class=cl>  <span class=n>actor</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>td_error</span><span class=p>)</span>  <span class=c1># true_gradient = grad[logPi(s,a) * td_error]</span>
</span></span><span class=line><span class=cl>  <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span></span></span></code></pre></td></tr></table></div></div><p>大家对照第三节的算法流程和代码应该可以比较容易理清这个过程。但是这个程序很难收敛。因此大家跑了后发现分数总是很低的话是可以理解的。我们需要优化这个问题。</p><h1 id=5-actor-critic算法小结>5. Actor-Critic算法小结</h1><p>基本版的Actor-Critic算法虽然思路很好，但是由于难收敛的原因，还需要做改进。</p><p>目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法我们在从DQN到Nature DQN的过程中已经用过一次了。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p><p>在后面的文章中，我们会继续讨论DDPG和A3C。</p><p>　</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-25 21:12:29">更新于 2024-02-25&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/23cd7c478708235ece71456fee411313e37ef46b rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 23cd7c478708235ece71456fee411313e37ef46b: feat: add rl learning note to 19"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>23cd7c4</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_14/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_14/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_14/ data-title="强化学习笔记 [14] | Actor-Critic" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_14/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_14/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_14/ data-title="强化学习笔记 [14] | Actor-Critic"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_14/ data-title="强化学习笔记 [14] | Actor-Critic"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_13/ class=post-nav-item rel=prev title="强化学习笔记 [13] | 策略梯度(Policy Gradient)"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>强化学习笔记 [13] | 策略梯度(Policy Gradient)</a>
<a href=/posts/rl_learning_note_15/ class=post-nav-item rel=next title="强化学习笔记 [15] | A3C">强化学习笔记 [15] | A3C<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.124.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>