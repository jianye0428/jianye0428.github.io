<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数的近似表示 在"><meta name=keywords content='RL'><meta itemprop=name content="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning"><meta itemprop=description content="0. 引言 在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数的近似表示 在"><meta itemprop=datePublished content="2024-02-23T13:17:44+08:00"><meta itemprop=dateModified content="2024-02-28T09:10:39+08:00"><meta itemprop=wordCount content="3935"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_8/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning"><meta property="og:description" content="0. 引言 在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数的近似表示 在"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-23T13:17:44+08:00"><meta property="article:modified_time" content="2024-02-28T09:10:39+08:00"><meta property="article:tag" content="RL"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning"><meta name=twitter:description content="0. 引言 在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数的近似表示 在"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_8/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_7/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_9/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_8\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":3935,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_8\/","datePublished":"2024-02-23T13:17:44+08:00","dateModified":"2024-02-28T09:10:39+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-23 13:17:44"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-23>2024-02-23</time></span>&nbsp;<span title="更新于 2024-02-28 09:10:39"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-28>2024-02-28</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3935 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 8 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><h1 id=0-引言>0. 引言</h1><p>在强化学习系列的<a href=https://www.cnblogs.com/pinard/p/9385570.html target=_blank rel="external nofollow noopener noreferrer">前七篇<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。</p><p>Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。</p><h1 id=1-为何需要价值函数的近似表示>1. 为何需要价值函数的近似表示</h1><p>在之前讲到了强化学习求解方法，无论是动态规划DP，蒙特卡罗方法MC，还是时序差分TD，使用的状态都是离散的有限个状态集合 $S$。此时问题的规模比较小，比较容易求解。但是假如我们遇到复杂的状态集合呢？甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。　　　　</p><p>比如经典的冰球世界(PuckWorld)强化学习问题，具体的动态demo见<a href=https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间步时长的一个大小固定的力，借此来改变大圆的速度。环境会在每一个时间步内告诉个体当前的水平与垂直坐标、当前的速度在水平和垂直方向上的分量以及目标的水平和垂直坐标共6项数据，奖励值为个体与目标两者中心距离的负数，也就是距离越大奖励值越低且最高奖励值为0。</p><p>在这个问题中，状态是一个6维的向量，并且是连续值。没法直接用之前离散集合的方法来描述状态。当然，你可以说，我们可以把连续特征离散化。比如把这个冰球场100x100的框按1x1的格子划分成10000个格子，那么对于运动员的坐标和冰球的坐标就有$10^4∗10^4=10^8$次种，如果再加上个体速度的分量就更是天文数字了，此时之前讲过的强化学习方法都会因为问题的规模太大而无法使用。怎么办呢？必须要对问题的建模做修改了，而价值函数的近似表示就是一个可行的方法。</p><h1 id=2-价值函数的近似表示方法>2. 价值函数的近似表示方法</h1><p>由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数 $\hat{v}$, 这个函数由参数 $w$ 描述，并接受状态 $s$ 作为输入，计算后得到状态 $s$ 的价值，即我们期望：</p><p>$$\hat{v}(s,w)\approx v_\pi(s)$$</p><p>类似的，引入一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到动作价值，即我们期望：</p><p>$$\hat{q}(s,a,w)\approx q_\pi(s,a)$$</p><p>价值函数近似的方法很多，比如最简单的线性表示法，用 $ϕ(s)$表示状态 $s$ 的特征向量，则此时我们的状态价值函数可以近似表示为：</p><p>$$\hat{v}(s,w)=\phi(s)^Tw$$</p><p>当然，除了线性表示法，我们还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。因此后面我们的近似表达方法如果没有特别提到，都是指的神经网络的近似表示。</p><p>对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：</p><br><center><img src=images/2_01.jpg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">神经网络拟合价值函数</div></center><br><p>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值 $\hat{v}(s,w)$。对于动作价值函数，有两种方法，一种是输入状态 $s$ 的特征向量和动作 $a$，输出对应的动作价值 $\hat{q}(s,a,w)$，另一种是只输入状态 $s$ 的特征向量，动作集合有多少个动作就有多少个输出 $\hat{q}(s,ai,w)$。这里隐含了我们的动作是有限个的离散动作。</p><p>对于我们前一篇讲到的Q-Learning算法，我们现在就价值函数的近似表示来将其改造，采用上面右边的第三幅图的动作价值函数建模思路来做，现在我们叫它Deep Q-Learning。</p><h1 id=3-deep-q-learning算法思路>3. Deep Q-Learning算法思路</h1><p>Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值s和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p><p>DQN的输入是我们的状态s对应的状态向量 $ϕ(s)$， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p><p>DQN主要使用的技巧是经验回放(experience replay), 即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是<strong>经验回放</strong>。</p><p>通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数 $w$，当 $w$ 收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。</p><p>下面我们总结下DQN的算法流程，基于NIPS 2013 DQN。　　　　</p><ul><li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, Q网络结构, 批量梯度下降的样本数 $m$。</li><li>输出：Q网络参数<ul><li><ol><li>随机初始化$Q$网络的所有参数 $w$，基于 $w$初始化所有的状态和动作对应的价值 $Q$。清空经验回放的集合 $D$。</li></ol></li><li><ol start=2><li>for i from 1 to T，进行迭代。</li></ol><ul><li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li><li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li><li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态<code>is_end</code></li><li>d) 将 $\\{ϕ(S),A,R,ϕ(S&rsquo;),is_end\\}$这个五元组存入经验回放集合D</li><li>e) $S=S'$</li><li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m$，计算当前目标Q值$y_j$：<ul><li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\mathrm{~}is\mathrm{~}true\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\mathrm{~}is\mathrm{~}false\end{array}\right.\right.$$</li></ul></li><li>g) 使用均方差损失函数$\frac1m\sum_{i=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li><li>h) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li></ul></li></ul></li></ul><p>注意，上述第二步的 $f$步和 $g$步的 $Q$值计算也都需要通过 $Q$网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p><h1 id=4-deep-q-learning实例>4. Deep Q-Learning实例</h1><p>下面我们用一个具体的例子来演示DQN的应用。这里使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href=https://github.com/openai/gym/wiki/CartPole-v0 target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p><p>完整的代码参见github: <a href=https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py target=_blank rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><p>代码参考了知乎上的一个<a href=https://zhuanlan.zhihu.com/p/21477488 target=_blank rel="external nofollow noopener noreferrer">DQN实例<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>，修改了代码中的一些错误，并用最新的Python3.6+Tensorflow1.8.0运行。要跑代码需要安装OpenAI的Gym库，使用<code>pip install gym</code>即可。</p><p>代码使用了一个三层的神经网络，输入层，一个隐藏层和一个输出层。下面我们看看关键部分的代码。</p><p>算法第2步的步骤b通过$ϵ−$贪婪法选择动作的代码如下，注意每次我们$ϵ−$贪婪法后都会减小$ϵ$值。</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>egreedy_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>Q_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Q_value</span><span class=o>.</span><span class=n>eval</span><span class=p>(</span><span class=n>feed_dict</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>:[</span><span class=n>state</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>})[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span> <span class=o>-=</span> <span class=p>(</span><span class=n>INITIAL_EPSILON</span> <span class=o>-</span> <span class=n>FINAL_EPSILON</span><span class=p>)</span> <span class=o>/</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span> <span class=o>-=</span> <span class=p>(</span><span class=n>INITIAL_EPSILON</span> <span class=o>-</span> <span class=n>FINAL_EPSILON</span><span class=p>)</span> <span class=o>/</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>Q_value</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>算法第2步的步骤c在状态S�执行当前动作A�的代码如下，这个交互是由Gym完成的。</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=n>next_state</span><span class=p>,</span><span class=n>reward</span><span class=p>,</span><span class=n>done</span><span class=p>,</span><span class=n>_</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># Define reward for agent</span>
</span></span><span class=line><span class=cl>  <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=k>if</span> <span class=n>done</span> <span class=k>else</span> <span class=mf>0.1</span></span></span></code></pre></td></tr></table></div></div><p>算法第2步的步骤d保存经验回放数据的代码如下：</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>perceive</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>state</span><span class=p>,</span><span class=n>action</span><span class=p>,</span><span class=n>reward</span><span class=p>,</span><span class=n>next_state</span><span class=p>,</span><span class=n>done</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>one_hot_action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>one_hot_action</span><span class=p>[</span><span class=n>action</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>replay_buffer</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>state</span><span class=p>,</span><span class=n>one_hot_action</span><span class=p>,</span><span class=n>reward</span><span class=p>,</span><span class=n>next_state</span><span class=p>,</span><span class=n>done</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>replay_buffer</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>REPLAY_SIZE</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>replay_buffer</span><span class=o>.</span><span class=n>popleft</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>replay_buffer</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>BATCH_SIZE</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>train_Q_network</span><span class=p>()</span></span></span></code></pre></td></tr></table></div></div><p>算法第2步的步骤f,g计算目标Q值，并更新Q网络的代码如下：</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_Q_network</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>time_step</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>  <span class=c1># Step 1: obtain random minibatch from replay memory</span>
</span></span><span class=line><span class=cl>  <span class=n>minibatch</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>replay_buffer</span><span class=p>,</span><span class=n>BATCH_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>state_batch</span> <span class=o>=</span> <span class=p>[</span><span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>data</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>action_batch</span> <span class=o>=</span> <span class=p>[</span><span class=n>data</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>data</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>reward_batch</span> <span class=o>=</span> <span class=p>[</span><span class=n>data</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=k>for</span> <span class=n>data</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>next_state_batch</span> <span class=o>=</span> <span class=p>[</span><span class=n>data</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=k>for</span> <span class=n>data</span> <span class=ow>in</span> <span class=n>minibatch</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Step 2: calculate y</span>
</span></span><span class=line><span class=cl>  <span class=n>y_batch</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=n>Q_value_batch</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>Q_value</span><span class=o>.</span><span class=n>eval</span><span class=p>(</span><span class=n>feed_dict</span><span class=o>=</span><span class=p>{</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>:</span><span class=n>next_state_batch</span><span class=p>})</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=n>BATCH_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>done</span> <span class=o>=</span> <span class=n>minibatch</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>4</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>y_batch</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>reward_batch</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>y_batch</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>reward_batch</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>GAMMA</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>Q_value_batch</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>feed_dict</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>y_input</span><span class=p>:</span><span class=n>y_batch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>action_input</span><span class=p>:</span><span class=n>action_batch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>:</span><span class=n>state_batch</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span></span></span></code></pre></td></tr></table></div></div><p>我们在每100轮迭代完后会去玩10次交互测试，计算10次的平均奖励。运行了代码后，我的3000轮迭代的输出如下：</p><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>episode: <span class=m>0</span> Evaluation Average Reward: 12.2
</span></span><span class=line><span class=cl>episode: <span class=m>100</span> Evaluation Average Reward: 9.4
</span></span><span class=line><span class=cl>episode: <span class=m>200</span> Evaluation Average Reward: 10.4
</span></span><span class=line><span class=cl>episode: <span class=m>300</span> Evaluation Average Reward: 10.5
</span></span><span class=line><span class=cl>episode: <span class=m>400</span> Evaluation Average Reward: 11.6
</span></span><span class=line><span class=cl>episode: <span class=m>500</span> Evaluation Average Reward: 12.4
</span></span><span class=line><span class=cl>episode: <span class=m>600</span> Evaluation Average Reward: 29.6
</span></span><span class=line><span class=cl>episode: <span class=m>700</span> Evaluation Average Reward: 48.1
</span></span><span class=line><span class=cl>episode: <span class=m>800</span> Evaluation Average Reward: 85.0
</span></span><span class=line><span class=cl>episode: <span class=m>900</span> Evaluation Average Reward: 169.4
</span></span><span class=line><span class=cl>episode: <span class=m>1000</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1100</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1200</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1300</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1400</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1500</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1600</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1700</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1800</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1900</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2000</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2100</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2200</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2300</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2400</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2500</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2600</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2700</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2800</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table></div></div><p>大概到第1000次迭代后，算法已经收敛，达到最高的200分。当然由于是$ϵ−$探索，每次前面的输出可能不同，但最后应该都可以收敛到200的分数。当然由于DQN不保证绝对的收敛，所以可能到了200分后还会有抖动。</p><h1 id=5-deep-q-learning小结>5. Deep Q-Learning小结　　　　</h1><p>DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。但是DQN有个问题，就是它并不一定能保证Q网络的收敛，也就是说，我们不一定可以得到收敛后的Q网络参数。这会导致我们训练出的模型效果很差。</p><p>针对这个问题，衍生出了DQN的很多变种，比如Nature DQN(NIPS 2015), Double DQN，Dueling DQN等。这些我们在下一篇讨论。</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-28 09:10:39">更新于 2024-02-28&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/d426456642d56dc3ffe2ec26e60d7ea7c402054d rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) d426456642d56dc3ffe2ec26e60d7ea7c402054d: feat: update post default setting"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>d426456</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_8/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_8/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_8/ data-title="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_8/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_8/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_8/ data-title="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_8/ data-title="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_7/ class=post-nav-item rel=prev title="强化学习笔记 [7] | 时序差分离线控制算法Q-Learning"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>强化学习笔记 [7] | 时序差分离线控制算法Q-Learning</a>
<a href=/posts/rl_learning_note_9/ class=post-nav-item rel=next title="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN">强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.125.7">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>