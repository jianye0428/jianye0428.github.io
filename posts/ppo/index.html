<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习 | PPO 论文解读 - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="1. 引言 ​ 首先在论文的引言部分给出了经典的强化学习算法的不足之处:许多的经典强化学习算法在大型的模型、数据采样效率、鲁棒性(无需手动超参调整)上都有很大的提升空间。Q-Learning算法(包括函数逼近类算法)在许多简单问题上应用存在局限性,例如要满足状态空间与动作空间的离散型要求，并且其理解起来也是"><meta name=keywords content='PPO'><meta itemprop=name content="强化学习 | PPO 论文解读"><meta itemprop=description content="1. 引言 ​ 首先在论文的引言部分给出了经典的强化学习算法的不足之处:许多的经典强化学习算法在大型的模型、数据采样效率、鲁棒性(无需手动超参调整)上都有很大的提升空间。Q-Learning算法(包括函数逼近类算法)在许多简单问题上应用存在局限性,例如要满足状态空间与动作空间的离散型要求，并且其理解起来也是"><meta itemprop=datePublished content="2023-07-14T08:43:50+08:00"><meta itemprop=dateModified content="2024-05-07T15:56:02+08:00"><meta itemprop=wordCount content="2835"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="PPO"><meta property="og:url" content="https://jianye0428.github.io/posts/ppo/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="强化学习 | PPO 论文解读"><meta property="og:description" content="1. 引言 ​ 首先在论文的引言部分给出了经典的强化学习算法的不足之处:许多的经典强化学习算法在大型的模型、数据采样效率、鲁棒性(无需手动超参调整)上都有很大的提升空间。Q-Learning算法(包括函数逼近类算法)在许多简单问题上应用存在局限性,例如要满足状态空间与动作空间的离散型要求，并且其理解起来也是"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-14T08:43:50+08:00"><meta property="article:modified_time" content="2024-05-07T15:56:02+08:00"><meta property="article:tag" content="PPO"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习 | PPO 论文解读"><meta name=twitter:description content="1. 引言 ​ 首先在论文的引言部分给出了经典的强化学习算法的不足之处:许多的经典强化学习算法在大型的模型、数据采样效率、鲁棒性(无需手动超参调整)上都有很大的提升空间。Q-Learning算法(包括函数逼近类算法)在许多简单问题上应用存在局限性,例如要满足状态空间与动作空间的离散型要求，并且其理解起来也是"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/ppo/><link rel=prev href=https://jianye0428.github.io/posts/dqn/><link rel=next href=https://jianye0428.github.io/posts/dpg/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习 | PPO 论文解读","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/ppo\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"PPO","wordcount":2835,"url":"https:\/\/jianye0428.github.io\/posts\/ppo\/","datePublished":"2023-07-14T08:43:50+08:00","dateModified":"2024-05-07T15:56:02+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习 | PPO 论文解读</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
</span></span><span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2023-07-14 08:43:50"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-14>2023-07-14</time></span>&nbsp;<span title="更新于 2024-05-07 15:56:02"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-05-07>2024-05-07</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2835 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 6 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习 | PPO 论文解读">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-引言>1. 引言</a></li><li><a href=#2-背景>2. 背景</a><ul><li><a href=#21-策略梯度方法policy-gradient-methods>2.1 策略梯度方法(Policy Gradient Methods)</a></li><li><a href=#22-置信域方法>2.2 置信域方法</a></li></ul></li></ul><ul><li><ul><li><a href=#4自适应罚函数系数方法>4.自适应罚函数系数方法</a></li><li><a href=#5算法部分>5.算法部分</a></li><li><a href=#6实验部分>6.实验部分</a></li></ul></li><li><a href=#ref>Ref:</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><h2 id=1-引言>1. 引言</h2><p>​ 首先在论文的引言部分给出了经典的强化学习算法的不足之处:许多的经典强化学习算法在大型的模型、数据采样效率、鲁棒性(无需手动超参调整)上都有很大的提升空间。Q-Learning算法(包括函数逼近类算法)在许多简单问题上应用存在局限性,例如要满足状态空间与动作空间的离散型要求，并且其理解起来也是一件很困难的事情、而vanilla policy gradient算法的数据效率与鲁棒性较差、置信域优化算法(TRPO)相对来说比较复杂，而且对于包含噪声或参数共享(在策略函数与价值函数之间有其他的辅助任务需求)的网络结构不兼容(比如dropout)。
​
该论文的主要目的是为了解决上述问题，<strong>在TRPO的基础上运用一阶优化提高其数据的使用效率与良好的表现</strong>。创新的使用了一种对目标函数使用限幅概率比(clipped probabilty ratios)的方法，对原有策略的表现做出悲观主义的估计(个人理解这里是对原有策略目标函数 $J(\theta)$ 的下界做出估计)。为了优化策略 $\pi_{\theta}$,我们交替从策略中采样数据，并对采样数据执行几个优化阶段。</p><p>​实验比较了好几个代理者算法，发现其中采用限幅概率比(clipped probabilty ratios)的方法表现效果最佳。相比于之前的一些强化学习算法，PPO在连续控制问题上比之前的算法效果都要好。在Atari游戏中，其表现要显著强于A2C和ACER算法，因为其是更加简单的(从策略的采样复杂度的角度上来说)。</p><h2 id=2-背景>2. 背景</h2><h3 id=21-策略梯度方法policy-gradient-methods>2.1 策略梯度方法(Policy Gradient Methods)</h3><p>策略梯度方法主要通过计算一个 $estimator s(\hat{g})$, 并且将其用于随机梯度下降算法中实现策略的梯度上升功能 $(\theta\leftarrow\theta+\alpha\hat{g})$。比较常用的一种estimator如下所示:</p><p>$$\hat{g}=\hat{E}<em>t\left[\nabla</em>\theta\log\pi_\theta\left(a_t\left|s_t\right.\right)\hat{A}_t\right]$$</p><p>其中 $\pi_{\theta}$ ​是随机策略函数，$\hat{A_t}$ 是在第 $t$个步长时的优势函数 $A(s_t,a_t)$ 的估计值。</p><blockquote><p>由之前的知识我们可以知道，这里的 $\hat{ A_t}$ 如果是用MonteCarlo方法来估计即: $\hat{A_t} = G_t-v_{\omega}(s_t)$ ，便是Reinforce with baseline 方法。如果采用 $\hat{A_t} = Q_{\pi_{\theta}}(s_t,a_t)-v_{\omega}(s_t)$ 来估计，便是A2C方法。</p></blockquote><p>其中: $\hat E_t[&mldr;]$表明采用在一群采样的样本之间采用经验平均值来估计，即: $\hat{E_t[&mldr;]}=\frac{1}{n}\sum_{t=1}^n[&mldr;]$。由于可以用带用自动梯度计算的软件对 $\hat{g}$ 进行计算，因此 $\hat{g}$ 可以视为对以下目标进行梯度计算 $\nabla_{\theta}$:</p><p>$$L^{PG}(\theta)=\hat{E}<em>t\left[\log\pi</em>\theta\left(a_t\left|s_t\right.\right)\hat{A}_t\right]$$</p><p>​ 尽管上式在用多个轨迹(trajectory)对误差 $L^{PG}(\theta)$ 进行多步的参数更新优化时有一定的优势。但是这么做理由并不充分(原话是doing so is not well-justified我不知道怎么翻译比较合适)，并且这常常会导致一个毁灭性的极大范围的策略参数 $\theta$ 的更新问题(个人认为这里想强调的是学习率$\alpha$ 不好调，容易造成参数的估计出现问题)。</p><h3 id=22-置信域方法>2.2 置信域方法</h3><p>在TRPO算法中，目标函数(或者称为代理函数)是一个被要求最大化的目标函数，其被约束在一系列的策略更新的约束下。具体而言该优化问题可以描述如下:</p><p>$$\max_\theta\hat{E}<em>t\left[\frac{\pi</em>\theta\left(a_t\left|s_t\right.\right)}{\pi_{\theta_{old}}\left(a_t\left|s_t\right.\right)}\hat{A}<em>t\right]\s.t.\hat{E}<em>t\left[KL[\pi</em>{\theta</em>{old}}\left(.|s_t\right),\pi_\theta\left(.|s_t\right)]\right]\leq\delta $$</p><p>​ 其中，$\theta_{old}$ 是在策略函数 $\pi$ 更新之前的 $\theta$，是个向量。该约束问题可以在对目标在 $\theta_{old}$ ​进行一阶近似(泰勒一阶展开)，在约束处对 $\theta_{old}$ ​进行二阶近似(泰勒二阶展开)如下后，利用共轭梯度法求解。</p><p>$$\max_\theta g(\theta_{old})(\theta-\theta_{old})\s.t.\frac12(\theta-\theta_{old})^TF(\theta_{old})(\theta-\theta_{old})\leq\delta $$</p><p>其中， $F(\theta)=E_{s_{t} ,a_{t}\sim\pi_{\theta}} [\nabla_{\theta} \operatorname{log}\pi(a_{t} |s_{t} ;\theta)[\nabla_{\theta} \operatorname{log}\pi(a_{t} |s_{t} ;\theta)]^{T}]$，而 $g\left(\theta\right)=\nabla_{\theta}E_{\pi(\theta_{old})}[\frac{\pi_\theta\left(a_t\mid s_t\right)}{\pi_{\theta_{old}\left(a_t\mid s_t\right)}}\hat{A}_{t}]$</p><p>事实上，上述的置信域优化问题的求解一般建议采用罚函数法求解，而并不是利用带约束优化问题的常规套路求解。将上述带约束的优化问题转化为如下的无约束优化问题：</p><p>$$\max_\theta\hat{E}<em>t\left[\frac{\pi</em>\theta\left(a_t\left|s_t\right.\right)}{\pi_{\theta_{old}\left(a_t\left|s_t\right.\right)}}\hat{A}<em>t-\beta KL[\pi</em>{\theta_{old}}\left(.|s_t\right),\pi_\theta\left(.|s_t\right)]\right]$$</p><p>​对于其中的 $\beta$ 为罚系数。事实上上式中对于 $KL$ 散度实际使用时，用最大散度替代平均散度实现约束效果。而TRPO算法使用时一般用硬约束而非罚函数，这是因为选择 $\beta$ 时，不同的 $\beta$ 的选择会产生许多不同的问题。因此为了实现我们一阶优化算法的目标，选择一个固定的惩罚系数 $\beta$ 用SGD优化上面的罚方程是不现实的(这里主要强调的是调超参 $\beta$ 的问题)。</p><h1 id=3剪裁代理目标clipped-surrogate-objective>3.剪裁代理目标(Clipped Surrogate Objective)</h1><p>设$r_t\left(\theta\right)=\frac{\pi_\theta\left(a_t\left|s_t\right.\right)}{\pi_{\theta_{old}}\left(a_t\left|s_t\right.\right)}$。TRPO算法最大化一个代理目标函数如下:</p><p>$$L^{CPI}(\theta)=\hat{E}<em>t\left[\frac{\pi</em>\theta\left(a_t\left|s_t\right.\right)}{\pi_{\theta_{old}}\left(a_t\left|s_t\right.\right)}\hat{A}_t\right]=\hat{E}_t\left[r_t\left(\theta\right)\hat{A}_t\right]$$</p><p>​其中CPI指的是保守策略迭代(conservative policy iteration)。如果没有KL散度的约束，最大化 $L^{CPI}(\theta)$ 将导致一个大的策略参数的过估计。<strong>因此我们考虑修正这个目标，去惩罚比率$r_t(\theta)$远离1时的情况</strong>。</p><p>我们主要考虑如下所示的最大化目标函数:</p><p>$$L^{CLIP}(\theta)=\hat{E}_t\left[\min(r_t\left(\theta\right)\hat{A}_t,\mathbf{clip}(r_t\left(\theta\right),1-\varepsilon,1+\varepsilon)\hat{A}_t)\right]$$</p><p>​而其中 $\varepsilon$ 是一个待调的超参，一般可以设为 \varepsilon = 0.2$，上式的第一个最小项是 $L^{CPI}(\theta)$。第二个最小项是 $\mathbf{clip}(r_t(\theta),1-\varepsilon,1+\varepsilon)\hat{A_t})$，通过切片比率修正了代理函数:</p><p>$$\left.\mathbf{clip}(r_t\left(\theta\right),1-\varepsilon,1+\varepsilon)=\left{\begin{array}{l}1-\varepsilon,r_t\left(\theta\right)\leq1-\varepsilon\r_t\left(\theta\right)\quad,1-\varepsilon\leq r_t\left(\theta\right)\leq1+\varepsilon\1+\varepsilon,r_t\left(\theta\right)\geq1+\varepsilon\end{array}\right.\right.$$</p><p>当 $\hat {A_t}$ ​取不同的符号时，其 $L^{CLIP}(\theta)$ 和 $r$ 的关系如下图所示:</p><br><center><img src=images/3_1.png width=640 height=220 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><blockquote><p>个人理解:当 $\hat A_t\geq 0$时，为了达到目标 $\max L^{CLIP}(\theta)$，限制 $r_t(\theta)$ 只在小于 $1+\varepsilon$ 时有增大趋势。而当 $r_t(\theta)$ 过大即 $r_t(\theta)\geq 1+\varepsilon$ 时，限制其 $L^{CLIP}(\theta)$ 的继续增大。这样对于那些使得 $r_t(\theta)$过大的 $\theta$ , 其目标函数 $L^{CLIP}(\theta)$ 不会更大，也就不在$ \theta$ 的搜索的考虑范围之内。因此采用搜索算法搜索 $\theta$ 时不容易搜索到使得 $r_t(\theta)$ 过大的 $\theta$。达到在尽可能置信域内搜索$\theta$的效果。当$\hat{A_t}\leq0$时，同理依然如此。</p></blockquote><p>对比不同方法在连续控制问题中的调整参数 $\theta$ 对散度 $KL(\theta,\theta_{old})$ 的影响效果如下图所示:</p><br><center><img src=images/3_2.png width=640 height=220 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><p>该图可以看出在初始参数 $\theta_{old}$ 的一次PPO算法迭代之后，$KL(\pi_{\theta},\pi_{\theta_{old}})$ 几乎最大值才是0.02。这充分保住了$\theta$ 在 $\theta_{old}$ 的置信域之内。该图来自于 Hopper-v1问题，超参数如下:</p><br><center><img src=images/3_3.png width=640 height=360 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><h3 id=4自适应罚函数系数方法>4.自适应罚函数系数方法</h3><p>前面讲到采用罚函数法进行参数更新时，主要是罚函数系数 $\beta$ 的选取比较困难。而现在一种克服方法是自适应调整系数 $\beta$。</p><p>其优化目标如下:</p><p>$$\max L^{KLPEN}(\theta)=\hat{E_t}\left[\frac{\pi_\theta\left(a_t\left|s_t\right.\right)}{\pi_{\theta_{old}\left(a_t\left|s_t\right.\right)}}\hat{A}<em>t-\beta KL[\pi</em>{\theta_{old}}\left(.|s_t\right),\pi_\theta\left(.|s_t\right)]\right]$$</p><p>​计算 $d=\hat{E}<em>{t}\left[KL[\pi</em>{\theta_{old}}\left(.|s_{t}\right),\pi_{\theta}\left(.|s_{t}\right)]\right]$, 当$d\leq\frac{d_{targ}}{1.5},\beta\leftarrow\frac\beta2$​; 当 $d\geq1.5d_{targ} ,\beta\leftarrow2\beta$。值得一提的是 $\beta$的初值以及参数1.5和2对算法本身并不敏感。这是因为算法会在使用中自动判断并且调整参数。</p><h3 id=5算法部分>5.算法部分</h3><br><center><img src=images/5_1.png width=640 height=180 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br>在网上找了一个比较详细的伪代码图:<br><center><img src=images/5_2.png width=640 height=280 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><p>算法中优势函数的计算如下(具体算法过程不过多描述请参考原论文)：</p><br><center><img src=images/5_3.png width=640 height=180 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><h3 id=6实验部分>6.实验部分</h3><p>不过多描述，就给张图：<br></p><center><img src=images/6_1.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><h2 id=ref>Ref:</h2><p>[1] <a href=https://blog.csdn.net/shengzimao/article/details/126493407 target=_blank rel="external nofollow noopener noreferrer">PPO算法经典论文阅读<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
[2] <a href=https://blog.csdn.net/weixin_42529756/article/details/131691832 target=_blank rel="external nofollow noopener noreferrer">PPO(Proximal Policy Optimization Algorithms)论文解读及实现<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
[3] <a href=https://zhuanlan.zhihu.com/p/512327050 target=_blank rel="external nofollow noopener noreferrer">影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="<nil> 支付宝" data-alt="<nil> 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="<nil> 微信" data-alt="<nil> 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-05-07 15:56:02">更新于 2024-05-07&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/823febaa29cb5a7b2b27fa5c1419416b35449ae2 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 823febaa29cb5a7b2b27fa5c1419416b35449ae2: feat: update pilot"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>823feba</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/ppo/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/PPO/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/ppo/ data-title="强化学习 | PPO 论文解读" data-hashtags=PPO><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/ppo/ data-hashtag=PPO><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/ppo/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/ppo/ data-title="强化学习 | PPO 论文解读"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/ppo/ data-title="强化学习 | PPO 论文解读"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/ppo/ class=post-tag>PPO</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/dqn/ class=post-nav-item rel=prev title=DQN><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>DQN</a>
<a href=/posts/dpg/ class=post-nav-item rel=next title=DPG>DPG<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.126.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>