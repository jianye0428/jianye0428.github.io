<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>RL学习笔记 [2] | 马尔科夫决策过程(MDP) - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP这一篇对应Sut"><meta name=keywords content='RL'><meta itemprop=name content="RL学习笔记 [2] | 马尔科夫决策过程(MDP)"><meta itemprop=description content="0. 引言 在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP这一篇对应Sut"><meta itemprop=datePublished content="2024-02-21T10:38:11+08:00"><meta itemprop=dateModified content="2024-02-29T09:37:40+08:00"><meta itemprop=wordCount content="2740"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_2/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="RL学习笔记 [2] | 马尔科夫决策过程(MDP)"><meta property="og:description" content="0. 引言 在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP这一篇对应Sut"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-21T10:38:11+08:00"><meta property="article:modified_time" content="2024-02-29T09:37:40+08:00"><meta property="article:tag" content="RL"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="RL学习笔记 [2] | 马尔科夫决策过程(MDP)"><meta name=twitter:description content="0. 引言 在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP这一篇对应Sut"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_2/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_1/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_3/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"RL学习笔记 [2] | 马尔科夫决策过程(MDP)","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_2\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":2740,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_2\/","datePublished":"2024-02-21T10:38:11+08:00","dateModified":"2024-02-29T09:37:40+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>RL学习笔记 [2] | 马尔科夫决策过程(MDP)</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-21 10:38:11"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-21>2024-02-21</time></span>&nbsp;<span title="更新于 2024-02-29 09:37:40"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-29>2024-02-29</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2740 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 6 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="RL学习笔记 [2] | 马尔科夫决策过程(MDP)">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><h1 id=0-引言>0. 引言</h1><p>在<a href=https://www.cnblogs.com/pinard/p/9385570.html target=_blank rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。</p><p>MDP这一篇对应Sutton书的第三章和UCL强化学习课程的第二讲。</p><h1 id=1-强化学习引入mdp的原因>1. 强化学习引入MDP的原因</h1><p>对于马尔科夫性本身，我之前讲过的<a href=http://www.cnblogs.com/pinard/p/6945257.html target=_blank rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（一）HMM模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>，<a href=http://www.cnblogs.com/pinard/p/7048333.html target=_blank rel="external nofollow noopener noreferrer">条件随机场CRF(一)从随机场到线性链条件随机场<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>以及<a href=http://www.cnblogs.com/pinard/p/6632399.html target=_blank rel="external nofollow noopener noreferrer">MCMC(二)马尔科夫链<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>都有讲到。它本身是一个比较简单的假设，因此这里就不专门对“马尔可夫性”做专门的讲述了。</p><p>除了对于环境的状态转化模型这个因素做马尔科夫假设外，我们还对强化学习第四个要素个体的策略(policy) $π$ 也做了马尔科夫假设。即在状态 $s$ 时采取动作 $a$ 的概率仅与当前状态 $s$ 有关，与其他的要素无关。用公式表示就是</p><p>$$\pi(a\mid s)=P(A_{t}=a\mid S_{t}=s)$$</p><p>对于第五个要素，价值函数 $v_π(s)$ 也是一样, $v_π(s)$ 现在仅仅依赖于当前状态了，那么现在价值函数 $v_π(s)$ 表示为:</p><p>$$\nu_{\pi}(s)=\mathrm{E}_{\pi}(G_{t}|S_{t}=s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)$$</p><p>其中，$G_t$ 代表收获(return), 是一个MDP中从某一个状态 $S_t$ 开始采样直到终止状态时所有奖励的有衰减的之和。</p><h1 id=2-mdp的价值函数与贝尔曼方程>2. MDP的价值函数与贝尔曼方程</h1><p>对于MDP，我们在第一节里已经讲到了它的价值函数 $v_π(s)$ 的表达式。但是这个表达式没有考虑到所采用的动作$a$带来的价值影响，因此我们除了 $v_π(s)$ 这个状态价值函数外，还有一个动作价值函数 $q_π(s,a)$，即：</p><p>$$q_{\pi}(s,a)=\operatorname{E}_{\pi}(G_{t}|S_{t}=s,A_{t}=a)=\operatorname{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s,A_{t}=a)$$</p><p>根据价值函数的表达式，我们可以推导出价值函数基于状态的递推关系，比如对于状态价值函数 $v_π(s)$，可以发现：</p><p>$$\begin{aligned}
V_{\pi}(s)& =\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s) \\\\
&=\mathrm{E}_{\pi}(R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)|S_{t}=s) \\\\
&=\mathrm{E}_{\pi}(R_{t+1}+\gamma G_{t+1}|S_{t}=s) \\\\
&=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})|S_{t}=s)
\end{aligned}$$</p><p>也就是说，在 $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 是满足递推关系的，即：</p><p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})\mid S_{t}=s)$$
　　　　
这个递推式子我们一般将它叫做<strong>贝尔曼方程</strong>。这个式子告诉我们，一个状态的价值由该状态的奖励以及后续状态价值按一定的衰减比例联合组成。</p><p>同样的方法，我们可以得到动作价值函数 $q_π(s,a)$ 的贝尔曼方程：</p><p>$$q_{\pi}(s,a)=\mathrm{E}_{\pi}(R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})\mid S_{t}=s,A_{t}=a)$$</p><h1 id=3-状态价值函数与动作价值函数的递推关系>3. 状态价值函数与动作价值函数的递推关系</h1><p>根据动作价值函数 $q_π(s,a)$ 和状态价值函数 $v_π(s)$ 的定义，我们很容易得到他们之间的转化关系公式：</p><p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$</p><p>也就是说，状态价值函数是所有动作价值函数基于策略 $π$ 的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值。</p><p>反过来，利用上贝尔曼方程，我们也很容易从状态价值函数 $v_π(s)$ 表示动作价值函数 $q_π(s,a)$，即：</p><p>$$q_{\pi}(s,a)=R_{s}^{a}+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;})$$</p><p>通俗说就是状态动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。</p><p>这两个转化过程也可以从下图中直观的看出：</p><br><center><img src=images/3_01.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">状态价值函数</div></center><br><br><center><img src=images/3_02.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">动作价值函数</div></center><br><p>把上面两个式子互相结合起来，我们可以得到：</p><p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;}))$$</p><p>$$q_\pi(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\sum_{a&rsquo; \in A}\pi(a&rsquo; \mid s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)$$</p><h1 id=4-最优价值函数>4. 最优价值函数</h1><p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略我们可以用 $π^*$表示。一旦找到这个最优策略$π^∗$，那么我们就解决了这个强化学习问题。一般来说，比较难去找到一个最优策略，但是可以通过比较若干不同策略的优劣来确定一个较好的策略，也就是局部最优解。</p><p>如何比较策略的优劣呢？一般是通过对应的价值函数来比较的，也就是说，寻找较优策略可以通过寻找较优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者，即：</p><p>$$\nu_{*}(s)=\max_{\pi}\nu_{\pi}(s)$$</p><p>同理也可以定义最优动作价值函数是所有策略下产生的众多动作状态价值函数中的最大者，即：</p><p>$$q_*(s,a)=\max_\pi q_\pi(s,a)$$</p><p>对于最优的策略，基于动作价值函数我们可以定义为：</p><p>$$\pi_<em>(a|s)=\begin{cases}1&\mathrm{if~}a=\mathrm{arg~}\max_{a\in A}q</em>(s,a)\\\\0&\mathrm{else}&\end{cases}$$</p><p>只要我们找到了最大的状态价值函数或者动作价值函数，那么对应的策略 $π^*$ 就是我们强化学习问题的解。同时，利用状态价值函数和动作价值函数之间的关系，我们也可以得到:</p><p>$$v_<em>(s)=\max_aq_</em>(s,a)$$</p><p>反过来的最优价值函数关系也很容易得到：</p><p>$$q_{<em>}(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss}^{a}{}_{</em>}(\mathrm{s&rsquo;})$$</p><p>利用上面的两个式子也可以得到和第三节末尾类似的式子：</p><p>$$\nu_<em>(s)=\max_a(R_s^a+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^a\nu_</em>(s&rsquo;))$$</p><p>$$q_<em>(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\max_{a&rsquo;}q_</em>(s&rsquo;,a&rsquo;)$$</p><h1 id=5-mdp实例>5. MDP实例</h1><p>上面的公式有点多，需要一些时间慢慢消化，这里给出一个UCL讲义上实际的例子，首先看看具体我们如何利用给定策略来计算价值函数。</p><br><center><img src=images/5_01.jpg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">MDP 举例</div></center><br><p>例子是一个学生学习考试的MDP。里面左下那个圆圈位置是起点，方框那个位置是终点。上面的动作有study, pub, facebook, quit, sleep，每个状态动作对应的即时奖励R已经标出来了。我们的目标是找到最优的动作价值函数或者状态价值函数，进而找出最优的策略。</p><p>为了方便，我们假设衰减因子 $γ=1$, $π(a|s)=0.5$。</p><p>对于终点方框位置，由于其没有下一个状态，也没有当前状态的动作，因此其状态价值函数为0。对于其余四个状态，我们依次定义其价值为<em>v</em>1,<em>v</em>2,<em>v</em>3,<em>v</em>4， 分别对应左上，左下，中下，右下位置的圆圈。我们基于$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s&rsquo;))$计算所有的状态价值函数。可以列出一个方程组。</p><ul><li><p>对于<em>v</em>1位置，我们有：$v_1=0.5*(-1+v_1)+0.5*(0+v_2)$</p></li><li><p>对于<em>v</em>2位置，我们有：$v_2=0.5*(-1+v_1)+0.5*(-2+v_3)$</p></li><li><p>对于<em>v</em>3位置，我们有：$v_3=0.5*(0+0)+0.5*(-2+v_4)$</p></li><li><p>对于<em>v</em>4位置，我们有：$v_4=0.5*(10+0)+0.5*(1+0.2<em>v_2+0.4</em>v_3+0.4*v_4)$</p></li></ul><p>解出这个方程组可以得到 $v_1=−2.3$, $v_2=−1.3$, $v_3=2.7$, $v_4=7.4$, 即每个状态的价值函数如下图：</p><br><center><img src=images/5_02.jpg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">MDP</div></center><br><p>上面我们固定了策略$ π(a|s)$, 虽然求出了每个状态的状态价值函数，但是却并不一定是最优价值函数。那么如何求出最优价值函数呢？这里由于状态机简单，求出最优的状态价值函数 $v*(s)$ 或者动作价值函数 $q*(s,a)$ s比较容易。</p><p>我们这次以动作价值函数 $q*(s,a)$ 来为例求解。首先终点方框处的好求。</p><p>$$q*(s_3,sleep)=0,q*(s_4,study)=10$$</p><p>接着我们就可利用 $q*(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\max_{a&rsquo;}q*(s&rsquo;,a&rsquo;)$ 列方程组求出所有的 $q∗(s,a)$ 。有了所有的 $q*(s,a)$,利用 $v_{<em>}(s)=\max_{a}q</em>(s,a)$ 就可以求出所有的 $v∗(s)$。最终求出的所有 $v∗(s)$ 和 $q∗(s,a)$ 如下图：</p><br><center><img src=images/5_03.jpg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">MDP</div></center><br><p>从而我们的最优决策路径是走6->6->8->10->结束。　　　　</p><h1 id=6-mdp小结>6. MDP小结</h1><p>MDP是强化学习入门的关键一步，如果这部分研究的比较清楚，后面的学习就会容易很多。因此值得多些时间在这里。虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。</p><p>下一篇讨论用动态规划的方法来求解强化学习的问题。</p><h1 id=7-ref>7. ref</h1><p><a href=https://www.cnblogs.com/pinard/p/9426283.html target=_blank rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9426283.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-29 09:37:40">更新于 2024-02-29&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/20f9fa383c95a06c0d4e26af4312983f3581333d rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 20f9fa383c95a06c0d4e26af4312983f3581333d: feat: update post default setting"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>20f9fa3</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_2/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_2/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_2/ data-title="RL学习笔记 [2] | 马尔科夫决策过程(MDP)" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_2/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_2/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_2/ data-title="RL学习笔记 [2] | 马尔科夫决策过程(MDP)"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_2/ data-title="RL学习笔记 [2] | 马尔科夫决策过程(MDP)"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_1/ class=post-nav-item rel=prev title="RL学习笔记 [1] | 模型基础"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>RL学习笔记 [1] | 模型基础</a>
<a href=/posts/rl_learning_note_3/ class=post-nav-item rel=next title="RL学习笔记 [3] | 用动态规划(DP)求解">RL学习笔记 [3] | 用动态规划(DP)求解<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.125.5">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>