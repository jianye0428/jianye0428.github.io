<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习笔记 [13] | 策略梯度(Policy Gradient) - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien"><meta name=keywords content='RL'><meta itemprop=name content="强化学习笔记 [13] | 策略梯度(Policy Gradient)"><meta itemprop=description content="0. 引言 在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien"><meta itemprop=datePublished content="2024-02-25T15:35:55+08:00"><meta itemprop=dateModified content="2024-02-25T21:12:29+08:00"><meta itemprop=wordCount content="3282"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL,"><meta property="og:title" content="强化学习笔记 [13] | 策略梯度(Policy Gradient)"><meta property="og:description" content="0. 引言 在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_13/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-25T15:35:55+08:00"><meta property="article:modified_time" content="2024-02-25T21:12:29+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习笔记 [13] | 策略梯度(Policy Gradient)"><meta name=twitter:description content="0. 引言 在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_13/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_12/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_14/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习笔记 [13] | 策略梯度(Policy Gradient)","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_13\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":3282,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_13\/","datePublished":"2024-02-25T15:35:55+08:00","dateModified":"2024-02-25T21:12:29+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习笔记 [13] | 策略梯度(Policy Gradient)</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-25 15:35:55"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span title="更新于 2024-02-25 21:12:29"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3282 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 7 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习笔记 [13] | 策略梯度(Policy Gradient)">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><h1 id=0-引言>0. 引言</h1><p>在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradient)，它是Policy Based强化学习方法，基于策略来学习。</p><p>本文参考了Sutton的强化学习书第13章和策略梯度的<a href=https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf target=_blank rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。</p><h1 id=1-value-based强化学习方法的不足>1. Value Based强化学习方法的不足</h1><p>DQN系列强化学习算法主要的 <strong><font color=red>问题</font></strong> 主要有三点。</p><ul><li><p>第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见<a href=https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。</p></li><li><p>第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。</p></li><li><p>第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。</p></li></ul><p>由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述类别问题的方法，比如Policy Based强化学习方法。</p><h1 id=2-policy-based强化学习方法引入>2. Policy Based强化学习方法引入</h1><p>回想我们在Value Based强化学习方法里，我们对价值函数进行了近似表示，引入了一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到近似的动作价值，即：</p><p>$$\hat{q}\left(s,a,w\right)\approx q_\pi(s,a)$$</p><p>在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略 $π$可以被被描述为一个包含参数 $θ$ 的函数,即：</p><p>$$\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)$$</p><p>将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？</p><h1 id=3-策略梯度的优化目标>3. 策略梯度的优化目标</h1><p>我们要用梯度上升来寻找最优的梯度，首先就要找到一个可以优化的函数目标。</p><p>最简单的优化目标就是初始状态收获的期望，即优化目标为：</p><p>$$J_1(\theta)=V_{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}(G_1)$$</p><p>但是有的问题是没有明确的初始状态的，那么我们的优化目标可以定义平均价值，即：
$$J_{avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$$</p><p>其中，$d_πθ(s)$ 是基于策略 $π_θ$生成的马尔科夫链关于状态的静态分布。</p><p>或者定义为每一时间步的平均奖励，即：</p><p>$$J_{avR}(\theta)==\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$$</p><p>无论我们是采用 $J_1$, $J_{av}V$, 还是 $J_{av}R$ 来表示优化目标，最终对 $θ$求导的梯度都可以表示为：</p><p>$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p><p>具体的证明过程这里就不再列了，如果大家感兴趣，可以去看策略梯度的<a href=https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf target=_blank rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>的附录1，里面有详细的证明。</p><p>当然我们还可以采用很多其他可能的优化目标来做梯度上升，此时我们的梯度式子里面的 $\nabla_\theta log\pi_\theta(s,a)$ 部分并不改变，变化的只是后面的 $Q_\pi(s,a)$ 部分。对于 $\nabla_\theta log\pi_\theta(s,a)$,我们一般称为<strong>分值函数</strong>(score function)。</p><p>现在梯度的式子已经有了，后面剩下的就是策略函数 $\pi_\theta(s,a)$的设计了。</p><h1 id=4-策略函数的设计>4. 策略函数的设计</h1><p>现在我们回头看一下策略函数 $\pi_\theta(s,a)$ 的设计，在前面它一直是一个数学符号。</p><p>最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征 $ϕ(s,a)$ 与参数 $θ$的线性组合来权衡一个行为发生的几率,即:</p><p>$$\pi_\theta(s,a)=\frac{e^{\phi(s,a)^T\theta}}{\sum_be^{\phi(s,b)^T\theta}}$$</p><p>则通过求导很容易求出对应的分值函数为：</p><p>$$\nabla_\theta log\pi_\theta(s,a)=\phi(s,a)-\mathbb{E}_{\pi_\theta}[\phi(s,.)]$$</p><p>另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布 $\mathbb{N}(\phi(\mathrm{s})^{\mathbb{T}}\theta,\sigma^2)$中产生。高斯策略对应的分值函数求导可以得到为:</p><p>$$\nabla_\theta log\pi_\theta(s,a)==\frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}$$</p><p>有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。</p><h1 id=5-蒙特卡罗策略梯度reinforce算法>5. 蒙特卡罗策略梯度reinforce算法</h1><p>这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数 $v(s)$ 来近似代替策略梯度公式里面的 $Q_π(s,a)$。算法的流程很简单，如下所示:</p><ul><li>输入：N个蒙特卡罗完整序列,训练步长 $α$</li><li>输出：策略函数的参数 $θ$<ul><li>(1). for 每个蒙特卡罗序列:<ul><li>a. 用蒙特卡罗法计算序列每个时间位置t的状态价值 $v_t$</li><li>b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数 $θ$：<ul><li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t$$</li></ul></li></ul></li><li>(2).返回策略函数的参数 $θ$</li></ul></li></ul><p>　　这里的策略函数可以是softmax策略，高斯策略或者其他策略。</p><h1 id=6-策略梯度实例>6. 策略梯度实例</h1><p>这里给出第5节的蒙特卡罗策略梯度reinforce算法的一个实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href=https://github.com/openai/gym/wiki/CartPole-v0 target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p><p>完整的代码参见我的github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py</p><p>这里我们采用softmax策略作为我们的策略函数，同时，softmax的前置部分，也就是我们的策略模型用一个三层的softmax神经网络来表示。这样好处就是梯度的更新可以交给神经网络来做。</p><p>我们的softmax神经网络的结构如下，注意这个网络不是价值Q网络，而是策略网络：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_softmax_network</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># network weights</span>
</span></span><span class=line><span class=cl>  <span class=n>W1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span> <span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>b1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>W2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>b2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=c1># input layer</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>state_input</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=s2>&#34;float&#34;</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>tf_acts</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;actions_num&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>tf_vt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=p>],</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;actions_value&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># hidden layers</span>
</span></span><span class=line><span class=cl>  <span class=n>h_layer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># softmax layer</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>softmax_input</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=c1>#softmax output</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>all_act_prob</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>softmax_input</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s1>&#39;act_prob&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>neg_log_prob</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>sparse_softmax_cross_entropy_with_logits</span><span class=p>(</span><span class=n>logits</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>softmax_input</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>tf_acts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>neg_log_prob</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>tf_vt</span><span class=p>)</span>  <span class=c1># reward guided loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>train_op</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>(</span><span class=n>LEARNING_RATE</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>loss</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>注意我们的损失函数是softmax交叉熵损失函数和状态价值函数的乘积，这样TensorFlow后面可以自动帮我们做梯度的迭代优化。</p><p>另一个要注意的点就是蒙特卡罗法里面价值函数的计算，一般是从后向前算，这样前面的价值的计算可以利用后面的价值作为中间结果，简化计算，对应代码如下：</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>discounted_ep_rs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ep_rs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>running_add</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ep_rs</span><span class=p>))):</span>
</span></span><span class=line><span class=cl>      <span class=n>running_add</span> <span class=o>=</span> <span class=n>running_add</span> <span class=o>*</span> <span class=n>GAMMA</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>ep_rs</span><span class=p>[</span><span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=n>discounted_ep_rs</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>=</span> <span class=n>running_add</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>discounted_ep_rs</span> <span class=o>-=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>discounted_ep_rs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>discounted_ep_rs</span> <span class=o>/=</span> <span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>discounted_ep_rs</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>其余部分和之前的DQN的代码类似。</p><h1 id=7-策略梯度小结>7. 策略梯度小结</h1><p>策略梯度提供了和DQN之类的方法不同的新思路，但是我们上面的蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，我们需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，我们的参数更新的方向很可能不是策略梯度的最优方向。</p><p>因此，Policy Based的强化学习方法还需要改进，注意到我们之前有Value Based强化学习方法，那么两者能不能结合起来一起使用呢？下一篇我们讨论Policy Based+Value Based结合的策略梯度方法Actor-Critic。</p><p>　　　　</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-25 21:12:29">更新于 2024-02-25&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/23cd7c478708235ece71456fee411313e37ef46b rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 23cd7c478708235ece71456fee411313e37ef46b: feat: add rl learning note to 19"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>23cd7c4</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_13/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_13/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_13/ data-title="强化学习笔记 [13] | 策略梯度(Policy Gradient)" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_13/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_13/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_13/ data-title="强化学习笔记 [13] | 策略梯度(Policy Gradient)"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_13/ data-title="强化学习笔记 [13] | 策略梯度(Policy Gradient)"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_12/ class=post-nav-item rel=prev title="强化学习笔记 [12] | Dueling DQN"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>强化学习笔记 [12] | Dueling DQN</a>
<a href=/posts/rl_learning_note_14/ class=post-nav-item rel=next title="强化学习笔记 [14] | Actor-Critic">强化学习笔记 [14] | Actor-Critic<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.123.6">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>