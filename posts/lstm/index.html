<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>长短期记忆网络 -- LSTM - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="一、传统的BP网络和CNN网络 BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。 BP Network CNN Network 图中的隐含层、卷积层、池"><meta name=keywords content='temporal sequences process'><meta itemprop=name content="长短期记忆网络 -- LSTM"><meta itemprop=description content="一、传统的BP网络和CNN网络 BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。 BP Network CNN Network 图中的隐含层、卷积层、池"><meta itemprop=datePublished content="2023-12-28T21:50:25+08:00"><meta itemprop=dateModified content="2024-01-29T20:28:03+08:00"><meta itemprop=wordCount content="4606"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="temporal sequences process,"><meta property="og:title" content="长短期记忆网络 -- LSTM"><meta property="og:description" content="一、传统的BP网络和CNN网络 BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。 BP Network CNN Network 图中的隐含层、卷积层、池"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/lstm/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-28T21:50:25+08:00"><meta property="article:modified_time" content="2024-01-29T20:28:03+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="长短期记忆网络 -- LSTM"><meta name=twitter:description content="一、传统的BP网络和CNN网络 BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。 BP Network CNN Network 图中的隐含层、卷积层、池"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/lstm/><link rel=prev href=https://jianye0428.github.io/posts/ch08_designing_concurrent_code/><link rel=next href=https://jianye0428.github.io/posts/argoverse2/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"长短期记忆网络 -- LSTM","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/lstm\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"temporal sequences process","wordcount":4606,"url":"https:\/\/jianye0428.github.io\/posts\/lstm\/","datePublished":"2023-12-28T21:50:25+08:00","dateModified":"2024-01-29T20:28:03+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>长短期记忆网络 -- LSTM</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/dl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> DL</a></span></div><div class=post-meta-line><span title="发布于 2023-12-28 21:50:25"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-12-28>2023-12-28</time></span>&nbsp;<span title="更新于 2024-01-29 20:28:03"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-01-29>2024-01-29</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 4606 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 10 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="长短期记忆网络 -- LSTM">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#一传统的bp网络和cnn网络>一、传统的BP网络和CNN网络</a></li><li><a href=#二lstm网络>二、LSTM网络</a><ul><li><a href=#21-lstm的原理>2.1 LSTM的原理</a></li></ul></li><li><a href=#三lstm的输入结构>三、LSTM的输入结构</a></li><li><a href=#四pytorch中的lstm>四、Pytorch中的LSTM</a><ul><li><a href=#41-pytorch中定义的lstm模型>4.1 Pytorch中定义的LSTM模型</a></li><li><a href=#42-喂给lstm的数据格式>4.2 喂给LSTM的数据格式</a></li><li><a href=#43-lstm的output格式>4.3 LSTM的output格式</a></li></ul></li><li><a href=#五lstm和其他网络组合>五、LSTM和其他网络组合</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><h2 id=一传统的bp网络和cnn网络>一、传统的BP网络和CNN网络</h2><p>BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。<br></p><center><img src=images/1_01.jpg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><center><img src=images/1_02.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">CNN Network</div></center><br><p>图中的隐含层、卷积层、池化层、全连接层等，都是实际存在的，一层层前后叠加，在空间上很好理解，因此在写代码的时候，基本就是看图写代码，比如用keras就是:</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 示例代码，没有实际意义</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>  <span class=c1># 添加卷积层</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>MaxPooling2D</span><span class=p>(</span><span class=n>pool_size</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>         <span class=c1># 添加池化层</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.25</span><span class=p>))</span>                          <span class=c1># 添加dropout层</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>  <span class=c1># 添加卷积层</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>MaxPooling2D</span><span class=p>(</span><span class=n>pool_size</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>         <span class=c1># 添加池化层</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.25</span><span class=p>))</span>                          <span class=c1># 添加dropout层</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>....</span>   <span class=c1># 添加其他卷积操作</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Flatten</span><span class=p>())</span>                            <span class=c1># 拉平三维数组为2维数组</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>        <span class=n>添加普通的全连接层</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>....</span>  <span class=c1># 训练网络</span></span></span></code></pre></td></tr></table></div></div><h2 id=二lstm网络>二、LSTM网络</h2><p>RNN网络的机构图如下图所示:</p><center><img src=images/2_01.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">2.1 RNN Architecture Overview</div></center><br><p><strong><font color=purple>RNN 面临的问题:短时记忆和梯度消失/梯度爆炸</font></strong></p><ul><li><p><font color=red>短时记忆</font>
<strong>问题描述</strong>：RNN在处理长序列时，由于信息的传递是通过隐藏状态进行的，随着时间的推移，较早时间步的信息可能会在传递到后面的时间步时逐渐消失或被覆盖。</p><p><strong>影响</strong>：这导致RNN难以捕捉和利用序列中的长期依赖关系，从而限制了其在处理复杂任务时的性能。</p></li><li><p><font color=red>梯度消失/梯度爆炸</font>
<strong>问题描述</strong>：在RNN的反向传播过程中，梯度会随着时间步的推移而逐渐消失（变得非常小）或爆炸（变得非常大）。</p><p><strong>影响</strong>：梯度消失使得RNN在训练时难以学习到长期依赖关系，因为较早时间步的梯度信息在反向传播到初始层时几乎为零。梯度爆炸则可能导致训练过程不稳定，权重更新过大，甚至导致数值溢出。</p></li></ul><p><strong><font color=blue>LSTM解决问题:</font></strong>
大脑和LSTM在处理信息时都选择性地保留重要信息，忽略不相关细节，并据此进行后续处理。这种机制使它们能够高效地处理和输出关键信息，解决了RNN（递归神经网络）在处理长序列时面临的问题。</p><table><body text=red><tr><td style=text-align:center;font-weight:700 bgcolor=yellow><font size=3 color=red>大脑记忆机制</font></td></tr></body></table><ul><li><p>大脑记忆机制：当浏览评论时，大脑倾向于记住重要的关键词。无关紧要的词汇和内容容易被忽略。回忆时，大脑提取并表达主要观点，忽略细节。</p></li><li><p>LSTM门控机制：LSTM通过输入门、遗忘门和输出门选择性地保留或忘记信息，使用保留的相关信息来进行预测，类似于大脑提取并表达主要观点。</p></li></ul><p>图2.1是RNN循环神经网络经典的结构图，LSTM只是对隐含层节点A做了改进，整体结构不变，因此本文讨论的也是这个结构的可视化问题。</p><p>中间的A节点隐含层，左边是表示只有一层隐含层的LSTM网络，所谓LSTM循环神经网络就是在时间轴上的循环利用，在时间轴上展开后得到右图。</p><p><strong>上图右边，我们看Xt表示序列，下标t是时间轴，所以，A的数量表示的是时间轴的长度，是同一个神经元在不同时刻的状态(Ht)，不是隐含层神经元个数。</strong></p><p>我们知道，LSTM网络在训练时会使用上一时刻的信息，加上本次时刻的输入信息来共同训练。</p><p>举个简单的例子: 在第一天我生病了(初始状态H0)，然后吃药(利用输入信息X1训练网络)，第二天好转但是没有完全好(H1)，再吃药(X2),病情得到好转(H2),如此循环往复知道病情好转。因此，输入Xt是吃药，时间轴T是吃多天的药，隐含层状态是病情状况。因此我还是我，只是不同状态的我。</p><p>实际上，LSTM的网络是这样的:</p><center><img src=images/2_02.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">LSTM Network</div></center><p>上面的图表示包含2个隐含层的LSTM网络，在T=1时刻看，它是一个普通的BP网络，在T=2时刻看也是一个普通的BP网络，只是沿时间轴展开后，T=1训练的隐含层信息H,C会被传递到下一个时刻T=2，如下图所示。上图中向右的五个常常的箭头，所指的也是隐含层状态在时间轴上的传递。</p><center><img src=images/2_03.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">LSTM Architecture Overview</div></center><p>注意，图中H表示隐藏层状态，C是遗忘门，后面会讲解它们的维度。</p><h3 id=21-lstm的原理>2.1 LSTM的原理</h3><table><body text=red><tr><td style=text-align:left;font-weight:700 bgcolor=yellow><font size=3 color=red>RNN工作原理:第一个词被转换成了机器可读的向量，然后 RNN 逐个处理向量序列。</font></td></tr></body></table><br><center><img src=images/2_1_01.gif width=640 height=180 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">RNN 工作原理</div></center><br><ul><li><p><strong><font color=purple>隐藏状态的传递</font></strong>
过程描述：在处理序列数据时，RNN将前一时间步的隐藏状态传递给下一个时间步。</p><p>作用：隐藏状态充当了神经网络的“记忆”，它包含了网络之前所见过的数据的相关信息。</p><p>重要性：这种传递机制使得RNN能够捕捉序列中的时序依赖关系。</p></li></ul><br><center><img src=images/2_1_02.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">将隐藏状态传递给下一个时间步</div></center><br><ul><li><p><strong><font color=purple>隐藏状态的计算</font></strong>
细胞结构：RNN的一个细胞接收当前时间步的输入和前一时间步的隐藏状态。</p><p>组合方式：当前输入和先前隐藏状态被组合成一个向量，这个向量融合了当前和先前的信息。</p><p>激活函数：组合后的向量经过一个tanh激活函数的处理，输出新的隐藏状态。这个新的隐藏状态既包含了当前输入的信息，也包含了之前所有输入的历史信息。</p></li></ul><br><center><img src=images/2_1_03.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BP Network</div></center><br><p>**<font color=red>输出:</font>**新的隐藏状态被输出，并被传递给下一个时间步，继续参与序列的处理过程。</p><br><center><img src=images/2_1_04.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">RNN的细胞结构和运算</div></center><br><table><body text=red><tr><td style=text-align:center;font-weight:700 bgcolor=yellow><font size=3 color=fuchsia>LSTM工作原理：</font></td></tr></body></table><br><center><img src=images/2_1_05.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">LSTM的细胞结构和运算</div></center><br><ul><li><p><strong><font color=red>输入门:</font></strong>
作用：决定哪些新信息应该被添加到记忆单元中。</p><p>组成：输入门由一个sigmoid激活函数和一个tanh激活函数组成。sigmoid函数决定哪些信息是重要的，而tanh函数则生成新的候选信息。</p><p>运算：输入门的输出与候选信息相乘，得到的结果将在记忆单元更新时被考虑。</p></li></ul><br><center><img src=images/2_1_06.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">输入门（sigmoid激活函数 + tanh激活函数）</div></center><br><ul><li><p><strong><font color=red>遗忘门:</font></strong>
作用：决定哪些旧信息应该从记忆单元中遗忘或移除。</p><p>组成：遗忘门仅由一个sigmoid激活函数组成。</p></li></ul><br><center><img src=images/2_1_07.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">sigmoid激活函数（区间0～1）</div></center><br><p>运算：sigmoid函数的输出直接与记忆单元的当前状态相乘，用于决定哪些信息应该被保留，哪些应该被遗忘。输出值越接近1的信息将被保留，而输出值越接近0的信息将被遗忘。</p><br><center><img src=images/2_1_08.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">遗忘门（sigmoid激活函数）</div></center><br><ul><li><p><strong><font color=red>输出门:</font></strong></p><p>作用：决定记忆单元中的哪些信息应该被输出到当前时间步的隐藏状态中。</p><p>组成：输出门同样由一个sigmoid激活函数和一个tanh激活函数组成。sigmoid函数决定哪些信息应该被输出，而tanh函数则处理记忆单元的状态以准备输出。</p><p>运算：sigmoid函数的输出与经过tanh函数处理的记忆单元状态相乘，得到的结果即为当前时间步的隐藏状态。</p></li></ul><br><center><img src=images/2_1_09.gif width=720 height=240 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">输出门(sigmoid激活函数 + tanh激活函数)</div></center><br><h2 id=三lstm的输入结构>三、LSTM的输入结构</h2><p>为了更好理解LSTM结构，还必须理解LSTM的数据输入情况。仿照3通道图像的样子，再加上时间轴后的多样本的多特征的不同时刻的数据立方体如下图所示:</p><center><img src=images/3_01.jpeg width=720 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Input Structure of LSTM Network</div></center><p>右边的图是我们常见模型的输入，比如XGBOOST，lightGBM，决策树等模型，输入的数据格式都是这种(N<em>F)的矩阵，而左边是加上时间轴后的数据立方体，也就是时间轴上的切片，它的维度是(N</em>T*F),第一维度是样本数，第二维度是时间，第三维度是特征数，如下图所示:</p><center><img src=images/3_02.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">天气数据立方体</div></center><p>这样的数据立方体很多，比如天气预报数据，把样本理解成城市，时间轴是日期，特征是天气相关的降雨风速PM2.5等，这个数据立方体就很好理解了。在NLP里面，一句话会被embedding成一个矩阵，词与词的顺序是时间轴T，索引多个句子的embedding三维矩阵如下图所示:</p><center><img src=images/3_03.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">NLP Embedding Matrix</div></center><h2 id=四pytorch中的lstm>四、Pytorch中的LSTM</h2><h3 id=41-pytorch中定义的lstm模型>4.1 Pytorch中定义的LSTM模型</h3><p>pytorch中定义的LSTM模型的参数如下</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>参数有</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>input_size</span><span class=p>:</span> <span class=n>x的特征维度</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>hidden_size</span><span class=p>:</span> <span class=n>隐藏层的特征维度</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>num_layers</span><span class=p>:</span> <span class=n>lstm隐层的层数</span><span class=err>，</span><span class=n>默认为1</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>bias</span><span class=p>:</span> <span class=n>False则bihbih</span><span class=o>=</span><span class=mi>0</span><span class=n>和bhhbhh</span><span class=o>=</span><span class=mf>0.</span> <span class=n>默认为True</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>batch_first</span><span class=p>:</span> <span class=n>True则输入输出的数据格式为</span> <span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>seq</span><span class=p>,</span> <span class=n>feature</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>dropout</span><span class=p>:</span> <span class=n>除最后一层</span><span class=err>，</span><span class=n>每一层的输出都进行dropout</span><span class=err>，</span><span class=n>默认为</span><span class=p>:</span>  <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>bidirectional</span><span class=p>:</span> <span class=n>True则为双向lstm默认为False</span></span></span></code></pre></td></tr></table></div></div><p>结合前面的图形，我们一个个看。</p><p>(1)input_size: x的特征维度，就是数据立方体中的F，在NLP中就是一个词被embedding后的向量长度，如下图所示:</p><center><img src=images/4_01.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">LSTM Feature Matrix</div></center><p>(2)hidden_size: 隐藏层的特征维度(隐藏层神经元个数)，如下图所示，我们有两个隐含层，每个隐藏层的特征维度都是5。注意，<strong>非双向LSTM的输出维度等于隐藏层的特征维度</strong>。</p><br><center><img src=images/4_02.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">隐藏层特征维度</div></center><br><p>(3)num_layers: lstm隐层的层数，上面的图我们定义了2个隐藏层。
(4)batch_first: 用于定义输入输出维度，后面再讲。
(5)bidirectional: 是否是双向循环神经网络，如下图是一个双向循环神经网络，因此在使用双向LSTM的时候我需要特别注意，正向传播的时候有(Ht, Ct),反向传播也有(Ht&rsquo;, Ct&rsquo;),前面我们说了非双向LSTM的输出维度等于隐藏层的特征维度，而<strong>双向LSTM的输出维度是隐含层特征数<em>2，而且H,C的维度是时间轴长度</em>2</strong>。</p><br><center><img src=images/4_03.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Bidirectional RNN</div></center><br><h3 id=42-喂给lstm的数据格式>4.2 喂给LSTM的数据格式</h3><p>pytorch中LSTM的输入数据格式默认如下:</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>input</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>input_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>参数有</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>seq_len</span><span class=p>:</span> <span class=n>序列长度</span><span class=err>，</span><span class=n>在NLP中就是句子长度</span><span class=err>，</span><span class=n>一般都会用pad_sequence补齐长度</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>batch</span><span class=p>:</span> <span class=n>每次喂给网络的数据条数</span><span class=err>，</span><span class=n>在NLP中就是一次喂给网络多少个句子</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>input_size</span><span class=p>:</span> <span class=n>特征维度</span><span class=err>，</span><span class=n>和前面定义网络结构的input_size一致</span><span class=err>。</span></span></span></code></pre></td></tr></table></div></div><p>前面也说到，如果LSTM的参数 batch_first=True，则要求输入的格式是:</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>input</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>input_size</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>刚好调换前面两个参数的位置。其实这是比较好理解的数据形式，下面以NLP中的embedding向量说明如何构造LSTM的输入。</p><p>之前我们的embedding矩阵如下图:<br></p><center><img src=images/4_04.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Embedding Matrix</div></center><br><p>如果把batch放在第一位，则三维矩阵的形式如下:</p><br><center><img src=images/4_05.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Batch First</div></center><br><p>其转换过程如下图所示:<br></p><center><img src=images/4_06.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">将三维矩阵转换成二维形式</div></center><br><p>看懂了吗，这就是输入数据的格式，是不是很简单。
LSTM的另外两个输入是 h0 和 c0，可以理解成网络的初始化参数，用随机数生成即可。</p><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>h0</span><span class=p>(</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>c0</span><span class=p>(</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>参数</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>num_layers</span><span class=p>:</span> <span class=n>隐藏层数</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>num_directions</span><span class=p>:</span> <span class=n>如果是单向循环网络</span><span class=err>，</span><span class=n>则num_directions</span><span class=o>=</span><span class=mi>1</span><span class=err>，</span><span class=n>双向则num_directions</span><span class=o>=</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>batch</span><span class=p>:</span> <span class=n>输入数据的batch</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>hidden_size</span><span class=p>:</span> <span class=n>隐藏层神经元个数</span></span></span></code></pre></td></tr></table></div></div><p>注意，如果我们定义的input格式是:</p><div class=highlight id=id-6><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>input</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>input_size</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>则H和C的格式也是要变的:</p><div class=highlight id=id-7><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>h0</span><span class=p>(</span><span class=n>batc</span><span class=err>，</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>c0</span><span class=p>(</span><span class=n>batc</span><span class=err>，</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><h3 id=43-lstm的output格式>4.3 LSTM的output格式</h3><p>LSTM的输出是一个tuple，如下:</p><div class=highlight id=id-8><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>output</span><span class=p>,(</span><span class=n>ht</span><span class=p>,</span> <span class=n>ct</span><span class=p>)</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>output</span><span class=p>:</span>  <span class=n>最后一个状态的隐藏层的神经元输出</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>ht</span><span class=p>:</span> <span class=n>最后一个状态的隐含层的状态值</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=n>ct</span><span class=p>:</span> <span class=n>最后一个状态的隐含层的遗忘门值</span></span></span></code></pre></td></tr></table></div></div><p>output的默认维度是:</p><div class=highlight id=id-9><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>output</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>hidden_size</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ht</span><span class=p>(</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ct</span><span class=p>(</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>和input的情况类似，如果我们前面定义的input格式是:</p><div class=highlight id=id-10><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>input</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>input_size</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>则ht和ct的格式也是要变的:</p><div class=highlight id=id-11><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ht</span><span class=p>(</span><span class=n>batc</span><span class=err>，</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ct</span><span class=p>(</span><span class=n>batc</span><span class=err>，</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>num_directions</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>说了这么多，我们回过头来看看ht和ct在哪里，请看下图:<br></p><center><img src=images/4_07.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">LSTM Network</div></center><br><p>output在哪里？请看下图:<br></p><center><img src=images/4_08.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">输出层</div></center><br><h2 id=五lstm和其他网络组合>五、LSTM和其他网络组合</h2><p>还记得吗，output的维度等于隐藏层神经元的个数，即hidden_size，在一些时间序列的预测中，会在output后，接上一个全连接层，全连接层的输入维度等于LSTM的hidden_size，之后的网络处理就和BP网络相同了，如下图:</p><br><center><img src=images/5_01.jpeg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">在LSTM网络后接上全连接层</div></center><br><p>用pytorch实现上面的结构:</p><div class=highlight id=id-12><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RegLSTM</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>RegLSTM</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义LSTM</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义回归层网络，输入的特征维度等于LSTM的输出，输出维度为1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reg</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>,</span> <span class=p>(</span><span class=n>ht</span><span class=p>,</span><span class=n>ct</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_len</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>reg</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></td></tr></table></div></div><p>当然，有些模型则是将输出当做另一个LSTM的输入，或者使用隐藏层ht,ct的信息进行建模，不一而足。
好了，以上就是我对LSTM的一些学习心得，看完记得关注点赞。</p><p>REF:
[[1]. 漂亮，LSTM模型结构的可视化](https: //mp.weixin.qq.com/s?__biz=MzU1OTYzNjg5OQ==&amp;mid=2247545117&amp;idx=1&amp;sn=670ba155d94b229d39c5bf0bf20239d5&amp;chksm=fc1639d1cb61b0c72434a00454b2af8f9022e7ac3030a4186cda22ef5594ef5994620dc5fd52&amp;mpshare=1&amp;scene=1&amp;srcid=0617kfSozC3sKY1lRjYg1f0u&amp;sharer_shareinfo=6833fdea9df7fee2c423a9474c0928be&amp;sharer_shareinfo_first=6833fdea9df7fee2c423a9474c0928be#rd)
[2].https: //zhuanlan.zhihu.com/p/94757947
[3].https: //zhuanlan.zhihu.com/p/59862381
[4].https: //zhuanlan.zhihu.com/p/36455374
[5].https: //www.zhihu.com/question/41949741/answer/318771336
[6].https: //blog.csdn.net/android_ruben/article/details/80206792
to be added:<br>[7].https: //www.analyticsvidhya.com/blog/2021/01/understanding-architecture-of-lstm/
[8]. <a href=https://mp.weixin.qq.com/s/p1jmj__DQIwDMDTCExHs5Q target=_blank rel="external nofollow noopener noreferrer">神经网络算法 - 一文搞懂LSTM(长短期记忆网络)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-01-29 20:28:03">更新于 2024-01-29&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/e154fb743ceeee6b6dc61b358dfbcc57056fa375 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) e154fb743ceeee6b6dc61b358dfbcc57056fa375: feat: add content for lstm"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>e154fb7</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/lstm/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/DL/LSTM/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/lstm/ data-title="长短期记忆网络 -- LSTM" data-hashtags="temporal sequences process"><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/lstm/ data-hashtag="temporal sequences process"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/lstm/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/lstm/ data-title="长短期记忆网络 -- LSTM"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/lstm/ data-title="长短期记忆网络 -- LSTM"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/temporal-sequences-process/ class=post-tag>Temporal Sequences Process</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/ch08_designing_concurrent_code/ class=post-nav-item rel=prev title="C++ Concurrency in Action [8] | CH08 Designing Concurrent Code"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>C++ Concurrency in Action [8] | CH08 Designing Concurrent Code</a>
<a href=/posts/argoverse2/ class=post-nav-item rel=next title="Argoverse 2 数据集">Argoverse 2 数据集<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.123.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>