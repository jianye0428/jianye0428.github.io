<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>变分自编码器 VAE 详解 - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content><meta name=keywords content='VAE'><meta itemprop=name content="变分自编码器 VAE 详解"><meta itemprop=description content><meta itemprop=datePublished content="2023-07-27T10:53:41+08:00"><meta itemprop=dateModified content="2023-09-03T19:04:36+08:00"><meta itemprop=wordCount content="7415"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="VAE"><meta property="og:url" content="https://jianye0428.github.io/posts/vae_1/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="变分自编码器 VAE 详解"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-27T10:53:41+08:00"><meta property="article:modified_time" content="2023-09-03T19:04:36+08:00"><meta property="article:tag" content="VAE"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="变分自编码器 VAE 详解"><meta name=twitter:description content><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/vae_1/><link rel=prev href=https://jianye0428.github.io/posts/clause_8/><link rel=next href=https://jianye0428.github.io/posts/clause_9/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"变分自编码器 VAE 详解","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/vae_1\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"VAE","wordcount":7415,"url":"https:\/\/jianye0428.github.io\/posts\/vae_1\/","datePublished":"2023-07-27T10:53:41+08:00","dateModified":"2023-09-03T19:04:36+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>变分自编码器 VAE 详解</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/ml/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> ML</a></span></div><div class=post-meta-line><span title="发布于 2023-07-27 10:53:41"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-27>2023-07-27</time></span>&nbsp;<span title="更新于 2023-09-03 19:04:36"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-09-03>2023-09-03</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 7415 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 15 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="变分自编码器 VAE 详解">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#引入>引入</a></li><li><a href=#一ae>一、AE</a></li><li><a href=#二ae-存在的问题>二、AE 存在的问题</a></li><li><a href=#三vae-结构预览>三、VAE 结构预览</a></li><li><a href=#四数学描述>四、数学描述</a><ul><li><a href=#41作者的-intuition>4.1、作者的 Intuition</a></li><li><a href=#42变分下界>4.2、变分下界</a></li><li><a href=#43loss-function>4.3、Loss Function</a></li><li><a href=#44蒙特卡洛法求梯度>4.4、蒙特卡洛法求梯度</a></li><li><a href=#45重参数化-trick>4.5、重参数化 Trick</a></li><li><a href=#46-generic-sgvb>4.6 Generic SGVB</a></li><li><a href=#47another-sgvb>4.7、Another SGVB</a></li></ul></li><li><a href=#五vae-结构回顾>五、VAE 结构回顾</a></li><li><a href=#六原文实验>六、原文实验</a></li><li><a href=#七torch复现-aevae>七、torch复现 AE、VAE</a></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><div class="details admonition warning open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-exclamation-triangle fa-fw" aria-hidden=true></i>警告<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>本文最后更新于 2023-09-03，文中内容可能已过时。</div></div></div><h2 id=引入>引入</h2><p><img loading=lazy src=images/VAE_0.png srcset="/posts/vae_1/images/VAE_0.png, images/VAE_0.png 1.5x, /posts/vae_1/images/VAE_0.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_0.png data-alt=/posts/vae_1/images/VAE_0.png width=1080 height=446 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><div class="details admonition Notes open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Notes<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>本文也是为写 Stable Diffusion 相关文章做的铺垫，主要参考了李宏毅老师的视频课以及B站的白板推导系列。有关GMM、蒙特卡洛、ELBO、变分推断、重参数化的细节本文不做详细介绍，主要围绕VAE的结构以及loss优化推导做讲解。</div></div></div><p>我们先来简单的引入一下：</p><ul><li>V：变分推断，它的意思来自于概率图模型，本文会给出变分下界的详细推导；</li><li>AE：Auto-Encoder，自编码器；</li><li>VAE：Variational Auto-Encoder，<strong>变分自编码器</strong>，将概率图模型和神经网络相结合的模型；</li></ul><h2 id=一ae>一、AE</h2><p><img loading=lazy src=images/VAE_1.png srcset="/posts/vae_1/images/VAE_1.png, images/VAE_1.png 1.5x, /posts/vae_1/images/VAE_1.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_1.png data-alt=/posts/vae_1/images/VAE_1.png width=1080 height=479 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>先来介绍一下自编码器(Auto-Encoder)，它是一种无监督学习方法，如上图所示，原理可概述为：</p><ul><li>将高维原始数据(如图片)送入 Encoder，利用 Encoder 将高维数据映射到一个低维空间，将n维压缩到m维($m&#171;n$)，我们用隐变量来表示；</li><li>然后将低维空间的特征送入 Decoder 进行解码，以此来重建原始输入数据。</li></ul><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>Encoder、Decoder网络可以为普通的全连接、也可以为CNN、或者类似于Unet都可以，没有固定的要求。</div></div></div><p>这里为和后文的推导联系起来，我们将 Encoder 网络的映射函数定义为 $q_{\phi}$ 、Decoder 网络定义为 $p_{\theta}$，其中 $\phi$、$\theta$ 皆为网络参数。</br>那么对于输入 $x$，我们可以通过Encoder得到 <code>Latent Variable</code>：$z = q_{\phi}(x)$，然后Decoder可以从隐变量z中对原始数据进行重建：$x&rsquo; = p_{\theta}(z) = p_{\theta}(q_{\phi}(x))$。</p><p>我们希望重建的数据和原来的数据近似一致，即最小化输入和输出之间的重构误差，那么AE的训练损失可以采用简单的MSE：</p><p>$$L_{\text{AE}}(\theta, \phi) = \frac{1}{n}\sum_{i=1}^{n} (x^{(i)} - x&rsquo;^{(i)})^2 =\frac{1}{n}\sum_{i=1}^{n} (x^{(i)} - p_{\theta}(q_{\phi}(x^{(i)})))^2$$</p><div class="details admonition Note"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>可以理解为比较输入和重构输入的像素点的误差。</div></div></div><h2 id=二ae-存在的问题>二、AE 存在的问题</h2><p>上面我们通过AE可以<font color=lightseablue><strong>构建一个重构图像的模型</strong></font>，但是这个模型并不能满足要求，或者说它并不是真正意义上的生成模型。对于一个生成模型而言，它满足：</p><ul><li><strong>Encoder 和 Decoder 可以独立拆分(类比 GAN 的 Generator 和 Discriminator)；</strong></li><li><strong>固定维度下<font color=red>任意采样</font>出来的编码，都应该能通过 Decoder 产生一张清晰且逼真的图片。</strong></li></ul><p>当然对于第一点它是满足的，我们主要分析第二点，也就是AE存在的问题，从而引出VAE。</p><p><img loading=lazy src=images/VAE_2.png srcset="/posts/vae_1/images/VAE_2.png, images/VAE_2.png 1.5x, /posts/vae_1/images/VAE_2.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_2.png data-alt=/posts/vae_1/images/VAE_2.png width=520 height=584 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>如上图所示，用一张全月图和一张半月图去训练一个AE，经过训练模型是能够很好的还原出这两张图片。</p><p>接下来，我们在 latent code 中任取一点，将其交给 Decoder 进行解码，直觉上我们会得到一张介于全月和半月之前的图片(比如阴影面积覆盖的样子)。然而<font color=red>实际上的输出图片不仅模糊而且还是乱码的</font>。</p><p>对于这个现象，一个直观的解释就是AE的 Encoder 和 Decoder 都用了DNN，那么NN只会干一件事情：学习、记住、用记住的东西预测，我们<u>从 latent space 中采样的点，编码器都没有学习过</u>，怎么能够指望它生成希望的值呢?</p><p>换句话说，<font color=red><strong>NN只记住了左边全月图片的隐向量和右边半月图片的隐向量，并不能泛化到中间就是$\frac{3}{4}$月亮的图片</strong></font>。</p><p>为了解决这个问题，一个最直接的思想就是<strong>引入噪声</strong>，扩大图片的编码区域，从而能够覆盖到失真的空白编码区，如下图所示：</p><p><img loading=lazy src=images/VAE_3.png srcset="/posts/vae_1/images/VAE_3.png, images/VAE_3.png 1.5x, /posts/vae_1/images/VAE_3.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_3.png data-alt=/posts/vae_1/images/VAE_3.png width=512 height=606 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>其实说白了就是<strong>通过增加输入的多样性从而增强输出的鲁棒性</strong>。</p><p>当我们给输入图片进行编码之前引入一点噪声，使得每张图片的编码点出现在绿色箭头范围内，这样一来所得到的 latent space 就能覆盖到更多的编码点。此时我们再从中间点抽取还原便可以得到一个比较希望的输出。</p><p>虽然我们给输入的图片增加了一些噪声，使得 latent space 能够覆盖到比较多的区域，但是还有不少地方没有覆盖到，比如上图的黄色点位置。</p><p>因此，我们是不是可以尝试利用更多的噪声，使得对于每一个输入样本，它的编码都能够覆盖到整个编码空间？只不过我们这里需要保证的是：<font color=red>对于编码附近的我们应该给定一个高的概率值，对于距离原编码点远的应该给定一个低的概率值</font>。</p><p>这样总体来说，我们就是要将原先的一个单点拉伸到整个编码空间，即将离散的编码点拉伸为一条连续的接近正太分布的编码曲线，如下图所示：</p><p><img loading=lazy src=images/VAE_4.png srcset="/posts/vae_1/images/VAE_4.png, images/VAE_4.png 1.5x, /posts/vae_1/images/VAE_4.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_4.png data-alt=/posts/vae_1/images/VAE_4.png width=570 height=586 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>这个其实就是VAE的思想，熟悉GMM的同学应该知道，它是K个高斯分布(Gaussian Distribution)的混合，其实<font color=green><strong>VAE可以说是无限个高斯分布的混合</strong></font>。</p><h2 id=三vae-结构预览>三、VAE 结构预览</h2><p><img loading=lazy src=images/VAE_5.png srcset="/posts/vae_1/images/VAE_5.png, images/VAE_5.png 1.5x, /posts/vae_1/images/VAE_5.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_5.png data-alt=/posts/vae_1/images/VAE_5.png width=1080 height=547 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>如上图所示VAE的结构，我们可以看到<strong>VAE里的编码器不是输出隐向量$z$，而是一个概率分布</strong>，分布的均值为$m$、方差为$\sigma$，$e$ 即为给编码添加的噪声，来自于正态分布。</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>tips<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>VAE的Encoder的输出不是隐向量，而是均值为$m$, 方差为$\sigma$的正态分布。</div></div></div><p>公式怎么得到的后面会给出推导，我们先来描述一下这个过程：</p><p>$$z_{i} = c_{i} = \exp(\sigma_i) * e_i + m_i$$</p><ul><li>$e$ 为噪声，$m$ 为均值， $\sigma$ 为控制噪声$e$的方差;</li><li>Encoder会计算出两组编码，一组为均值m、一组为控制噪声干扰程度的方差$\sigma$;</li><li>方差$\sigma$主要用来为噪声编码 $e$ 分配权重;</li><li>取指数主要是为了保证分配到的权重是正值;</li><li>也就是说数据分布会在 $\exp(\sigma_i) * e$ 方差范围内采样一个值，得到一个偏移量，就是相当于把原始的样本加上了一个噪声。从结构图中我们可以看到，损失除了AE的 重构损失(reconstruction error)外，还多出了下面这一项:
$$c = (c_1, c_2, c_3) = \sum_{i=1}^{3} (e^{\sigma_i} - (1 + \sigma_i) + (m_i)^2)$$</li></ul><p>这个辅助loss可以认为是一个约束，也就是说生成的 $\sigma$ 要满足这个约束。</p><p><strong>为什么要加这个辅助loss？</strong></p><ul><li>我们最小化了 reconstruction error，如果不加这个辅助loss的话，Encoder肯定希望噪声对自身生成的图片干扰越小越好，为了保证生成图片的质量，于是分配给噪声的权重也就是越低。如果不加这个约束的话，网络只需要将方差设置为接近负无穷大的值 $\exp ^ {-\infty} = 0$，即可消除噪声带来的影响，这样必然会过拟合导致鲁棒性不佳。</li></ul><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>tips<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>添加辅助loss是为了防止过拟合，提高模型的鲁棒性。</div></div></div><p><strong>为什么加这个辅助loss有用？</strong></p><ul><li>我们对 $\sigma$ 求导可得 $c = e^{\sigma} - 1$，令其等于0可求出 $\sigma = 0$ 时取得极小值，这样一来便可以约束方差不会一路走向负无穷，从而起到正则化约束的作用；</li><li>如下图所示，$e^{\sigma}$ 是蓝色曲线，$1 + \sigma$ 是红色线条，那么 $e^{\sigma} - (1 + \sigma)$就是蓝色曲线减去红色直线，得到绿色曲线，显而易见的可以发现它的最小值为0。</li></ul><p><img loading=lazy src=images/VAE_6.png srcset="/posts/vae_1/images/VAE_6.png, images/VAE_6.png 1.5x, /posts/vae_1/images/VAE_6.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_6.png data-alt=/posts/vae_1/images/VAE_6.png width=872 height=860 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h2 id=四数学描述>四、数学描述</h2><h3 id=41作者的-intuition>4.1、作者的 Intuition</h3><p><img loading=lazy src=images/VAE_8.png srcset="/posts/vae_1/images/VAE_8.png, images/VAE_8.png 1.5x, /posts/vae_1/images/VAE_8.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_8.png data-alt=/posts/vae_1/images/VAE_8.png width=1080 height=106 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
<img loading=lazy src=images/VAE_7.png srcset="/posts/vae_1/images/VAE_7.png, images/VAE_7.png 1.5x, /posts/vae_1/images/VAE_7.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_7.png data-alt=/posts/vae_1/images/VAE_7.png width=1080 height=406 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>借用作者原文的表述，我们来引入定义。如上图所示，首先我们会有一个高维的随机变量，与之相关联的我们叫它隐变量 $z$，$z$ 的维度一般要比 $x$ 低很多，用来描述 $x$ 中所包含的信息。</p><p>我们假设 $z$ 满足分布 $p_{\theta}(z)$，$x$ 也是一个条件概率，也就是说：</p><ul><li>在已知 $z$ 的情况下，$p_{\theta}(z)$能生成一个sample $x$ ；</li><li>给定一个sample $x$，$q_{\phi}(x)$ 就可以尝试推测出这个来。</li></ul><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>因为假设$z$满足一定分布，所以也有从$\theta$到$z$的箭头；</p><p>之后提到的$z$都是Decoder里的参数。</p></div></div></div><p>这么说可能有点抽象，我们举个例子：</p><p><img loading=lazy src=images/VAE_9.png srcset="/posts/vae_1/images/VAE_9.png, images/VAE_9.png 1.5x, /posts/vae_1/images/VAE_9.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_9.png data-alt=/posts/vae_1/images/VAE_9.png width=1080 height=975 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>如上图所示，假设有一个图像，里面有3个颜色不一致的形状，这个就是我们的输入$x$。通过右图的参数，可以控制$x$，这就是隐变量$z$。</p><p>那么回到实际的应用场景，我们想要通过$x$获得$z$，又想通过$z$得到相应的$x$，也就是图中的双箭头就是我们想要做的事情。</p><p>那么对于生成模型而言，VAE的数据产生包括两个过程：</p><ul><li>从一个先验分布 $p_{\theta}(z)$ 中采样一个 $z^{(i)}$；</li><li>根据条件分布 $p_{\theta}(x|z)$，用 $z^{(i)}$ 生成 $x^{(i)}$。</li></ul><p>我们希望找到一个参数 $\theta^*$ 来<strong>最大化生成真实数据的概率</strong>：</p><p>$$\theta^*=\argmax_{\theta} \prod_{i=1}^{n}p_{\theta}(x^{(i)})$$</p><p>这里 $p_{\theta}(x^{(i)})$ 可以通过对 $z$ 积分得到：</p><p>$$p_{\theta}(x^{(i)}) = \int_{z} p_{\theta}(x, z) \mathrm{d}{z} = \int_{z} p_{\theta}(z) p_{\theta}(x^{(i)}|z)\mathrm{d}{z}$$</p><p>实际上我们要根据上述积分是不可能实现的，先验分布 $p_{\theta}(z)$ 是未知的，而且如果分布比较复杂且高维，对其穷举计算也是不现实的。</p><p><strong>变分推断引入后验概率来联合建模</strong>，即given $x$ 想要得到它的 $z$，根据贝叶斯公式表示为：</p><p>$$p_{\theta}(z | x) = \frac{p_{\theta}(x|z) p_{\theta}(z)}{p_{\theta}(x)}$$</p><p>我们又回到最上面的图：</p><p><img loading=lazy src=images/VAE_10.png srcset="/posts/vae_1/images/VAE_10.png, images/VAE_10.png 1.5x, /posts/vae_1/images/VAE_10.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_10.png data-alt=/posts/vae_1/images/VAE_10.png width=394 height=406 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><ul><li>实线箭头就是我们要得到的生成模型 $p_{\theta}(z) p_{\theta}(x|z)$，这里 $p_{\theta} (z)$ 往往是事先定义好的，比如标准正态分布，而 $p_{\theta}(x|z)$ 可以用一个网络来学习，它就可以看成是 <strong>Probabilistic Decoder</strong>。</li><li>虚线箭头代表对后验分布 $p_{\theta}(z|x)$ 的变分估计，它也可以用一个网络去近似，我们记为 $q_{\phi}(z|x)$，则这个网络称为 <strong>Probabilistic Encoder</strong>。</li></ul><p>所以VAE的优化目标就有了，为了<strong>达到从x估计z的过程</strong>，对于估计的后验 $q_{\phi}(z|x)$，我们希望它<strong>接近</strong>真实的后验分布 $p_{\theta}(z|x)$ ，即：</p><p>$$p_{\theta}(z|x) \cong q_{\phi}(z|x)$$</p><p>说白了就是使用另一个模型，参数由 $\phi$ 表示，在参数 $\phi$ 的帮助下，有了一个分布 $q$ ，现在希望分布 $q$ 能够尽量接近 $p$，从而达到从 $x$ 估计 $z$ 的过程。</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>可以看到VAE和AE架构上还是相似的，VAE的最终目标是得到生成模型即Decoder，Encoder只是辅助建模。</p><p>而AE常常是为了得到Encoder来进行特征提取或压缩。</p></div></div></div><h3 id=42变分下界>4.2、变分下界</h3><p>为了衡量两个 distribution 的相似程度，我们应该很自然的想到了KL divergence，因为我们实际上计算的是分布 $q$ ，所以我们从 $q$ 的视角来计算它到 $p$ 的KL散度：</p><p>$$q_{\phi}(z|x) \cong p_{\theta}(z|x) \rightarrow D_{\text{KL}}(q_{\phi}(z|x) || p_{\theta}(z|x))$$</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><p>需要再次强调的是：</p><p>$\theta$ 为 decoder 的参数；</p><p>$\phi$ 为 encoder 的参数。</p></div></div></div><p>根据定义我们将KL divergence展开，对 $z$ 求和，表示如下：</p><p>$$
\begin{align}
D_{\text{KL}}(q_{\phi}(z|x) || p_{\theta}(z|x)) &= \sum_{z} q_{\phi}(z | x) \log (\frac{q_{\phi}(z | x)}{p_{\theta}(z | x)}) \cr
&= - \sum_{z} q_{\phi}(z | x) \log (\frac{p_{\theta}(z | x)}{q_{\phi}(z | x)})\cr
&= - \sum_{z} q_{\phi}(z | x) \log (\frac{\frac{p_{\theta}(z,x)}{p_{\theta}(x)}}{q_{\phi}(z | x)})\cr
&= - \sum_{z} q_{\phi}(z | x) [\log({\frac{p_{\theta}(x, z)}{p_{\theta}(x)}}) - \log({q_{\phi}(z | x)})]\cr
&= - \sum_{z} q_{\phi}(z | x) [\log({\frac{p_{\theta}(x, z)}{q_{\phi}(z|x)}}) - \log({p_{\theta}(x)})]\cr
\end{align}
$$</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>$p_{\theta}(z | x)$ 是根据条件概率公式拆开的。</div></div></div><p>这个时候我们注意到 $\log(p_{\theta}(x))$ 是和 $z$ 没有关系的，并且log项是常数，所以在乘求和的时候直接提到 $\sum$ 外面去就可以了，并且 $q_{\phi} (z | x)$ 对 $z$ 求和的结果是1，那所以 $-\sum_{z}(q_{\phi}(z|x))(-\log(p_{\theta}(x)))$ 的结果就是 $\log(p_{\theta}(x))$，它是个const。</p><p>我们将它移到等式的左边，表示如下：</p><p>$$
\begin{align}
\log(p_{\theta}(x)) &= D_{KL}(q_{\phi}(z|x) || p_{\theta}(z|x)) + \sum_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x, z)}{q_{\phi}(z|x)}) \cr
&= D_{KL}(q_{\phi}(z|x) || p_{\theta}(z|x)) + L(\theta, \phi; x)
\end{align}
$$</p><p>我们将 $\sum_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x, z)}{q_{\phi}(z|x)})$ 写成 $L(\theta, \phi; x)$ ，等式左边是一个const，也就是说不管 $x$ 的分布是什么样，它对 $\theta$ 来说没什么影响。等式右边，KL divergence是一个非负的，所以我们只要把 $L(\theta, \phi; x)$ 的值尽可能的拉大，那么KL divergence的值就会随之缩小。</p><p><strong>想要最大化的$L(\theta, \phi; x)$，就被称为变分下界(Variational lower bound)。</strong></p><h3 id=43loss-function>4.3、Loss Function</h3><p>现在我们只要想办法将这个 lower bound 提升就可以了，那么这个 lower bound 就可以作为我们的 loss function：</p><p>$$
\begin{aligned}
L(\theta, \phi; x) &= \sum_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x, z)}{q_{\phi}(z|x)}) \cr
&= \sum_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x|z) p_{\theta}(z)}{q_{\phi}(z|x)}) \cr
&= \sum_{z}q_{\phi}(z|x)[\log(p_{\theta}(x|z)) + \log(\frac{p_{\theta}(z)}{q_{\phi}(z | x)})] \cr
&= {E}<em>{q</em>{\phi}(z|x)}[\log(p_{\theta}(x|z))] - D_{KL}(q_{\theta}(z | x) || p_{\theta}(z))
\end{aligned}
$$</p><p>上述等式，我们将 lower bound 再展开，将 $p_{\theta}(x, z)$ 展成条件概率，然后再将log拆分。</p><p>第三行中括号内，左边的可以写成期望的形式，右边的因为都有 $q_{\phi}$ 和 $p_{\theta}$ 所以符合KL divergence的公式。</p><ul><li>我们将 ${E}<em>{q</em>{\phi}(z|x)}[\log(p_{\theta}(x|z))]$ 称为<strong>Reconstruction Loss</strong>，</li><li>将 $-D_{KL}(q_{\theta}(z | x) || p_{\theta}(z))$ 称为 <strong>Regularization Loss</strong>。</li></ul><p>所以我们只需要估计出这两项的梯度来，就可以对 lower bound 进行优化了。</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>我们的目的是想让 Probabilistic Encoder 接近于 $p_{\theta}(z)$，因为两个损失，这样KL divergence就越大越好，实际-KL才是训练用的loss。</div></div></div><h3 id=44蒙特卡洛法求梯度>4.4、蒙特卡洛法求梯度</h3><p>接下来讲如何求出这两项的导数，来优化提升 lower bound。我们看到想要优化的这个loss，其实是可以写成期望的形式的，假定期望里的这一项是 $f(z)$，对于估计这种期望它的导数，最直接的我们就想到了蒙特卡洛的方法。</p><p><img loading=lazy src=images/VAE_11.png srcset="/posts/vae_1/images/VAE_11.png, images/VAE_11.png 1.5x, /posts/vae_1/images/VAE_11.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_11.png data-alt=/posts/vae_1/images/VAE_11.png width=976 height=414 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>虽然这里有个 $\phi$，但我们假定这个 $f(z)$ 和 $\phi$ 是没有关系的。(假设！！！！)</div></div></div><p>使用蒙特卡洛方法，对 $f(z)$ 在 $q_{\phi}$ 上的期望，对 $\phi$ 求导数，表示如下：</p><p>$$
\begin{aligned}
\eta &= \triangle_{\phi} E_{q_{\phi}(z)}[f(z)]\cr
&= \triangle_{\phi} \int {q_{\phi}(z)}f(z) \mathrm{d}z\cr
&= \int \triangle_{\phi}{q_{\phi}(z)}f(z) \mathrm{d}z\cr
&= \int {q_{\phi}(z)}f(z)\triangle_{\phi} \log {q_{\phi}(z)}\mathrm{d}z\cr
&= E_{q_{\phi}(z)}[f(z)\triangle_{\phi} \log {q_{\phi}(z)}]
\end{aligned}
$$</p><ul><li>$line 1 \sim 2:$ 根据期望的定义展开，因为我们假设 $f(z)$ 和 $\phi$ 没有关系，所以可以将导数符号拿进来；</li><li>$line 3: $ 根据变换 $\triangle_{\phi} \log q_{\phi}(z) = \frac{\triangle_{\phi}q_{\phi}(z)}{q_{\phi}(z)}$ 带入可得;</li></ul><p>套用蒙特卡洛公式，最终表示如下：</p><p>$$
\begin{aligned}
\triangle_{\phi}E_{q_{\phi}(z)}[f(z)] &= E_{q_{\phi}(z)}[f(z) \triangle_{q_{\phi}(z)} \log{q_{\phi}(z)}] \cr
&\cong \frac{1}{L}\sum_{l=1}^{L} f(z) \triangle_{q_{\phi}(z^{(l)})} \log{q_{\phi}(z^{(l)})}, where z^{(l)} \sim q_{\phi}(z|x^{(i)})
\end{aligned}
$$</p><p>但是作者实验发现使用这个 estimator 是有很高的 variance 的，就是直观上来说会导致训练很不稳定。</p><p>在此基础上作者提出了 Generic Stochastic Gradient Variational Bayes (<strong>SGVB</strong>) estimator，并使用**重参数化(Reparameterization)**trick，我们先来说下重参数化。</p><h3 id=45重参数化-trick>4.5、重参数化 Trick</h3><p>上面我们用蒙特卡洛的时候，有一个非常强的假设，那就是假设 $f(z)$ 和 $\phi$ 是没有关系的，但实际表达式中：
<img loading=lazy src=images/VAE_12.png srcset="/posts/vae_1/images/VAE_12.png, images/VAE_12.png 1.5x, /posts/vae_1/images/VAE_12.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_12.png data-alt=/posts/vae_1/images/VAE_12.png width=948 height=398 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>我们可以看到它还是有关系的，所以我们得考虑它们之间存在的关系、这个关系会带来什么样的问题。</p><p>我们把它打开来看：</p><p>$$
\begin{aligned}
\triangle_{\phi}E_{q_{\phi}}[f(z)] &= \triangle_{\phi}\int q_{\phi}(z)f(z) \mathrm{d}z \cr
&= \int \triangle_{\phi}[q_{\phi}(z)f(z)] \mathrm{d}z\cr
&= \int f(z) \triangle_{\phi}q_{\phi}(z) \mathrm{d}z + \int q_{\phi}(z)\triangle_{\phi}f(z) \mathrm{d}z\cr
&= \underbrace{\int f(z) \triangle_{\phi}q_{\phi}(z) \mathrm{d}z}<em>{what \ about \ this \ ?} + E</em>{q_{\phi}(z)}[\triangle_{\phi}f(z)]
\end{aligned}
$$</p><p>分别求导之后，后面一项可以写成期望的形式，但是前面这一项就无法处理了，为了解决这个问题，作者使用了<strong>重参数化技巧(Reparameterization Trick)</strong>。</p><p>核心思想就是引入一个辅助的随机变量 $\epsilon$，$\epsilon \in p(\epsilon)$，这个随机变量和其它变量没有关系，它是一个独立的随机变量，用来表示产生 $z$ 的过程中所有的随机性。也就是说抽样产生 $z$ 的过程中，所有的随机性都是由这个 $\epsilon \in p(\epsilon)$ 产生的。</p><p>这样我们就可以把 $z$ 写成这种形式： $z = g_{\phi}(\epsilon, x)$，从而可以把 $q_{\phi}(z)$ 这个概率分布转移到 $p_{\epsilon}$ 上，而 $\epsilon$ 有一个非常好的特性，那就是和 $\phi$ 是没有关系的。</p><p>这种 trick 就是重参数化，得到新的变形后重新对 $\phi$ 求导：</p><p>$$
\begin{aligned}
E_{q_{\phi(z)}}[f(z^{(i)})] &= E_{p(\epsilon)}[f(g_{\phi}(\epsilon, x^i))] \cr
\triangle_{\phi}E_{q_{\phi(z)}}[f(z^{(i)})] &= \triangle_{\phi}E_{p(\epsilon)}[f(g_{\phi}(\epsilon, x^i)]\cr
&=E_{p(\epsilon)}[\triangle_{\phi}f(g_{\phi}(\epsilon, x^i)]\cr
&\approx \frac{1}{L} \sum_{l=1}^{L} \triangle_{\phi}f(g_{\phi}(\epsilon^{(l)}, x^{(i)}))
\end{aligned}
$$</p><p>估计这个期望也是采样然后求平均得到最后的式子，这样就可以把loss的梯度给估计出来了。</p><p>以上是从数学角度来分析的重参数化技巧，这里作者给出了一个更加直观的表达：</p><p><img loading=lazy src=images/VAE_13.png srcset="/posts/vae_1/images/VAE_13.png, images/VAE_13.png 1.5x, /posts/vae_1/images/VAE_13.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_13.png data-alt=/posts/vae_1/images/VAE_13.png width=1080 height=662 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><ul><li>左图为原来的形式，我们使用 $\phi$ 和 $x$ 产生一个distribution，然后在这个distribution中抽样产生一个z，然后再得到最终的 $f$ 。但是在传递梯度的时候，怎么把梯度通过抽样这个过程传递回来呢？这个是没法传递梯度的。
在使用了重参数化trick后，随机性移动到了 $\epsilon$ 上，之前所有抽样的过程包括的随机性，都让 $\epsilon$ 包括了，这样就可以顺利地将梯度通过 $z$ 传递到 $\phi$，这是一个非常巧妙的方法.</li></ul><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>可以理解成用多余参数逼近抽样的过程。</div></div></div><h3 id=46-generic-sgvb>4.6 Generic SGVB</h3><p>简单说完重参数化，我们回到SGVB:
<img loading=lazy src=images/VAE_14.png srcset="/posts/vae_1/images/VAE_14.png, images/VAE_14.png 1.5x, /posts/vae_1/images/VAE_14.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_14.png data-alt=/posts/vae_1/images/VAE_14.png width=1060 height=334 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>这里是想求这一串期望，它就是我们的 $f(z)$ ，根据之前的 Reparameterization Trick，我们把 $z$ 写成这样的形式：</p><p>$$z^{(i, l)} = g_{\phi} (\epsilon^{(i,l)}, x^{(i)}) \ and \ \epsilon^{(l)} \sim p(\epsilon)$$</p><p>让 $\epsilon$ 从这个 distribution 中抽样产生，$\epsilon$ 是一个与 $\phi$ 、$\theta$ 都没有关系的随机变量，然后 loss 就变成：</p><p>$$L(\theta, \phi, x^{(i)}) = \frac{1}{L} \sum_{l=1}^{L} \log p_{\theta}(x^{(i)}, z^{(i,l)}) - \log q_{\phi}(z^{(i,l)} | x^{(i)})$$</p><p>想要求它对 $\phi$ 的导数，只需要两边同时求导即可。</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>因为期望这个积分已经被替换成了 $\epsilon$ 的distribution，它跟 $\phi$ 是没有关系的，所以我们在估计整个loss的导数的时候，我们直接对 $\phi$ 求导就可以了。</div></div></div><p>这个就是作者提出的第一种估计梯度的方法。</p><h3 id=47another-sgvb>4.7、Another SGVB</h3><p>在此基础上作者还发现，有一些好的性质可以直接拿来利用，比如期望的一些性质。</p><p>在 4.3 节中我们讲到原来的 loss 可以写成 KL散度 + 期望 的形式：</p><p>$$L(\theta, \phi, x^{(i)}) = -D_{KL}(q_{\phi}(z|x^{(i)})||p_{\theta}(z)) + E_{q_{\phi}(z|x)}[\log(p_{\theta}(x^{(i)}|z))]$$</p><p>这里我们假设这两个distribution：$q_{\phi}$ 、$p_{\theta}$ 都是 Gaussian distribution，说白了就是0均值1方差，根据定义：</p><p>$$
\begin{cases}
D_{KL}(P||Q) = E_{x \sim P}[\log(\frac{P(x)}{Q(x)})]\cr
E_{x\sim P(x)} = \int P(x)Q(x)\mathrm{d}x
\end{cases}
$$</p><p>我们根据上述定义打开这个KL divergence：</p><p>$$-D_{KL}(q_{\phi}(z|x)||p_{\theta}(z)) = \int q_{\phi}(z|x)(\log p_{\theta}(z)) - \log q_{\phi}(z|x) \mathrm{d}z$$</p><p>我们先来看 $\int q_{\phi}(z|x)\log p_{\theta}(z) \mathrm{d}z$:</p><p>$$
\begin{align}
\int q_{\phi}(z|x)\log p_{\theta}(z) \mathrm{d}z &= \int N(z; \mu, \sigma^2) \log N(z; 0, 1)\mathrm{d}z\cr
&= \int N(z; \mu, \sigma^2) (-\frac{1}{2}z^2 - \frac{1}{2}\log(2\pi))\mathrm{d}z\cr
&= -\frac{1}{2} \int N(z; \mu, \sigma^2) z^2\mathrm{d}z - \frac{J}{2}\log(2\pi) \cr
&= -\frac{J}{2} \log (2\pi) - \frac{1}{2}(E_{z \sim N(z;\mu, \sigma^2)}[z]^2 + Var(z))\cr
&= -\frac{J}{2} \log (2\pi) - \frac{1}{2}\sum_{J}^{j=1}(\mu^2 + \sigma_j^2)
\end{align}
$$</p><ul><li>$line 1\sim 2:$ 我们让左面分布 $N(z; \mu, \sigma^2)$ 保持不动，将 normal distribution 的PDF带进去；<ul><li>normal distribution 的PDF为:
<img loading=lazy src=images/VAE_15.png srcset="/posts/vae_1/images/VAE_15.png, images/VAE_15.png 1.5x, /posts/vae_1/images/VAE_15.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_15.png data-alt=/posts/vae_1/images/VAE_15.png width=432 height=108 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></li><li>将常数项直接拿出来，指数的部分也通过log直接拿下来了；</li></ul></li><li>$line 3:$ 因为 $\frac{1}{2}\log(2\pi)$ 是个常数、$N(z; \mu, \sigma^2)$ 这个分布积分之后是1，所以可以直接把常数项拿到积分外面；但是因为 $z$ 是一个向量，我们假设 $z$ 有 $J$ 个元素element，那么每个元素都会积出一个值来，所以要乘上 $J$，即 $\frac{J}{2}\log(2\pi)$;</li><li>$line 4:$ 对于积分 $\int N(z;\mu,\sigma^2) z^2\mathrm{d}z$ 我们可以换个角度理解它：这里我们把它就当成一个概率分布，所以整个这个积分其实也是一个期望的形式，不过它是对 $z^2$ 的期望，经过变形可以写成 $-\frac{1}{2} E_{z \sim N(z; \mu, \sigma^2)}[z]^2$
。在这个基础上我们使用期望的性质 $E[z^2] = E[z]^2 + variance(z)$，即 $z^2$ 的期望等于期望的平方加上 $z$ 的方差；</li><li>那么对于一个 normal distribution 来说它的期望和方差是显而易见的：$\mu$ 和 $\sigma$，对于 $z$ 里的每个元素(脚标是 $j$)都加起来就好了，这样最开始的积分就可以简化成最后的形式。</li></ul><p>我们再来看 $\int q_{\phi}(z|x)\log q_{\phi}(z|x)\mathrm{d}z$:</p><p>$$
\begin{aligned}
\int q_{\phi}(z|x)\log q_{\phi}(z|x)\mathrm{d}z &=\int N(z; \mu, \sigma^2)\log N(z;\mu,\sigma^2)\mathrm{d}z\cr
&= \int N(z; \mu, \sigma^2)(-\frac{1}{2}(\frac{z - \mu}{\sigma})^2- \frac{1}{2}\log (2 \pi) - \frac{1}{2}\log(\sigma^2))\mathrm{d}z\cr
&=-\frac{1}{2}\int N(z;\mu,\sigma^{2})(\frac{z-\mu}{\sigma})^{2}\mathrm{d}z-\frac{J}{2}log(2\pi)-\frac{1}{2}\sum_{j=1}^{J}log(\sigma_{j}^{2}) \cr
&=-\frac J2log(2\pi)-\frac12\sum_{j=1}^{J}log(\sigma_{j}^{2})-\frac12E_{z\sim N(z;\mu,\sigma^{2})}[(\frac{z-\mu}\sigma)^{2}] \cr
&=-\frac{J}{2}log(2\pi)-\frac{1}{2}\sum_{j=1}^{J}log(\sigma_{j}^{2})-\frac{1}{2}(E_{z\sim N(z;\mu,\sigma^{2})}[\frac{z-\mu}{\sigma}]^{2}+Var(\frac{z-\mu}{\sigma})) \cr
&=-\frac{J}{2}log(2\pi)-\frac{1}{2}\sum_{j=1}^{J}(1+log(\sigma_j^2))
\end{aligned}
$$</p><p>同样的还是把它的PDF带进来，展成上面相似的形式，但是这个地方的常数项和变量要显得复杂一点，相似的是我们一样可以把常数部分拿到积分外面去，然后对于前面这项积分也把它理解成期望的形式，同样利用期望的性质将平方化简，就可以得到最后的结果。</p><p>随后我们把 KL散度 这两项给合并起来：
$$
\begin{aligned}
-D_{KL}(q_{\phi}(z\mid x)\mid\mid p_{\theta}(z))& =\int q_\phi(z\mid x)(logp_\theta(z))-logq_\phi(z\mid x))\mathrm{d}z \cr
&=\frac12\sum_{j=1}^J(1+log((\sigma_j)^2)-(\mu_j)^2-(\sigma_j)^2)
\end{aligned}
$$</p><p>把刚刚上面的结果带进来做减法即可得到这个等式，也就是说可以通过这个式子来估计出KL散度。</p><p>对于另一部分的loss $E_{q_{\phi}(z|x)}[\log(p_{\theta}(x^{(i)} | z))]$，就像我们上面说的，这部分的概率我们希望given $z$ 产生的 $x$ 尽量的接近输入 $x$，为了实现这个逼近，我们使用MSE来让$f(z)$逼近这个x，就可以最大化这个loss：</p><p><img loading=lazy src=images/VAE_16.png srcset="/posts/vae_1/images/VAE_16.png, images/VAE_16.png 1.5x, /posts/vae_1/images/VAE_16.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_16.png data-alt=/posts/vae_1/images/VAE_16.png width=864 height=260 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>以上就是最终使用的SGVB，作者通过 KL散度 的性质和 Regularization Loss 的近似，给我们提供了一种相对稳定的估计loss和梯度的方法。</p><h2 id=五vae-结构回顾>五、VAE 结构回顾</h2><p><img loading=lazy src=images/VAE_17.png srcset="/posts/vae_1/images/VAE_17.png, images/VAE_17.png 1.5x, /posts/vae_1/images/VAE_17.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_17.png data-alt=/posts/vae_1/images/VAE_17.png width=1080 height=461 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>总的来看 Variational Auto-Encoder 的model就是：</p><ul><li>输入一个 $x$，进了Encoder，这个 Encoder 是由参数来决定的，Encoder 会产生 $μ$和 $σ$；</li><li>$μ$和 $σ$首先被我们用来计算 KL divergence，作为<strong>辅助损失</strong>；</li><li>同时在 $μ$ 和 $σ$之后我们对它抽样产生一个 $z$，加上 $\epsilon$ 帮我们产生随机的项；</li><li>得到隐变量 $z$后，放到 Decoder 里，它是由参数 $\theta$ 来决定的；</li><li>经过这个 Decoder 之后，我们重建出了一个 $x$；</li><li>对比重建后的 $x$ 和输入 $x$ 之间的 MSE 就构成了loss的另一部分，</li><li>两个loss加起来就是最终的loss。</li></ul><p>这个就是最经典的 Variational Auto-Encoder。</p><p>对比第一大节AE的图，可以画成一下形式：</p><p><img loading=lazy src=images/VAE_18.png srcset="/posts/vae_1/images/VAE_18.png, images/VAE_18.png 1.5x, /posts/vae_1/images/VAE_18.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_18.png data-alt=/posts/vae_1/images/VAE_18.png width=1080 height=508 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h2 id=六原文实验>六、原文实验</h2><p>作者基于MNIST 和 Frey Face做了实验验证，看下原文的结果图：</p><p><img loading=lazy src=images/VAE_19.png srcset="/posts/vae_1/images/VAE_19.png, images/VAE_19.png 1.5x, /posts/vae_1/images/VAE_19.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_19.png data-alt=/posts/vae_1/images/VAE_19.png width=1080 height=548 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>首先作者说了使用不同的学习方法能把这个 lower bound 提升多少，lower bound 的提升越大，说明 Encoder 和我们想要逼近的这个Distruction，它的KL散度是越来越小。</p><div class="details admonition Note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>由图中可以看出AEVB与wake-sleep算法的比较，可以看出AEVB训练效果更好。且随着隐变量维度增大，并未出现过拟合现象。</div></div></div><p><img loading=lazy src=images/VAE_20.png srcset="/posts/vae_1/images/VAE_20.png, images/VAE_20.png 1.5x, /posts/vae_1/images/VAE_20.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_20.png data-alt=/posts/vae_1/images/VAE_20.png width=1080 height=847 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
<img loading=lazy src=images/VAE_21.png srcset="/posts/vae_1/images/VAE_21.png, images/VAE_21.png 1.5x, /posts/vae_1/images/VAE_21.png 2x" sizes=auto data-title=/posts/vae_1/images/VAE_21.png data-alt=/posts/vae_1/images/VAE_21.png width=1080 height=384 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>图4是限定2个维度的隐变量 $z$，并调节两个维度的值，生成的图片。</p><p>图5是不同维度的隐变量随机采样的图片。</p><h2 id=七torch复现-aevae>七、torch复现 AE、VAE</h2><p><a href=https://wangguisen.blog.csdn.net/article/details/128476638 target=_blank rel="external nofollow noopener noreferrer">https://wangguisen.blog.csdn.net/article/details/128476638<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><h2 id=references>References</h2><p>[1]. <a href=https://arxiv.org/abs/1312.6114 target=_blank rel="external nofollow noopener noreferrer">https://arxiv.org/abs/1312.6114<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[2]. <a href=http://www.gwylab.com/note-vae.html target=_blank rel="external nofollow noopener noreferrer">http://www.gwylab.com/note-vae.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[3]. <a href=https://zhuanlan.zhihu.com/p/452743042 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/452743042<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[4]. <a href=https://www.bilibili.com/video/BV1q64y1y7J2/ target=_blank rel="external nofollow noopener noreferrer">https://www.bilibili.com/video/BV1q64y1y7J2/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[5]. <a href="https://www.bilibili.com/video/av15889450/?p=33" target=_blank rel="external nofollow noopener noreferrer">https://www.bilibili.com/video/av15889450/?p=33<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[6]. <a href=https://gregorygundersen.com/blog/2018/04/29/reparameterization/ target=_blank rel="external nofollow noopener noreferrer">https://gregorygundersen.com/blog/2018/04/29/reparameterization/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br></br>[7]. <a href="https://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==&amp;mid=2247486014&amp;idx=1&amp;sn=2ff34f72c869907408ed1b08bec1a238&amp;chksm=c337b7a7f4403eb14a1b5cdc3e1a1b11dca6f957591957cc29a4c270f0ace0a4674a7ae33214&amp;scene=21#wechat_redirect" target=_blank rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==&mid=2247486014&idx=1&sn=2ff34f72c869907408ed1b08bec1a238&chksm=c337b7a7f4403eb14a1b5cdc3e1a1b11dca6f957591957cc29a4c270f0ace0a4674a7ae33214&scene=21#wechat_redirect<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-09-03 19:04:36">更新于 2023-09-03&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/53b5e933a28addb4a38a246a89f72dae256440ea rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 53b5e933a28addb4a38a246a89f72dae256440ea: feat: modify key words in article FastBEV"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>53b5e93</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/vae_1/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/ML/VAE/VAE_1/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/vae_1/ data-title="变分自编码器 VAE 详解" data-hashtags=VAE><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/vae_1/ data-hashtag=VAE><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/vae_1/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/vae_1/ data-title="变分自编码器 VAE 详解"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/vae_1/ data-title="变分自编码器 VAE 详解"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/vae/ class=post-tag>VAE</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/clause_8/ class=post-nav-item rel=prev title="Effective STL [8] | 永不建立auto_ptr的容器"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Effective STL [8] | 永不建立auto_ptr的容器</a>
<a href=/posts/clause_9/ class=post-nav-item rel=next title="Effective STL [9] | 在删除选项中仔细选择">Effective STL [9] | 在删除选项中仔细选择<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.125.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>