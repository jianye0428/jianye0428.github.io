<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Diffusion 扩散模型（DDPM） - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="一、引入 近年AIGC的爆火离不开人工智能在图像生成、文本生成以及多模态等领域的不断累积，其中生成模型的发展占据了很大功劳，如：生成对抗网络 GAN 及其一系列变体、变分自编码器 VAE 及其一系列变体、自回归模型 AR、流模型 flow ，以及近年大火的扩散模型 Diffusion Model 等。 扩散模型的大火并非横空出世，早在2015年就有人提出了类似"><meta name=keywords content="draft"><meta itemprop=name content="Diffusion 扩散模型（DDPM）"><meta itemprop=description content="一、引入 近年AIGC的爆火离不开人工智能在图像生成、文本生成以及多模态等领域的不断累积，其中生成模型的发展占据了很大功劳，如：生成对抗网络 GAN 及其一系列变体、变分自编码器 VAE 及其一系列变体、自回归模型 AR、流模型 flow ，以及近年大火的扩散模型 Diffusion Model 等。 扩散模型的大火并非横空出世，早在2015年就有人提出了类似"><meta itemprop=datePublished content="2023-07-31T15:57:07+08:00"><meta itemprop=dateModified content="2023-08-08T09:24:08+08:00"><meta itemprop=wordCount content="5595"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="draft,"><meta property="og:title" content="Diffusion 扩散模型（DDPM）"><meta property="og:description" content="一、引入 近年AIGC的爆火离不开人工智能在图像生成、文本生成以及多模态等领域的不断累积，其中生成模型的发展占据了很大功劳，如：生成对抗网络 GAN 及其一系列变体、变分自编码器 VAE 及其一系列变体、自回归模型 AR、流模型 flow ，以及近年大火的扩散模型 Diffusion Model 等。 扩散模型的大火并非横空出世，早在2015年就有人提出了类似"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/ddpm/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-31T15:57:07+08:00"><meta property="article:modified_time" content="2023-08-08T09:24:08+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="Diffusion 扩散模型（DDPM）"><meta name=twitter:description content="一、引入 近年AIGC的爆火离不开人工智能在图像生成、文本生成以及多模态等领域的不断累积，其中生成模型的发展占据了很大功劳，如：生成对抗网络 GAN 及其一系列变体、变分自编码器 VAE 及其一系列变体、自回归模型 AR、流模型 flow ，以及近年大火的扩散模型 Diffusion Model 等。 扩散模型的大火并非横空出世，早在2015年就有人提出了类似"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/ddpm/><link rel=prev href=https://jianye0428.github.io/posts/effective_cpp_part_four/><link rel=next href=https://jianye0428.github.io/posts/clause_12/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Diffusion 扩散模型（DDPM）","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/ddpm\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"draft","wordcount":5595,"url":"https:\/\/jianye0428.github.io\/posts\/ddpm\/","datePublished":"2023-07-31T15:57:07+08:00","dateModified":"2023-08-08T09:24:08+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Diffusion 扩散模型（DDPM）</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/ml/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> ML</a></span></div><div class=post-meta-line><span title="发布于 2023-07-31 15:57:07"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-31>2023-07-31</time></span>&nbsp;<span title="更新于 2023-08-08 09:24:08"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-08-08>2023-08-08</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 5595 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 12 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="Diffusion 扩散模型（DDPM）">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#一引入>一、引入</a></li><li><a href=#二扩散原理阐述>二、扩散原理阐述</a><ul><li><a href=#21直观理解>2.1、直观理解</a></li><li><a href=#22前向过程扩散>2.2、前向过程（扩散）</a></li><li><a href=#23反向过程去噪>2.3、反向过程（去噪）</a></li><li><a href=#24模型训练>2.4、模型训练</a></li></ul></li><li><a href=#三算法流程概述>三、算法流程概述</a></li><li><a href=#四数学描述>四、数学描述</a><ul><li><a href=#42反向过程去噪>4.2、反向过程（去噪）</a></li><li><a href=#43训练损>4.3、训练损</a></li></ul></li><li><a href=#五torch复现>五、torch复现</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><div class="details admonition warning open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-exclamation-triangle fa-fw" aria-hidden=true></i>警告<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>本文最后更新于 2023-08-08，文中内容可能已过时。</div></div></div><h2 id=一引入>一、引入</h2><p><img loading=lazy src=images/DDPM_0.png srcset="/posts/ddpm/images/DDPM_0.png, images/DDPM_0.png 1.5x, /posts/ddpm/images/DDPM_0.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_0.png data-alt=/posts/ddpm/images/DDPM_0.png width=1308 height=393 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>近年AIGC的爆火离不开人工智能在图像生成、文本生成以及多模态等领域的不断累积，其中<strong>生成模型</strong>的发展占据了很大功劳，如：<mark>生成对抗网络 GAN</mark> 及其一系列变体、<mark>变分自编码器 VAE</mark> 及其一系列变体、<mark>自回归模型 AR</mark>、<mark>流模型 flow</mark> ，以及近年大火的<strong>扩散模型 Diffusion Model</strong> 等。</p><p>扩散模型的大火并非横空出世，早在2015年就有人提出了类似的想法，直到2020年才提出了经典的 <strong>Denoising Diffusion Probabilistic Models（DDPM）</strong>，像OpenAI、NovelAI、NVIDIA和Google成功的训练了大规模模型之后，它们吸引了很多人注意，后续有了很多基于扩散模型的变体，比如有：GLIDE、DALLE-2、Imagen和年底爆火的完全开源的稳定扩散模型（Stable Diffusion）。</p><p>扩散模型与之前所有的生成方法有着本质的区别：</p><p><img loading=lazy src=images/DDPM_1.png srcset="/posts/ddpm/images/DDPM_1.png, images/DDPM_1.png 1.5x, /posts/ddpm/images/DDPM_1.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_1.png data-alt=/posts/ddpm/images/DDPM_1.png width=1080 height=728 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>直观的说它是<mark>将图像生成过程（采样）分解为许多小的去噪步骤</mark>，其实 Diffusion 的含义本质上就是一个迭代过程，实线箭头用于扩散步骤中添加随机噪声，虚线箭头代表的是通过学习逆向扩散过程<mark>从噪声中重构所需的数据样本</mark>。<strong>引入噪声导致了信息的衰减，再通过噪声尝试还原原始数据，多次迭代最小化损失后，能够使模型在给定噪声输入的情况下学习生成新图像。</strong></p><p>所以Diffusion模型和其它生成模型的区别是，它不是直接的<strong>图像->潜变量、潜变量->图像</strong>的一步到位，它是一步一步的<mark><font color=red><strong>逐渐分解、逐渐去噪</strong></font></mark>的过程。</p><p>当然有关Diffusion的理解和变体有很多，但是扩散模型从本质上讲就是DDPM，所以本文主要对DDPM的原理进行讲解，并给出DDPM的扩散过程、去噪过程、训练损失的详细推导，对于掌握Diffusion算法原理只需要抓住以下四点即可：</p><ul><li>前向过程（扩散）；</li><li>反向过程（去噪、采样）；</li><li>如何训练；</li><li>如何推断。</li></ul><h2 id=二扩散原理阐述>二、扩散原理阐述</h2><p>扩散模型包括 <strong>前向扩散过程</strong> 和 <strong>反向去噪过程(采样)</strong>，前向阶段对图像逐步施加噪声，直至图像被破坏变成完全的高斯噪声，然后在反向阶段学习从高斯噪声还原为原始图像的过程。</p><h3 id=21直观理解>2.1、直观理解</h3><ul><li>扩散模型的目的是什么？<ul><li>学习从纯噪声生成图片的方法。</li></ul></li><li>扩散模型是怎么做的？<ul><li>训练一个UNet，接受一系列加了噪声的图片，学习预测所加的噪声。</li></ul></li><li>前向过程在干什么？<ul><li>逐步向真实图片添加噪声最终得到一个纯噪声；</li><li>对于训练集中的每张图片，都能生成一系列的噪声程度不同的加噪图片；</li><li>在训练时，这些 【不同程度的噪声图片 + 生成它们所用的噪声】 是实际的训练样本。</li></ul></li><li>反向过程在干什么？<ul><li>训练好模型后，采样、生成图片。</li></ul></li></ul><h3 id=22前向过程扩散>2.2、前向过程（扩散）</h3><p><img loading=lazy src=images/DDPM_2.png srcset="/posts/ddpm/images/DDPM_2.png, images/DDPM_2.png 1.5x, /posts/ddpm/images/DDPM_2.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_2.png data-alt=/posts/ddpm/images/DDPM_2.png width=1080 height=174 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>前向过程在原始输入图像$x_0$上逐步添加随机噪声，这个噪声服从高斯分布$N(0, 1)$，每一步得到的图像$x_t$只和上一步的加噪结果$x_{t-1}$相关，逐步添加噪声至$T$步，可以得到趋向于纯粹噪声的图像，如下图所示：
<img loading=lazy src=images/DDPM_3.png srcset="/posts/ddpm/images/DDPM_3.png, images/DDPM_3.png 1.5x, /posts/ddpm/images/DDPM_3.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_3.png data-alt=/posts/ddpm/images/DDPM_3.png width=1080 height=235 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><blockquote><p>后面有详细的推导，公式比较多，这里先提前把主要的列一下方便阐述。</p></blockquote><p>对于将一张图片，从$x_{t-1}\rightarrow x_{t}$的逐步加噪破坏的公式为：</p><p>$$x_t=\sqrt{\alpha_t}\left.x_{t-1}+\sqrt{1-\alpha_t}\right.\varepsilon_t\quad\quad\quad\quad\quad\quad(1)$$</p><p>其中:</p><ul><li>$x_t$表示第$t$步的图像；</li><li>$\varepsilon$ 是一个满足正态分布的随机噪声，$\varepsilon \sim N(0, 1)$；</li><li>$\sqrt{\alpha_{t}}$ 是图片的权重，$\sqrt{1 - \alpha_{t}}$ 是噪声的权重；</li></ul><p>定义：</p><ul><li>$\alpha_t=1-\beta_t$</li><li>$\overline{\alpha}=\prod_{s=1}^t\alpha_s$</li></ul><p>随着$t$的增加，<strong>噪声的占比会越来越大</strong>，所以添加的<strong>噪声强度也会越来越大</strong>，也就是说图片的权重要越来越小，噪声的权重要越来越大。因为随着扩散过程的增加，图像中噪声的占比也会越来越大，我们想要进一步破坏它的结构，就需要添加更多的噪声。</p><blockquote><p>换句话说，一开始图像比较清晰，这个时候添加的噪声小一些，随着图像的噪声越来越多，这个时候再加一点噪声的话，对原来的图像就没什么影响了，因为它本身就有好多噪声了，所以随着图像的噪声越来越多，后面的步骤就要加更多的噪声。</p></blockquote><p>实际训练过程中会比较大（DDPM原文中为1000），所以会有从$x_0$递推到$x_t$的公式：</p><p>$$x_t=\sqrt{\overline{\alpha}_t}\left.x_0+\sqrt{1-\overline{\alpha}_t}\right.\varepsilon\quad\quad\quad\quad(2)$$</p><p>其中：</p><ul><li>$\alpha_t$、$\beta_t$ 有一个固定的已知函数，是可以直接进行计算的；</li><li>$\varepsilon$ 为随机产生的噪声；</li></ul><p>所以整个式子是已知的，式 $(1)$、$(2)$ 就可以描述前向过程了，$(1)$ 用于将一张图片的逐步破坏，$(2)$ 用于一步到位的破坏。</p><h3 id=23反向过程去噪>2.3、反向过程（去噪）</h3><p>反向过程则是不断去除噪声的过程，给定一个噪声图片 $x_T$，对它一步步的去噪还原，直至最终将原始图像 $x_0$ 给恢复出来，如下图所示：</p><p><img loading=lazy src=images/DDPM_4.png srcset="/posts/ddpm/images/DDPM_4.png, images/DDPM_4.png 1.5x, /posts/ddpm/images/DDPM_4.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_4.png data-alt=/posts/ddpm/images/DDPM_4.png width=1080 height=239 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>去噪的过程，$x_t$、$\alpha_t$、$\beta_t$ 都是已知的，只有公式 $(2)$ 中的真实噪声是未知的，因为它是随机采样的。所以需要一个神经网络把 $\varepsilon$ 给学出来，也就是说训练一个由 $x_t$ 和 $t$ 估测噪声的模型:</p><p>$$x_{t-1}=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}<em>t}}\varepsilon</em>\theta(x_t,t))$$</p><p>其中 $\theta$ 就是模型的参数，通常使用UNet作为预估噪声的模型。</p><h3 id=24模型训练>2.4、模型训练</h3><p>所以说反向过程其实就是<strong>训练网络去学习分解过程每一步的噪声</strong>，当网络训练好之后，输入一张噪声图片，通过网络就能把加的噪声给求出来，噪声有了代入公式，就能把 $x_{t-1}$ 步的比较清晰的图给求出来了，一步步往前迭代就行了。</p><p>采用L2距离刻画相近程度就可以，DDPM的关键是训练 $\varepsilon_{\theta}(x_t, t)$，目的就是使预测的噪声与真实用于破坏的噪声相近：</p><p>$$Loss=\mid\mid\varepsilon-\varepsilon_\theta(x_t,t)\mid\mid^2=\mid\mid\varepsilon-\varepsilon_\theta(\sqrt{\overline{\alpha}_t}~x_0+\sqrt{1-\overline{\alpha}_t}~\varepsilon_t,t)\mid\mid^2$$</p><p><img loading=lazy src=images/DDPM_5.png srcset="/posts/ddpm/images/DDPM_5.png, images/DDPM_5.png 1.5x, /posts/ddpm/images/DDPM_5.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_5.png data-alt=/posts/ddpm/images/DDPM_5.png width=1080 height=560 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>模型训练完后，只要给定随机高斯噪声，就可以生成一张从未见过的图像。</p><p>UNet本文不做介绍，结构图为：</p><p><img loading=lazy src=images/DDPM_6.png srcset="/posts/ddpm/images/DDPM_6.png, images/DDPM_6.png 1.5x, /posts/ddpm/images/DDPM_6.png 2x" sizes=auto data-title="U_Net 结构图" data-alt="U_Net 结构图" width=1080 height=731 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><blockquote><p>额外强调的是：Unet里有一个位置编码，是关于时间步的，每个时间步是有一个线性调度器的，每个时间添加的噪声的方差是不一样的，所以将时间步作为编码嵌入的话，可以将模型预测的噪声更加的准确。</p></blockquote><h2 id=三算法流程概述>三、算法流程概述</h2><p><img loading=lazy src=images/DDPM_7.png srcset="/posts/ddpm/images/DDPM_7.png, images/DDPM_7.png 1.5x, /posts/ddpm/images/DDPM_7.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_7.png data-alt=/posts/ddpm/images/DDPM_7.png width=1080 height=241 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>再次总结，扩散模型两个步骤如下：</p><ul><li>一个固定的（预先定义好的）前向扩散过程 $q(x_t | x_{t-1})$：逐步向图片增加噪声直到最终得到一张纯粹的噪声图；</li><li>一个学习得到的去噪过程 $p_{\theta}(x_{t-1} | x_t)$：训练一个神经网络去逐渐的从一张纯噪声中消除噪声，直到得到一张真正的图片。</li></ul><p><img loading=lazy src=images/DDPM_8.png srcset="/posts/ddpm/images/DDPM_8.png, images/DDPM_8.png 1.5x, /posts/ddpm/images/DDPM_8.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_8.png data-alt=/posts/ddpm/images/DDPM_8.png width=923 height=230 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>算法1 为训练流程：</p><ul><li>line2：从数据中采样 $x_0$，$q(x_0)$ 的意思是给 $x_0$ 加上噪声；</li><li>line3：随机选取 time step $t$；<ul><li>真实训练过程中我们不可能一步步的从 $t$ 到 $T$，因为会很大，这就意味着每输入一张图片 $x$，就会产生张噪声图像，也就是一张图像的网络要训练 $T$ 个噪声样本，非常耗时。</li><li>所以对 $T$ 进行了采样，$t$ 就是从 $T$ 里采集若干个的意思。</li><li>举个例子：假设采集 $t$ 的分别为100、20、3，对应的 $x$ 为 $x_{100}$、$x_{20}$、$x_{3}$，对应噪声为 $\varepsilon_{100}$、$\varepsilon_{20}$、$\varepsilon_{3}$，对于的预测噪声为 $\hat{\varepsilon}<em>{100}$、$\hat{\varepsilon}</em>{20}$、$\hat{\varepsilon}_{3}$, 只需要将 $\varepsilon$ 和 $\hat{\varepsilon}$ 代入MSE公式即可（相减、平方、最小化）。</li></ul></li><li>line 4：生成随机高斯噪声；</li><li>line 5：调用模型估计 $\varepsilon_{\theta}(\sqrt{\overline{\alpha}_t}~x_0+\sqrt{1-\overline{\alpha}_t}~\varepsilon_t,t)$ ，计算真实噪声与估计噪声之间的MSE Loss，反向传播更新模型。<ul><li>网络的作用是预测噪声，随着的增加，噪声强度会越来越大，因此预测的噪声是和迭代是直接相关的，所以要把作为参数送入到网络当中。</li></ul></li><li>直到收敛。</li></ul><p>算法2 为采样流程：</p><ul><li>line 1：从高斯分布采样 $x_T$；</li><li>line 2：按照 $T, &mldr;, 1$ 的顺序进行迭代；</li><li>line 3：如果 $t = 1$ 令 $z = 0$；如果 $t > 1$ ，从高斯分布中采样；</li><li>line 4：利用公式求出均值和方差，进而求得 $x_{t-1}$；</li><li>经过上述迭代，恢复 $x_0$。</li></ul><h2 id=四数学描述>四、数学描述</h2><p>我们来推导如何从原始图像直接到第t时刻的图像 $(X_0 - X_t)$。</p><p>首先回顾 2.1小节 的两个定义：</p><ul><li>$\alpha_t = 1 - \beta_{t}$, $\beta_t$ 要越大越好，论文中从0.0001到0.02;</li><li>$\overline{\alpha}=\prod_{s=1}^t\alpha_s$累乘，下面会用到；</li><li>$x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\varepsilon_t\text{,}\varepsilon_t\sim N(0,1)$ 每一时刻添加的噪声均独立；</li></ul><p>我们要求$x_t$时刻的图像，它需要一步步的加噪迭代，这样太慢了。因为每一步添加的噪声独立且服从正太分布，我们可以做如下推导：</p><blockquote><p>为了不混淆，只需要记住：<strong>下标越小，噪声越小</strong>，即 $x_{t-1}$ 的噪声是小于 $x_t$ 的。</p></blockquote><p>$$
\begin{aligned}
q(x_{t}\mid x_{t-1})& =N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I) \cr
&=\underbrace{\sqrt{\alpha_t}x_{t-1}}<em>{x</em>{t-2}\text{来表示}x_{t-1}}+\sqrt{1-\alpha_t}\varepsilon_t \cr
&=\sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}}\right.x_{t-2}+\sqrt{1-\alpha_{t-1}}\left.\varepsilon_{t-1}\right)+\sqrt{1-\alpha_t}\left.\varepsilon_t\right. \cr
&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\underbrace{\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}\varepsilon_{t-1}+\sqrt{1-\alpha_t}\varepsilon_t}<em>{\text{两个独立正太分布相加}} \cr
&=\sqrt{\alpha_t\alpha</em>{t-1}}\left.x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\right.\varepsilon \cr
&\text{&mldr;} \
&=\sqrt{\overline{\alpha}_t}\left.x_0+\sqrt{1-\overline{\alpha}_t}\right.\varepsilon \cr
&\therefore q(x_t\mid x_0)=N(x_t;\sqrt{\overline{\alpha}_t}x_0,\sqrt{1-\overline{\alpha}_t}I)
\end{aligned}
$$</p><blockquote><p>上述用的就是重参数化技巧。</p></blockquote><p>方差参数 $\beta_{t}$ 可以固定为一个常数，也可以选择作为 $T$ 时间段的一个时间表。事实上，人们可以定义一个方差表，它可以是线性的、二次的、余弦的等等。最初的DDPM作者利用了一个从 $\beta_1 = 10^{-4}$ 到$\beta_T = 0.02$增加的线性时间表。Nichol等人2021年的研究表明，采用余弦时间表效果更好。</p><p><img loading=lazy src=images/DDPM_9.png srcset="/posts/ddpm/images/DDPM_9.png, images/DDPM_9.png 1.5x, /posts/ddpm/images/DDPM_9.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_9.png data-alt=/posts/ddpm/images/DDPM_9.png width=1080 height=215 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h3 id=42反向过程去噪>4.2、反向过程（去噪）</h3><p>接下来是反向过程的推导：
$$p(x_{t-1}\mid x_t)=N(x_{t-1};\underbrace{\mu_\theta(x_t,t)}<em>\text{要反预测这个},\overbrace{\Sigma</em>\theta(x_t,t)}^{fixed})$$</p><p>给定$x_t$要预测 $x_{t-1}$，它是一个高斯分布，$x_t$和$t$的方差是固定的，论文作者使用原始的噪声调度器作为方差，也就是说噪声调度器一旦确立，方差的大小也就固定了。所以我们只需要预测这个均值就好了，下面给出具体的推导过程：</p><p>我们先看整个损失函数，是个<strong>负对数似然</strong>：</p><p>$$-\log{p_{\theta}(x_0)}$$</p><p>希望神经网络的参数 $\theta$，可以使得生成 $x_0$的概率越大越好。</p><p>但问题在于 $x_0$ 的概率不好计算，因为它依赖于 $x_0$ 之前的所有步长，从 $x_T$ 开始。作为一种解决方案，我们可以计算这个目标的<strong>变分下界</strong>，并得到一个更易于计算的公式：</p><p>$$-log(p_\theta(x_0))\leq-log(p_\theta(x_0))+D_{KL}(q(x_{1:T}\mid x_0)\parallel p_\theta(x_{1:T}\mid x_0))$$</p><p>其中：</p><ul><li>$x_{1:T}$ 指的是 $x_1, &mldr;, x_T$ 整个序列。</li></ul><p>现在依然无法计算，我们继续推导：</p><p>$$
\begin{gathered}
-log(p_\theta(x_0)) \leq-log(p_\theta(x_0))+D_{KL}(q(x_{1:T}\mid x_0)\mid\mid p_\theta(x_{1:T}\mid x_0)) \cr
\leq-log(p_\theta(x_0))+log(\frac{q(x_{1:T}\mid x_0)}{p_\theta(x_{1:T}\mid x_0)})
\end{gathered}
$$</p><p>我们将 KL divergence 改写后，再利用贝叶斯公式进行变形，即分母可以改写为：</p><p>$$
\begin{aligned}
p_\theta(x_{1:T}\mid x_0) &=\frac{p_\theta(x_0\mid x_{1:T})\mathrm{~}p_\theta(x_{1:T})}{p_\theta(x_0)} \cr
&=\frac{p_\theta(x_0,x_{1:T})}{p_\theta(x_0)} \cr
&=\frac{p_\theta(x_{0:T})}{p_\theta(x_0)}
\end{aligned}
$$</p><p>将其代回原式：</p><p>$$
\begin{aligned}
log(\frac{q(x_{1:T}\mid x_0)}{p_\theta(x_{1:T}\mid x_0)})& =log(\frac{q(x_{1:T}\mid x_0)}{\frac{p_\theta(x_{0:T})}{p_\theta(x_0)}}) \cr
&=log(\frac{q(x_{1:T}\mid x_0)}{p_\theta(x_{0:T})})+log(p_\theta(x_0))
\end{aligned}
$$</p><p>所以原式可简化为：</p><p>$$-log(p_\theta(x_0))\leq\underbrace{log(\frac{q(x_{1:T}\mid x_0)}{p_\theta(x_{0:T})})}_{\text{变分下界,可以优化它}}$$</p><ul><li><p>分子，就是前向过程，它是固定的，从 $x_0$ 到 $x_{1:T}$ 的采样，换句话说就是从我们数据中的一些图像开始；</p></li><li><p>分母，$p_\theta(x_{0:T})=p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}\mid x_t)$；</p><ul><li>将 $p(x_T)$ 提出来，是因为 $p(x_T)$ 是指当前图像，它是不依赖于网络参数 $\theta$ 的.</li></ul><p>$$
\begin{aligned}
log(\frac{q(x_{1:T}\mid x_0)}{p_{\theta}(x_{0:T})})& =log(\frac{\prod_{t=1}^Tq(x_t\mid x_{t-1})}{p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1}\mid x_t)}) \cr
&=-log(p(x_T))+log(\frac{\prod_{t=1}^Tq(x_t\mid x_{t-1})}{\prod_{t=1}^Tp_\theta(x_{t-1}\mid x_t)}) \cr
&=-log(p(x_T))+\sum_{t=1}^Tlog(\frac{q(x_t\mid x_{t-1})}{p_\theta(x_{t-1}\mid x_t)}) \cr
&=-log(p(x_T))+\sum_{t=2}^Tlog(\frac{q(x_t\mid x_{t-1})}{p_\theta(x_{t-1}\mid x_t)})+\underbrace{log(\frac{q(x_1\mid x_0)}{p_\theta(x_0\mid x_1)})}_{t=1}
\end{aligned}
$$</p></li></ul><p><img loading=lazy src=images/DDPM_10.png srcset="/posts/ddpm/images/DDPM_10.png, images/DDPM_10.png 1.5x, /posts/ddpm/images/DDPM_10.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_10.png data-alt=/posts/ddpm/images/DDPM_10.png width=1080 height=407 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>$q(x_t|x_{t-1})$ 根据贝叶斯公式可以变换如下：</p><p>$$q(x_t\mid x_{t-1})=\frac{q(x_{t-1}\mid x_t)q(x_t)}{q(x_{t-1})}$$</p><p>$q(x_{t-1}|x_{t})$具有比较高的方差，因为根据这张照片，我们无法确定它来自哪里，但是引入 $x_0$，我们就可以容易的预测出 $x_{t-1}$，</p><p><img loading=lazy src=images/DDPM_11.png srcset="/posts/ddpm/images/DDPM_11.png, images/DDPM_11.png 1.5x, /posts/ddpm/images/DDPM_11.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_11.png data-alt=/posts/ddpm/images/DDPM_11.png width=676 height=326 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>因此我们使用：</p><p>$$\frac{q(x_{t-1}\mid x_t,x_0)\mathrm{~}q(x_t\mid x_0)}{q(x_{t-1}\mid x_0)}$$</p><p>替换贝叶斯重写后的式子，我们得到：</p><p>$$
\begin{aligned}
log(\frac{q(x_{1:T}\mid x_0)}{p_{\theta}(x_{0:T})})& =-log(p(x_T))+\sum_{t=2}^Tlog(\frac{q(x_{t-1}\mid x_t,x_0)q(x_t\mid x_0)}{p_\theta(x_{t-1}\mid x_t)q(x_{t-1}\mid x_0)})+log(\frac{q(x_1\mid x_0)}{p_\theta(x_0\mid x_1)}) \cr
&=-log(p(x_T))+\sum_{t=2}^Tlog(\frac{q(x_{t-1}\mid x_t,x_0)}{p_\theta(x_{t-1}\mid x_t)})+\underbrace{\sum_{t=2}^Tlog(\frac{q(x_t\mid x_0)}{q(x_{t-1}\mid x_0)})}+log(\frac{q(x_1\mid x_0)}{p_\theta(x_0\mid x_1)})
\end{aligned}
$$</p><p>上述标记的式子，也可以简化，我们假设 $t=4$：</p><p>$$
\begin{gathered}
\begin{aligned}\sum_{t=2}^{T=4}log(\frac{q(x_t\mid x_0)}{q(x_{t-1}\mid x_0)})\end{aligned} =log(\frac{q(x_2\mid x_0)}{q(x_1\mid x_0)}\cdot\frac{q(x_3\mid x_0)}{q(x_2\mid x_0)}\cdot\frac{q(x_4\mid x_0)}{q(x_3\mid x_0)}) \
=log(\frac{q(x_4\mid x_0)}{q(x_1\mid x_0)})
\end{gathered}
$$</p><p>因此我们可以简化为：</p><p>$$
\begin{aligned}
&=-log(p(x_T))+\sum_{t=2}^Tlog(\frac{q(x_{t-1}\mid x_t,x_0)}{p_\theta(x_{t-1}\mid x_t)})+log(\frac{q(x_t\mid x_0)}{q(x_1\mid x_0)})+log(\frac{q(x_1\mid x_0)}{p_\theta(x_0\mid x_1)}) \cr
&=-log(p(x_T))+\sum_{t=2}^Tlog(\frac{q(x_{t-1}\mid x_t,x_0)}{p_\theta(x_{t-1}\mid x_t)})+log(q(x_t\mid x_0))-log(p_\theta(x_0\mid x_1)) \cr
&=log(\frac{q(x_t\mid x_0)}{p(x_T)})+\sum_{t=2}^Tlog(\frac{q(x_{t-1}\mid x_t,x_0)}{p_\theta(x_{t-1}\mid x_t)})-log(p_\theta(x_0\mid x_1))\cr
&=\overbrace{\underbrace{D_{KL}(q(x_t\mid x_0)\mid\mid p(x_T))}<em>{q\text{只是个正向过程没有可学习参数}}}^{\text{可以忽略}} + \sum</em>{t=2}^TD_{KL}(q(x_{t-1}\mid x_t,x_0)\mid\mid p_\theta(x_{t-1}\mid x_t))-log(p_\theta(x_0\mid x_1))
\end{aligned}
$$</p><ul><li>第一项KL散度可以忽略，因为$q$只是个正向过程，没有可学习参数，换句话说就是它是固定的。</li><li>第二项KL散度，左边和右边都是正太分布，分别服从 $N(x_{t-1};\tilde{\mu_t}(x_t,x_0),\tilde{\mathsf{\beta}<em>t}I)$ 、$N(x</em>{t-1};\mu_\theta(x_t,t),\text{β}I)$：</li></ul><p>$$
\sum_{t=2}^TD_{KL}(\underbrace{q(x_{t-1}\mid x_t,x_0)}<em>{N(x</em>{t-1};\tilde{\mu}<em>t(x_t,x_0),\tilde{\mathsf{\beta}}<em>tI)}\mid\mid\overbrace{p</em>\theta(x</em>{t-1}\mid x_t)}^{N(x_{t-1};\mu_\theta(x_t,t),\mathsf{\beta}I})
$$</p><p>第一项的 $\tilde{\mu_{t}}(x_{t},x_{0})$、$\tilde{\beta_{t}}$ 就是我们要求的值，这里省略了这部分的推导，不影响算法的理解，</p><p>$$
\begin{gathered}\tilde{\mu}<em>t(x_t,x_0)=\frac{\sqrt{\alpha_t}(1-\overline{\alpha}</em>{t-1})}{1-\overline{\alpha}<em>t}x_t+\frac{\sqrt{\alpha}</em>{t-1}\beta_t}{1-\overline{\alpha}_t}x_0\\tilde{\mathsf{\beta}}<em>t=\frac{1-\overline{\alpha}</em>{t-1}}{1-\overline{\alpha}_t}\beta_t\end{gathered}
$$</p><blockquote><p>凡是涉及到 $\alpha_t$ 的，就是学习调度器的，我们不需要关注它</p></blockquote><p>我们可以化简 $\tilde{\mu}_{t}$，我们知道 $x_t=\sqrt{\overline{\alpha}_t}x_0+\sqrt{1-\overline{\alpha}_t}\varepsilon $, 即:</p><p>$$
x_0=\frac1{\sqrt{\overline{\alpha}_t}}(x_t-\sqrt{1-\overline{\alpha}_t}\left.\varepsilon\right)
$$</p><p>还知道: $\overline{\alpha}=\prod_{s=1}^t\alpha_s$、$\alpha_t=1-\beta_t$:</p><p>代入 $\tilde{\mu}_{t}$ 得到：</p><p>$$
\begin{aligned}
\underbrace{\tilde{\mu}<em>t(x_t,x_0)}</em>{\text{不再依赖}x_0}& =\frac{\sqrt{\alpha_{t}}(1-\overline{\alpha}<em>{t-1})}{1-\overline{\alpha}</em>{t}}x_{t}+\frac{\sqrt{\overline{\alpha}<em>{t-1}}\beta</em>{t}}{1-\overline{\alpha}<em>{t}}\frac{1}{\sqrt{\overline{\alpha}</em>{t}}}(x_{t}-\sqrt{1-\overline{\alpha}<em>{t}}\varepsilon) \cr
&=\frac{\alpha_t(1-\overline{\alpha}</em>{t-1})x_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)}+\frac{\beta_t}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)}(x_t-\sqrt{1-\overline{\alpha}_t}\left.\varepsilon\right) \cr
&=\frac{\alpha_tx_t-\overline{\alpha}_tx_t+(1-\alpha_t)x_t-(1-\alpha_t)\sqrt{1-\overline{\alpha}_t}\varepsilon}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)} \cr
&=\frac{x_t(1-\overline{\alpha}_t)-(1-\alpha_t)\sqrt{1-\overline{\alpha}_t}\varepsilon}{\sqrt{\alpha_t}(1-\overline{\alpha}_t)} \cr
&=\frac{x_t}{\sqrt{\alpha_t}}-\frac{(1-\alpha_t)\varepsilon}{\sqrt{\alpha_t}\sqrt{(1-\overline{\alpha}_t)}} \cr
&=\frac1{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}_t}}\left.\varepsilon\right)
\end{aligned}
$$</p><p>代入之后我们发现它就不再依赖于 $x_0$ 了，它就是和 $x_t$ 的一个关系式，式中的 $\alpha_t$、$\beta_t$、$\varepsilon$都是已知的，最后的本质就是我们只是从中减去缩放的随机噪声。</p><p>$$\therefore x_{t-1}=N(x_{t-1};\frac1{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}<em>t}}\left.\varepsilon</em>\theta(x_t,t)\right),\Sigma_\theta(x_t,t))$$</p><p>这样一来，DDPM的每一步推断可以总结为：</p><ul><li>每个时间步通过 $x_t$ 和 $t$ 来预测高斯噪声，图中用 $z$ 表示，根据上述公式计算得到均值 $\mu$；</li><li>得到方差 $\Sigma_\theta(x_t,t)$</li><li>入公式得到 $q(x_{t-1}\mid x_t)$ ，利用重参数化得到 $x_{t-1}$ 。</li></ul><p><img loading=lazy src=images/DDPM_12.png srcset="/posts/ddpm/images/DDPM_12.png, images/DDPM_12.png 1.5x, /posts/ddpm/images/DDPM_12.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_12.png data-alt=/posts/ddpm/images/DDPM_12.png width=1080 height=254 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h3 id=43训练损>4.3、训练损</h3><p>下面我们来看损失的推导，我们来回顾第二项：</p><p><img loading=lazy src=images/DDPM_13.png srcset="/posts/ddpm/images/DDPM_13.png, images/DDPM_13.png 1.5x, /posts/ddpm/images/DDPM_13.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_13.png data-alt=/posts/ddpm/images/DDPM_13.png width=1080 height=469 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>我们需要减小KL散度，由于<mark>方差是固定的，我们无法优化，所以需要将它们的均值之差减小</mark>，原论文中使用的是简单的均方误差：</p><p>将$\mu$表达式代入：</p><p>$$
\begin{aligned}
L_{t}& =\frac1{2\sigma_t^2}\mid|\tilde{\mu}<em>t(x_t,x_0)-\mu</em>\theta(x_t,t)||^2 \cr
&=\frac1{2\sigma_t^2}\mid\mid\frac1{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}<em>t}}\left.\varepsilon\right)-\frac1{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\overline{\alpha}<em>t}}\left.\varepsilon</em>\theta(x_t,t)\right)\mid\mid^2 \cr
&=\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\overline{\alpha}<em>t)}\underbrace{\mid\mid\varepsilon-\varepsilon</em>\theta(x_t,t)\mid\mid^2}</em>{mse} \cr
&->\mid\mid\varepsilon-\varepsilon_\theta(x_t,t)\mid\mid^2=\mid\mid\varepsilon-\varepsilon_\theta(\sqrt{\overline{\alpha}_t}\left.x_0+\sqrt{1-\overline{\alpha}_t}\left.\varepsilon_t,t\right)\mid\mid^2\right.
\end{aligned}
$$</p><p>研究人员发现，忽略前面的系数项会变得更简单，采样质量也会得到提高，所以前面这个系数项我们直接忽略，它是和噪声调度器有关的，我们加噪的话也会使计算复杂。</p><p>我们最小化 $\mid\mid\varepsilon-\varepsilon_\theta(x_t, t)\mid\mid^2$ 也就是<strong>最小化了KL散度</strong>，KL散度变小了也就是变分上限优化到最小，所以那个负对数似然也会变小。</p><p>上面还剩了最后一项 $-log(p_\theta(x_0\mid x_1))$ ，这个作者决定去掉它，即在 $t=1$ 时，我们不添加噪声。也就是下面横线的地方，只有 $t>1$ 的时候才服从高斯分布，如果 $t\leq {1}$，直接让 $z=0$，即噪声设置为0。</p><p><img loading=lazy src=images/DDPM_14.png srcset="/posts/ddpm/images/DDPM_14.png, images/DDPM_14.png 1.5x, /posts/ddpm/images/DDPM_14.png 2x" sizes=auto data-title=/posts/ddpm/images/DDPM_14.png data-alt=/posts/ddpm/images/DDPM_14.png width=1080 height=272 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>回顾上面整个推导过程：我们从<strong>负对数似然 -> 优化下界 -> 简化下界 -> 预测噪声</strong>。</p><h2 id=五torch复现>五、torch复现</h2><p><a href=https://wangguisen.blog.csdn.net/article/details/128821008 target=_blank rel="external nofollow noopener noreferrer">https://wangguisen.blog.csdn.net/article/details/128821008<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><p>ref:
[1]. <a href=https://arxiv.org/abs/2006.11239 target=_blank rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2006.11239<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[2]. <a href=https://kexue.fm/archives/9119 target=_blank rel="external nofollow noopener noreferrer">https://kexue.fm/archives/9119<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[3]. <a href=https://zhuanlan.zhihu.com/p/576475987 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/576475987<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[4]. <a href=https://zhuanlan.zhihu.com/p/525106459 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/525106459<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[5]. <a href=https://www.bilibili.com/video/BV1b541197HX target=_blank rel="external nofollow noopener noreferrer">https://www.bilibili.com/video/BV1b541197HX<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[6]. <a href=https://www.bilibili.com/video/BV1WD4y1E7X5 target=_blank rel="external nofollow noopener noreferrer">https://www.bilibili.com/video/BV1WD4y1E7X5<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[7]. <a href=https://huggingface.co/blog/annotated-diffusion target=_blank rel="external nofollow noopener noreferrer">https://huggingface.co/blog/annotated-diffusion<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[8]. <a href=https://www.datalearner.com/blog/1051664857725795 target=_blank rel="external nofollow noopener noreferrer">https://www.datalearner.com/blog/1051664857725795<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[9]. <a href=https://lilianweng.github.io/posts/2021-07-11-diffusion-models target=_blank rel="external nofollow noopener noreferrer">https://lilianweng.github.io/posts/2021-07-11-diffusion-models<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[10]. <a href="https://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==&amp;mid=2247486128&amp;idx=1&amp;sn=7ffef5d8c1bbf24565d0597eb5eaeb16&amp;chksm=c337b729f4403e3f4ca4fcc1bc04704f72c1dc02876a2bf83c330e7857eba567864da6a64e18&amp;scene=21#wechat_redirect" target=_blank rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s?__biz=Mzk0MzIzODM5MA==&mid=2247486128&idx=1&sn=7ffef5d8c1bbf24565d0597eb5eaeb16&chksm=c337b729f4403e3f4ca4fcc1bc04704f72c1dc02876a2bf83c330e7857eba567864da6a64e18&scene=21#wechat_redirect<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
[11]. <a href=https://arxiv.org/pdf/2006.11239.pdf target=_blank rel="external nofollow noopener noreferrer">paper link<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-08-08 09:24:08">更新于 2023-08-08&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/c3f1587d684ef542efa12140ad3ef20479483da7 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) c3f1587d684ef542efa12140ad3ef20479483da7: feat: add cluase 17 for c++ effective"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>c3f1587</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/ddpm/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/ML/Diffusion/DDPM/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/ddpm/ data-title="Diffusion 扩散模型（DDPM）" data-hashtags=draft><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/ddpm/ data-hashtag=draft><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/ddpm/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/ddpm/ data-title="Diffusion 扩散模型（DDPM）"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/ddpm/ data-title="Diffusion 扩散模型（DDPM）"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/draft/ class=post-tag>draft</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/effective_cpp_part_four/ class=post-nav-item rel=prev title="Effective C++ (第3版) 精读总结 [4]"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Effective C++ (第3版) 精读总结 [4]</a>
<a href=/posts/clause_12/ class=post-nav-item rel=next title="Effective STL [12] | 对STL容器线程安全性的期待现实一些">Effective STL [12] | 对STL容器线程安全性的期待现实一些<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.122.0">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>