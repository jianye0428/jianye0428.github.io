<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>所有文章 - yejian's blog</title><link>https://jianye0428.github.io/posts/</link><description>所有文章 | yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sun, 25 Feb 2024 19:53:22 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>RL学习笔记 [2] | 马尔科夫决策过程(MDP)</title><link>https://jianye0428.github.io/posts/rl_learning_note_2/</link><pubDate>Wed, 21 Feb 2024 10:38:11 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_2/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。</p>
<p>MDP这一篇对应Sutton书的第三章和UCL强化学习课程的第二讲。</p>
<h1 id="1-强化学习引入mdp的原因">1. 强化学习引入MDP的原因</h1>
<p>对于马尔科夫性本身，我之前讲过的<a href="http://www.cnblogs.com/pinard/p/6945257.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（一）HMM模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="http://www.cnblogs.com/pinard/p/7048333.html"target="_blank" rel="external nofollow noopener noreferrer">条件随机场CRF(一)从随机场到线性链条件随机场<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>以及<a href="http://www.cnblogs.com/pinard/p/6632399.html"target="_blank" rel="external nofollow noopener noreferrer">MCMC(二)马尔科夫链<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>都有讲到。它本身是一个比较简单的假设，因此这里就不专门对“马尔可夫性”做专门的讲述了。</p>
<p>除了对于环境的状态转化模型这个因素做马尔科夫假设外，我们还对强化学习第四个要素个体的策略(policy) $π$ 也做了马尔科夫假设。即在状态 $s$ 时采取动作 $a$ 的概率仅与当前状态 $s$ 有关，与其他的要素无关。用公式表示就是</p>
<p>$$\pi(a\mid s)=P(A_{t}=a\mid S_{t}=s)$$</p>
<p>对于第五个要素，价值函数 $v_π(s)$ 也是一样, $v_π(s)$ 现在仅仅依赖于当前状态了，那么现在价值函数 $v_π(s)$ 表示为:</p>
<p>$$\nu_{\pi}(s)=\mathrm{E}_{\pi}(G_{t}|S_{t}=s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)$$</p>
<p>其中，$G_t$ 代表收获(return), 是一个MDP中从某一个状态 $S_t$ 开始采样直到终止状态时所有奖励的有衰减的之和。</p>
<h1 id="2-mdp的价值函数与贝尔曼方程">2. MDP的价值函数与贝尔曼方程</h1>
<p>对于MDP，我们在第一节里已经讲到了它的价值函数 $v_π(s)$ 的表达式。但是这个表达式没有考虑到所采用的动作$a$带来的价值影响，因此我们除了 $v_π(s)$ 这个状态价值函数外，还有一个动作价值函数 $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=\operatorname{E}_{\pi}(G_{t}|S_{t}=s,A_{t}=a)=\operatorname{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s,A_{t}=a)$$</p>
<p>根据价值函数的表达式，我们可以推导出价值函数基于状态的递推关系，比如对于状态价值函数 $v_π(s)$，可以发现：</p>
<p>$$\begin{aligned}
V_{\pi}(s)&amp; =\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)  \\\\
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)|S_{t}=s) \\\\
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma G_{t+1}|S_{t}=s) \\\\
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})|S_{t}=s)
\end{aligned}$$</p>
<p>也就是说，在 $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 是满足递推关系的，即：</p>
<p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})\mid S_{t}=s)$$
　　　　
这个递推式子我们一般将它叫做<strong>贝尔曼方程</strong>。这个式子告诉我们，一个状态的价值由该状态的奖励以及后续状态价值按一定的衰减比例联合组成。</p>
<p>同样的方法，我们可以得到动作价值函数 $q_π(s,a)$ 的贝尔曼方程：</p>
<p>$$q_{\pi}(s,a)=\mathrm{E}_{\pi}(R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})\mid S_{t}=s,A_{t}=a)$$</p>
<h1 id="3-状态价值函数与动作价值函数的递推关系">3. 状态价值函数与动作价值函数的递推关系</h1>
<p>根据动作价值函数 $q_π(s,a)$ 和状态价值函数 $v_π(s)$ 的定义，我们很容易得到他们之间的转化关系公式：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$</p>
<p>也就是说，状态价值函数是所有动作价值函数基于策略 $π$ 的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值。</p>
<p>反过来，利用上贝尔曼方程，我们也很容易从状态价值函数 $v_π(s)$ 表示动作价值函数 $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=R_{s}^{a}+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;})$$</p>
<p>通俗说就是状态动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。</p>
<p>这两个转化过程也可以从下图中直观的看出：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">状态价值函数</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">动作价值函数</div>
</center>
<br>
<p>把上面两个式子互相结合起来，我们可以得到：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;}))$$</p>
<p>$$q_\pi(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\sum_{a&rsquo; \in A}\pi(a&rsquo; \mid s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)$$</p>
<h1 id="4-最优价值函数">4. 最优价值函数</h1>
<p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略我们可以用 $π^*$表示。一旦找到这个最优策略$π^∗$，那么我们就解决了这个强化学习问题。一般来说，比较难去找到一个最优策略，但是可以通过比较若干不同策略的优劣来确定一个较好的策略，也就是局部最优解。</p>
<p>如何比较策略的优劣呢？一般是通过对应的价值函数来比较的，也就是说，寻找较优策略可以通过寻找较优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者，即：</p>
<p>$$\nu_{*}(s)=\max_{\pi}\nu_{\pi}(s)$$</p>
<p>同理也可以定义最优动作价值函数是所有策略下产生的众多动作状态价值函数中的最大者，即：</p>
<p>$$q_*(s,a)=\max_\pi q_\pi(s,a)$$</p>
<p>对于最优的策略，基于动作价值函数我们可以定义为：</p>
<p>$$\pi_<em>(a|s)=\begin{cases}1&amp;\mathrm{if~}a=\mathrm{arg~}\max_{a\in A}q</em>(s,a)\\\\0&amp;\mathrm{else}&amp;\end{cases}$$</p>
<p>只要我们找到了最大的状态价值函数或者动作价值函数，那么对应的策略 $π^*$ 就是我们强化学习问题的解。同时，利用状态价值函数和动作价值函数之间的关系，我们也可以得到:</p>
<p>$$v_<em>(s)=\max_aq_</em>(s,a)$$</p>
<p>反过来的最优价值函数关系也很容易得到：</p>
<p>$$q_{<em>}(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss}^{a}{}_{</em>}(\mathrm{s&rsquo;})$$</p>
<p>利用上面的两个式子也可以得到和第三节末尾类似的式子：</p>
<p>$$\nu_<em>(s)=\max_a(R_s^a+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^a\nu_</em>(s&rsquo;))$$</p>
<p>$$q_<em>(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\max_{a&rsquo;}q_</em>(s&rsquo;,a&rsquo;)$$</p>
<h1 id="5-mdp实例">5. MDP实例</h1>
<p>上面的公式有点多，需要一些时间慢慢消化，这里给出一个UCL讲义上实际的例子，首先看看具体我们如何利用给定策略来计算价值函数。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP 举例</div>
</center>
<br>
<p>例子是一个学生学习考试的MDP。里面左下那个圆圈位置是起点，方框那个位置是终点。上面的动作有study, pub, facebook, quit, sleep，每个状态动作对应的即时奖励R已经标出来了。我们的目标是找到最优的动作价值函数或者状态价值函数，进而找出最优的策略。</p>
<p>为了方便，我们假设衰减因子 $γ=1$, $π(a|s)=0.5$。</p>
<p>对于终点方框位置，由于其没有下一个状态，也没有当前状态的动作，因此其状态价值函数为0。对于其余四个状态，我们依次定义其价值为<em>v</em>1,<em>v</em>2,<em>v</em>3,<em>v</em>4， 分别对应左上，左下，中下，右下位置的圆圈。我们基于$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s&rsquo;))$计算所有的状态价值函数。可以列出一个方程组。</p>
<ul>
<li>
<p>对于<em>v</em>1位置，我们有：$v_1=0.5*(-1+v_1)+0.5*(0+v_2)$</p>
</li>
<li>
<p>对于<em>v</em>2位置，我们有：$v_2=0.5*(-1+v_1)+0.5*(-2+v_3)$</p>
</li>
<li>
<p>对于<em>v</em>3位置，我们有：$v_3=0.5*(0+0)+0.5*(-2+v_4)$</p>
</li>
<li>
<p>对于<em>v</em>4位置，我们有：$v_4=0.5*(10+0)+0.5*(1+0.2<em>v_2+0.4</em>v_3+0.4*v_4)$</p>
</li>
</ul>
<p>解出这个方程组可以得到 $v_1=−2.3$, $v_2=−1.3$, $v_3=2.7$, $v_4=7.4$, 即每个状态的价值函数如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>上面我们固定了策略$ π(a|s)$, 虽然求出了每个状态的状态价值函数，但是却并不一定是最优价值函数。那么如何求出最优价值函数呢？这里由于状态机简单，求出最优的状态价值函数 $v*(s)$ 或者动作价值函数 $q*(s,a)$ s比较容易。</p>
<p>我们这次以动作价值函数 $q*(s,a)$ 来为例求解。首先终点方框处的好求。</p>
<p>$$q*(s_3,sleep)=0,q*(s_4,study)=10$$</p>
<p>接着我们就可利用 $q*(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\max_{a&rsquo;}q*(s&rsquo;,a&rsquo;)$ 列方程组求出所有的 $q∗(s,a)$ 。有了所有的 $q*(s,a)$,利用 $v_{<em>}(s)=\max_{a}q</em>(s,a)$ 就可以求出所有的 $v∗(s)$。最终求出的所有 $v∗(s)$ 和 $q∗(s,a)$ 如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>从而我们的最优决策路径是走6-&gt;6-&gt;8-&gt;10-&gt;结束。　　　　</p>
<h1 id="6-mdp小结">6. MDP小结</h1>
<p>MDP是强化学习入门的关键一步，如果这部分研究的比较清楚，后面的学习就会容易很多。因此值得多些时间在这里。虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。</p>
<p>下一篇讨论用动态规划的方法来求解强化学习的问题。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9426283.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [3] | 用动态规划(DP)求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_3/</link><pubDate>Thu, 22 Feb 2024 08:59:02 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_3/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。</p>
<p>动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲。</p>
<h1 id="1-动态规划和强化学习问题的联系">1. 动态规划和强化学习问题的联系</h1>
<p>对于动态规划，相信大家都很熟悉，很多使用算法的地方都会用到。就算是机器学习相关的算法，使用动态规划的也很多，比如之前讲到的<a href="https://www.cnblogs.com/pinard/p/6955871.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="https://www.cnblogs.com/pinard/p/6991852.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 都是动态规划的典型例子。</p>
<p>动态规划的关键点有两个：一是问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解。第二是可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题的状态。而强化学习的问题恰好是满足这两个条件的。</p>
<p>我们先看看强化学习的两个基本问题。</p>
<p>第一个问题是预测，即给定强化学习的6个要素：状态集 $S$, 动作集$A$, 模型状态转化概率矩阵$P$, 即时奖励$R$，衰减因子$γ$, 给定策略$π$， 求解该策略的状态价值函数$v(π)$</p>
<p>第二个问题是控制，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 模型状态转化概率矩阵$P$, 即时奖励$R$，衰减因子$γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
<p>那么如何找到动态规划和强化学习这两个问题的关系呢？</p>
<p>回忆一下上一篇<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中状态价值函数的贝尔曼方程：</p>
<p>$$v_\pi(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_\pi(s&rsquo;))$$</p>
<p>从这个式子我们可以看出，我们可以定义出子问题求解每个状态的状态价值函数，同时这个式子又是一个递推的式子, 意味着利用它，我们可以使用上一个迭代周期内的状态价值来计算更新当前迭代周期某状态 $s$ 的状态价值。可见，使用动态规划来求解强化学习问题是比较自然的。</p>
<h1 id="2-策略评估求解预测问题">2. 策略评估求解预测问题</h1>
<p>首先，我们来看如何使用动态规划来求解强化学习的预测问题，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p>
<p>策略评估的基本思路是从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。</p>
<p>假设我们在第k轮迭代已经计算出了所有的状态的状态价值，那么在第 $k+1$ 轮我们可以利用第k轮计算出的状态价值计算出第k+1+1轮的状态价值。这是通过贝尔曼方程来完成的，即：</p>
<p>$$v_{k+1}(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_k(s&rsquo;))$$</p>
<p>和上一节的式子唯一的区别是由于我们的策略 $π$ 已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变很小(收敛)，那么我们就得出了预测问题的解，即给定策略的状态价值函数 $v(π)$。</p>
<p>下面我们用一个具体的例子来说明策略评估的过程。</p>
<h1 id="3-策略评估求解实例">3. 策略评估求解实例</h1>
<p>这是一个经典的Grid World的例子。我们有一个4x4的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为γ=1=1。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率P=1=1。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Grid World</div>
</center>
<br>
<p>首先我们初始化所有格子的状态价值为0，如上图 $k=0$ 的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。在 $k=1$ 的时候，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：</p>
<p>$$v_1^{(21)}=\frac14[(-1+0)+(-1+0)+(-1+0)+(-1+0)]=-1$$</p>
<p>第二行第二个格子的价值是：</p>
<p>$$v_1^{(22)}=\frac14[(-1+0)+(-1+0)+(-1+0)+(-1+0)]=-1$$</p>
<p>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图 $k=1$ 的时候。现在我们第一轮迭代完了。开始动态规划迭代第二轮了。还是看第二行第一个格子的价值：</p>
<p>$$v_2^{(21)}=\frac14[(-1+0)+(-1-1)+(-1-1)+(-1-1)]=-1.75$$</p>
<p>第二行第二个格子的价值是：</p>
<p>$$v_2^{(22)}=\frac14[(-1-1)+(-1-1)+(-1-1)+(-1-1)]=-2$$</p>
<p>最终得到的结果是上图 $k=2$ 的时候。第三轮的迭代如下：</p>
<p>$$v_3^{(21)}=\frac14[(-1-1.7)+(-1-2)+(-1-2)+(-1+0)]=-2.425$$</p>
<p>$$v_3^{(22)}=\frac14[(-1-1.7)+(-1-1.7)+(-1-2)+(-1-2)]=-2.85$$</p>
<p>最终得到的结果是上图 $k=3$ 的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h1 id="4-策略迭代求解控制问题">4. 策略迭代求解控制问题</h1>
<p>上面我们讲了使用策略评估求解预测问题，现在我们再来看如何使用动态规划求解强化学习的第二个问题控制问题。一种可行的方法就是根据我们之前基于任意一个给定策略评估得到的状态价值来及时调整我们的动作策略，这个方法我们叫做策略迭代(Policy Iteration)。</p>
<p>如何调整呢？最简单的方法就是贪婪法。考虑一种如下的贪婪策略：个体在某个状态下选择的行为是其能够到达后续所有可能的状态中状态价值最大的那个状态。还是以第三节的例子为例，如上面的图右边。当我们计算出最终的状态价值后，我们发现，第二行第一个格子周围的价值分别是0,-18,-20，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-14,-14,-20,-20。那么我们整行动策略为向状态价值为-14的方向移动，也就是图中的向左向上。</p>
<p>如果用一副图来表示策略迭代的过程的话，如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Policy Iteration</div>
</center>
<br>
<p>在策略迭代过程中，我们循环进行两部分工作，第一步是使用当前策略 $π∗$ 评估计算当前策略的最终状态价值 $v∗$，第二步是根据状态价值 $v∗$ 根据一定的方法（比如贪婪法）更新策略 $π∗$，接着回到第一步，一直迭代下去，最终得到收敛的策略 $π∗$ 和状态价值 $v∗$。</p>
<h1 id="5-价值迭代求解控制问题">5. 价值迭代求解控制问题</h1>
<p>观察第三节的图发现，我们如果用贪婪法调整动作策略，那么当k=3=3的时候，我们就已经得到了最优的动作策略。而不用一直迭代到状态价值收敛才去调整策略。那么此时我们的策略迭代优化为价值迭代。</p>
<p>还是以第三节的例子为例，如上面的图右边。比如当k=2=2时，第二行第一个格子周围的价值分别是0,-2,-2，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-1.7,-1.7,-2, -2。那么我们整行动策略为向状态价值为-1.7的方向移动，也就是图中的向左向上。</p>
<p>和上一节相比，我们没有等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时我们的状态价值的更新方法也和策略迭代不同。现在的贝尔曼方程迭代式子如下：</p>
<p>$$v_{k+1}(s)=\max_{a\in A}(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_k(s&rsquo;))$$</p>
<p>可见由于策略调整，我们现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。</p>
<h1 id="6-异步动态规划算法">6. 异步动态规划算法</h1>
<p>在前几节我们讲的都是同步动态规划算法，即每轮迭代我会计算出所有的状态价值并保存起来，在下一轮中，我们使用这些保存起来的状态价值来计算新一轮的状态价值。</p>
<p>另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。</p>
<p>常见的异步动态规划算法有三种：</p>
<p>第一种是原位动态规划 (in-place dynamic programming)， 此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。</p>
<p>第二种是优先级动态规划 (prioritised sweeping)：该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。</p>
<p>第三种是实时动态规划 (real-time dynamic programming)：实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢。</p>
<h1 id="7-动态规划求解强化学习问题小结">7. 动态规划求解强化学习问题小结</h1>
<p>动态规划是我们讲到的第一个系统求解强化学习预测和控制问题的方法。它的算法思路比较简单，主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。</p>
<p>动态规划算法使用全宽度（full-width）的回溯机制来进行状态价值的更新，也就是说，无论是同步还是异步动态规划，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此我们还需要寻找其他的针对复杂问题的强化学习问题求解方法。</p>
<p>下一篇我们讨论用蒙特卡罗方法来求解强化学习预测和控制问题的方法。</p>
<p>ref:
<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9463815.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [4] | 用蒙特卡罗法（MC）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_4/</link><pubDate>Thu, 22 Feb 2024 13:00:24 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_4/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（三）用动态规划（DP）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型 $P$ 都无法知道，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？本文要讨论的蒙特卡罗(Monte-Calo, MC)就是一种可行的方法。</p>
<p>蒙特卡罗法这一篇对应Sutton书的第五章和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-不基于模型的强化学习问题定义">1. 不基于模型的强化学习问题定义</h1>
<p>在动态规划法中，强化学习的两个问题是这样定义的:</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的6个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>可见, 模型状态转化概率矩阵 $P$ 始终是已知的，即MDP已知，对于这样的强化学习问题，我们一般称为<mark>基于模型的强化学习</mark>问题。</p>
<p>不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵 $P$ ，这时如果仍然需要我们求解强化学习问题，那么这就是<mark>不基于模型的强化学习</mark>问题了。它的两个问题一般的定义是：</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$ , 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>本文要讨论的蒙特卡罗法就是上述不基于模型的强化学习问题。</p>
<h1 id="2-蒙特卡罗法求解特点">2. 蒙特卡罗法求解特点</h1>
<p>蒙特卡罗这个词之前的博文也讨论过，尤其是在之前的<a href="https://www.cnblogs.com/pinard/p/MCMC%28%e4%b8%80%29%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%96%b9%e6%b3%95"target="_blank" rel="external nofollow noopener noreferrer">MCMC系列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中。它是一种通过采样近似求解问题的方法。这里的蒙特卡罗法虽然和MCMC不同，但是采样的思路还是一致的。那么如何采样呢？</p>
<p>蒙特卡罗法通过采样若干经历完整的状态序列(episode)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。</p>
<p>从特卡罗法法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。</p>
<h1 id="3-蒙特卡罗法求解强化学习预测问题">3. 蒙特卡罗法求解强化学习预测问题</h1>
<p>这里我们先来讨论蒙特卡罗法求解强化学习预测问题的方法，即策略评估。一个给定策略 $π$ 的完整有T个状态的状态序列如下：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</p>
<p>回忆下<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中对于价值函数 $v_π(s)$的定义:</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(G_t|S_t=s)=\mathbb{E}_\pi(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t=s)$$</p>
<p>可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解，也就是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>$$v_\pi(s)\approx average(G_t),s.t.S_t=s$$</p>
<p>可以看出，预测问题的求解思路还是很简单的。不过有几个点可以优化考虑。</p>
<ul>
<li>
<p>第一个点是: 同样一个状态可能在一个完整的状态序列中重复出现，那么该状态的收获该如何计算？有两种解决方法。第一种是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：首次访问(first visit) 和每次访问(every visit) 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p>
</li>
<li>
<p>第二个点是累进更新平均值(incremental mean)。在上面预测问题的求解公式里，我们有一个average的公式，意味着要保存所有该状态的收获值之和最后取平均。这样浪费了太多的存储空间。一个较好的方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。通过下面的公式就很容易理解这个过程：</p>
</li>
</ul>
<p>$$\mu_k=\frac1k\sum_{j=1}^kx_j=\frac1k(x_k+\sum_{j=1}^{k-1}x_j)=\frac1k(x_k+(k-1)\mu_{k-1})=\mu_{k-1}+\frac1k(x_k-\mu_{k-1})$$</p>
<p>这样上面的状态价值公式就可以改写成：</p>
<p>$$N(S_t)=N(S_t)+1$$</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>这样我们无论数据量是多还是少，算法需要的内存基本是固定的 。</p>
<p>有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数 $N(S_t)$,这时我们可以用一个系数 $α$ 来代替，即：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t))$$</p>
<p>对于动作价值函数 $Q(S_t,A_t)$,也是类似的，比如对上面最后一个式子，动作价值函数版本为：</p>
<p>$$Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t))$$</p>
<p>以上就是蒙特卡罗法求解预测问题的整个过程，下面我们来看控制问题求解。</p>
<h1 id="4-蒙特卡罗法求解强化学习控制问题">4. 蒙特卡罗法求解强化学习控制问题</h1>
<p>蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。回忆下动态规划价值迭代的的思路， 每轮迭代先做策略评估，计算出价值 $v_k(s)$ ，然后基于据一定的方法（比如贪婪法）更新当前策略 $π$。最后得到最优价值函数 $v∗$ 和最优策略 $π∗$。</p>
<p>和动态规划比，蒙特卡罗法不同之处体现在三点:</p>
<ul>
<li>一是预测问题策略评估的方法不同，这个第三节已经讲了。</li>
<li>第二是蒙特卡罗法一般是优化最优动作价值函数 $q∗$，而不是状态价值函数 $v∗$。</li>
<li>三是动态规划一般基于贪婪法更新策略。而蒙特卡罗法一般采用 $ϵ−$贪婪法更新。这个 $ϵ$ 就是我们在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中讲到的第8个模型要素 $ϵ$。$ϵ−$贪婪法通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 $m$ 个可选行为中选择行为。用公式可以表示为：
$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</li>
</ul>
<p>在实际求解控制问题时，为了使算法可以收敛，一般 $ϵ$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Mento Carlo 搜索过程示意</div>
</center>
<br>
<h1 id="5-蒙特卡罗法控制问题算法流程">5. 蒙特卡罗法控制问题算法流程</h1>
<p>在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。在线和离线的区别我们在后续的文章里面会讲。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。</p>
<p>在线蒙特卡罗法求解强化学习控制问题的算法流程如下:</p>
<ul>
<li>输入：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率$ϵ$</li>
<li>输出：最优的动作价值函数 $q∗$ 和最优策略 $π∗$</li>
<li>
<ol>
<li>初始化所有的动作价值 $Q(s,a)=0$ ， 状态次数 $N(s,a)=0$，采样次数 $k=0$，随机初始化一个策略 $π$</li>
</ol>
</li>
<li>
<ol start="2">
<li>$k=k+1$, 基于策略 $π$ 进行第k次蒙特卡罗采样，得到一个完整的状态序列:
$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</li>
</ol>
</li>
<li>
<ol start="3">
<li>对于该状态序列里出现的每一状态行为对 $(S_t,A_t)$，计算其收获 $G_t$, 更新其计数 $N(s,a)$ 和行为价值函数 $Q(s,a)$：
$$\begin{gathered}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T \\\\
N(S_t,A_t)=N(S_t,A_t)+1 \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\frac1{N(S_t,A_t)}(G_t-Q(S_t,A_t))
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="4">
<li>基于新计算出的动作价值，更新当前的 $ϵ−$贪婪策略：
$$\begin{gathered}
\epsilon=\frac1k \\\\
\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;ifa^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="5">
<li>如果所有的 $Q(s,a)$ 收敛，则对应的所有 $Q(s,a)$ 即为最优的动作价值函数 $q∗$。对应的策略 $π(a|s)$ 即为最优策略 $π∗$。否则转到第二步。</li>
</ol>
</li>
</ul>
<h1 id="6-蒙特卡罗法求解强化学习问题小结">6. 蒙特卡罗法求解强化学习问题小结</h1>
<p>蒙特卡罗法是我们第二个讲到的求解强化问题的方法，也是第一个不基于模型的强化问题求解方法。它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点，这就是它每次采样都需要一个完整的状态序列。如果我们没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了， 也就是说，我们还需要寻找其他的更灵活的不基于模型的强化问题求解方法。</p>
<p>下一篇我们讨论用时序差分方法来求解强化学习预测和控制问题的方法。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9492980.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [6] | 时序差分在线控制算法SARSA</title><link>https://jianye0428.github.io/posts/rl_learning_note_6/</link><pubDate>Thu, 22 Feb 2024 16:29:33 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_6/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。</p>
<p>SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-sarsa算法的引入">1. SARSA算法的引入</h1>
<p>SARSA算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解不需要环境的状态转化模型，是<strong>不基于模型的强化学习问题</strong>求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作。而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。</p>
<p>我们的SARSA算法，属于在线控制这一类，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是 $ϵ−$贪婪法，在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们对于 $ϵ−$贪婪法有详细讲解，即通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 m 个可选行为中选择行为。用公式可以表示为：</p>
<p>$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</p>
<p>π(a|s)={ϵ/m+1−ϵifa∗=argmaxa∈AQ(s,a)ϵ/melse�(�|�)={�/�+1−����∗=arg⁡max�∈��(�,�)�/�����</p>
<h1 id="2-sarsa算法概述">2. SARSA算法概述</h1>
<p>作为SARSA算法的名字本身来说，它实际上是由 $S,A,R,S,A$ 几个字母组成的。而 $S,A,R$ 分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">SARSA Transition</div>
</center>
<br>
<p>在迭代的时候，我们首先基于 $ϵ−$贪婪法在当前状态 $S$ 选择一个动作 $A$ ，这样系统会转到一个新的状态 $S′$, 同时给我们一个即时奖励 $R$ , 在新的状态 $S′$，我们会基于 $ϵ−$贪婪法在状态 $S′$ 选择一个动作 $A′$，但是注意这时候我们并不执行这个动作 $A′$，只是用来更新的我们的价值函数，价值函数的更新公式是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</p>
<p>其中，$γ$ 是衰减因子，$α$ 是迭代步长。这里和蒙特卡罗法求解在线控制问题的迭代公式的区别主要是，收获 $G_t$的表达式不同，对于时序差分，收获 $G_t$的表达式是 $R+\gamma Q(S&rsquo;,A&rsquo;)$ 。这个价值函数更新的贝尔曼公式我们在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节有详细讲到。</p>
<p>除了收获 $G_t$的表达式不同，SARSA算法和蒙特卡罗在线控制算法基本类似。</p>
<h1 id="3-sarsa算法流程">3. SARSA算法流程</h1>
<p>下面我们总结下SARSA算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出：所有的状态和动作对应的价值 $Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。设置 $A$ 为 $ϵ−$贪婪法在当前状态$S$ 选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$ ,得到新状态 $S′$ 和 奖励 $R$</li>
<li>c) 用 $\epsilon-$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S′$, $A=A′$</li>
<li>f) 如果 $S′$ 是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>这里有一个要注意的是，步长 $α$一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数 $Q$ 可以收敛。当 $Q$ 收敛时，我们的策略 $ϵ−$贪婪法也就收敛了。</p>
<h1 id="4-sarsa算法实例windy-gridworld">4. SARSA算法实例：Windy GridWorld</h1>
<p>下面我们用一个著名的实例Windy GridWorld来研究SARSA算法。</p>
<p>如下图一个10×7的长方形格子世界，标记有一个起始位置 S 和一个终止目标位置 G，格子下方的数字表示对应的列中一定强度的风。当个体进入该列的某个格子时，会按图中箭头所示的方向自动移动数字表示的格数，借此来模拟世界中风的作用。同样格子世界是有边界的，个体任意时刻只能处在世界内部的一个格子中。个体并不清楚这个世界的构造以及有风，也就是说它不知道格子是长方形的，也不知道边界在哪里，也不知道自己在里面移动移步后下一个格子与之前格子的相对位置关系，当然它也不清楚起始位置、终止目标的具体位置。但是个体会记住曾经经过的格子，下次在进入这个格子时，它能准确的辨认出这个格子曾经什么时候来过。格子可以执行的行为是朝上、下、左、右移动一步，每移动一步只要不是进入目标位置都给予一个 -1 的惩罚，直至进入目标位置后获得奖励 0 同时永久停留在该位置。现在要求解的问题是个体应该遵循怎样的策略才能尽快的从起始位置到达目标位置。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Windy GridWorld</div>
</center>
<br>
<p>逻辑并不复杂，完整的代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">我的github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。这里我主要看一下关键部分的代码。</p>
<p>算法中第2步步骤a,初始化 $S$,使用 $ϵ−$贪婪法在当前状态 $S$ 选择的动作的过程：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤b,在状态S�执行当前动作A�,得到新状态S′�′的过程，由于奖励不是终止就是-1，不需要单独计算：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤c,用 $ϵ−$贪婪法在状态 $S&rsquo;$选择新的动作 $A′$的过程：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤d,e, 更新价值函数 $Q(S,A)$ 以及更新当前状态动作的过程：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">  <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl"><span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span></span></span></code></pre></td></tr></table>
</div>
</div><p>代码很简单，相信大家对照算法，跑跑代码，可以很容易得到这个问题的最优解，进而搞清楚SARSA算法的整个流程。</p>
<h1 id="5-sarsaλ">5. SARSA(λ)</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讲到了多步时序差分 $TD(λ)$ 的价值函数迭代方法，那么同样的，对应的多步时序差分在线控制算法，就是我们的 $SARSA(λ)$。</p>
<p>$TD(\lambda)$有前向和后向两种价值函数迭代方式，当然它们是等价的。在控制问题的求解时，基于反向认识的 $SARSA(\lambda)$算法将可以有效地在线学习，数据学习完即可丢弃。因此 $SARSA(\lambda)$算法默认都是基于反向来进行价值函数迭代。</p>
<p>在上一篇我们讲到了$TD(\lambda)$状态价值函数的反向迭代，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(S)\end{gathered}$$</p>
<p>对应的动作价值函数的迭代公式可以找样写出，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\\\Q(S_t,A_t)=Q(S_t,A_t)+\alpha\delta_tE_t(S,A)\end{gathered}$$</p>
<p>除了状态价值函数 $Q(S,A)$ 的更新方式，多步参数 $λ$ 以及反向认识引入的效用迹 $E(S,A)$ ，其余算法思想和 $SARSA$ 类似。这里我们总结下 $SARSA(λ)$的算法流程。　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率$ϵ$, 多步参数$λ$</li>
<li>输出：所有的状态和动作对应的价值$Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 对于终止状态其 $Q$值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化所有状态动作的效用迹 $E$ 为0，初始化S为当前状态序列的第一个状态。设置$A$为 $ϵ−$贪婪法在当前状态 $S$选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>c) 用$ϵ−$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新效用迹函数 $E(S,A)$和TD误差 $δ$:
<ul>
<li>$$\begin{gathered}E(S,A)=E(S,A)+1\\\\\delta=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\end{gathered}$$</li>
</ul>
</li>
<li>e) 对当前序列所有出现的状态s和对应动作 $a$, 更新价值函数 $Q(s,a)$和效用迹函数 $E(s,a)$:
<ul>
<li>$$\begin{gathered}Q(s,a)=Q(s,a)+\alpha\delta E(s,a)\\\\E(s,a)=\gamma\lambda E(s,a)\end{gathered}$$</li>
</ul>
</li>
<li>f) $S=S&rsquo;$, $A=A'$</li>
<li>g) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>对于步长$α$，和SARSA一样，一般也需要随着迭代的进行逐渐变小才能保证动作价值函数$Q$收敛。</p>
<h1 id="6-sarsa小结">6. SARSA小结</h1>
<p>SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和蒙特卡罗法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。</p>
<p>但是SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在 SARSA 算法中，$Q(S,A)$ 的值使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。</p>
<p>下一篇我们讨论SARSA的姊妹算法，时序差分离线控制算法Q-Learning。</p>
]]></description></item><item><title>RL学习笔记 [5] | 用时序差分法（TD）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_5/</link><pubDate>Thu, 22 Feb 2024 17:25:21 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_5/</guid><description><![CDATA[<h1 id="0-引言">0 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化学习问题的方法：时序差分(Temporal-Difference, TD)。</p>
<p>时序差分这一篇对应Sutton书的第六章部分和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-时序差分td简介">1. 时序差分TD简介</h1>
<p>时序差分法和蒙特卡罗法类似，都是<strong>不基于模型的强化学习问题</strong>求解方法。所以在上一篇定义的不基于模型的强化学习控制问题和预测问题的定义，在这里仍然适用。</p>
<p>预测问题：即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
<p>控制问题：也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
<p>回顾蒙特卡罗法中计算状态收获的方法是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>而对于时序差分法来说，我们没有完整的状态序列，只有部分的状态序列，那么如何可以近似求出某个状态的收获呢？回顾<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中的贝尔曼方程：</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s)$$</p>
<p>这启发我们可以用 $R_{t+1}+\gamma v(S_{t+1})$ 来近似的代替收获 $G_t$,一般我们把 $R_{t+1}+\gamma V(S_{t+1})$ 称为TD目标值。$R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 称为TD误差，将用TD目标值近似代替收获 $G(t)$ 的过程称为引导(bootstrapping)。这样我们只需要两个连续的状态与对应的奖励，就可以尝试求解强化学习问题了。</p>
<p>现在我们有了自己的近似收获 $G_t$ 的表达式，那么就可以去求解时序差分的预测问题和控制问题了。</p>
<h1 id="2-时序差分td的预测问题求解">2. 时序差分TD的预测问题求解</h1>
<p>时序差分的预测问题求解和蒙特卡罗法类似，但是主要有两个不同点。一是收获 $G_t$ 的表达式不同，时序差分 $G(t)$ 的表达式为：</p>
<p>$$G(t)=R_{t+1}+\gamma V(S_{t+1})$$</p>
<p>二是迭代的式子系数稍有不同，回顾蒙特卡罗法的迭代式子是：</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>由于在时序差分我们没有完整的序列，也就没有对应的次数 $N(S_t)$ ,一般就用一个[0,1]的系数 $α$ 代替。这样时序差分的价值函数迭代式子是：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t)) \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t)) $$</p>
<p>这里我们用一个简单的例子来看看蒙特卡罗法和时序差分法求解预测问题的不同。</p>
<p>假设我们的强化学习问题有A,B两个状态，模型未知，不涉及策略和行为。只涉及状态转化和即时奖励。一共有8个完整的状态序列如下：</p>
<p>　　① A,0,B,0 ②B,1 ③B,1 ④ B,1 ⑤ B,1 ⑥B,1 ⑦B,1 ⑧B,0</p>
<p>只有第一个状态序列是有状态转移的，其余7个只有一个状态。设置衰减因子 $γ=1$。</p>
<p>首先我们按蒙特卡罗法来求解预测问题。由于只有第一个序列中包含状态A，因此A的价值仅能通过第一个序列来计算，也就等同于计算该序列中状态A的收获：</p>
<p>$$V(A)=G(A)=R_A+\gamma R_B=0$$</p>
<p>对于B，则需要对其在8个序列中的收获值来平均，其结果是6/8。</p>
<p><strong>再来看看时序差分法求解的过程</strong>。其收获是在计算状态序列中某状态价值时是应用其后续状态的预估价值来计算的，对于B来说，它总是终止状态，没有后续状态，因此它的价值直接用其在8个序列中的收获值来平均，其结果是6/8。</p>
<p>对于A，只在第一个序列出现，它的价值为：</p>
<p>$$V(A)=R_A+\gamma V(B)=\frac68$$</p>
<p>从上面的例子我们也可以看到蒙特卡罗法和时序差分法求解预测问题的区别。</p>
<p>一是时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。</p>
<p>二是时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。</p>
<p>三是虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。</p>
<p>从上面的描述可以看出时序差分法的优势比较大，因此现在主流的强化学习求解方法都是基于时序差分的。后面的文章也会主要基于时序差分法来扩展讨论。</p>
<h1 id="3-n步时序差分">3. n步时序差分</h1>
<p>在第二节的时序差分法中，我们使用了用 $R_{t+1}+\gamma v(S_{t+1})$ 来近似的代替收获 $G_t$。即向前一步来近似我们的收获 $G_{t}$,那么能不能向前两步呢？当然可以，这时我们的收获 $G_t$ 的近似表达式为：</p>
<p>$$G_t^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2})$$</p>
<p>从两步，到三步，再到n步，我们可以归纳出n步时序差分收获 $G^{(n)}_t$表达式为：$$G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})$$</p>
<p>当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了。</p>
<p>对于n步时序差分来说，和普通的时序差分的区别就在于收获的计算方式的差异。那么既然有这个n步的说法，那么n到底是多少步好呢？如何衡量n的好坏呢？我们在下一节讨论。</p>
<h1 id="4-tdλ">4. TD(λ)</h1>
<p>n步时序差分选择多少步数作为一个较优的计算参数是需要尝试的超参数调优问题。为了能在不增加计算复杂度的情况下综合考虑所有步数的预测，我们引入了一个新[0,1]的参数 $\lambda$,定义入—收获是 $n$ 从 $1$ 到 $\infty$ 所有步的收获乘以权重的和。每一步的权重是 $(1-\lambda)\lambda^{n-1}$,这样 $\lambda-$收获的计算公式表示为:</p>
<p>$$G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}$$</p>
<p>进而我们可以得到 $TD(λ)$ 的价值函数的迭代公式：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t^\lambda-V(S_t)) \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t^\lambda-Q(S_t,A_t)) $$</p>
<p>每一步收获的权重定义为 $(1−λ)λ^{n−1}$ 的原因是什么呢？其图像如下图所示，可以看到随着n的增大，其第n步收获的权重呈几何级数的衰减。当在T时刻到达终止状态时，未分配的权重全部给予终止状态的实际收获值。这样可以使一个完整的状态序列中所有的n步收获的权重加起来为1，离当前状态越远的收获其权重越小。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">TD(λ)</div>
</center>
<br>
<p>从前向来看 $TD(λ)$， 一个状态的价值 $V(St)$由 $G_t$得到，而Gt��又间接由所有后续状态价值计算得到，因此可以认为更新一个状态的价值需要知道所有后续状态的价值。也就是说，必须要经历完整的状态序列获得包括终止状态的每一个状态的即时奖励才能更新当前状态的价值。这和蒙特卡罗法的要求一样，因此TD(λ)��(�)有着和蒙特卡罗法一样的劣势。当 $λ=0$ 时,就是第二节讲到的普通的时序差分法，当 $λ=1$ 时,就是蒙特卡罗法。</p>
<p>从反向来看 $TD(λ)$，它可以分析我们状态对后续状态的影响。比如老鼠在依次连续接受了3 次响铃和1 次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为频率启发(frequency heuristic) 式；而把电击归因于最近少数几次状态的影响，则称为就近启发(recency heuristic) 式。</p>
<p>如果给每一个状态引入一个数值：效用(eligibility, E) 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发。而所有状态的效用值总称为效用迹(eligibility traces,ES)。定义为：</p>
<p>$$ E_0(s)=0 \\\\ \left.E_t(s)=\gamma\lambda E_{t-1}(s)+1(S_t=s)=\left\\{\begin{array}{ll}0&amp;t&lt;k\\\\(\gamma\lambda)^{t-k}&amp;t\geq k\end{array}\right.\right.,\quad s.t.\quad\lambda,\gamma\in[0,1],s\textit{ is visited once at time k}$$</p>
<p>此时我们$TD(λ)$的价值函数更新式子可以表示为：</p>
<p>$$\delta_t=R_{t+1}+\gamma v(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(s)$$</p>
<p>也许有人会问，这前向的式子和反向的式子看起来不同啊，是不是不同的逻辑呢？其实两者是等价的。现在我们从前向推导一下反向的更新式子。</p>
<p>$$\begin{aligned}
G_t^\lambda-V(S_t)&amp; =-V(S_t)+(1-\lambda)\lambda^0(R_{t+1}+\gamma V(S_{t+1})) &amp;&amp; \text{(1)}  \\\\
&amp;+(1-\lambda)\lambda^1(R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2}))&amp;&amp; (2)  \\\\
&amp;+(1-\lambda)\lambda^2(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3V(S_{t+3}))&amp;&amp; (3)  \\\\
&amp;+\ldots &amp;&amp; \text{(4)}  \\\\
&amp;=-V(S_t)+(\gamma\lambda)^0(R_{t+1}+\gamma V(S_{t+1})-\gamma\lambda V(S_{t+1}))&amp;&amp; (5)  \\\\
&amp;+(\gamma\lambda)^1(R_{t+2}+\gamma V(S_{t+2})-\gamma\lambda V(S_{t+2}))&amp;&amp; \text{(6)}  \\\\
&amp;+(\gamma\lambda)^2(R_{t+3}+\gamma V(S_{t+3})-\gamma\lambda V(S_{t+3}))&amp;&amp; \text{(7)}  \\\\
&amp;\begin{array}{c}+\ldots\end{array}&amp;&amp; \text{(8)}  \\\\
&amp;=(\gamma\lambda)^0(R_{t+1}+\gamma V(S_{t+1})-V(S_t))&amp;&amp; \left(9\right)  \\\\
&amp;+(\gamma\lambda)^1(R_{t+2}+\gamma V(S_{t+2})-V(S_{t+1}))&amp;&amp; \text{(10)}  \\\\
&amp;+(\gamma\lambda)^2(R_{t+3}+\gamma V(S_{t+3})-V(S_{t+2}))&amp;&amp; (11)  \\\\
&amp;\begin{array}{c}+\ldots\end{array}&amp;&amp; (12)  \\\\
&amp;=\delta_t+\gamma\lambda\delta_{t+1}+(\gamma\lambda)^2\delta_{t+2}+\ldots &amp;&amp; (13)
\end{aligned}$$</p>
<p>可以看出前向TD误差和反向的TD误差实际上一致的。</p>
<h1 id="5-时序差分的控制问题求解">5. 时序差分的控制问题求解</h1>
<p>现在我们回到普通的时序差分，来看看它控制问题的求解方法。回想上一篇蒙特卡罗法在线控制的方法，我们使用的是$ϵ−$贪婪法来做价值迭代。对于时序差分，我们也可以用$ϵ−$贪婪法来价值迭代，和蒙特卡罗法在线控制的区别主要只是在于收获的计算方式不同。时序差分的在线控制(on-policy)算法最常见的是SARSA算法，我们在下一篇单独讲解。</p>
<p>而除了在线控制，我们还可以做离线控制(off-policy)，离线控制和在线控制的区别主要在于在线控制一般只有一个策略(最常见的是$ϵ−$贪婪法)。而离线控制一般有两个策略，其中一个策略(最常见的是$ϵ−$贪婪法)用于选择新的动作，另一个策略(最常见的是贪婪法)用于更新价值函数。时序差分的离线控制算法最常见的是Q-Learning算法，我们在下下篇单独讲解。</p>
<h1 id="6-时序差分小结">6. 时序差分小结</h1>
<p>时序差分和蒙特卡罗法比它更加灵活，学习能力更强，因此是目前主流的强化学习求解问题的方法，现在绝大部分强化学习乃至深度强化学习的求解都是以时序差分的思想为基础的。因此后面我们会重点讨论。</p>
<p>下一篇我们会讨论时序差分的在线控制算法SARSA。</p>
]]></description></item><item><title>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_8/</link><pubDate>Fri, 23 Feb 2024 13:17:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_8/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在强化学习系列的<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">前七篇<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。</p>
<p>Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。</p>
<h1 id="1-为何需要价值函数的近似表示">1. 为何需要价值函数的近似表示</h1>
<p>在之前讲到了强化学习求解方法，无论是动态规划DP，蒙特卡罗方法MC，还是时序差分TD，使用的状态都是离散的有限个状态集合 $S$。此时问题的规模比较小，比较容易求解。但是假如我们遇到复杂的状态集合呢？甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。　　　　</p>
<p>比如经典的冰球世界(PuckWorld)强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间步时长的一个大小固定的力，借此来改变大圆的速度。环境会在每一个时间步内告诉个体当前的水平与垂直坐标、当前的速度在水平和垂直方向上的分量以及目标的水平和垂直坐标共6项数据，奖励值为个体与目标两者中心距离的负数，也就是距离越大奖励值越低且最高奖励值为0。</p>
<p>在这个问题中，状态是一个6维的向量，并且是连续值。没法直接用之前离散集合的方法来描述状态。当然，你可以说，我们可以把连续特征离散化。比如把这个冰球场100x100的框按1x1的格子划分成10000个格子，那么对于运动员的坐标和冰球的坐标就有$10^4∗10^4=10^8$次种，如果再加上个体速度的分量就更是天文数字了，此时之前讲过的强化学习方法都会因为问题的规模太大而无法使用。怎么办呢？必须要对问题的建模做修改了，而价值函数的近似表示就是一个可行的方法。</p>
<h1 id="2-价值函数的近似表示方法">2. 价值函数的近似表示方法</h1>
<p>由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数 $\hat{v}$, 这个函数由参数 $w$ 描述，并接受状态 $s$ 作为输入，计算后得到状态 $s$ 的价值，即我们期望：</p>
<p>$$\hat{v}(s,w)\approx v_\pi(s)$$</p>
<p>类似的，引入一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到动作价值，即我们期望：</p>
<p>$$\hat{q}(s,a,w)\approx q_\pi(s,a)$$</p>
<p>价值函数近似的方法很多，比如最简单的线性表示法，用 $ϕ(s)$表示状态 $s$ 的特征向量，则此时我们的状态价值函数可以近似表示为：</p>
<p>$$\hat{v}(s,w)=\phi(s)^Tw$$</p>
<p>当然，除了线性表示法，我们还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。因此后面我们的近似表达方法如果没有特别提到，都是指的神经网络的近似表示。</p>
<p>对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络拟合价值函数</div>
</center>
<br>
<p>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值 $\hat{v}(s,w)$。对于动作价值函数，有两种方法，一种是输入状态 $s$ 的特征向量和动作 $a$，输出对应的动作价值 $\hat{q}(s,a,w)$，另一种是只输入状态 $s$ 的特征向量，动作集合有多少个动作就有多少个输出 $\hat{q}(s,ai,w)$。这里隐含了我们的动作是有限个的离散动作。</p>
<p>对于我们前一篇讲到的Q-Learning算法，我们现在就价值函数的近似表示来将其改造，采用上面右边的第三幅图的动作价值函数建模思路来做，现在我们叫它Deep Q-Learning。</p>
<h1 id="3-deep-q-learning算法思路">3. Deep Q-Learning算法思路</h1>
<p>Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值s和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p>
<p>DQN的输入是我们的状态s对应的状态向量 $ϕ(s)$， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p>
<p>DQN主要使用的技巧是经验回放(experience replay), 即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是<strong>经验回放</strong>。</p>
<p>通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数 $w$，当 $w$ 收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。</p>
<p>下面我们总结下DQN的算法流程，基于NIPS 2013 DQN。　　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, Q网络结构, 批量梯度下降的样本数 $m$。</li>
<li>输出：Q网络参数
<ul>
<li>
<ol>
<li>随机初始化$Q$网络的所有参数 $w$，基于 $w$初始化所有的状态和动作对应的价值 $Q$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S&rsquo;),is_end\\}$这个五元组存入经验回放集合D</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\mathrm{~}is\mathrm{~}true\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\mathrm{~}is\mathrm{~}false\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{i=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的 $f$步和 $g$步的 $Q$值计算也都需要通过 $Q$网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-deep-q-learning实例">4. Deep Q-Learning实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。这里使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码参考了知乎上的一个<a href="https://zhuanlan.zhihu.com/p/21477488"target="_blank" rel="external nofollow noopener noreferrer">DQN实例<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，修改了代码中的一些错误，并用最新的Python3.6+Tensorflow1.8.0运行。要跑代码需要安装OpenAI的Gym库，使用<code>pip install gym</code>即可。</p>
<p>代码使用了一个三层的神经网络，输入层，一个隐藏层和一个输出层。下面我们看看关键部分的代码。</p>
<p>算法第2步的步骤b通过$ϵ−$贪婪法选择动作的代码如下，注意每次我们$ϵ−$贪婪法后都会减小$ϵ$值。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">egreedy_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:[</span><span class="n">state</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">})[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤c在状态S�执行当前动作A�的代码如下，这个交互是由Gym完成的。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Define reward for agent</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mf">0.1</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤d保存经验回放数据的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">perceive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span><span class="n">one_hot_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">REPLAY_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤f,g计算目标Q值，并更新Q网络的代码如下：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 1: obtain random minibatch from replay memory</span>
</span></span><span class="line"><span class="cl">  <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">y_input</span><span class="p">:</span><span class="n">y_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">action_input</span><span class="p">:</span><span class="n">action_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">state_batch</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们在每100轮迭代完后会去玩10次交互测试，计算10次的平均奖励。运行了代码后，我的3000轮迭代的输出如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 12.2
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.4
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 10.4
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.5
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 11.6
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 12.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 29.6
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 48.1
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 85.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 169.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>大概到第1000次迭代后，算法已经收敛，达到最高的200分。当然由于是$ϵ−$探索，每次前面的输出可能不同，但最后应该都可以收敛到200的分数。当然由于DQN不保证绝对的收敛，所以可能到了200分后还会有抖动。</p>
<h1 id="5-deep-q-learning小结">5. Deep Q-Learning小结　　　　</h1>
<p>DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。但是DQN有个问题，就是它并不一定能保证Q网络的收敛，也就是说，我们不一定可以得到收敛后的Q网络参数。这会导致我们训练出的模型效果很差。</p>
<p>针对这个问题，衍生出了DQN的很多变种，比如Nature DQN(NIPS 2015), Double DQN，Dueling DQN等。这些我们在下一篇讨论。</p>
]]></description></item><item><title>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_9/</link><pubDate>Fri, 23 Feb 2024 13:17:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_9/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9714655.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（八）价值函数的近似表示与Deep Q-Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Nature DQN的论文。</p>
<h1 id="1-dqnnips-2013的问题">1. DQN(NIPS 2013)的问题</h1>
<p>在上一篇我们已经讨论了DQN(NIPS 2013)的算法原理和代码实现，虽然它可以训练像CartPole这样的简单游戏，但是有很多问题。这里我们先讨论第一个问题。</p>
<p>注意到DQN(NIPS 2013)里面，我们使用的目标 $Q$值的计算方式：</p>
<p>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>这里目标Q值的计算使用到了当前要训练的Q网络参数来计算$Q(\phi(S_j^{\prime}),A_j^{\prime},w)$，而实际上，我们又希望通过 $y_j$来后续更新 $Q$网络参数。这样两者循环依赖，迭代起来两者的相关性就太强了。不利于算法的收敛。</p>
<p>因此，一个改进版的DQN: Nature DQN尝试<strong>用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系</strong>。下面我们来看看Nature DQN是怎么做的。</p>
<h1 id="2-nature-dqn的建模">2. Nature DQN的建模</h1>
<p>Nature DQN的两个Q网络分别命名为当前Q网络和目标Q网络。</p>
<p>Nature DQN使用了两个Q网络，一个<strong>当前Q网络</strong>$Q$用来选择动作，更新模型参数，另一个<strong>目标Q网络</strong> $Q&rsquo;$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。</p>
<p>要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。</p>
<p>Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。</p>
<h1 id="3-nature-dqn的算法流程">3. Nature DQN的算法流程</h1>
<p>下面我们来总结下Nature DQN的算法流程， 基于DQN NIPS 2015：</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率$C$。</p>
<p>输出：$Q$网络参数</p>
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S′),is_end\\}$这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，计算当前目标Q值 $y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果 $i%C=1$, 则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$ 需要随着迭代的进行而变小。</p>
<h1 id="4-nature-dqn算法实例">4. Nature DQN算法实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Nature DQN和上一节的NIPS 2013 DQN的代码的不同之处。</p>
<p>首先是Q网络，上一篇的DQN是一个三层的神经网络，而这里我们有两个一样的三层神经网络，一个是当前Q网络，一个是目标Q网络，网络的定义部分如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2t</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对于定期将目标Q网络的参数更新的代码如下面两部分：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">t_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;target_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">e_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;current_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;soft_replacement&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">t_params</span><span class="p">,</span> <span class="n">e_params</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_target_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># update target Q netowrk</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">REPLACE_TARGET_FREQ</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#print(&#39;episode &#39;+str(episode) +&#39;, target Q network params replaced!&#39;)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此外，注意下我们计算目标Q值的部分，这里使用的目标Q网络的参数，而不是当前Q网络的参数：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分基本和上一篇DQN的代码相同。这里给出我跑的某一次的结果:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 9.6
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.0
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 34.8
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 177.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 198.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 193.2
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>注意，由于DQN不保证稳定的收敛，所以每次跑的结果会不同，如果你跑的结果后面仍然收敛的不好，可以把代码多跑几次，选择一个最好的训练结果。</p>
<h1 id="5-nature-dqn总结">5. Nature DQN总结</h1>
<p>Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：</p>
<ul>
<li>1） 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？</li>
<li>2） 随机采样的方法好吗？按道理不同样本的重要性是不一样的。</li>
<li>3） Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？</li>
</ul>
<p>第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN，这三个DQN的改进版我们在下一篇来讨论。</p>
]]></description></item><item><title>强化学习笔记 [11] | Prioritized Replay DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_11/</link><pubDate>Sun, 25 Feb 2024 11:16:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_11/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay DQN。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Prioritized Replay DQN的论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<h1 id="1-prioritized-replay-dqn之前算法的问题">1. Prioritized Replay DQN之前算法的问题</h1>
<p>在Prioritized Replay DQN之前，我们已经讨论了很多种DQN，比如Nature DQN， DDQN等，他们都是通过经验回放来采样，进而做目标Q值的计算的。在采样的时候，我们是一视同仁，在经验回放池里面的所有的样本都有相同的被采样到的概率。</p>
<p>但是注意到在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，由于TD误差小，对反向梯度的计算影响不大。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。</p>
<p>这样如果TD误差的绝对值 $|δ(t)|$较大的样本更容易被采样，则我们的算法会比较容易收敛。下面我们看看Prioritized Replay DQN的算法思路。</p>
<h1 id="2-prioritized-replay-dqn算法的建模">2. Prioritized Replay DQN算法的建模</h1>
<p>Prioritized Replay DQN根据每个样本的TD误差绝对值 $|δ(t)|$，给定该样本的优先级正比于 $|δ(t)|$，将这个优先级的值存入经验回放池。回忆下之前的DQN算法，我们仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。</p>
<p>由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。</p>
<p>具体的SumTree树结构如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">sum_tree 结构图</div>
</center>
<br>
<p>所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。</p>
<p>这样保存有什么好处呢？主要是方便采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样，采样到哪个区间，就是哪个样本。比如我们采样到了26， 在（25-29）这个区间，那么就是第四个叶子节点被采样到。而注意到第三个叶子节点优先级最高，是12，它的区间13-25也是最长的，会比其他节点更容易被采样到。</p>
<p>如果要采样两个样本，我们可以在[0,21],[21,42]两个区间做均匀采样，方法和上面采样一个样本类似。</p>
<p>类似的采样算法思想我们在<a href="https://www.cnblogs.com/pinard/p/7249903.html"target="_blank" rel="external nofollow noopener noreferrer">word2vec原理(三) 基于Negative Sampling的模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第四节中也有讲到。</p>
<p>除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：</p>
<p>$$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>现在我们新的考虑了样本优先级的损失函数是</p>
<p>$$\frac1m\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>其中 $w_j$是第j个样本的优先级权重，由TD误差 $|δ(t)|$归一化得到。</p>
<p>第三个要注意的点就是当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。</p>
<p>除了以上三个部分，Prioritized Replay DQN和DDQN的算法流程相同。</p>
<h1 id="3-prioritized-replay-dqn算法流程">3. Prioritized Replay DQN算法流程</h1>
<p>下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，采样权重系数 $β$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$, SumTree的叶子节点数 $S$。</li>
<li>输出：Q网络参数。</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级 $p_j$为1。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S&rsquo;),is_end}$这个五元组存入SumTree</li>
<li>e) $S=S'$</li>
<li>f) 从SumTree中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，每个样本被采样的概率基于 $P(j)=\frac{p_j}{\sum_i(p_i)}$，损失函数权重 $w_j=(N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left\\{\begin{matrix}R_j&amp;is_end_j\textit{is true}\\\\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{is false}\end{matrix}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\begin{aligned}\frac{1}{m}\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2\end{aligned}$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 重新计算所有样本的TD误差 $\delta_j=y_j-Q(\phi(S_j),A_j,w)$，更新SumTree中所有节点的优先级 $p_j=|\delta_j|$</li>
<li>i) 如果i%C=1,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>j) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-prioritized-replay-ddqn算法流程">4. Prioritized Replay DDQN算法流程</h1>
<p>下面我们给出Prioritized Replay DDQN算法的实例代码。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 代码中的SumTree的结构和经验回放池的结构参考了morvanzhou的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py"target="_blank" rel="external nofollow noopener noreferrer">github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>这里重点讲下和第三节中算法描述不同的地方，主要是 $w_j$的计算。注意到：</p>
<p>$$w_j=\frac{(N<em>P(j))^{-\beta}}{\max_i(w_i)}=\frac{(N</em>P(j))^{-\beta}}{\max_i((N*P(i))^{-\beta})}=\frac{(P(j))^{-\beta}}{\max_i((P(i))^{-\beta})}=(\frac{P_j}{\min_iP(i)})^{-\beta}$$</p>
<p>因此代码里面$w_j$，即ISWeights的计算代码是这样的：</p>
<p><a href="javascript:void%280%29;"></a></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">pri_seg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span> <span class="o">/</span> <span class="n">n</span>       <span class="c1"># priority segment</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_increment_per_sampling</span><span class="p">])</span>  <span class="c1"># max = 1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">min_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">capacity</span><span class="p">:])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>     <span class="c1"># for later calculate ISweight</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">min_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_prob</span> <span class="o">=</span> <span class="mf">0.00001</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">get_leaf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prob</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>
</span></span><span class="line"><span class="cl">    <span class="n">ISWeights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prob</span><span class="o">/</span><span class="n">min_prob</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">b_memory</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上述代码的采样在第二节已经讲到。根据树的优先级的和total_p和采样数n，将要采样的区间划分为n段，每段来进行均匀采样，根据采样到的值落到的区间，决定被采样到的叶子节点。当我们拿到第i段的均匀采样值v以后，就可以去SumTree中找对应的叶子节点拿样本数据，样本叶子节点序号以及样本优先级了。代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree structure and array storage:
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree index:
</span></span></span><span class="line"><span class="cl"><span class="s2">        0         -&gt; storing priority sum
</span></span></span><span class="line"><span class="cl"><span class="s2">      / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">    1     2
</span></span></span><span class="line"><span class="cl"><span class="s2">    / \   / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">  3   4 5   6    -&gt; storing priority for transitions
</span></span></span><span class="line"><span class="cl"><span class="s2">  Array type for storing:
</span></span></span><span class="line"><span class="cl"><span class="s2">  [0,1,2,3,4,5,6]
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">parent_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>     <span class="c1"># the while loop is faster than the method in the reference code</span>
</span></span><span class="line"><span class="cl">    <span class="n">cl_idx</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">parent_idx</span> <span class="o">+</span> <span class="mi">1</span>         <span class="c1"># this leaf&#39;s left and right kids</span>
</span></span><span class="line"><span class="cl">    <span class="n">cr_idx</span> <span class="o">=</span> <span class="n">cl_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cl_idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">):</span>        <span class="c1"># reach bottom, end search</span>
</span></span><span class="line"><span class="cl">      <span class="n">leaf_idx</span> <span class="o">=</span> <span class="n">parent_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>       <span class="c1"># downward search, always search for a higher priority node</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cl_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cr_idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">data_idx</span> <span class="o">=</span> <span class="n">leaf_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">leaf_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">leaf_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了采样部分，要注意的就是当梯度更新完毕后，我们要去更新SumTree的权重，代码如下，注意叶子节点的权重更新后，要向上回溯，更新所有祖先节点的权重。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">batch_update</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">)</span>  <span class="c1"># update priority</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">batch_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">abs_errors</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>  <span class="c1"># convert to abs and avoid 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">clipped_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">abs_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs_err_upper</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">clipped_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">ps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ti</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">change</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># then propagate the change through tree</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">tree_idx</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>    <span class="c1"># this method is faster than the recursive loop in the reference code</span>
</span></span><span class="line"><span class="cl">      <span class="n">tree_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">change</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，和DDQN比，TensorFlow的网络结构流程中多了一个TD误差的计算节点，以及损失函数多了一个ISWeights系数。此外，区别不大。</p>
<h1 id="5-prioritized-replay-dqn小结">5. Prioritized Replay DQN小结</h1>
<p>Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。</p>
<p>下一篇我们讨论DQN家族的另一个优化算法Duel DQN，它将价值Q分解为两部分，第一部分是仅仅受状态但不受动作影响的部分，第二部分才是同时受状态和动作影响的部分，算法的效果也很好。</p>
]]></description></item><item><title>RL学习笔记 [1] | 模型基础</title><link>https://jianye0428.github.io/posts/rl_learning_note_1/</link><pubDate>Wed, 21 Feb 2024 10:38:07 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_1/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>　从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。</p>
<p>　第一篇会从强化学习的基本概念讲起，对应Sutton书的第一章和UCL课程的第一讲。</p>
<h1 id="1-强化学习在机器学习中的位置">1. 强化学习在机器学习中的位置</h1>
<p>强化学习的学习思路和人比较类似，是在实践中学习，比如学习走路，如果摔倒了，那么我们大脑后面会给一个负面的奖励值，说明走的姿势不好。然后我们从摔倒状态中爬起来，如果后面正常走了一步，那么大脑会给一个正面的奖励值，我们会知道这是一个好的走路姿势。那么这个过程和之前讲的机器学习方法有什么区别呢？</p>
<p>强化学习是和监督学习，非监督学习并列的第三种机器学习方法，从下图我们可以看出来。</p>
  <br>
  <center>
    
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">RL、SL、UL与ML的区别联系</div>
  </center>
  <br>
<p>与监督学习相比，强化学习最大的区别是它没有监督学习已经准备好的训练数据输出值。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的，比如上面的例子里走路摔倒了才得到大脑的奖励值。同时，强化学习的每一步与时间顺序前后关系紧密。而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。</p>
<p>再来看看强化学习和非监督学习的区别。也还是在奖励值这个地方。非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。</p>
<h1 id="2-强化学习的建模">2. 强化学习的建模</h1>
<p>我们现在来看看强化学习这样的问题我们怎么来建模，简单的来说，是下图这样的：</p>
  <br>
  <center>
    
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">大脑与环境的交互</div>
  </center>
  <br>
<p>上面的大脑代表我们的算法执行个体，我们可以操作个体来做决策，即选择一个合适的动作（Action）$A_t$。下面的地球代表我们要研究的环境,它有自己的状态模型，我们选择了动作 $A_t$ 后，环境的状态(State)会变，我们会发现环境状态已经变为 $S_{t+1}$,同时我们得到了我们采取动作 $A_t$ 的延时奖励(Reward) $R_{t+1}$。然后个体可以继续选择下一个合适的动作，然后环境的状态又会变，又有新的奖励值&hellip;这就是强化学习的思路。</p>
<p>那么我们可以整理下这个思路里面出现的强化学习要素。</p>
<ul>
<li>
<p>第一个是环境的状态 $S$, $t$ 时刻环境的状态 $S_t$ 是它的环境状态集中某一个状态。</p>
</li>
<li>
<p>第二个是个体的动作 $A$, $t$ 时刻个体采取的动作 $A_t$ 是它的动作集中某一个动作。</p>
</li>
<li>
<p>第三个是环境的奖励 $R$, $t$ 时刻个体在状态 $S_t$ 采取的动作 $A_t$ 对应的奖励 $R_{t+1}$ 会在 $t+1$ 时刻得到。</p>
</li>
<li>
<p>第四个是个体的策略(policy) $π$,它代表个体采取动作的依据，即个体会依据策略 $π$ 来选择动作。最常见的策略表达方式是一个条件概率分布 $π(a|s)$, 即在状态 $s$ 时采取动作 $a$ 的概率。即 $π(a|s)=P(A_t=a|S_t=s)$.此时概率大的动作被个体选择的概率较高。</p>
</li>
<li>
<p>第五个是个体在策略 $π$ 和状态 $s$ 时，采取行动后的价值(value)，一般用 $v_π(s)$ 表示。这个价值一般是一个期望函数。虽然当前动作会给一个延时奖励 $R_{t+1}$,但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表到了 $t+1$, $t+2$,&hellip;时刻的后续奖励也高。比如下象棋，我们可以某个动作可以吃掉对方的车，这个延时奖励是很高，但是接着后面我们输棋了。此时吃车的动作奖励值高但是价值并不高。因此我们的价值要综合考虑当前的延时奖励和后续的延时奖励。价值函数 $v_{\pi}(s)$ 一般可以表示为下式，不同的算法会有对应的一些价值函数变种，但思路相同。
$$v_{\pi}(s)=\mathbb{E}_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+&hellip;|S_t=s)$$</p>
</li>
<li>
<p>其中 $γ$ 是第六个模型要素，即奖励衰减因子，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。大多数时候，我们会取一个0到1之间的数字，即当前延时奖励的权重比后续奖励的权重大。</p>
</li>
<li>
<p>第七个是环境的状态转化模型，可以理解为一个概率状态机，它可以表示为一个概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率，表示为 $P^a_{ss&rsquo;}$。</p>
</li>
<li>
<p>第八个是探索率 $ϵ$，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。因此我们在训练选择最优动作时，会有一定的概率 $ϵ$ 不选择使当前轮迭代价值最大的动作，而选择其他的动作。</p>
</li>
</ul>
<p>以上8个就是强化学习模型的基本要素了。当然，在不同的强化学习模型中，会考虑一些其他的模型要素，或者不考虑上述要素的某几个，但是这8个是大多数强化学习模型的基本要素。</p>
<h1 id="3-强化学习的简单实例">3. 强化学习的简单实例</h1>
<p>这里给出一个简单的强化学习例子Tic-Tac-Toe。这是一个简单的游戏，在一个3x3的九宫格里，两个人轮流下，直到有个人的棋子满足三个一横一竖或者一斜，赢得比赛游戏结束，或者九宫格填满也没有人赢，则和棋。</p>
<p>这个例子的完整代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/introduction.py"target="_blank" rel="external nofollow noopener noreferrer">github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。例子只有一个文件，很简单，代码首先会用两个电脑选手训练模型，然后可以让人和机器对战。当然，由于这个模型很简单，所以只要你不乱走，最后的结果都是和棋，当然想赢电脑也是不可能的。</p>
<p>我们重点看看这个例子的模型，理解上面第二节的部分。如何训练强化学习模型可以先不管。代码部分大家可以自己去看，只有300多行。</p>
<ul>
<li>
<p>首先看第一个要素环境的状态 $S$。这是一个九宫格，每个格子有三种状态，即没有棋子(取值0)，有第一个选手的棋子(取值1)，有第二个选手的棋子(取值-1)。那么这个模型的状态一共有$3^9=19683$个。</p>
</li>
<li>
<p>接着我们看个体的动作 $A$，这里只有9个格子，每次也只能下一步，所以最多只有9个动作选项。实际上由于已经有棋子的格子是不能再下的，所以动作选项会更少。实际可以选择动作的就是那些取值为0的格子。</p>
</li>
<li>
<p>第三个是环境的奖励 $R$，这个一般是我们自己设计。由于我们的目的是赢棋，所以如果某个动作导致的改变到的状态可以使我们赢棋，结束游戏，那么奖励最高，反之则奖励最低。其余的双方下棋动作都有奖励，但奖励较少。特别的，对于先下的棋手，不会导致结束的动作奖励要比后下的棋手少。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># give reward to two players</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">giveReward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">currentState</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p1Symbol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">currentState</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p2Symbol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第四个是个体的策略(policy) $π$，这个一般是学习得到的，我们会在每轮以较大的概率选择当前价值最高的动作，同时以较小的概率去探索新动作，在这里AI的策略如下面代码所示。里面的exploreRate就是我们的第八个要素探索率 $ϵ$。即策略是以 $1−ϵ$ 的概率选择当前最大价值的动作，以 $ϵ$ 的概率随机选择新动作。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># determine next action</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">takeAction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">nextStates</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">nextPositions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BOARD_ROWS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BOARD_COLS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextPositions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">nextState</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span><span class="o">.</span><span class="n">getHash</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploreRate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nextPositions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Not sure if truncating is the best way to deal with exploratory step</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Maybe it&#39;s better to only skip this step rather than forget all the history</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span> <span class="o">=</span> <span class="n">nextPositions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">action</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="nb">hash</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nextStates</span><span class="p">,</span> <span class="n">nextPositions</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="nb">hash</span><span class="p">],</span> <span class="n">pos</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">values</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">action</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第五个是价值函数，代码里用value表示。价值函数的更新代码里只考虑了当前动作的现有价值和得到的奖励两部分，可以认为我们的第六个模型要素衰减因子 $γ$ 为0。具体的代码部分如下，价值更新部分的代码加粗。具体为什么会这样更新价值函数我们以后会讲。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># update estimation according to reward</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">feedReward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="o">.</span><span class="n">getHash</span><span class="p">()</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">latestState</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">stepSize</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">      <span class="n">target</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第七个是环境的状态转化模型, 这里由于每一个动作后，环境的下一个模型状态是确定的，也就是九宫格的每个格子是否有某个选手的棋子是确定的，因此转化的概率都是1，不存在某个动作后会以一定的概率到某几个新状态，比较简单。</p>
</li>
</ul>
<p>从这个例子，相信大家对于强化学习的建模会有一个初步的认识了。　　　　　　　　</p>
<p>以上就是强化学习的模型基础，下一篇会讨论马尔科夫决策过程。</p>
]]></description></item><item><title>C++ 基础知识[一]</title><link>https://jianye0428.github.io/posts/basics_one/</link><pubDate>Tue, 11 Jul 2023 19:37:05 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/basics_one/</guid><description><![CDATA[<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>Overview<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">c++ 八股文 第一部分</div>
    </div>
  </div>
<h2 id="1-基础知识一">1. 基础知识(一)</h2>
<h3 id="11-c语言的特点">1.1 C++语言的特点</h3>
<blockquote>
<p>①C++在C的基础上引入了<u><font color=red><strong>面向对象</strong></font></u>机制，同时也兼容C语言；</br>
②C++三大特性：<font color=red><em>封装</em>、<em>继承</em>、<em>多态</em></font>；</br>
③C++程序结构清晰、易于扩充、程序可读性好；</br>
④C++代码质量高，<strong>运行效率高</strong>、仅比汇编语言慢10%~20%；</br>
⑥C++<strong>可复用性高</strong>，C++引入了模板的概念，有专门的模板库(STL)；</br>
⑦C++是不断发展的语言，C++11中新引入了nullptr、auto变量、Lambda匿名函数、右值引用、智能指针。</br></p>
</blockquote>
<table><body text=red><tr><td style="text-align:left;font-weight:bold" bgcolor=yellow><font size="3" color="red">C++面向对象的三大特征</font></td></tr></body></table>
<blockquote>
<p><font color=red><strong>封装性：</strong></font> 将客观事物抽象成类，每个类对自身的<u>数据</u>和<u>方法</u>实行<font color=darkblue>访问控制</font>，包括(private，protected，public)。</br>
<font color=red><strong>继承性：</strong></font> 广义的继承有三种实现形式：<u>实现继承</u>(使用基类的属性和方法而无需额外编码的能力)、<u>可视继承</u>(子窗体使用父窗体的外观和实现代码)、<u>接口继承</u>(仅使用属性和方法，实现滞后到子类实现)。</br>
<font color=red><strong>多态性：</strong></font> 是将父类对象设置成为和一个或更多它的子对象相等的技术。用子类对象给父类对象赋值之后，父类对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。</br></p>
</blockquote>
<h3 id="12-c和c语言的区别">1.2 C++和C语言的区别</h3>
<ul>
<li>① C语言是C++的子集，C++可以很好<strong>兼容C语言</strong>。但是C++又有很多新特性，如引用、智能指针、auto变量等；</br></li>
<li>② C++是面对<strong>对象</strong>(object-oriented)的编程语言；C语言是面对<strong>过程</strong>(process-oriented)的编程语言；</br></li>
<li>③ C语言有一些不安全的语言特性，如指针使用的潜在危险、强制转换的不确定性、内存泄露等。而C++对此增加了不少新特性来改善安全性，如const常量、引用、cast转换、智能指针、try—catch等等；</br></li>
<li>④ C++可复用性高，C++引入了模板的概念，后面在此基础上，实现了方便开发的标准模板库STL。C++的STL库相对于C语言的函数库更灵活、更通用。</br></li>
</ul>
<h3 id="13-c中-struct-和-class-的区别">1.3 C++中 struct 和 class 的区别</h3>
<blockquote>
<p>① struct 一般用于描述一个<u>数据结构集合</u>，而 class 是强调<u>对一个对象数据的<strong>封装</strong></u>；</br>
② struct 中默认的访问控制权限是 public 的，而 class 中默认的访问控制权限是 private 的；</br>
③ 在<u>继承关系</u>中，struct 默认是<strong>公有继承</strong>，而 class 是<strong>私有继承</strong>；</br>
④ class关键字可以用于定<strong>义模板参数</strong>，就像typename，而 struct 不能用于定义模板参数。</br></p>
</blockquote>
<h3 id="14-include头文件的顺序以及双引号和尖括号的区别">1.4 include头文件的顺序以及双引号<code>&quot;&quot;</code>和尖括号<code>&lt;&gt;</code>的区别</h3>
<blockquote>
<p>区别：</br>
① 尖括号<code>&lt; &gt;</code>的头文件是<font color=red>系统文件</font>，双引号<code>&quot; &quot;</code>的头文件是自定义文件;</br>
② 编译器预处理阶段查找头文件的路径不一样；</br>
查找路径：</br>
① 使用尖括号<code>&lt;  &gt;</code>(系统文件)的头文件的查找路径：编译器设置的头文件路径$\rightarrow$系统变量;</br>
② 使用双引号<code>&quot;  &quot;</code>(自定义文件)的头文件的查找路径：当前头文件目录$\rightarrow$编译器设置的头文件路径$\rightarrow$系统变量。</br></p>
</blockquote>
<h3 id="15-c结构体和c结构体的区别">1.5 C++结构体和C结构体的区别</h3>
<blockquote>
<p>①C的结构体内不允许有函数存在，C++允许有内部成员函数，且允许该函数是虚函数；</br>
②C的结构体对内部成员变量的访问权限<strong>只能是public</strong>，而C++允许 <font color=red>public</font>，<font color=red>protected</font>，<font color=red>private</font>三种；</br>
③C 中使用结构体需要加上 struct 关键字，或者对结构体使用 typedef 取别名，而 C++ 中可以省略 struct 关键字直接使用；</br>
④C语言的结构体是<strong>不可以继承的</strong>，C++的结构体是可以从其他的结构体或者类继承过来的。</br></p>
</blockquote>
<h3 id="16-导入c函数的关键字是什么c编译时和c有什么不同">1.6 导入C函数的关键字是什么，C++编译时和C有什么不同？</h3>
<blockquote>
<p><strong>关键字：</strong> 在C++中，导入C函数的关键字是<code>extern</code>，表达形式为<code>extern &quot;C&quot;</code>， <code>extern &quot;C&quot;</code> 的主要作用就是为了能够正确实现C++代码调用其他C语言代码。<font color=red>加上<code>extern &quot;C&quot;</code>后，会指示编译器这部分代码按C语言的进行编译</font>，而不是C++的。</br></p>
</blockquote>
<blockquote>
<p><strong>编译区别：</strong> 由于C++支持函数重载，因此<u>编译器编译函数的过程中会将函数的参数类型也加到编译后的代码中</u>，而不仅仅是函数名；而C语言并不支持函数重载，因此编译C语言代码的函数时不会带上函数的参数类型，一般只包括函数名。</br>
总结: 区别在于<font color=red>在编译过程中是否带上函数的参数类型，c++带，c不带</font>。</p>
</blockquote>
<h3 id="17-简述c从代码到可执行二进制文件的过程">1.7 简述C++从代码到可执行二进制文件的过程</h3>
<blockquote>
<p><strong>预编译、编译、汇编、链接</strong> </br></p>
</blockquote>
<ul>
<li>①<strong>预编译</strong>：这个过程主要的处理操作如下：</br>
<ul>
<li>(1) 将所有的#define删除，并且展开所有的宏定义</br></li>
<li>(2) 处理所有的<u><font color=purple>条件预编译指令</font></u>，如#if、#ifdef</br></li>
<li>(3) 处理#include预编译指令，将被包含的文件插入到该预编译指令的位置。</br></li>
<li>(4) 过滤所有的注释</br></li>
<li>(5) 添加行号和文件名标识</br></li>
</ul>
</li>
<li>②<strong>编译</strong>：这个过程主要的处理操作如下：</br>
<ul>
<li>(1) 词法分析：将源代码的字符序列分割成一系列的记号。</br></li>
<li>(2) 语法分析：对记号进行语法分析，产生语法树。</br></li>
<li>(3) 语义分析：判断表达式是否有意义。</br></li>
<li>(4) 代码优化：</br></li>
<li>(5) 目标代码生成：<strong>生成汇编代码</strong>。</br></li>
<li>(6) 目标代码优化</br></li>
</ul>
</li>
<li>③<strong>汇编</strong>：这个过程主要是将汇编代码转变成机器可以执行的指令(汇编代码转为机器码)。</br></li>
<li>④<strong>链接</strong>：将不同的源文件产生的目标文件进行链接，从而形成一个可以执行的程序(链接目标文件，形成可执行程序)。</br>
​</li>
</ul>
<p><strong>链接分为<font color=red>静态链接</font>和<font color=red>动态链接</font>。</strong></br></p>
<ul>
<li>(1) <strong>静态链接</strong>，是在链接的时候就已经把要调用的函数或者过程链接到了生成的可执行文件中，就算你再去把静态库删除也不会影响可执行程序的执行；生成的静态链接库，Windows下以.lib为后缀，Linux下以.a为后缀。</br></li>
<li>(2) <strong>动态链接</strong>，是在链接的时候没有把调用的函数代码链接进去，而是<font color=red>在执行的过程中，再去找要链接的函数，生成的可执行文件中没有函数代码</font>，只包含函数的重定位信息，所以当你删除动态库时，可执行程序就不能运行。生成的动态链接库，Windows下以.dll为后缀，Linux下以.so为后缀。</br></li>
</ul>
<h3 id="18-static关键字的作用">1.8 static关键字的作用</h3>
<ul>
<li>①<strong>定义全局静态变量和局部静态变量</strong>：在变量前面加上static关键字。static的变量默认初始化为0。初始化的静态变量会在<font color=red><strong>数据段</strong></font>分配内存，未初始化的静态变量会在<font color=red><strong>BSS段</strong></font>分配内存。直到程序结束，静态变量始终会维持前值。只不过全局静态变量(在整个工程文件有效)和局部静态变量(在当前定义的文件内有效)的作用域不一样；(什么是数据段和BBS段内存分配?)</br></li>
<li>②<strong>定义静态函数</strong>：在函数返回类型前加上static关键字，函数即被定义为静态函数。静态函数只能在本源文件中使用；<code>static int func()</code></br></li>
<li>③在变量类型前加上static关键字，变量即被定义为静态变量。静态变量只能在本源文件中使用;</br></li>
<li>④<font color=red><strong>类内静态成员变量:</strong></font> 在c++中，static关键字可以用于定义<strong>类中的静态成员变量</strong>：使用静态数据成员，它既可以被当成全局变量那样去存储，但又被隐藏在类的内部。类中的static静态数据成员拥<strong>有一块单独的存储区</strong>，而<u>不管创建了多少个该类的对象。所有这些对象的静态数据成员都共享这一块静态存储空间，static修饰的变量要在<font color=purplr><a href="https://blog.csdn.net/sevenjoin/article/details/81772792"target="_blank" rel="external nofollow noopener noreferrer">类外初始化<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></font></u>。</br></li>
<li>⑤<font color=red><strong>类内静态成员函数:</strong></font>在c++中，static关键字可以用于定义<strong>类中的静态成员函数</strong>：与静态成员变量类似，类里面同样可以定义静态成员函数。只需要在函数前加上关键字static即可。如静态成员函数也是类的一部分，而不是对象的一部分。所有这些对象的静态数据成员都共享这一块静态存储空间，只能访问类的static成员变量，static修饰的变量要在类外初始化。</br></li>
</ul>
<h3 id="19-数组和指针的区别">1.9 数组和指针的区别</h3>
<ul>
<li><strong>概念：</strong></br>
<ul>
<li>(1)数组：数组是用于储存多个<strong>相同类型数据</strong>的集合。数组名是首元素的地址。</br></li>
<li>(2)指针：指针相当于一个变量，但是它和一般变量不一样，它存放的是其它变量在内存中的地址。指针名指向了内存的首地址。</br></li>
</ul>
</li>
<li><strong>区别：</strong></br>
<ul>
<li>赋值：同类型指针变量可以相互赋值；数组不行，只能一个一个元素的赋值或拷贝；</br></li>
</ul>
</li>
<li><strong>存储方式：</strong></br>
<ul>
<li>数组：数组在<strong>内存中是连续</strong>存放的，开辟一块连续的内存空间。数组是根据数组的下标进行访问的，数组的存储空间，不是在静态区就是在栈上。</br></li>
<li>指针：指针很灵活，它可以指向任意类型的数据。指针的类型说明了它所指向地址空间的内存。由于指针本身就是一个变量，再加上它所存放的也是变量，所以指针的存储空间不能确定。</br></li>
</ul>
</li>
</ul>
<h3 id="110-什么是函数指针如何定义函数指针有什么使用场景">1.10 什么是函数指针，如何定义函数指针，有什么使用场景</h3>
<ul>
<li>
<p><strong>概念：</strong> 函数指针就是指向函数的指针变量。每一个函数都有一个入口地址，该函数入口地址就是函数指针所指向的地址。</br>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">函数指针指向函数的入口地址！</div>
    </div>
  </div></p>
</li>
<li>
<p><strong>定义形式：</strong></br></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">func</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">);</span> <span class="c1">// 函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">f</span><span class="p">)(</span><span class="kt">int</span> <span class="n">a</span><span class="p">);</span> <span class="c1">// 函数指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">f</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">func</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用场景： 回调(callback)。我们调用别人提供的 API函数(Application Programming Interface,应用程序编程接口)，称为Call；如果别人的库里面调用我们的函数，就叫Callback。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//以库函数qsort排序函数为例，它的原型如下：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">qsort</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span><span class="c1">//void*类型，代表原始数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">size_t</span> <span class="n">nmemb</span><span class="p">,</span> <span class="c1">//第二个是size_t类型，代表数据数量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="c1">//第三个是size_t类型，代表单个数据占用空间大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">int</span><span class="p">(</span><span class="o">*</span><span class="n">compar</span><span class="p">)(</span><span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">,</span><span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="c1">//第四个参数是函数指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//第四个参数告诉qsort，应该使用哪个函数来比较元素，
</span></span></span><span class="line"><span class="cl"><span class="c1">//即只要我们告诉qsort比较大小的规则，它就可以帮我们对任意数据类型的数组进行排序。
</span></span></span><span class="line"><span class="cl"><span class="c1">//在库函数qsort调用我们自定义的比较函数，这就是回调的应用。
</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//示例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="n">num</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">cmp_int</span><span class="p">(</span><span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">_a</span> <span class="p">,</span> <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">_b</span><span class="p">){</span><span class="c1">//参数格式固定
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">_a</span><span class="p">;</span>    <span class="c1">//强制类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">_b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">*</span><span class="n">a</span> <span class="o">-</span> <span class="o">*</span><span class="n">b</span><span class="p">;</span>　　
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">qsort</span><span class="p">(</span><span class="n">num</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">num</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">cmp_int</span><span class="p">);</span> <span class="c1">//回调cmp_int函数
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="111-静态变量什么时候初始化">1.11 静态变量什么时候初始化</h3>
<blockquote>
<p>对于C语言的全局和静态变量，初始化发生在任何代码执行之前，属于<font color=red><strong>编译期</strong></font>初始化。</br>
而C++标准规定：全局或静态对象当且仅当对象<font color=red>首次用到时</font>才进行构造。</p>
</blockquote>
<h3 id="112-nullptr调用成员函数可以吗为什么">1.12 nullptr调用成员函数可以吗？为什么？</h3>
<p>可以。因为<font color=red>在编译时对象就绑定了函数地址</font>，和指针空不空没关系。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="c1">//给出实例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">animal</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">sleep</span><span class="p">()</span> <span class="p">{</span> <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;animal sleep&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">breathe</span><span class="p">()</span> <span class="p">{</span> <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;animal breathe haha&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">fish</span> <span class="o">:</span><span class="k">public</span> <span class="n">animal</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">breathe</span><span class="p">(){</span> <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;fish bubble&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">animal</span> <span class="o">*</span><span class="n">pAn</span><span class="o">=</span><span class="k">nullptr</span><span class="p">;</span>    <span class="c1">//类指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">pAn</span><span class="o">-&gt;</span><span class="n">breathe</span><span class="p">();</span>   <span class="c1">// 输出：animal breathe haha
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">fish</span> <span class="o">*</span><span class="n">pFish</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">pFish</span><span class="o">-&gt;</span><span class="n">breathe</span><span class="p">();</span> <span class="c1">// 输出：fish bubble
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// 原因：因为在编译时对象就绑定了函数地址，和指针空不空没关系。
</span></span></span><span class="line"><span class="cl"><span class="c1">// pAn-&gt;breathe();编译的时候，函数的地址就和指针pAn绑定了；
</span></span></span><span class="line"><span class="cl"><span class="c1">// 调用breath(*this), this就等于pAn。由于函数中没有需要解引用this的地方，所以函数运行不会出错，
</span></span></span><span class="line"><span class="cl"><span class="c1">// 但是若用到this，因为this=nullptr，运行出错。
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="113-什么是野指针怎么产生的如何避免">1.13 什么是野指针，怎么产生的，如何避免？</h3>
<blockquote>
<p><strong>概念：</strong> 野指针就是指针指向的位置是不可知的(随机的、不正确的、没有明确限制的)；</p>
</blockquote>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">指向位置不可知称为野指针！</div>
    </div>
  </div>
<blockquote>
<p>产生原因：<u>释放内存后指针不及时置空(野指针)</u>，依然指向了该内存，那么可能出现非法访问的错误。这些我们都要注意避免。(内存泄露)</p>
</blockquote>
<blockquote>
<p>避免办法：</br>
(1)初始化置NULL</br>
(2)申请内存后判空</br>
(3)指针释放后置NULL</br>
(4)使用智能指针</br></p>
</blockquote>
<h3 id="114-静态局部变量全局变量局部变量的特点以及使用场景">1.14 静态局部变量，全局变量，局部变量的特点，以及使用场景</h3>
<ul>
<li>
<p>①首先从作用域考虑: </br></p>
<ul>
<li>C++里作用域可分为6种: 全局，局部，类，语句，命名空间和文件作用域。</br>
<ul>
<li>全局变量: 全局作用域，可以通过extern作用于其他非定义的源文件。</br></li>
<li>静态全局变量: 全局作用域+文件作用域，所以无法在其他文件中使用。</br></li>
<li>局部变量: 局部作用域，比如函数的参数，函数内的局部变量等等。</br></li>
<li>静态局部变量: 局部作用域，只被初始化一次，直到程序结束。</br></li>
</ul>
</li>
</ul>
</li>
<li>
<p>②从所在空间考虑：除了局部变量在栈上外，其他都在静态存储区。因为静态变量都在静态存储区，所以下次调用函数的时候还是能取到原来的值。</br></p>
</li>
<li>
<p>③生命周期： 局部变量在栈上，出了作用域就回收内存；而全局变量、静态全局变量、静态局部变量都在静态存储区，直到程序结束才会回收内存。</br></p>
</li>
<li>
<p>④使用场景：从它们各自特点就可以看出各自的应用场景，不再赘述。</br></p>
</li>
</ul>
<h3 id="115-c继承">1.15 C++继承</h3>
<blockquote>
<p>①<strong>公有继承public</strong>：基类的公有成员和保护成员作为派生类的成员时，它们都保持原有的状态，而基类的私有成员仍然是私有的，不能被这个派生类的子类所访问。</br>
②<strong>私有继承private</strong>：私有继承的特点是基类的公有成员和保护成员都作为派生类的私有成员，并且不能被这个派生类的子类所访问。</br>
③<strong>保护继承protect</strong>：保护继承的特点是基类的所有公有成员和保护成员都成为派生类的保护成员，并且只能被它的派生类成员函数或友元访问，基类的私有成员仍然是私有的</br></p>
</blockquote>
<h3 id="116-常量指针和指针常量">1.16 常量指针和指针常量</h3>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>常量指针和指针常量的区别<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">常量指针: 内存里的值不变</br>
指针常量: 指针指向的内存地址不变</br></div>
    </div>
  </div>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="mf">1.</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">a</span><span class="p">;</span>     <span class="c1">//指的是a是一个常量，不允许修改。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="mf">2.</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">;</span>    <span class="c1">//a指针所指向的内存里的值不变，即(*a)不变  常量指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="mf">3.</span> <span class="kt">int</span> <span class="k">const</span> <span class="o">*</span><span class="n">a</span><span class="p">;</span>    <span class="c1">//同const int *a;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="mf">4.</span> <span class="kt">int</span> <span class="o">*</span><span class="k">const</span> <span class="n">a</span><span class="p">;</span>    <span class="c1">//a指针所指向的内存地址不变，即a不变     指针常量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="mf">5.</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="k">const</span> <span class="n">a</span><span class="p">;</span>   <span class="c1">//都不变，即(*a)不变，a也不变
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="117-内联函数和函数的区别">1.17 内联函数和函数的区别</h3>
<ul>
<li>①内联函数比普通函数多了关键字inline；</br></li>
<li>②内联函数避免了<strong>函数调用的开销</strong>；普通函数有调用的开销；</br></li>
<li>③普通函数在被调用的时候，需要寻址(函数入口地址)；<u>内联函数不需要寻址</u>。</br></li>
<li>④内联函数有一定的限制，内联函数体要求代码简单，不能包含复杂的结构控制语句(内联函数内不允许用循环语句和开关语句。普通函数没有这个要求。</br></li>
</ul>
<h3 id="118-简述c有几种传值方式之间的区别是什么">1.18 简述C++有几种传值方式，之间的区别是什么？</h3>
<ul>
<li><strong>值传递、引用传递、指针传递</strong></br>
<ul>
<li>①值传递: 形参即使在函数体内值发生变化，也不会影响实参的值；</br></li>
<li>②引用传递: 形参在函数体内值发生变化，会影响实参的值；</br></li>
<li>③指针传递: 在指针指向没有发生改变的前提下，形参在函数体内值发生变化，会影响实参的值；</br></li>
</ul>
</li>
</ul>
<h3 id="119-内联函数和宏函数的区别">1.19 内联函数和宏函数的区别</h3>
<blockquote>
<p><strong>宏常量&amp;宏函数</strong></br></p>
</blockquote>
<p>定义:</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// a. 定义一个宏常量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#define MAX 1024 </span><span class="c1">// 宏常量  MAX称为符号常量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// b. 定义一个宏函数
</span></span></span><span class="line"><span class="cl"><span class="c1">// 宏函数:宏函数就是使用宏定义定义出来的函数,并不是真正意义上的函数。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#define GETSUM(x, y) ((x) + (y)) </span><span class="c1">// 宏函数
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>使用宏函数的注意事项: </br></p>
<blockquote>
<ol>
<li>要保证运算的完整性；</br></li>
<li>宏函数的使用场景:频繁调用和短小的函数,封装成宏函数；</br></li>
<li>使用宏函数的优点:以空间换时间；</br></li>
</ol>
</blockquote>
<p>宏定义和函数的区别:</br></p>
<blockquote>
<ol>
<li>宏在 <font color=red>预处理阶段完成替换</font>，之后被替换的文本参与编译，相当于 <font color=red>直接插入代码</font>，运行时不存在函数调用，执行起来更快；函数调用在运行时需要跳转到具体调用函数;</br></li>
<li>宏定义属于在结构中插入代码，<font color=red>没有返回值</font>; 函数调用具有返回值;</br></li>
<li>宏定义参数没有类型，不进行类型检查；函数参数具有类型，需要检查类型;</br></li>
<li>宏定义不要在最后加分号；</br></li>
</ol>
</blockquote>
<p>宏定义和typedef的区别:</br></p>
<ul>
<li>
<ol>
<li>宏主要用于 <font color=red>定义常量及书写复杂的内容</font>；typedef主要用于 <font color=red>定义类型别名</font>；</br></li>
</ol>
</li>
<li>
<ol start="2">
<li>宏替换发生在<strong>预编译阶段</strong>，属于文本插入替换；typedef是<strong>编译</strong>的一部分；</br></li>
</ol>
</li>
<li>
<ol start="3">
<li>宏不检查类型；typedef会检查数据类型；</br></li>
</ol>
</li>
<li>
<ol start="4">
<li>宏不是语句，不需要在最后加分号；typedef是语句，要加分号标识结束；</br></li>
</ol>
</li>
<li>
<ol start="5">
<li>注意对指针的操作，<code>typedef char * p_char</code>和<code>#define p_char char *</code>区别巨大；</br></li>
</ol>
</li>
</ul>
<p>宏函数和内联函数的区别:</br></p>
<ul>
<li>1.在使用时，宏只做简单字符串替换(编译前或者预编译阶段)。而内联函数可以进行参数类型检查(编译时)，且具有返回值；</br></li>
<li>2.内联函数在编译时直接将函数代码嵌入到目标代码中，省去函数调用的开销来提高执行效率，并且进行参数类型检查，具有返回值，可以实现重载；</br></li>
<li>3.宏定义时要注意书写(参数要括起来)否则容易出现歧义(保证运算的完整性)，内联函数不会产生歧义；</br></li>
<li>4.内联函数有类型检查、语法判断等功能，而宏没有；</br></li>
</ul>
<p>define宏定义和const的区别:</br></p>
<ul>
<li>
<p>处理阶段: define是在编译的<strong>预处理</strong>阶段起作用，而const是在<strong>编译、运行</strong>的时候起作用；</p>
</li>
<li>
<p>安全性：</br></p>
<ul>
<li>
<ol>
<li>define只做替换，不做类型检查和计算，也不求解，容易产生错误，一般最好加上一个大括号包含住全部的内容，要不然很容易出错；</br></li>
</ol>
</li>
<li>
<ol start="2">
<li>const常量有数据类型，编译器可以对其进行类型安全检查；</br></li>
</ol>
</li>
</ul>
</li>
<li>
<p>内存占用：</br></p>
<ul>
<li>
<ol>
<li>define只是将宏名称进行替换，在内存中会产生多份相同的备份。const在程序运行中只有一份备份，且可以执行<strong>常量折叠</strong>，能将复杂的的表达式计算出结果放入常量表；</br></li>
</ol>
</li>
<li>
<ol start="2">
<li>宏定义的数据没有分配内存空间，只是插入替换掉；const定义的变量只是值不能改变，但要分配内存空间；</br></li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="120四种cast类型转换">1.20 四种cast类型转换</h3>
<blockquote>
<p>作用：克服c语言中强制类型转化带来的风险，C++引入四种更加安全的强制类型转换运算符(明确转换的目的，便于程序的维护和分析)</p>
</blockquote>
<ol>
<li>const_cast：去除const属性</li>
</ol>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 1.去除const属性，将只读变为只读写
</span></span></span><span class="line"><span class="cl"><span class="c1">// 2.针对常量指针、常量引用和常量对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">char</span> <span class="o">*</span><span class="n">p1</span> <span class="o">=</span> <span class="k">const_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">p</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>static_cast: 内置数据类型、基类-派生类之间的转换</li>
</ol>
<ul>
<li>内置数据类型之间的转换，int转double，char转int</br></li>
<li>基类指针与派生类之间的转换，只能转换有继承或派生关系的类。用于类层次结构之间基类和派生类指针和引用之间的转换，进行向上转型是安全的，但是进行向下转型是不安全的，但是是可以转换的;
<ul>
<li>向上转型(向基类转换 -&gt; 安全)：我们知道基类的引用和指针都可以指向派生类的对象，那么将派生类的指针或者引用强转为基类的指针或者引用，那么这就是向上转型，也就是向父类转;</br></li>
<li>向下转型(向派生类转换 -&gt; 不安全)：向下转型就和向上转型相反，它是将父类的指针或者引用，强制转换为子类的指针或者引用</br></li>
</ul>
</li>
<li>把void类型指针转换为目标类型的指针</br></li>
<li>任何类型的表达式转化为void类型</br></li>
</ul>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 整形转浮点型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">double</span> <span class="n">b</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="n">a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//基类指针转派生类
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">A</span><span class="p">{};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">public</span> <span class="n">A</span><span class="p">{};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">*</span><span class="n">pA</span> <span class="o">=</span> <span class="k">new</span> <span class="n">A</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">*</span><span class="n">pB</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">B</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">pA</span><span class="p">);</span> <span class="c1">// 向下转换不安全
</span></span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>reinterpret_cast:</li>
</ol>
<ul>
<li>可以将一个类型的<strong>指针</strong>转换为其它任意类型的指针，也可以用在指针和整形数据之间的转换。它是很危险的，如果我们没有使用它的充分理由，那么就不要使用它</br></li>
<li>为运算对象的位模式提供较低层次上的重新解释</br></li>
<li>用于底层的强制转换，依赖于机器，一般使用较少</br></li>
</ul>
<ol start="4">
<li>dynamic_cast: 运行时处理；基类向派生类转换时比static_cast更安全</li>
</ol>
<ul>
<li>dynamic_cast是<font color=red>运行时处理</font>的，运行时进行类型检查，其他三种是编译时处理的</br></li>
<li>不能用于内置数据类型之间的转换</br></li>
<li>dynamic_cast在进行上行转换时和static_cast效果是一样的，但是进行下行转换时会进行类型检查，比static_cast更加安全，下行转换是否成功取决于转换对象的实际类型与目标类型是否相同</br></li>
<li>要求基类必须具有虚函数，否则编译不通过</br></li>
<li>若转换成功，返回的是指向目标的指针或引用，不成功返回NULL</br></li>
</ul>
<h2 id="2-基础知识二">2. 基础知识(二)</h2>
<h3 id="21-写出-int-bool-float-指针变量与-零值比较的if-语句">2.1 写出 int 、bool、 float 、指针变量与 “零值”比较的if 语句</h3>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//int与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span> <span class="n">n</span> <span class="o">!=</span> <span class="mi">0</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//bool与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="n">flag</span><span class="p">)</span> <span class="c1">// 表示flag为真
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">flag</span><span class="p">)</span> <span class="c1">// 表示flag为假
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">//float与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">EPSINON</span> <span class="o">=</span> <span class="mf">0.00001</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">((</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="o">-</span> <span class="n">EPSINON</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">EPSINON</span><span class="p">)</span> <span class="c1">//其中EPSINON是允许的误差(即精度)。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">//指针变量与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="22-变量的声明和定义有什么区别">2.2 变量的声明和定义有什么区别</h3>
<blockquote>
<p>① 变量的定义为变量<u><em>分配地址和存储空间</em></u>， 变量的声明不分配地址。</br>
② 一个变量可以在多个地方声明， 但是只在一个地方定义。<font color=red>声明多次，定义一次。</font></br>
③ 加入extern 修饰的是变量的声明，说明此变量将在文件外部或在文件后面部分定义。</br>
④ 说明：很多时候一个变量，只是声明，不分配内存空间，直到具体使用时才初始化，分配内存空间， 如外部变量。</br></p>
</blockquote>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="k">extern</span> <span class="kt">int</span> <span class="n">A</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="c1">//这是个声明而不是定义，声明A是一个已经定义了的外部变量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="c1">//注意：声明外部变量时可以把变量类型去掉如：extern A;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">dosth</span><span class="p">();</span> <span class="c1">//执行函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">A</span><span class="p">;</span> <span class="c1">//是定义，定义了A为整型的外部变量
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="23-简述-ifdefelseendif和ifndef的作用">2.3 简述 <code>#ifdef</code>、<code>#else</code>、<code>#endif</code>和<code>#ifndef</code>的作用</h3>
<p>利用 <code>#ifdef</code>、<code>#endif</code> <u>将某程序功能模块包括进去，以向特定用户提供该功能</u>。在不需要时用户可轻易将其屏蔽。</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="cp">#ifdef MATH
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="cp">#include “math.c”
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">//在子程序前加上标记，以便于追踪和调试。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="cp">#ifdef DEBUG
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="n">printf</span> <span class="p">(</span><span class="err">“</span><span class="n">Indebugging</span><span class="err">…</span><span class="o">!</span><span class="err">”</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="cp">#endif</span></span></span></code></pre></td></tr></table>
</div>
</div><p>应对硬件的限制。由于一些具体应用环境的硬件不一样，限于条件，本地缺乏这种设备，只能绕过硬件，直接写出预期结果。</br>
注意：虽然不用条件编译命令而直接用if语句也能达到要求，但那样做目标程序长(因为所有语句都编译)，运行时间长(因为在程序运行时间对if语句进行测试)。而采用<u><strong>条件编译</strong></u>，可以减少被编译的语句，从而减少目标程序的长度，减少运行时间。</p>
<h3 id="24-结构体可以直接赋值吗">2.4 结构体可以直接赋值吗?</h3>
<blockquote>
<p>①结构体声明时可以直接初始化，同一结构体的不同对象之间也可以直接赋值，但是当结构体中含有指针“成员”时一定要小心。</br>
②注意：当有多个指针指向同一段内存时，某个指针释放这段内存可能会导致其他指针的非法操作。因此在释放前一定要确保其他指针不再使用这段内存空间。</p>
</blockquote>
<h3 id="25-sizeof-和strlen-的区别">2.5 sizeof 和strlen 的区别</h3>
<blockquote>
<p>①sizeof是一个<strong>操作符</strong>，strlen是<strong>库函数</strong>。</br>
②sizeof的参数可以是<strong>数据的类型</strong>，也可以是<strong>变量</strong>，而strlen只能以结尾为‘\0’的字符串作参数。</br>
③编译器在编译时就计算出了sizeof的结果，而strlen函数必须在运行时才能计算出来。并且sizeof计算的是数据类型占内存的大小，而strlen计算的是字符串实际的长度。</br>
④数组做sizeof的参数不退化，传递给strlen就退化为指针了</br></p>
</blockquote>
<h3 id="26-sizeof求类型大小">2.6 sizeof求类型大小</h3>
<p>ref: <a href="https://www.cnblogs.com/maji233/p/11439880.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/maji233/p/11439880.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>①类的大小为类的非静态成员数据的类型大小之和，也就是说<font color=red>静态成员数据不作考虑</font>。
普通成员函数与sizeof无关。</br>
②虚函数由于要维护虚函数表，所以要占据一个指针大小，也就是4字节。
类的总大小也遵守类似class字节对齐的，调整规则。</br></p>
</blockquote>
<p>ref:</br></p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">=&gt;(32 位)
</span></span><span class="line"><span class="cl">指针都是  4个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">char     1个字节
</span></span><span class="line"><span class="cl">short    2个字节
</span></span><span class="line"><span class="cl">int      4个字节
</span></span><span class="line"><span class="cl">long     4个字节
</span></span><span class="line"><span class="cl">long int 4个字节
</span></span><span class="line"><span class="cl">float    4个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">double    8个字节
</span></span><span class="line"><span class="cl">long double  8个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">=&gt;(64 位)
</span></span><span class="line"><span class="cl">指针都是一个字长, 8个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">char    1个字节
</span></span><span class="line"><span class="cl">short   2个字节
</span></span><span class="line"><span class="cl">int     4个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">long    8个字节
</span></span><span class="line"><span class="cl">long int  8个字节
</span></span><span class="line"><span class="cl">double    8个字节
</span></span><span class="line"><span class="cl">long double 也可以变长了, 16个字节</span></span></code></pre></td></tr></table>
</div>
</div><p>例如有如下结构体：</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="k">struct</span> <span class="nc">Stu</span>  <span class="c1">//自定义的数据类型，允许用户存储不同的数据类型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">id</span><span class="p">;</span> <span class="c1">// 4个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">char</span> <span class="n">sex</span><span class="p">;</span> <span class="c1">// 1个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">float</span> <span class="n">hight</span><span class="p">;</span> <span class="c1">// 4个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">};</span></span></span></code></pre></td></tr></table>
</div>
</div><p>那么一个这样的结构体变量占多大内存呢？也就是 <code>cout&lt;&lt;sizeof(Stu)&lt;&lt;endl;</code>  会输出什么？
在了解字节对齐方式之前想当然的会以为：sizeof(Stu) = sizeof(int)+sizeof(char)+sizeof(float) = 9.
然而事实并非如此！</p>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>字节对齐原则<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">在系统默认的对齐方式下：每个成员相对于这个结构体变量地址的偏移量正好是该成员类型所占字节的整数倍，且最终占用字节数为成员类型中最大占用字节数的整数倍。</div>
    </div>
  </div>
<p>在这个例子中，id的偏移量为0(0=4x0)，sex的偏移量为4(4=1x4)，height的偏移量为8(8=2x4)，此时占用12字节，也同时满足12=3x4.所以sizeof(Stu)=12.</p>
<blockquote>
<p>总结：</br>
①最终大小一定是最大数据类型的整数倍；</br>
②静态变量不占空间</br>
③每种类型的偏移量为自身的n倍；</br>
详细请查阅：<a href="https://blog.csdn.net/weixin_30412577/article/details/95141536?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task"target="_blank" rel="external nofollow noopener noreferrer">struct/class等内存字节对齐问题详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
</blockquote>
<p>ref:</br>
<a href="https://www.cnblogs.com/always-chang/p/6084973.html"target="_blank" rel="external nofollow noopener noreferrer">struct地址偏移量计算<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="27-c语言的关键字static和c的关键字static有什么区别">2.7 C语言的关键字<code>static</code>和C++的关键字<code>static</code>有什么区别</h3>
<ul>
<li>①在 C 中 static 用来修饰局部静态变量和外部静态变量、函数。而 C++中除了上述功能外，还用来定义类的成员变量和函数。即静态成员和静态成员函数。</br></li>
<li>②注意：编程时 static 的记忆性和全局性的特点可以让在不同时期调用的函数进行通信，传递信息，而 C++的静态成员则可以在多个对象实例间进行通信，传递信息。</br></li>
</ul>
<h3 id="28-ｃ语言的malloc和ｃ中的new有什么区别">2.8 Ｃ语言的<code>malloc</code>和Ｃ＋＋中的<code>new</code>有什么区别</h3>
<ul>
<li>①new 、delete 是操作符，可以重载，只能在C++ 中使用。</br></li>
<li>②malloc、free 是函数，可以覆盖，C、C++ 中都可以使用。</br></li>
<li>③new 可以调用对象的构造函数，对应的delete 调用相应的析构函数。</br></li>
<li>④malloc 仅仅分配内存，free 仅仅回收内存，并不执行构造和析构函数。</br></li>
<li>⑤new 、delete 返回的是<strong>某种数据类型指针</strong>，malloc、free 返回的是**<code>void</code>指针**。</br>
注意：<code>malloc</code>申请的内存空间要用<code>free</code>释放，而<code>new</code>申请的内存空间要用<code>delete</code>释放，不要混用。</br></li>
</ul>
<p>ref: <a href="https://jianye0428.github.io/posts/basics_one/#211-new%E5%92%8Cmalloc%E7%9A%84%E5%8C%BA%E5%88%AB%E5%90%84%E8%87%AA%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-delete-%E5%92%8C-free%E7%B1%BB%E4%BC%BC"target="_blank" rel="external nofollow noopener noreferrer">2.11 new 和 malloc的区别<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="29-写一个-标准-宏min">2.9 写一个 “标准” 宏MIN</h3>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#define min(a,b) ((a)&lt;=(b)?(a):(b))</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="210-i和i的区别">2.10 ++i和i++的区别</h3>
<blockquote>
<p>++i先自增1，再返回；i++先返回i,再自增1</br>
前置版本将对象本身作为左值返回，后置版本将对象原始值的副本作为右值返回。</p>
</blockquote>
<h3 id="211-new和malloc的区别各自底层实现原理delete和free类似">2.11 <code>new</code>和<code>malloc</code>的区别，各自底层实现原理(<code>delete</code>和<code>free</code>类似)</h3>
<blockquote>
<p>①new(delete)是操作符，而malloc(free)是函数。</br>
②new在调用的时候先分配内存，再调用构造函数，释放的时候调用析构函数；而malloc没有构造函数和析构函数。</br>
③malloc需要给定申请内存的大小，返回的指针需要强转(返回void指针)；new会调用构造函数，不用指定内存的大小，返回指针不用强转。</br>
④new是操作符，可以被重载; malloc不行</br>
⑤new分配内存, 更直接和安全。</br>
⑥new发生错误抛出异常，malloc返回null</br></p>
</blockquote>
<h3 id="212-const-和-define-的区别">2.12 const 和 define 的区别</h3>
<p><strong>区别</strong></br></p>
<blockquote>
<p>(1)就<font color=red>起作用的阶段</font>而言：<code>#define</code>是在编译的<strong>预处理</strong>阶段起作用，而<code>const</code>是在 <strong>编译</strong>、<strong>运行</strong>的时候起作用。</br>
(2)就<font color=red>起作用的方式</font>而言：<code>#define</code>只是<u>简单的字符串替换，没有类型检查</u>。而<code>const</code>有对应的数据类型，是要进行判断的，可以避免一些低级的错误。</br>
(3)就<font color=red>存储方式</font>而言：<code>#define</code>只是进行展开，有多少地方使用，就替换多少次，它定义的宏常量在内存中有若干个备份；const定义的只读变量在程序运行过程中只有一份备份。</br>
(4)从<font color=red>代码调试的方便程度</font>而言： <code>const</code>常量可以进行调试的，<code>define</code>是不能进行调试的，因为在预编译阶段就已经替换掉了。</br></p>
</blockquote>
<p><strong>const优点：</strong></p>
<blockquote>
<p>(1)const常量有数据类型，而宏常量没有数据类型。编译器可以对前者进行类型安全检查。而对后者只进行字符替换，没有类型安全检查，并且在字符替换可能会产生意料不到的错误。</br>
(2)有些集成化的调试工具可以对const常量进行调试，但是不能对宏常量进行调试。</br>
(3)const可节省空间，避免不必要的内存分配，提高效率</br></p>
</blockquote>
<h3 id="213c中函数指针和指针函数的区别">2.13 C++中函数指针和指针函数的区别</h3>
<ol>
<li>定义不同</li>
</ol>
<blockquote>
<p>指针函数本质是一个函数，其返回值为指针。</br>
函数指针本质是一个指针，其指向一个函数。</br></p>
</blockquote>
<ol start="2">
<li>写法不同</li>
</ol>
<blockquote>
<p>指针函数：int *fun(int x, int y);</br>
函数指针：int (*fun)(int x, int y);</br></p>
</blockquote>
<ol start="3">
<li>用法不同</li>
</ol>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//指针函数示例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">_Data</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span><span class="n">Data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">//指针函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">Data</span><span class="o">*</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span><span class="kt">int</span> <span class="n">b</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="n">Data</span> <span class="o">*</span> <span class="n">data</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//调用指针函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">Data</span> <span class="o">*</span> <span class="n">myData</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//Data * myData = static_cast&lt;Data*&gt;(f(4,5));
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//函数指针示例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">x</span><span class="p">,</span><span class="kt">int</span> <span class="n">y</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">//函数指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">fun</span><span class="p">)(</span><span class="kt">int</span> <span class="n">x</span><span class="p">,</span><span class="kt">int</span> <span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//赋值, 函数指针指向函数add
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">fun</span> <span class="o">=</span> <span class="n">add</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">//调用
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;(*fun)(1,2) = &#34;</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="o">*</span><span class="n">fun</span><span class="p">)(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">//输出结果
</span></span></span><span class="line"><span class="cl"><span class="c1">//(*fun)(1,2) =  3
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="214使用指针需要注意什么">2.14 使用指针需要注意什么？</h3>
<blockquote>
<p>①定义指针时，先初始化为NULL空指针。</br>
②用malloc或new申请内存之后，应该立即检查指针值是否为NULL。防止使用指针值为NULL的内存。</br>
③不要忘记为数组和动态内存赋初值。防止将未被初始化的内存作为右值使用。</br>
④避免数字或指针的下标越界，特别要当心发生“多1”或者“少1”操作。</br>
⑤动态内存的申请与释放必须配对，防止内存泄漏。</br>
⑥用free或delete释放了内存之后，立即将指针设置为NULL，防止“野指针”。</br></p>
</blockquote>
<h3 id="215volatile有什么作用">2.15 volatile有什么作用</h3>
<blockquote>
<p>①volatile为状态寄存器一类的并行设备硬件寄存器。</br>
②一个中断服务子程序会访问到的非自动变量。</br>
③多线程间被几个任务共享的变量。</br>
注意：虽然volatile在嵌入式方面应用比较多，但是在PC软件的多线程中，volatile修饰的临界变量也是非常实用的。</p>
</blockquote>
<p>C++中volatile的作用:</br>
<font color=red>总结: 建议编译器不要对该变量进行优化，每次都从内存中读取该变量，而不是从缓存(寄存器)中读取变量。</font></p>
<blockquote>
<p>volatile是“易变/不稳定”的意思。volatile是C的一个较为少用的关键字，解决变量在“共享”环境下容易出现读取错误的问题。</br></p>
</blockquote>
<blockquote>
<p>定义为volatile的变量是说这变量可能会被意想不到地改变，即在你程序运行过程中一直会变，<font color=red>你希望这个值被正确地处理，每次从内存中去读这个值，而不是因编译器优化从缓存的地方读取</font>，比如读取缓存在寄存器中的数值，从而保证volatile变量被正确的读取。</br></p>
</blockquote>
<blockquote>
<p>在单任务的环境中，一个函数体内部，如果在两次读取变量的值之间的语句没有对变量的值进行修改，那么编译器就会设法对可执行代码进行优化。由于访问寄存器的速度要快过RAM(从RAM中读取变量的值到寄存器)，以后只要变量的值没有改变，就一直从寄存器中读取变量的值，而不对RAM进行访问。</br></p>
</blockquote>
<blockquote>
<p>而在多任务环境中，虽然在一个函数体内部，在两次读取变量之间没有对变量的值进行修改，但是该变量仍然有可能被其他的程序(如中断程序、另外的线程等)所修改。如果这时还是从寄存器而不是从RAM中读取，就会出现被修改了的变量值不能得到及时反应的问题。</br></p>
</blockquote>
<h3 id="216-一个参数可以既是const又是volatile吗">2.16 一个参数可以既是const又是volatile吗</h3>
<blockquote>
<p>可以。用const和volatile同时修饰变量，表示这个变量在程序内部是只读的，不能改变的，只在程序外部条件变化下改变，并且编译器不会优化这个变量。每次使用这个变量时，都要小心地去内存读取这个变量的值，而不是去寄存器读取它的备份。</br>
注意：在此一定要注意const的意思，const只是不允许程序中的代码改变某一变量，其在编译期发挥作用，它并<font color=red>没有实际地禁止某段内存的读写特性</font><br></p>
</blockquote>
<h3 id="217a和a有什么区别">2.17 <code>*a</code>和<code>&amp;a</code>有什么区别</h3>
<ul>
<li><code>&amp;a</code>：其含义就是“变量a的地址”。</br></li>
<li><code>*a</code>：用在不同的地方，含义也不一样。</br>
<ul>
<li>①在声明语句中，<code>*a</code>只说明a是一个指针变量，如<code>int *a</code>；</li>
<li>②在其他语句中，<code>*a</code>前面没有操作数且a是一个指针时，<code>*a</code>代表指针a指向的地址内存放的数据(<font color=red>解引用</font>)，如<code>b=*a</code>；</li>
<li>③<code>*a</code>前面有操作数且a是一个普通变量时，a代表乘以a，如c=ba</li>
</ul>
</li>
</ul>
<h3 id="218-用c-编写一个死循环程序">2.18 用C 编写一个死循环程序</h3>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl">    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：很多种途径都可实现同一种功能，但是不同的方法时间和空间占用度不同，特别是对于嵌入式软件，处理器速度比较慢，存储空间较小，所以时间和空间优势是选择各种方法的首要考虑条件。</p>
</blockquote>
<h3 id="219全局变量和局部变量有什么区别是怎么实现的操作系统和编译器是怎么知道的">2.19 全局变量和局部变量有什么区别？是怎么实现的？操作系统和编译器是怎么知道的？</h3>
<ul>
<li>①全局变量是整个程序都可访问的变量，谁都可以访问，生存期在整个程序从运行到结束(在程序结束时所占内存释放)；</br></li>
<li>②而局部变量存在于模块(子程序，函数)中，只有所在模块可以访问，其他模块不可直接访问，模块结束(函数调用完毕)，局部变量消失，所占据的内存释放。</br></li>
<li>③操作系统和编译器，可能是通过内存分配的位置来知道的，全局变量分配在全局数据段并且在程序开始运行的时候被加载.局部变量则分配在堆栈里面。</br></li>
</ul>
<h3 id="220-结构体内存对齐问题">2.20 结构体内存对齐问题</h3>
<p>请写出以下代码的输出结果：</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cm">/**************************************************************
</span></span></span><span class="line"><span class="cl"><span class="cm">*		结构体内存对⻬问题
</span></span></span><span class="line"><span class="cl"><span class="cm">*   从偏移为0的位置开始存储；
</span></span></span><span class="line"><span class="cl"><span class="cm">*	如果没有定义 #pragma pack(n)
</span></span></span><span class="line"><span class="cl"><span class="cm">*	sizeof 的最终结果必然是结构内部最⼤成员的整数倍，不够补⻬；
</span></span></span><span class="line"><span class="cl"><span class="cm">*	结构内部各个成员的⾸地址必然是⾃身⼤⼩的整数倍；
</span></span></span><span class="line"><span class="cl"><span class="cm">*
</span></span></span><span class="line"><span class="cl"><span class="cm">***************************************************************/</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">S1</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">i</span> <span class="p">;</span>  <span class="c1">//起始偏移0，sizeof(i)=4; 地址0、1、2、3分配给成员i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">char</span> <span class="n">j</span> <span class="p">;</span> <span class="c1">//起始偏移4，sizeof(j)=1;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">a</span> <span class="p">;</span>	 <span class="c1">//sizeof(a)=4,内存对齐到8个字节，从偏移量为8处存放a;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">double</span> <span class="n">b</span><span class="p">;</span><span class="c1">//sizeof(b)=8,内存对齐到16个字节，再存放b,结构体总大小24;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="c1">//结构体成员的首地址必须是自身大小的整数倍
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">S3</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">char</span> <span class="n">j</span><span class="p">;</span><span class="c1">//起始偏移0，sizeof(j)=1;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">float</span> <span class="n">i</span><span class="p">;</span><span class="c1">//sizeof(i)=4，内存对齐到4，起始偏移量为4,再存放i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">double</span> <span class="n">b</span><span class="p">;</span><span class="c1">//当前地址为8，是b大小的整数倍，无需对齐，直接存放成员b 8个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">a</span><span class="p">;</span><span class="c1">//sizeof(a)=4,内存对齐到20，再存放a,总大小24字节；
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">printf</span><span class="p">(</span><span class="s">&#34;%d</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">S1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">	<span class="n">printf</span><span class="p">(</span><span class="s">&#34;%d</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">S3</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>输出:</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">24
</span></span><span class="line"><span class="cl">24</span></span></code></pre></td></tr></table>
</div>
</div><p>说明：</br></p>
<blockquote>
<p>①结构体作为一种复合数据类型，其构成元素既可以是基本数据类型的变量，也可以是一些复合型类型数据。对此，编译器会自动进行成员变量的对齐以提高运算效率。</br>
②默认情况下，按自然对齐条件分配空间。各个成员按照它们被声明的顺序在内存中顺序存储，第一个成员的地址和整个结构的地址相同，向结构体成员中size最大的成员对齐。</br>
③许多实际的计算机系统对基本类型数据在内存中存放的位置有限制，它们会要求这些数据的首地址的值是某个数k(通常它为4或8)的倍数，而这个k则被称为该数据类型的对齐模数。</br></p>
</blockquote>
<h2 id="3-基础知识三">3 基础知识(三)</h2>
<h3 id="31-简述cc程序编译的内存分配情况">3.1 简述C、C++程序编译的内存分配情况</h3>
<ul>
<li>
<p>①从<font color=red>静态存储区域</font>分配：</br>
内存在程序编译时就已经分配好，这块内存在程序的整个运行期间都存在。速度快、不容易出错， 因为有系统会善后。例如全局变量，static 变量，常量字符串等。</p>
</li>
<li>
<p>②在<font color=red>栈上</font>分配：</br>
在执行函数时，函数内局部变量的存储单元都在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。大小为2M。</p>
</li>
<li>
<p>③从<font color=red>堆上</font>分配：</br>
即动态内存分配。程序在运行的时候用 malloc 或new 申请任意大小的内存，程序员自己负责在何时用free 或delete 释放内存。动态内存的生存期由程序员决定，使用非常灵活。如果在堆上分配了空间，就有责任回收它，否则运行的程序会出现内存泄漏，另外频繁地分配和释放不同大小的堆空间将会产生<mark>堆内碎块</mark>。</p>
</li>
</ul>
<p><strong>一个C、C++程序编译时内存分为5大存储区：堆区、栈区、全局区、文字常量区、程序代码区。</strong></p>
<h3 id="32简述strcpysprintf-与memcpy-的区别">3.2 简述strcpy、sprintf 与memcpy 的区别</h3>
<ul>
<li>① <font color=red>操作对象不同</font>，strcpy 的两个操作对象均为字符串，sprintf 的操作源对象可以是多种数据类型， 目的操作对象是字符串，memcpy 的两个对象就是两个任意可操作的内存地址，并不限于何种数据类型。</br></li>
<li>② <font color=red>执行效率不同</font>，memcpy 最高，strcpy 次之，sprintf 的效率最低。</br></li>
<li>③ <font color=red>实现功能不同</font>，strcpy 主要实现字符串变量间的拷贝，sprintf 主要实现其他数据类型格式到字符串的转化，memcpy 主要是内存块间的拷贝。</br>
注意：strcpy、sprintf 与memcpy 都可以实现拷贝的功能，但是针对的对象不同，根据实际需求，来选择合适的函数实现拷贝功能。</li>
</ul>
<h3 id="33-请解析void---0-的含义">3.3 请解析((void ()( ) )0)( )的含义</h3>
<blockquote>
<p><code>void (0)( )</code> ：是一个返回值为void，参数为空的函数指针0。</br>
<code>(void ()( ))0</code>：把0转变成一个返回值为void，参数为空的函数指针。</br>
<code>((void ()( ))0()</code>：在上句的基础上加表示整个是一个返回值为void，无参数，并且起始地址为0的函数的名字。</br>
<code>((void (*)( ))0)( )</code>：这就是上句的函数名所对应的函数的调用。</br></p>
</blockquote>
<h3 id="34-typedef-和define-有什么区别">3.4 typedef 和define 有什么区别</h3>
<ul>
<li>①用法不同：</br>
<ul>
<li>typedef 用来定义一种数据类型的别名，增强程序的可读性。define 主要用来定义常量，以及书写复杂使用频繁的宏。</br></li>
</ul>
</li>
<li>②执行时间不同：</br>
<ul>
<li>typedef 是编译过程的一部分，有类型检查的功能。define 是宏定义，是预编译的部分，其发生在编译之前，只是简单的进行字符串的替换，不进行类型的检查。</br></li>
</ul>
</li>
<li>③作用域不同：</br>
<ul>
<li>typedef 有作用域限定：define 不受作用域约束，只要在define 声明后的引用都是正确的。</br></li>
</ul>
</li>
<li>④对指针的操作不同：</br>
-typedef 和define 定义的指针时有很大的区别。</br></li>
</ul>
<p>注意：typedef 定义是语句，因为句尾要加上分号。而define 不是语句，千万不能在句尾加分号。</br></p>
<h3 id="35指针常量与常量指针区别">3.5 指针常量与常量指针区别</h3>
<blockquote>
<p>指针常量是指定义了一个指针，这个指针的值只能在定义时初始化，其他地方不能改变。</br>
常量指针是指定义了一个指针，这个指针指向一个只读的对象，不能通过常量指针来改变这个对象的值。</br></p>
<blockquote>
<p>指针常量强调的是指针的不可改变性，而常量指针强调的是指针对其所指对象的不可改变性。</br></p>
</blockquote>
</blockquote>
<blockquote>
<p>注意：无论是指针常量还是常量指针，其最大的用途就是作为函数的形式参数，保证实参在被调用函数中的不可改变特性。</p>
</blockquote>
<h3 id="36简述队列和栈的异同">3.6 简述队列和栈的异同</h3>
<blockquote>
<p>队列和栈都是<font color=red>线性存储结构</font>，但是两者的插入和删除数据的操作不同，队列是“先进先出”，栈是 “后进先出”。
注意：区别栈区和堆区。堆区的存取是“顺序随意”，而栈区是“后进先出”。栈由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。堆一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS 回收。分配方式类似于链表。 它与本题中的堆和栈是两回事。堆栈只是一种数据结构，而堆区和栈区是程序的不同内存存储区域。</p>
</blockquote>
<h3 id="37设置地址为0x67a9-的整型变量的值为0xaa66">3.7 设置地址为0x67a9 的整型变量的值为0xaa66</h3>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="o">*</span><span class="n">ptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ptr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="p">)</span><span class="mh">0x67a9</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="o">*</span><span class="n">ptr</span> <span class="o">=</span> <span class="mh">0xaa66</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：这道题就是强制类型转换的典型例子，无论在什么平台，地址长度和整型数据的长度是一样的， 即一个整型数据可以强制转换成地址指针类型，只要有意义即可。</p>
</blockquote>
<h3 id="38编码实现字符串转化为数字">3.8 编码实现字符串转化为数字</h3>
<blockquote>
<p>编码实现函数atoi()，设计一个程序，把一个字符串转化为一个整型数值。例如数字：“5486321 ”， 转化成字符：5486321。</p>
</blockquote>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">myAtoi</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span> <span class="n">str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="kt">int</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//保存转换后的数值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="kt">int</span> <span class="n">isNegative</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//记录字符串中是否有负号
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">   <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="kt">char</span> <span class="o">*</span><span class="n">p</span> <span class="o">=</span> <span class="n">str</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">if</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="c1">//判断指针的合法性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">while</span><span class="p">(</span><span class="o">*</span><span class="n">p</span><span class="o">++</span> <span class="o">!=</span> <span class="sc">&#39;\0&#39;</span><span class="p">)</span> <span class="c1">//计算数字符串度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">n</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="n">p</span> <span class="o">=</span> <span class="n">str</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="sc">&#39;-&#39;</span><span class="p">)</span> <span class="c1">//判断数组是否有负号
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">isNegative</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="sc">&#39;0&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="n">p</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">       <span class="k">if</span><span class="p">(</span><span class="n">temp</span> <span class="o">&gt;</span> <span class="sc">&#39;9&#39;</span> <span class="o">||</span><span class="n">temp</span> <span class="o">&lt;</span> <span class="sc">&#39;0&#39;</span><span class="p">)</span> <span class="c1">//滤除非数字字符
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span><span class="p">(</span><span class="n">num</span> <span class="o">!=</span><span class="mi">0</span> <span class="o">||</span> <span class="n">temp</span> <span class="o">!=</span> <span class="sc">&#39;0&#39;</span><span class="p">)</span> <span class="c1">//滤除字符串开始的0 字符
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="n">temp</span> <span class="o">-=</span> <span class="mh">0x30</span><span class="p">;</span> <span class="c1">//将数字字符转换为数值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">num</span> <span class="o">+=</span> <span class="n">temp</span> <span class="o">*</span><span class="kt">int</span><span class="p">(</span> <span class="n">pow</span><span class="p">(</span><span class="mi">10</span> <span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span><span class="n">i</span><span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">       <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">if</span><span class="p">(</span><span class="n">isNegative</span><span class="p">)</span> <span class="c1">//如果字符串中有负号，将数值取反
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="p">(</span><span class="mi">0</span> <span class="o">-</span> <span class="n">num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">else</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">num</span><span class="p">;</span> <span class="c1">//返回转换后的数值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="39c语言的结构体struct和c的类class有什么区别">3.9 C语言的结构体(struct)和C++的类(class)有什么区别</h3>
<blockquote>
<p>①C语言的结构体是不能有成员函数的，而C++的类可以有。</br>
②C语言的结构体中数据成员是没有private、public和protected访问限定的。而C++的类的成员有这些访问权限限定。</br>
③C语言的结构体是没有继承关系的，而C++的类却有丰富的继承关系。</br>
注意：虽然C的结构体和C++的类有很大的相似度，但是类是实现面向对象的基础。而结构体只可以简单地理解为类的前身。</br></p>
</blockquote>
<h3 id="310-简述指针常量与常量指针的区别">3.10 简述指针常量与常量指针的区别</h3>
<blockquote>
<p>①指针常量是指定义了一个指针，这个指针的值只能在定义时初始化，其他地方不能改变。常量指针是定义了一个指针，这个指针指向一个只读的对象，不能通过常量指针来改变这个对象的值。指针常量的值只能在定义时初始化，常量指针指向一个只读的对象</br>
②指针常量强调的是指针的不可改变性，而常量指针强调的是指针对其所指对象的不可改变性。</br>
注意：无论是指针常量还是常量指针，其最大的用途就是作为函数的形式参数，保证实参在被调用函数中的不可改变特性。</br></p>
</blockquote>
<h3 id="311-哪些情况会导致野指针以及如何避免">3.11 哪些情况会导致“野指针”以及如何避免</h3>
<ul>
<li>①指针变量声明时没有被初始化。解决办法：指针声明时初始化，可以是具体的地址值，也可让它指向NULL。</br></li>
<li>②指针p被free或者delete之后，没有置为NULL。解决办法：指针指向的内存空间被释放后指针应该指向NULL。</br></li>
<li>③指针操作超越了变量的作用范围。解决办法：在变量的作用域结束前释放掉变量的地址空间并且让指针指向NULL。</br></li>
</ul>
<h3 id="312句柄和指针的区别和联系是什么">3.12 句柄和指针的区别和联系是什么？</h3>
<p>句柄和指针其实是两个截然不同的概念。Windows系统用句柄标记系统资源，隐藏系统的信息。你只要知道有这个东西，然后去调用就行了，它是个32bit的uint。指针则标记某个物理内存地址，两者是不同的概念。</p>
<h3 id="313newdelete与mallocfree的区别是什么">3.13 new/delete与malloc/free的区别是什么</h3>
<blockquote>
<p>new能自动计算需要分配的内存空间，而malloc需要手工计算字节数。</p>
</blockquote>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="o">*</span><span class="n">p</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="o">*</span><span class="n">q</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span> <span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="mi">2</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>①new与delete直接带具体类型的指针，malloc和free返回void类型的指针。</br></li>
<li>②new类型是安全的，而malloc不是。例如<code>int *p = new float[2];</code>就会报错；而<code>int p = malloc(2sizeof(int))</code>编译时编译器就无法指出错误来。</br></li>
<li>③new一般分为两步：new操作和调用构造函数。new操作对应与malloc，但new操作可以重载，可以自定义内存分配策略，不做内存分配，甚至分配到非内存设备上，而malloc不行。</br></li>
<li>④new调用构造函数，malloc不能；delete调用析构函数，而free不能。</br></li>
<li>⑤malloc/free需要库文件stdlib.h的支持，new/delete则不需要！</br></li>
<li>⑥new/delete是C++的关键字,申请内存失败时会抛出异常，malloc/free是库函数，申请内存失败后返回null。</br></li>
<li>⑦new/delete是C++的内存分配和回收机制，malloc/free是C的内存分配和回收机制。</br></li>
</ul>
<blockquote>
<p>注意：delete和free被调用后，内存不会立即回收，指针也不会指向空，delete或free仅仅是告诉操作系统，这一块内存被释放了，可以用作其他用途。但是由于没有重新对这块内存进行写操作，所以内存中的变量数值并没有发生变化，出现野指针的情况。因此，释放完内存后，应该讲该指针指向NULL。</br>
<a href="https://blog.csdn.net/qq_44443986/article/details/114800593"target="_blank" rel="external nofollow noopener noreferrer">new delete 详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<h3 id="314说一说externc">3.14 说一说extern“C”</h3>
<blockquote>
<p><code>extern &quot;C&quot;</code>的主要作用就是为了能够正确实现C++代码调用C语言代码。加上<code>extern &quot;C&quot;</code>后，会指示编译器这部分代码按C语言(而不是C++)的方式进行编译。由于C++支持函数重载，因此编译器编译函数的过程中会将函数的参数类型也加到编译后的代码中，而不仅仅是函数名；而C语言并不支持函数重载，因此编译C语言代码的函数时不会带上函数的参数类型，一般只包括函数名。</br></p>
</blockquote>
<blockquote>
<p>这个功能十分有用，因为在C++出现以前，很多代码(包括很多底层的库)都是C语言写的，为了更好地支持原来的C代码和已经写好的C语言库，需要在C++中尽可能的支持C，而<code>extern &quot;C&quot;</code>就是其中的一个策略。</br></p>
</blockquote>
<blockquote>
<p>C++代码调用C语言代码在C++的头文件中使用在多个人协同开发时，可能有的人比较擅长C语言，而有的人擅长C++，这样的情况下也会有用到。</br></p>
</blockquote>
<h3 id="315请你来说一下c中struct和class的区别">3.15 请你来说一下C++中struct和class的区别</h3>
<p>在C++中，class和struct做类型定义是只有两点区别：</p>
<blockquote>
<p>①<strong>默认继承权限</strong>不同，class继承默认是private继承，而struct默认是public继承</br>
②class还可用于定义模板参数，像typename，但是关键字struct不能同于定义模板参数</br>
③C++保留struct关键字，原因：保证与C语言的向下兼容性，C++必须提供一个struct</br>
④C++中的struct定义必须百分百地保证与C语言中的struct的向下兼容性，把C++中的最基本的对象单元规定为class而不是struct，就是为了避免各种兼容性要求的限制</br>
⑤对struct定义的扩展使C语言的代码能够更容易的被移植到C++中</br></p>
</blockquote>
<h3 id="316c类内可以定义引用数据成员吗">3.16 C++类内可以定义引用数据成员吗？</h3>
<blockquote>
<p>可以，必须通过成员函数初始化列表初始化。</p>
</blockquote>
<h3 id="317c中类成员的访问权限">3.17 C++中类成员的访问权限</h3>
<blockquote>
<p>①C++通过 <code>public</code>、<code>protected</code>、<code>private</code> 三个关键字来控制成员变量和成员函数的访问权限，它们分别表示公有的、受保护的、私有的，被称为成员访问限定符。</br>
②在类的内部(定义类的代码内部)，无论成员被声明为 public、protected 还是 private，都是可以互相访问的，没有访问权限的限制。</br>
③在类的外部(定义类的代码之外)，只能通过对象访问成员，并且通过对象只能访问 public 属性的成员，不能访问 private、protected 属性的成员</br></p>
</blockquote>
<h3 id="318什么是右值引用跟左值又有什么区别">3.18 什么是右值引用，跟左值又有什么区别？</h3>
<p>左值和右值的概念：</p>
<blockquote>
<p>①左值：</br>
能取地址，或者具名对象，表达式结束后依然存在的持久对象；</br>
右值：不能取地址，匿名对象，表达式结束后就不再存在的临时对象；</br>
②区别：</br>
左值能寻址，右值不能；</br>
左值能赋值，右值不能；</br>
左值可变，右值不能(仅对基础类型适用，用户自定义类型右值引用可以通过成员函数改变)；</br></p>
</blockquote>
<h3 id="319面向对象的三大特征">3.19 面向对象的三大特征</h3>
<blockquote>
<p>封装性：将客观事物抽象成类，每个类对自身的数据和方法实行<strong>访问权限保护</strong>(private ， protected ， public)。</br>
继承性：广义的继承有三种实现形式：实现继承(使用基类的属性和方法而无需额外编码的能力)、可视继承(子窗体使用父窗体的外观和实现代码)、接口继承(仅使用属性和方法,实现滞后到子类实现)。</br>
多态性：是将父类对象设置成为和一个或更多它的子对象相等的技术。用子类对象给父类对象赋值之后，父类对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。</br></p>
</blockquote>
<h3 id="320c的空类有哪些成员函数">3.20 C++的空类有哪些成员函数</h3>
<p><a href="https://blog.csdn.net/weixin_45805339/article/details/128089198"target="_blank" rel="external nofollow noopener noreferrer">C++空类成员函数<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<blockquote>
<p>缺省构造函数。</br>
缺省拷贝构造函数。</br>
缺省析构函数。</br>
缺省赋值运算符。</br>
缺省取址运算符。</br>
缺省取址运算符 const 。</br>
注意：有些书上只是简单的介绍了前四个函数。没有提及后面这两个函数。但后面这两个函数也是空类的默认函数。另外需要注意的是，只有当实际使用这些空类成员函数的时候，编译器才会去定义它们。</br></p>
</blockquote>
<h2 id="4-基础知识四">4. 基础知识(四)</h2>
<h3 id="41-说一说c中四种cast转换">4.1 说一说c++中四种cast转换</h3>
<p>C++中四种类型转换是：<code>static_cast</code>, <code>dynamic_cast</code>, <code>const_cast</code>, <code>reinterpret_cast</code></p>
<blockquote>
<p>1、const_cast</br>
用于将const变量转为非const</br>
2、static_cast</br>
用于各种隐式转换，比如非const转const，void*转指针等, static_cast能用于多态向上转化，如果向下转能成功但是不安全，结果未知；</br>
3、dynamic_cast</br>
用于动态类型转换。只能用于含有虚函数的类，用于类层次间的向上和向下转化。只能转指针或引用。向下转化时，如果是非法的对于指针返回NULL，对于引用抛异常。要深入了解内部转换的原理。</br></p>
<ul>
<li>向上转换：指的是子类向基类的转换</br></li>
<li>向下转换：指的是基类向子类的转换</br>
它通过判断在执行到该语句的时候变量的运行时类型和要转换的类型是否相同来判断是否能够进行向下转换。</br></li>
</ul>
<p>4、reinterpret_cast</br>
几乎什么都可以转，比如将int转指针，可能会出问题，尽量少用；</br>
5、为什么不使用C的强制转换？</br>
C的强制转换表面上看起来功能强大什么都能转，但是转化不够明确，不能进行错误检查，容易出错。</br></p>
</blockquote>
<h3 id="42-对c中的smart-pointer四个智能指针的理解shared_ptrunique_ptrweak_ptrauto_ptr">4.2 对c++中的smart pointer四个智能指针的理解：shared_ptr,unique_ptr,weak_ptr,auto_ptr</h3>
<blockquote>
<p>①C++里面的四个智能指针: auto_ptr, shared_ptr, weak_ptr, unique_ptr 其中后三个是c++11支持，并且第一个已经被C++11弃用。</br>
②智能指针的作用是管理一个指针，因为存在以下这种情况：</br>
申请的空间在函数结束时忘记释放，造成<strong>内存泄漏</strong>。使用智能指针可以很大程度上的避免这个问题，因为智能指针就是一个类，当超出了类的作用域是，类会自动调用析构函数，析构函数会自动释放资源。所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。</br>
③auto_ptr(c++98的方案，cpp11已经抛弃)</p>
</blockquote>
<p><font color=red>采用所有权模式。</font></p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="nf">p3</span> <span class="p">(</span><span class="k">new</span> <span class="n">string</span> <span class="p">(</span><span class="err">“</span><span class="k">auto</span><span class="err">”</span><span class="p">));</span> <span class="c1">//#4
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">unique_ptr</span> <span class="n">p4</span><span class="err">；</span> <span class="c1">//#5
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">p4</span> <span class="o">=</span> <span class="n">p3</span><span class="p">;</span><span class="c1">//此时会报错！！
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>编译器认为p4=p3非法，避免了p3不再指向有效数据的问题。因此，unique_ptr比auto_ptr更安全。</br></p>
<p>另外unique_ptr还有更聪明的地方：当程序试图将一个 unique_ptr 赋值给另一个时，如果源 unique_ptr 是个临时右值，编译器允许这么做；如果源 unique_ptr 将存在一段时间，编译器将禁止这么做，比如：</p>
<div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="nf">pu1</span><span class="p">(</span><span class="k">new</span> <span class="n">string</span> <span class="p">(</span><span class="err">“</span><span class="n">hello</span> <span class="n">world</span><span class="err">”</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="n">pu2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">pu2</span> <span class="o">=</span> <span class="n">pu1</span><span class="p">;</span> <span class="c1">// #1 not allowed
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">unique_ptr</span> <span class="n">pu3</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">pu3</span> <span class="o">=</span> <span class="n">unique_ptr</span><span class="p">(</span><span class="k">new</span> <span class="n">string</span> <span class="p">(</span><span class="err">“</span><span class="n">You</span><span class="err">”</span><span class="p">));</span> <span class="c1">// #2 allowed
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其中#1留下悬挂的 unique_ptr(pu1)，这可能导致危害。而#2不会留下悬挂的unique_ptr，因为它调用 unique_ptr 的构造函数，该构造函数创建的临时对象在其所有权让给 pu3 后就会被销毁。这种随情况而已的行为表明，unique_ptr 优于允许两种赋值的auto_ptr 。</p>
<p>注：如果确实想执行类似与#1的操作，要安全的重用这种指针，可给它赋新值。C++有一个标准库函数std::move()，让你能够将一个unique_ptr赋给另一个。例如：</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="n">ps1</span><span class="p">,</span> <span class="n">ps2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ps1</span> <span class="o">=</span> <span class="n">demo</span><span class="p">(</span><span class="err">“</span><span class="n">hello</span><span class="err">”</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">ps2</span> <span class="o">=</span> <span class="n">move</span><span class="p">(</span><span class="n">ps1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">ps1</span> <span class="o">=</span> <span class="n">demo</span><span class="p">(</span><span class="err">“</span><span class="n">alexia</span><span class="err">”</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">ps2</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">ps1</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>shared_ptr实现共享式拥有概念。</strong><u>多个智能指针可以指向相同对象，该对象和其相关资源会在“最后一个引用被销毁”时候释放。</u>从名字share就可以看出了资源可以被多个指针共享，它使用计数机制来表明资源被几个指针共享。可以通过成员函数use_count()来查看资源的所有者个数。除了可以通过new来构造，还可以通过传入auto_ptr, unique_ptr,weak_ptr来构造。当我们调用release()时，当前指针会释放资源所有权，计数减一。当计数等于0时，资源会被释放。</p>
<p>shared_ptr 是为了解决 auto_ptr 在对象所有权上的局限性(auto_ptr 是独占的), 在使用引用计数的机制上提供了可以共享所有权的智能指针。</p>
<p><strong>成员函数：</strong></p>
<blockquote>
<p>use_count 返回引用计数的个数</br>
unique 返回是否是独占所有权( use_count 为 1)</br>
swap 交换两个 shared_ptr 对象(即交换所拥有的对象)</br>
reset 放弃内部对象的所有权或拥有对象的变更, 会引起原有对象的引用计数的减少</br>
get 返回内部对象(指针), 由于已经重载了()方法, 因此和直接使用对象是一样的.如 shared_ptrsp(new int(1)); sp 与 sp.get()是等价的</br></p>
</blockquote>
<p><strong>weak_ptr:</strong></p>
<blockquote>
<p>weak_ptr 是一种不控制对象生命周期的智能指针, 它指向一个 shared_ptr 管理的对象. 进行该对象的内存管理的是那个强引用的shared_ptr. weak_ptr只是提供了对管理对象的一个访问手段。</br></br>
weak_ptr 设计的目的是为配合 shared_ptr 而引入的一种智能指针来协助 shared_ptr 工作, 它只可以从一个 shared_ptr 或另一个 weak_ptr 对象构造, 它的构造和析构不会引起引用记数的增加或减少。</br></br>
weak_ptr是用来<strong>解决shared_ptr相互引用时的死锁问题</strong>,如果说两个shared_ptr相互引用,那么这两个指针的引用计数永远不可能下降为0,资源永远不会释放。它是对对象的一种弱引用，不会增加对象的引用计数，和shared_ptr之间可以相互转化，shared_ptr可以直接赋值给它，它可以通过调用lock函数来获得shared_ptr。</br></p>
</blockquote>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">B</span><span class="o">&gt;</span> <span class="n">pb_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">A</span><span class="p">(){</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">&#34;A delete&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">A</span><span class="o">&gt;</span> <span class="n">pa_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">B</span><span class="p">(){</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">&#34;B delete&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">fun</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">B</span><span class="o">&gt;</span> <span class="n">pb</span><span class="p">(</span><span class="k">new</span> <span class="n">B</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">A</span><span class="o">&gt;</span> <span class="n">pa</span><span class="p">(</span><span class="k">new</span> <span class="n">A</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">pb</span><span class="o">-&gt;</span><span class="n">pa_</span> <span class="o">=</span> <span class="n">pa</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">pa</span><span class="o">-&gt;</span><span class="n">pb_</span> <span class="o">=</span> <span class="n">pb</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">pb</span><span class="p">.</span><span class="n">use_count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">pa</span><span class="p">.</span><span class="n">use_count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">fun</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到fun函数中pa ，pb之间互相引用，两个资源的引用计数为2，当要跳出函数时，智能指针pa，pb析构时两个资源引用计数会减一，但是两者引用计数还是为1，导致跳出函数时资源没有被释放(A B的析构函数没有被调用)，如果把其中一个改为weak_ptr就可以了，我们把类A里面的shared_ptr pb_; 改为weak_ptr pb_; 运行结果如下，这样的话，资源B的引用开始就只有1，当pb析构时，B的计数变为0，B得到释放，B释放的同时也会使A的计数减一，同时pa析构时使A的计数减一，那么A的计数为0，A得到释放。</br></p>
<p>注意：不能通过weak_ptr直接访问对象的方法，比如B对象中有一个方法print(),我们不能这样访问，pa-&gt;pb_-&gt;print(); 英文pb_是一个weak_ptr，应该先把它转化为shared_ptr,如：shared_ptr p = pa-&gt;pb_.lock(); p-&gt;print();</br></p>
<h3 id="43-说说强制类型转换运算符">4.3 说说强制类型转换运算符</h3>
<p><strong>①static_cast 用于非多态类型的转换</strong></p>
<blockquote>
<p>用于非多态类型的转换</br>
不执行运行时类型检查(转换安全性不如 dynamic_cast)</br>
通常用于转换数值数据类型(如 float -&gt; int)</br>
可以在整个类层次结构中移动指针，子类转化为父类安全(向上转换)，父类转化为子类不安全(因为子类可能有不在父类的字段或方法)</br></p>
</blockquote>
<p><strong>②dynamic_cast 用于多态类型的转换</strong></p>
<blockquote>
<p>用于多态类型的转换</br>
执行行运行时类型检查</br>
只适用于指针或引用</br>
对不明确的指针的转换将失败(返回 nullptr)，但不引发异常</br>
可以在整个类层次结构中移动指针，包括向上转换、向下转换</br></p>
</blockquote>
<p><strong>③const_cast</strong></p>
<blockquote>
<p>用于删除 const、volatile 和 __unaligned 特性(如将 const int 类型转换为 int 类型 )</p>
</blockquote>
<p><strong>④reinterpret_cast</strong></p>
<blockquote>
<ul>
<li>用于位的简单重新解释</br></li>
<li>滥用 reinterpret_cast 运算符可能很容易带来风险。除非所需转换本身是低级别的，否则应使用其他强制转换运算符之一。</br></li>
<li>允许将任何指针转换为任何其他指针类型(如 char* 到 int* 或 One_class* 到 Unrelated_class* 之类的转换，但其本身并不安全)
也允许将任何整数类型转换为任何指针类型以及反向转换。</br></li>
<li>reinterpret_cast 运算符不能丢掉 const、volatile 或 __unaligned 特性。</br></li>
<li>reinterpret_cast 的一个实际用途是在哈希函数中，即，通过让两个不同的值几乎不以相同的索引结尾的方式将值映射到索引。</br></li>
</ul>
</blockquote>
<p><strong>⑤bad_cast</strong></p>
<blockquote>
<p>由于强制转换为引用类型失败，dynamic_cast 运算符引发 bad_cast 异常。
bad_cast 使用:</p>
</blockquote>
<div class="highlight" id="id-26"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Circle</span><span class="o">&amp;</span> <span class="n">ref_circle</span> <span class="o">=</span> <span class="k">dynamic_cast</span><span class="o">&lt;</span><span class="n">Circle</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="n">ref_shape</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">catch</span> <span class="p">(</span><span class="n">bad_cast</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Caught: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">b</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="44-谈谈你对拷贝构造函数和赋值运算符的认识">4.4 谈谈你对拷贝构造函数和赋值运算符的认识</h3>
<p>拷贝构造函数和赋值运算符重载有以下两个不同之处：</br></p>
<blockquote>
<p>①拷贝构造函数生成新的类对象，而赋值运算符不能。</br>
②由于拷贝构造函数是直接构造一个新的类对象，所以在初始化这个对象之前不用检验原对象是否和新建对象相同，而赋值运算符则需要这个操作;</br>
③另外，赋值运算中，如果原来的对象中有内存分配要先把内存释放掉。</br>
注意：当有类中有指针类型的成员变量时，一定要重写拷贝构造函数和赋值运算符，不要使用默认的。</p>
</blockquote>
<h3 id="45-在c中使用malloc申请的内存能否通过delete释放使用new申请的内存能否用free">4.5 在C++中，使用malloc申请的内存能否通过delete释放？使用new申请的内存能否用free？</h3>
<blockquote>
<p>不能，malloc /free主要为了兼容C，new和delete 完全可以取代malloc /free的。</br>
①malloc /free的操作对象都是必须明确大小的。<font color=red>而且不能用在动态类上</font>。</br>
②new 和delete会自动进行类型检查和大小，malloc/free不能执行构造函数与析构函数，所以动态对象它是不行的。</br></p>
</blockquote>
<p>当然从理论上说使用malloc申请的内存是可以通过delete释放的。不过一般不这样写的。而且也不能保证每个C++的运行时都能正常。</p>
<h3 id="46-用c设计一个不能被继承的类">4.6 用C++设计一个不能被继承的类</h3>
<p>ref: <a href="https://blog.csdn.net/wei_cheng18/article/details/81043858"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/wei_cheng18/article/details/81043858<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<div class="highlight" id="id-27"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span> <span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">friend</span> <span class="n">T</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">A</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">   <span class="o">~</span><span class="n">A</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">virtual</span> <span class="k">public</span> <span class="n">A</span><span class="o">&lt;</span><span class="n">B</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">B</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"> <span class="o">~</span><span class="n">B</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">C</span> <span class="o">:</span> <span class="k">virtual</span> <span class="k">public</span> <span class="n">B</span><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">C</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"> <span class="o">~</span><span class="n">C</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">main</span><span class="p">(</span> <span class="kt">void</span> <span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">B</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">//C c;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意：<font color=red>构造函数是实现继承的关键</font>，每次子类对象构造时，首先调用的是父类的构造函数，然后才是自己的。</p>
<ul>
<li>
<p>这里需要说明的是：我们设计的不能被继承的类B对基类A的继承必须是虚继承，这样一来C类继承B类时会去直接调用A的构造函数，而不是像普通继承那样，先调用B的构造函数再调用A的构造函数；</p>
</li>
<li>
<p>C类直接调用A类的构造函数，由于A类的构造函数是私有的，而B是A的友元，C类不是A的友元，友元关系不会继承，因此会编译报错。</p>
</li>
</ul>
<h3 id="48-访问基类的私有虚函数">4.8 访问基类的私有虚函数</h3>
<p>写出以下程序的输出结果：</p>
<div class="highlight" id="id-28"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">class</span> <span class="nc">A</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">g</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;A::g&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">   <span class="k">virtual</span> <span class="kt">void</span> <span class="n">f</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;A::f&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">public</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="kt">void</span> <span class="nf">g</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;B::g&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">h</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;B::h&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="nf">void</span><span class="p">(</span> <span class="o">*</span><span class="n">Fun</span> <span class="p">)(</span> <span class="kt">void</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="n">B</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="n">Fun</span> <span class="n">pFun</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">pFun</span> <span class="o">=</span> <span class="p">(</span> <span class="n">Fun</span> <span class="p">)</span><span class="o">*</span><span class="p">(</span> <span class="p">(</span> <span class="kt">int</span><span class="o">*</span> <span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="kt">int</span><span class="o">*</span> <span class="p">)(</span> <span class="o">&amp;</span><span class="n">b</span> <span class="p">)</span> <span class="o">+</span> <span class="n">i</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="n">pFun</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>输出结果:</p>
<div class="highlight" id="id-29"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">B</span><span class="o">::</span><span class="n">g</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span><span class="o">::</span><span class="n">f</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span><span class="o">::</span><span class="n">h</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：考察了面试者对虚函数的理解程度。一个对虚函数不了解的人很难正确的做出本题。 在学习面向对象的多态性时一定要深刻理解虚函数表的工作原理。</p>
</blockquote>
<blockquote>
<p>虚函数：通过基类访问派生类定义的函数，多态时使用，使用虚函数加上virtual关键字。</br>
虚函数就是在基类定义一个未实现的函数名，为了提高程序的可读性</br>
<a href="https://blog.csdn.net/weixin_45138932/article/details/125667041"target="_blank" rel="external nofollow noopener noreferrer">虚函数详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/qq_42048450/article/details/117282640?spm=1001.2014.3001.5502"target="_blank" rel="external nofollow noopener noreferrer">C++虚函数详解_疯狂的麦克斯_max的博客-CSDN博客_c++虚函数<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<p><a href="https://blog.csdn.net/weixin_43700340/article/details/89471069"target="_blank" rel="external nofollow noopener noreferrer">菱形继承1<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/Y673789476/article/details/128271855#t9"target="_blank" rel="external nofollow noopener noreferrer">菱形继承2<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="49-对虚函数和多态的理解">4.9 对虚函数和多态的理解</h3>
<ul>
<li>①多态的实现主要分为<strong>静态多态</strong>和<strong>动态多态</strong>
<ul>
<li>静态多态主要是重载，在编译的时候就已经确定；</li>
<li>动态多态是用虚函数机制实现的，在运行期间动态绑定。</li>
<li>举个例子: 一个父类类型的指针指向一个子类对象时候，使用父类的指针去调用子类中重写了的父类中的虚函数的时候，会调用子类重写过后的函数，在父类中声明为加了virtual关键字的函数，在子类中重写时候不需要加virtual也是虚函数。</br></li>
</ul>
</li>
<li>②虚函数的实现: 在有虚函数的类中，类的最开始部分是一个虚函数表的指针，这个指针指向一个虚函数表，表中放了虚函数的地址，实际的虚函数在代码段(.text)中。当子类继承了父类的时候也会继承其虚函数表，当子类重写父类中虚函数时候，会将其继承到的虚函数表中的地址替换为重新写的函数地址。使用了虚函数，会增加访问内存开销，降低效率。</br></li>
</ul>
<h3 id="410-简述类成员函数的重写overwrite重载overload和隐藏的区别">4.10 简述类成员函数的重写(overwrite)、重载(overload)和隐藏的区别</h3>
<p>(1)重写和重载主要有以下几点不同。</p>
<blockquote>
<p>①范围的区别：被重写的函数和重写的函数在两个类中，而重载和被重载的函数在同一个类中。</br>
②参数的区别：被重写函数和重写函数的参数列表一定相同，而被重载函数和重载函数的参数列表一定不同。</br>
③virtual的区别：重写的基类中被重写的函数必须要有virtual修饰，而重载函数和被重载函数可以被virtual修饰，也可以没有。</br></p>
</blockquote>
<p>(2)隐藏和重写、重载有以下几点不同。</p>
<blockquote>
<p>与重载的范围不同：和重写一样，隐藏函数和被隐藏函数不在同一个类中。</br>
参数的区别：隐藏函数和被隐藏的函数的参数列表可以相同，也可不同，但是函数名肯定要相同。 当参数不相同时，无论基类中的参数是否被virtual 修饰，基类的函数都是被隐藏，而不是被重写。</br>
注意：虽然重载和覆盖都是实现多态的基础，但是两者实现的技术完全不相同，达到的目的也是完全不同的，覆盖是动态态绑定的多态，而重载是静态绑定的多态。</p>
</blockquote>
<h3 id="411-链表和数组有什么区别">4.11 链表和数组有什么区别</h3>
<p><strong>存储形式:</strong></p>
<blockquote>
<p>数组是一块连续的空间，声明时就要确定长度。</br>
链表是一块可不连续的动态空间， 长度可变，每个结点要保存相邻结点指针。</br></p>
</blockquote>
<p><strong>数据查找:</strong></p>
<blockquote>
<p>数组的线性查找速度快，查找操作直接使用偏移地址。</br>
链表需要按顺序检索结点， 效率低。</br>
数据插入或删除: 链表可以快速插入和删除结点，而数组则可能需要大量数据移动。</br></p>
</blockquote>
<p><strong>越界问题：</strong></p>
<blockquote>
<p>链表不存在越界问题，数组有越界问题。</p>
</blockquote>
<p><strong>注意：</strong></p>
<blockquote>
<p>在选择数组或链表数据结构时，一定要根据实际需要进行选择。数组便于查询，链表便于插入删除。数组节省空间但是长度固定，链表虽然变长但是占了更多的存储空间。</p>
</blockquote>
<h3 id="412-用两个栈实现一个队列的功能">4.12 用两个栈实现一个队列的功能</h3>
<div class="highlight" id="id-30"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">node</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="kt">int</span> <span class="n">data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="n">node</span> <span class="o">*</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span><span class="n">node</span><span class="p">,</span><span class="o">*</span><span class="n">LinkStack</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//创建空栈：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LinkStack</span> <span class="nf">CreateNULLStack</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="n">S</span> <span class="o">=</span> <span class="p">(</span><span class="n">LinkStack</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="k">sizeof</span><span class="p">(</span> <span class="n">node</span> <span class="p">)</span> <span class="p">);</span> <span class="c1">// 申请新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;Fail to malloc a new node.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">S</span><span class="o">-&gt;</span><span class="n">data</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//初始化新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//栈的插入函数：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LinkStack</span> <span class="nf">Push</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">,</span> <span class="kt">int</span> <span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="p">)</span> <span class="c1">//检验栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;There no node in stack!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">LinkStack</span> <span class="n">p</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">LinkStack</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="k">sizeof</span><span class="p">(</span> <span class="n">node</span> <span class="p">)</span> <span class="p">);</span> <span class="c1">// 申请新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;Fail to malloc a new node.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">p</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="k">else</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">p</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">p</span><span class="o">-&gt;</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">;</span> <span class="c1">//初始化新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">p</span><span class="p">;</span> <span class="c1">//插入新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//出栈函数：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">node</span> <span class="nf">Pop</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="n">node</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span><span class="p">.</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="p">)</span> <span class="c1">//检验栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;There no node in stack!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;The stack is NULL,can&#39;t pop!</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">LinkStack</span> <span class="n">p</span> <span class="o">=</span> <span class="n">S</span> <span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span> <span class="c1">//节点出栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">free</span><span class="p">(</span> <span class="n">p</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"> <span class="n">p</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//双栈实现队列的入队函数：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LinkStack</span> <span class="nf">StackToQueuPush</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">,</span> <span class="kt">int</span> <span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="n">node</span> <span class="n">n</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">LinkStack</span> <span class="n">S1</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">CreateNULLStack</span><span class="p">(</span> <span class="n">S1</span> <span class="p">);</span> <span class="c1">//创建空栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="k">while</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">!=</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="p">)</span> <span class="c1">//S 出栈入S1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">n</span> <span class="o">=</span> <span class="n">Pop</span><span class="p">(</span> <span class="n">S</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Push</span><span class="p">(</span> <span class="n">S1</span><span class="p">,</span> <span class="n">n</span><span class="p">.</span><span class="n">data</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">Push</span><span class="p">(</span> <span class="n">S1</span><span class="p">,</span> <span class="n">data</span> <span class="p">);</span> <span class="c1">//新结点入栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="k">while</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">!=</span> <span class="n">S1</span><span class="o">-&gt;</span><span class="n">next</span> <span class="p">)</span> <span class="c1">//S1 出栈入S
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">n</span> <span class="o">=</span> <span class="n">Pop</span><span class="p">(</span> <span class="n">S1</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Push</span><span class="p">(</span> <span class="n">S</span><span class="p">,</span> <span class="n">n</span><span class="p">.</span><span class="n">data</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意：用两个栈能够实现一个队列的功能，那用两个队列能否实现一个栈的功能呢？结果是否定的，因为栈是先进后出，将两个栈连在一起，就是先进先出。而队列是现先进先出，无论多少个连在一起都是先进先出，而无法实现先进后出。</p>
<h3 id="413-共享数据的保护">4.13 共享数据的保护</h3>
<p>①常引用：使所引用的形参不能被更新</p>
<div class="highlight" id="id-31"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">display</span><span class="p">(</span><span class="k">const</span> <span class="kt">double</span><span class="o">&amp;</span> <span class="n">a</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><p>②常对象：在生存期内不能被更新，但必须被初始化</p>
<div class="highlight" id="id-32"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">A</span> <span class="k">const</span> <span class="nf">a</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><p>③常成员函数：
不能修改对象中数据成员，也不能调用类中没有被const 修饰的成员函数(常对象唯一的对外接口).如果声明了一个常对象，则该对象只能调用他的常函数！-&gt;可以用于对重载函数的区分;</p>
<div class="highlight" id="id-33"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">print</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">print</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div><p>④<code>extern int a</code>:使其他文件也能访问该变量</p>
<ul>
<li>声明一个函数或定义函数时，冠以static的话，函数的作用域就被限制在了当前编译单元，当前编译单元内也必须包含函数的定义，也只在其编译单元可见，其他单元不能调用这个函数(每一个cpp文件就是一个编译单元)。</li>
</ul>
<h3 id="414-程序内存分配方式以及它们的区别">4.14 程序内存分配方式以及它们的区别</h3>
<p>内存分配大致上可以分成5块：</p>
<ul>
<li><strong>栈区(stack)</strong></br>
<ul>
<li>栈，就是那些由编译器在需要时分配，在不需要的时候自动清除的变量的存储区。里面的变量通常是<u>局部变量</u>、<u>函数参数</u>等。(由编译器管理)</br></li>
</ul>
</li>
<li><strong>堆区(heap)</strong></br>
<ul>
<li>一般由程序员分配、释放，若程序员不释放，程序结束时可能由系统回收。注意，它与数据结构中的堆是两回事，分配方式类似于链表。</br></li>
</ul>
</li>
<li><strong>全局区(静态区)(static)</strong></br>
<ul>
<li>全局变量和静态变量被分配到同一块内存中。程序结束后由系统释放。</br></li>
</ul>
</li>
<li><strong>常量存储区</strong></br>
<ul>
<li>常量字符串就是放在这里的，不允许修改，程序结束后由系统释放。</br></li>
</ul>
</li>
<li><strong>程序代码区</strong></br>
<ul>
<li>存放函数体的二进制代码。</br></li>
</ul>
</li>
</ul>
<p>C++程序在执行时，将内存大方向划分为4个区域:</p>
<ul>
<li>
<p>程序运行前</p>
<ul>
<li><font color=red>代码区</font>：存放函数体的二进制代码，由操作系统进行管理的</br></li>
<li><font color=red>全局区</font>：存放全局变量和静态变量以及常量</br></li>
</ul>
</li>
<li>
<p>程序运行后</p>
<ul>
<li><font color=red>栈区</font>：由编译器自动分配释放, 存放函数的参数值,局部变量等</br></li>
<li><font color=red>堆区</font>：由程序员分配和释放,若程序员不释放,程序结束时由操作系统回收</br></li>
</ul>
</li>
</ul>
<p>内存四区意义：</p>
<blockquote>
<p>不同区域存放的数据，赋予不同的生命周期, 给我们更大的灵活编程</p>
</blockquote>
<h3 id="415-explicit">4.15 explicit</h3>
<p>函数声明时加上explicit可以<u>阻止函数参数被隐式转换</u>。</p>
<div class="highlight" id="id-34"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="n">Class</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="k">explicit</span> <span class="nf">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Void</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="n">A</span> <span class="n">a1</span><span class="o">=</span><span class="mi">12</span><span class="p">;</span>   <span class="c1">//不加explicit时会被隐式转换位 A a1=A(12);加了此时编译器会报错。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>被声明为explicit的构造函数通常比non-explicit 函数更受欢迎。</p>
<h3 id="416-mutable关键字">4.16 mutable关键字</h3>
<blockquote>
<p>mutable的中文意思是“可变的，易变的”，跟constant(既C++中的const)是反义词。在C++中，mutable也是为了突破const的限制而设置的。</br>
被mutable修饰的变量(mutable只能用于修饰类的非静态数据成员)，将永远处于可变的状态，即使在一个const函数中。</br>
我们知道，假如类的成员函数不会改变对象的状态，那么这个成员函数一般会声明为const。但是，有些时候，我们<font color=purple>需要在const的函数里面修改一些跟类状态无关的数据成员，那么这个数据成员就应该被mutalbe来修饰</font>。(使用mutable修饰的数据成员可以被const成员函数修改)。</br></p>
</blockquote>
<h3 id="417-用const修饰函数的返回值">4.17 用const修饰函数的返回值</h3>
<p>如果给以“指针传递”方式的函数返回值加const修饰，那么函数返回值(即指针)的内容不能被修改，该返回值只能被赋给加const修饰的同类型指针。例如函数：</p>
<div class="highlight" id="id-35"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span> <span class="nf">GetString</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 如下语句将出现编译错误：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">char</span><span class="o">*</span><span class="n">str</span> <span class="o">=</span> <span class="n">GetString</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 正确的用法是
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">str</span> <span class="o">=</span><span class="n">GetString</span><span class="p">();</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="418-宏const和enum">4.18 宏、const和enum</h3>
<ul>
<li><code>#define</code>不被视为语言的一部分。对于单纯常量，最好用const对象或者enum替换<code>#define</code>。</br></li>
<li>对于类似函数的宏，尽量使用内联函数inline替换掉<code>#define</code></br></li>
<li>enum枚举类型是被当做 int 或者 unsigned int 类型来处理的。</br></li>
</ul>
<h3 id="419-static-对象和-non-local-static对象区别">4.19 static 对象和 non-local static对象区别</h3>
<blockquote>
<p>①C++中的static对象是指存储区不属于stack和heap、&ldquo;寿命&quot;从被构造出来直至程序结束为止的对象。</br>
②这些对象包括全局对象，定义于namespace作用域的对象，在class、function以及file作用域中被声明为static的对象。</br>
③其中，函数内的static对象称为local static对象，而其它static对象称为non-local static对象。</br></p>
</blockquote>
<p>local static 对象和non-local static对象在何时被初始化(构造)这个问题上存在细微的差别:</p>
<blockquote>
<p>①对于local static对象，在其所属的函数被调用之前，该对象并不存在，即只有在第一次调用对应函数时，local static对象才被构造出来。</br>
②而对于non-local static对象，在main()函数开始前就已经被构造出来，并在main()函数结束后被析构。</br></p>
</blockquote>
<p>&lt;/font color=red&gt;建议：</font></p>
<blockquote>
<p>1.对内置对象进行手工初始化，因为C++不保证初始化它们。</br>
2.构造函数最好使用成员初值列，而不要在构造函数本体中使用赋值操作。初值列中列出的成员变量，其排序次序应该和它们在class中的声明次序相同(初始化顺序与声明变量顺序一致)。</br>
3.为免除“跨编译单元的初始化次序问题”，尽量以local static对象替换non-local static对象。</br></p>
</blockquote>
<h3 id="420-全局变量和static变量的区别">4.20 全局变量和static变量的区别</h3>
<blockquote>
<p>①全局变量本身就是静态存储方式， 静态全局变量当然也是静态存储方式。 这两者在存储方式上并无不同。</br>
②这两者的区别在于非静态全局变量的作用域是整个源程序， 当一个源程序由多个源文件组成时，非静态的全局变量在各个源文件中都是有效的。</br>
③而静态全局变量则限制了其作用域， 即只在定义该变量的源文件内有效， 在同一源程序的其它源文件中不能使用它。</br>
④由于静态全局变量的作用域局限于一个源文件内，只能为该源文件内的函数公用， 因此可以避免在其它源文件中引起错误。</br></p>
</blockquote>
<h2 id="5-基础知识五">5. 基础知识(五)</h2>
<h3 id="51-为什么栈要比堆速度要快">5.1 为什么栈要比堆速度要快</h3>
<ul>
<li>①首先, 栈是本着LIFO原则的存储机制, 对栈数据的定位相对比较快速, 而堆则是随机分配的空间, 处理的数据比较多, 无论如何, 至少要两次定位.</li>
<li>②其次, 栈是由CPU提供指令支持的, 在指令的处理速度上, 对栈数据进行处理的速度自然要优于由操作系统支持的堆数据.</li>
<li>③再者, 栈是在一级缓存中做缓存的, 而堆则是在二级缓存中, 两者在硬件性能上差异巨大.</li>
<li>④最后, 各语言对栈的优化支持要优于对堆的支持, 比如swift语言中, 三个字及以内的struct结构, 可以在栈中内联, 从而达到更快的处理速度.</li>
</ul>
<h3 id="52-c-析构函数调用时间">5.2 c++ 析构函数调用时间</h3>
<ul>
<li>对象生命周期结束，被销毁时</li>
<li>delete指向对象的指针时，或delete指向对象的基类类型指针，而其基类析构函数是虚函数时</li>
<li>对象i是对象o的成员，o的析构函数被调用时，对象i的析构函数也被调用</li>
</ul>
<h3 id="53-静态绑定-动态绑定-也叫动态连编静态连编">5.3 静态绑定 动态绑定 (也叫动态连编，静态连编)</h3>
<blockquote>
<p>如果父类中存在有虚函数，那么编译器便会为之生成虚表(属于类)与虚指针(属于某个对象)，在程序运行时，根据虚指针的指向，来决定调用哪个虚函数，这称之与动态绑定，与之相对的是静态绑定，静态绑定在编译期就决定了。</p>
</blockquote>
<p>class和template都支持接口与多态；
①对classes而言，接口是显式的，以函数签名为中心。多态则是通过virtual函数(虚函数)发生于运行期；
②对template参数而言，接口是隐式的，奠基于有效表达式。多态则是通过template具现化和函数重载解析发生于编译期。
<strong>泛型</strong>
泛型是通过参数化类型来实现在同一份代码上操作多种数据类型。利用“参数化类型”将类型抽象化，从而实现灵活的复用。</p>
<h3 id="54-c语言的指针和c的引用有什么区别">5.4 C语言的指针和c++的引用有什么区别？</h3>
<blockquote>
<ul>
<li>指针有自己的一块空间，指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元，即指针是一个实体。而引用只是一个别名；</br></li>
<li>使用sizeof看一个指针的大小是4，而引用则是被引用对象的大小；</br></li>
<li>指针可以被初始化为NULL，而引用必须被初始化且必须是一个已有对象的引用；</br></li>
<li>作为参数传递时，指针需要被解引用才可以对对象进行操作，而直接对引用的修改都会改变引用所指向的对象；</br></li>
</ul>
</blockquote>
<h3 id="55-请你说说c语言是怎么进行函数调用的">5.5 请你说说C语言是怎么进行函数调用的</h3>
<blockquote>
<p>每一个函数调用都会分配函数栈，在栈内进行函数执行过程。调用前，先把返回地址压栈，然后把当前函数的esp指针压栈。(ESP(Extended Stack Pointer)为扩展栈指针寄存器，是指针寄存器的一种，用于存放函数栈顶指针)</br></p>
</blockquote>
<p>C语言参数压栈顺序？：从右到左</p>
<h3 id="56-c中拷贝赋值函数的形参能否进行值传递">5.6 C++中拷贝赋值函数的形参能否进行值传递？</h3>
<p>不能。如果是这种情况下，调用拷贝构造函数的时候，首先要将实参传递给形参，这个传递的时候又要调用拷贝构造函数(aa = ex.aa; //此处调用拷贝构造函数)。如此循环，无法完成拷贝，栈也会满。</p>
<h3 id="57-include头文件的顺序以及双引号和尖括号的区别">5.7 include头文件的顺序以及双引号””和尖括号&lt;&gt;的区别</h3>
<p>编译器预处理阶段查找头文件的路径不一样</p>
<blockquote>
<p>使用双引号包含的头文件，查找头文件路径的顺序为：</br>
①当前头文件目录</br>
②编译器设置的头文件路径(编译器可使用-I显式指定搜索路径)</br>
③系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径</br>
对于使用尖括号包含的头文件，查找头文件的路径顺序为：</br>
①编译器设置的头文件路径(编译器可使用-I显式指定搜索路径)</br>
②系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径</br></p>
</blockquote>
<h3 id="58-一个c源文件从文本到可执行文件经历的过程">5.8 一个C++源文件从文本到可执行文件经历的过程</h3>
<p>对于C/C++编写的程序，从源代码到可执行文件，一般经过下面四个步骤：</br></p>
<ul>
<li>预编译，预编译的时候做一些简单的文本替换，比如宏替换，而不进行语法的检查；</li>
<li>编译，在编译阶段，编译器将检查一些语法错误，但是，如果使用的函数事先没有定义这种情况，不再这一阶段检查，编译后，得到.s文件</li>
<li>汇编，将C/C++代码变为汇编代码，得到.o或者.obj文件</li>
<li>链接，将所用到的外部文件链接在一起，在这一阶段，就会检查使用的函数有没有定义</li>
</ul>
<p>链接过后，形成可执行文件.exe
详细请参阅: <a href="https://blog.csdn.net/daaikuaichuan/article/details/89060957"target="_blank" rel="external nofollow noopener noreferrer">一个C++源文件从文本到可执行文件经历的过程<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="59-内存泄漏原因和判断方法">5.9 内存泄漏原因和判断方法</h3>
<p>内存泄漏通常是因为调用了malloc/new等内存申请操作，但是缺少了对应的free/delete。
为了判断内存是否泄漏，我们一方面可以使用Linux环境下的内存泄漏检查工具Valgrind，另一方面我们写代码的时候，可以添加内存申请和释放的统计功能，统计当前申请和释放的内存是否一致，以此来判断内存是否有泄漏。</p>
<p>内存泄漏分类：</p>
<ul>
<li>堆内存泄漏(heap leak)。堆内存值得是程序运行过程中根据需要分配通过malloc\realloc\new等从堆中分配的一块内存，再完成之后必须要通过调用对应的free或者delete删除。如果程序的设计的错误导致这部分内存没有被释放，那么此后这块内存将不会被使用，就会产生Heap Leak。</li>
<li>系统资源泄露(Resource Leak)。主要指程序使用系统分配的资源比如 Bitmap，handle，SOCKET等没有使用相应的函数释放掉，导致系统资源的浪费，严重可导致系统效能降低，系统运行不稳定。</li>
<li>没有将基类的析构函数定义为虚函数。当基类指针指向子类对象时，如果基类的析构函数不是virtual，那么子类的析构函数将不会被调用，子类的资源没有正确的释放，从而造成内存泄漏。</li>
</ul>
<h3 id="510-段错误的产生原因">5.10 段错误的产生原因</h3>
<p><strong>段错误是什么?</strong></p>
<blockquote>
<p>一句话来说，段错误是指访问的内存超出了系统给这个程序所设定的内存空间，例如访问了不存在的内存地址、访问了系统保护的内存地址、访问了只读的内存地址等等情况。这里贴一个对于“段错误”的准确定义。</br></p>
</blockquote>
<p><strong>段错误产生的原因</strong></p>
<blockquote>
<p>访问不存在的内存地址</br>
访问系统保护的内存地址</br>
访问只读的内存地址</br>
栈溢出</br>
详细请参阅：<a href="https://www.cnblogs.com/lidabo/p/4545625.html"target="_blank" rel="external nofollow noopener noreferrer">Linux环境下段错误的产生原因及调试方法小结<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
</blockquote>
<h3 id="511-c-函数调用过程">5.11 C++ 函数调用过程</h3>
<p>总结起来整个过程就三步：</p>
<blockquote>
<p>1)根据调用的函数名找到函数入口；</br>
2)在栈中申请调用函数中的参数及函数体内定义的变量的内存空间</br>
3)函数执行完后，释放函数在栈中的申请的参数和变量的空间，最后返回值(如果有的话)</br></p>
</blockquote>
<p>详细请查阅：<a href="https://www.cnblogs.com/biyeymyhjob/archive/2012/07/20/2601204.html"target="_blank" rel="external nofollow noopener noreferrer">函数调用过程 / C/C++函数调用过程分析<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="512-如何调试c多线程程序">5.12 如何调试c++多线程程序？</h3>
<ol>
<li>打印日志，日志中加上线程ID；(简单粗暴)
gdb有thread相关命令，如infothread(简写infoth)显示线程消息，bxxthreadyy可以</br></li>
<li>对某个thread设置断点，threadxx(简写成thrxx)切换到某个thread。再配合frame(简写f)相关的命令(比如up，down在不同frame间跳转)，基本可以处理若干个不同的线程间的debug……</br>
详细请查阅：<a href="https://www.cnblogs.com/LuckCoder/p/10948242.html"target="_blank" rel="external nofollow noopener noreferrer">C++(vs)多线程调试 (转)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ol>
<h3 id="513面向对象和面向过程的区别">5.13 面向对象和面向过程的区别</h3>
<blockquote>
<p>①面向对象方法中，把数据和数据操作放在一起，组成对象；对同类的对象抽象出其共性组成类；类通过简单的接口与外界发生联系，对象和对象之间通过消息进行通信。</br>
②面向对象的三大特性是&quot;封装、“多态”、“继承”，五大原则是&quot;单一职责原则&rdquo;、“开放封闭原则”、“里氏替换原则”、“依赖倒置原则”、“接口分离原则”。</br>
③而面向过程方法是以过程为中心的开发方法，它自顶向下顺序进行， 程序结构按照功能划分成若干个基本模块，这些模块形成树状结构。</br></p>
</blockquote>
<p><strong>(过程)优点：</strong></p>
<ul>
<li>性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗源;比如嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。</li>
<li>缺点：没有面向对象易维护、易复用、易扩展。</li>
</ul>
<p><strong>(对象)优点：</strong></p>
<ul>
<li>易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统。</li>
<li>缺点：性能比面向过程低。</li>
</ul>
<h3 id="514-关于引用赋值的多态">5.14 关于引用赋值的多态：</h3>
<div class="highlight" id="id-36"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">B</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">Class</span> <span class="nl">D</span> <span class="p">:</span> <span class="k">public</span> <span class="n">B</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">B</span><span class="o">&amp;</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">D</span><span class="o">&amp;</span> <span class="n">d</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span><span class="o">&amp;</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">d</span> <span class="p">;</span>  <span class="c1">//父类可以作为子类的引用，此时b1表现和指针形式一致(会调用B的非虚函数)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">D</span><span class="o">&amp;</span> <span class="n">d1</span> <span class="o">=</span> <span class="n">b</span><span class="err">；</span> <span class="c1">//错误，不能将子类作为父类的引用
</span></span></span><span class="line"><span class="cl"><span class="c1">//父类可以作为子类的引用，此时b1表现和指针形式一致(会调用B的非虚函数) 
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="515-模板的声明和实现不能分开的原因">5.15 模板的声明和实现不能分开的原因</h3>
<blockquote>
<p>链接的时候，需要实例化模板，这时候就需要找模板的具体实现了。假设在main函数中调用了一个模板函数，这时候就需要去实例化该类型的模板。注意main函数里面只包含了.h文件，也就是只有模板的声明，没有具体实现。就会报错。
而模板的实现.cpp里面，虽然有模板的具体实现，但是没有谁在该.cpp里面使用一个模板函数，就不会生成一个具体化的实例
详细请参阅：<a href="https://www.cnblogs.com/callme/articles/6142129.html"target="_blank" rel="external nofollow noopener noreferrer">C++ 模板类的声明与实现分离问题<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> / ​​​​​<a href="https://blog.csdn.net/weixin_40539125/article/details/83375452?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param"target="_blank" rel="external nofollow noopener noreferrer">​C++ 模板类的声明与实现分离问题(模板实例化)​​​​​​<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<h3 id="516-c类中引用成员和常量成员的初始化初始化列表">5.16 C++类中引用成员和常量成员的初始化(初始化列表)</h3>
<p>如果一个类是这样定义的：</p>
<div class="highlight" id="id-37"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">pram1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram2</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">      <span class="kt">int</span> <span class="n">a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="kt">int</span> <span class="o">&amp;</span><span class="n">b</span><span class="p">;</span> <span class="c1">// 引用成员
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">c</span><span class="p">;</span> <span class="c1">// 常量成员
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>假如在构造函数中对三个私有变量进行赋值则通常会这样写：</p>
<div class="highlight" id="id-38"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">A</span><span class="o">::</span><span class="n">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">pram1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram2</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">a</span><span class="o">=</span><span class="n">pram1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">b</span><span class="o">=</span><span class="n">pram2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">c</span><span class="o">=</span><span class="n">pram3</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>但是，这样是编译不过的。因为常量和引用初始化必须赋值。所以上面的构造函数的写法只是简单的赋值，并不是初始化。</p>
<p>正确写法应该是：</p>
<div class="highlight" id="id-39"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="n">A</span><span class="o">::</span><span class="n">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">pram1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram2</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram3</span><span class="p">)</span><span class="o">:</span><span class="n">b</span><span class="p">(</span><span class="n">pram2</span><span class="p">),</span><span class="n">c</span><span class="p">(</span><span class="n">pram3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="o">=</span><span class="n">pram1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>采用初始化列表实现了对常量和引用的初始化。采用括号赋值的方法，括号赋值只能用在变量的初始化而不能用在定义之后的赋值。
凡是有引用类型的成员变量或者常量类型的变量的类，<font color=red>不能有缺省构造函数</font>。默认构造函数没有对引用成员提供默认的初始化机制，也因此造成引用未初始化的编译错误。并且必须<strong>使用初始化列表进行初始化const对象、引用对象</strong>。</p>
</blockquote>
<h3 id="517-memset为int型数组初始化问题">5.17 memset为int型数组初始化问题</h3>
<p>头文件：<code>#include &lt;string.h&gt;</code>
<code>memset()</code> 函数用来将指定内存的前n个字节设置为特定的值，其原型为：</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>函数说明<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content"><div class="highlight" id="id-42"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="o">*</span> <span class="nf">memset</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span> <span class="n">ptr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">value</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">num</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><p>参数说明：</br></p>
<ul>
<li><code>ptr</code> 为要操作的内存的指针。</br></li>
<li><code>value</code> 为要设置的值。你既可以向 value 传递 int 类型的值，也可以传递 char 类型的值，int 和 char 可以根据 ASCII 码相互转换。</br></li>
<li><code>num</code> 为 ptr 的前 num 个字节，size_t 就是unsigned int。</br></li>
</ul>
<p>memset() 会将 ptr 所指的内存区域的前 num 个字节的值都设置为 value，然后返回指向 ptr 的指针。</p>
</div>
    </div>
  </div>
<p>无法下面这样初始化，这样的结果是a被赋值成168430090，168430090&hellip;..</p>
<div class="highlight" id="id-40"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span> <span class="c1">// array
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这是因为int由4个字节(说)表示，并且不能得到数组a中整数的期望值。</p>
<p>但我经常看到程序员使用memset将int数组元素设置为 0 或 -1。其他值不行！</p>
<div class="highlight" id="id-41"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="n">memset</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">b</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="c1">//假设a为int型数组：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mh">0x7f</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="c1">//a数组每个空间将被初始化为0x7f7f7f7f,原因是C函数传参过程中的指针降级，导致sizeof(a)，返回的是一个 something*指针类型大小的的字节数，如果是32位，就是4字节。所以memset按字节赋值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="mh">0xaf</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="c1">//a数组每个空间将被初始化为0xafafafaf
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="518-编译器对-inline-函数的处理步骤">5.18 编译器对 inline 函数的处理步骤</h3>
<p>编译器对 inline 函数的处理:</p>
<ul>
<li>将 inline 函数体复制到 inline 函数调用点处;</br></li>
<li>为所有 inline 函数中的局部变量分配内存空间;</br></li>
<li>将 inline 函数的输入参数和返回值映射到调用方法的局部变量空间中;</br></li>
<li>如果 inline 函数有多个返回点，将其转变为 inline 函数代码块末尾的分支(使用 GOTO);</br></li>
</ul>
<p>优点:</p>
<ul>
<li>内联函数同宏函数一样将在被调用处进行代码展开，省去了<u>参数压栈</u>、<u>栈帧开辟与回收</u>，<u>结果返回</u>等，从而提高程序运行速度。</br></li>
<li>内联函数相比宏函数来说，在代码展开时，会做安全检查或自动类型转换(同普通函数)，而宏定义则不会。</br></li>
<li>在类中声明同时定义的成员函数，自动转化为内联函数，因此内联函数可以访问类的成员变量，宏定义则不能。</br></li>
<li>内联函数在运行时可调试，而宏定义不可以。</br></li>
</ul>
<p>缺点:</p>
<ul>
<li>代码膨胀。内联是以代码膨胀(复制)为代价，消除函数调用带来的开销。如果执行函数体内代码的时间，相比于函数调用的开销较大，那么效率的收获会很少。另一方面，每一处内联函数的调用都要复制代码，将使程序的总代码量增大，消耗更多的内存空间。</br></li>
<li>inline 函数无法随着函数库升级而升级。inline函数的改变需要重新编译，不像 non-inline 可以直接链接。</br></li>
<li>是否内联，程序员不可控。内联函数只是对编译器的建议，是否对函数内联，决定权在于编译器。</br></li>
</ul>
<h3 id="519-虚函数virtual可以是内联函数inline吗">5.19 虚函数(virtual)可以是内联函数(inline)吗？</h3>
<blockquote>
<p>虚函数可以是内联函数，内联是可以修饰虚函数的，但是<strong>当虚函数表现多态性的时候不能内联</strong>。</br>
内联是在<strong>编译期</strong>建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时(运行期)不可以内联。</br>
inline virtual 唯一可以内联的时候是：编译器知道所调用的对象是哪个类(如 <code>Base::who()</code>)，这只有在编译期具有实际对象而不是对象的指针或引用时才会发生;</br></p>
</blockquote>
<h3 id="520静态库和动态库比较">5.20 静态库和动态库比较</h3>
<p>静态库 (.a、.lib):</p>
<ul>
<li>将静态库的内容添加到程序中，此时程序的空间，变成了源程序空间大小 + 静态库空间大小。</li>
</ul>
<p>动态库(共享库)(.so、.dll):</p>
<ul>
<li>常驻内存，当程序需要调用相关函数时，会从内存调用。</li>
</ul>
<p>区别:</p>
<ul>
<li>静态库：对空间要求较低，而时间要求较高的核心程序中。(.a、.lib) </br></li>
<li>动态库：对时间要求较低，对空间要求较高。(.so、.dll) </br></li>
</ul>
<p><a href="https://blog.csdn.net/m0_46245582/article/details/124027320"target="_blank" rel="external nofollow noopener noreferrer">hash<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="6-基础知识六">6 基础知识(六)</h2>
<h3 id="61-构造函数为什么不能定义为虚函数-析构函数般写成虚函数的原因-">6.1 构造函数为什么不能定义为虚函数？ ⽽析构函数⼀般写成虚函数的原因 ？</h3>
<p>构造函数不能声明为虚函数的原因是:</p>
<blockquote>
<p>1 构造一个对象的时候，必须知道对象的实际类型，而虚函数行为是在运行期间确定实际类型的。而在构造一个对象时，由于对象还未构造成功。编译器无法知道对象的实际类型，是该类本身，还是该类的一个派生类，或是更深层次的派生类。无法确定。</br>
2 虚函数的执行依赖于虚函数表。而<strong>虚函数表在构造函数中进行初始化工作</strong>，即初始化vptr，让他指向正确的虚函数表。而在构造对象期间，虚函数表还没有被初始化，将无法进行。</br></p>
</blockquote>
<p>虚函数的意思就是开启动态绑定，程序会根据对象的动态类型来选择要调用的方法。然而在构造函数运行的时候，这个对象的动态类型还不完整，没有办法确定它到底是什么类型，故构造函数不能动态绑定。(动态绑定是根据对象的动态类型而不是函数名，在调用构造函数之前，这个对象根本就不存在，它怎么动态绑定？)
编译器在调用基类的构造函数的时候并不知道你要构造的是一个基类的对象还是一个派生类的对象。</p>
<blockquote>
<p>析构函数设为虚函数的作用:
解释：在类的继承中，如果有基类指针指向派生类，那么用基类指针delete时，如果不定义成虚函数，派生类中派生的那部分无法析构。(如果基类的析构函数不是虚函数，那么在delete 基类指针时，只调用基类的析构函数，不会调用派生类的析构函数，故派生类部分不会被析构。)</p>
</blockquote>
<p>ref:</br>
[1]. <a href="https://blog.csdn.net/Yangy_Jiaojiao/article/details/127588598"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Yangy_Jiaojiao/article/details/127588598<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2]. <a href="https://blog.csdn.net/Yangy_Jiaojiao/article/details/128145609"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Yangy_Jiaojiao/article/details/128145609<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
<p>参考(待补充):</br>
[1]. <a href="https://zhuanlan.zhihu.com/p/401341063"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/401341063<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2]. <a href="https://zhuanlan.zhihu.com/p/602866792"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/602866792<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
<p>ref: <a href="https://blog.csdn.net/m0_46245582/category_11569287.html"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/m0_46245582/category_11569287.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>强化学习笔记 [12] | Dueling DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_12/</link><pubDate>Sun, 25 Feb 2024 11:16:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_12/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9797695.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十一) Prioritized Replay DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。</p>
<h1 id="1-dueling-dqn的优化点考虑">1. Dueling DQN的优化点考虑</h1>
<p>在前面讲到的DDQN中，我们通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，我们通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，我们尝试通过<font color=red>优化神经网络的结构</font>来优化算法。</p>
<p>具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态 $S$有关，与具体要采用的动作 $A$无关，这部分我们叫做<strong>价值函数部分</strong>，记做 $V(S,w,α)$,第二部分同时与状态状态 $S$ 和动作 $A$有关，这部分叫做**优势函数(Advantage Function)**部分,记为 $A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+A(S,A,w,\beta)$$</p>
<p>其中，$w$ 是公共部分的网络参数，而 $α$ 是价值函数独有部分的网络参数，而 $β$ 是优势函数独有部分的网络参数。</p>
<h1 id="2-dueling-dqn网络结构">2. Dueling DQN网络结构</h1>
<p>由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。为了简化算法描述，这里不使用原论文的CNN网络结构，而是使用前面文中用到的最简单的三层神经网络来描述。是否使用CNN对Dueling DQN算法本身无影响。</p>
<p>在前面讲到的DDQN等DQN算法中，我使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络与Dueling DQN</div>
</center>
<br>
<p>而在Dueling DQN中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。</p>
<p>我们可以直接使用上一节的价值函数的组合公式得到我们的动作价值，但是这个式子无法辨识最终输出里面 $V(S,w,α)$ 和 $A(S,A,w,β)$各自的作用，为了可以体现这种可辨识性(identifiability),实际使用的组合公式如下：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+(A(S,A,w,\beta)-\frac1{\mathcal{A}}\sum_{a^{\prime}\in\mathcal{A}}A(S,a^{\prime},w,\beta))$$</p>
<p>其实就是对优势函数部分做了中心化的处理。以上就是Dueling DQN的主要算法思路。由于它仅仅涉及神经网络的中间结构的改进，现有的DQN算法可以在使用Duel DQN网络结构的基础上继续使用现有的算法。由于算法主流程和其他算法没有差异，这里就不单独讲Duel DQN的算法流程了。</p>
<h1 id="3-dueling-dqn实例">3. Dueling DQN实例</h1>
<p>下面我们用一个具体的例子来演示Dueling DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>这个实例代基于Nature DQN，并将网络结构改为上图中右边的Dueling DQN网络结构，完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Dueling DQN和Nature DQN的代码的不同之处。也就是网络结构定义部分，主要的代码如下，一共有两个相同结构的Q网络，每个Q网络都有状态函数和优势函数的定义，以及组合后的Q网络输出，如代码红色部分：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W21</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b21</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W21</span><span class="p">)</span> <span class="o">+</span> <span class="n">b21</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W22</span><span class="p">)</span> <span class="o">+</span> <span class="n">b22</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2v</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2a</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分代码和Nature DQN基本相同。当然，我们可以也在前面DDQN，Prioritized Replay DQN代码的基础上，把网络结构改成上面的定义，这样Dueling DQN也可以起作用。</p>
<h1 id="4-dqn总结">4. DQN总结</h1>
<p>DQN系列我花了5篇来讲解，一共5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。</p>
<p>当然DQN家族的算法远远不止这些，还有一些其他的DQN算法我没有详细介绍，比如使用一些较复杂的CNN和RNN网络来提高DQN的表达能力，又比如改进探索状态空间的方法等，主要是在DQN的基础上持续优化。</p>
<p>DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。虽然NAF DQN可以解决这个问题，但是方法过于复杂了。而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题，从下一篇我们开始讨论Policy-Based深度强化学习。</p>
]]></description></item><item><title>强化学习笔记 [13] | 策略梯度(Policy Gradient)</title><link>https://jianye0428.github.io/posts/rl_learning_note_13/</link><pubDate>Sun, 25 Feb 2024 15:35:55 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_13/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradient)，它是Policy Based强化学习方法，基于策略来学习。</p>
<p>本文参考了Sutton的强化学习书第13章和策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-value-based强化学习方法的不足">1. Value Based强化学习方法的不足</h1>
<p>DQN系列强化学习算法主要的 <strong><font color=red>问题</font></strong> 主要有三点。</p>
<ul>
<li>
<p>第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。</p>
</li>
<li>
<p>第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。</p>
</li>
<li>
<p>第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。</p>
</li>
</ul>
<p>由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述类别问题的方法，比如Policy Based强化学习方法。</p>
<h1 id="2-policy-based强化学习方法引入">2. Policy Based强化学习方法引入</h1>
<p>回想我们在Value Based强化学习方法里，我们对价值函数进行了近似表示，引入了一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到近似的动作价值，即：</p>
<p>$$\hat{q}\left(s,a,w\right)\approx q_\pi(s,a)$$</p>
<p>在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略 $π$可以被被描述为一个包含参数 $θ$ 的函数,即：</p>
<p>$$\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)$$</p>
<p>将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？</p>
<h1 id="3-策略梯度的优化目标">3. 策略梯度的优化目标</h1>
<p>我们要用梯度上升来寻找最优的梯度，首先就要找到一个可以优化的函数目标。</p>
<p>最简单的优化目标就是初始状态收获的期望，即优化目标为：</p>
<p>$$J_1(\theta)=V_{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}(G_1)$$</p>
<p>但是有的问题是没有明确的初始状态的，那么我们的优化目标可以定义平均价值，即：
$$J_{avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$$</p>
<p>其中，$d_πθ(s)$ 是基于策略 $π_θ$生成的马尔科夫链关于状态的静态分布。</p>
<p>或者定义为每一时间步的平均奖励，即：</p>
<p>$$J_{avR}(\theta)==\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$$</p>
<p>无论我们是采用 $J_1$, $J_{av}V$, 还是 $J_{av}R$ 来表示优化目标，最终对 $θ$求导的梯度都可以表示为：</p>
<p>$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>具体的证明过程这里就不再列了，如果大家感兴趣，可以去看策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的附录1，里面有详细的证明。</p>
<p>当然我们还可以采用很多其他可能的优化目标来做梯度上升，此时我们的梯度式子里面的 $\nabla_\theta log\pi_\theta(s,a)$ 部分并不改变，变化的只是后面的 $Q_\pi(s,a)$ 部分。对于 $\nabla_\theta log\pi_\theta(s,a)$,我们一般称为<strong>分值函数</strong>(score function)。</p>
<p>现在梯度的式子已经有了，后面剩下的就是策略函数 $\pi_\theta(s,a)$的设计了。</p>
<h1 id="4-策略函数的设计">4. 策略函数的设计</h1>
<p>现在我们回头看一下策略函数 $\pi_\theta(s,a)$ 的设计，在前面它一直是一个数学符号。</p>
<p>最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征 $ϕ(s,a)$ 与参数 $θ$的线性组合来权衡一个行为发生的几率,即:</p>
<p>$$\pi_\theta(s,a)=\frac{e^{\phi(s,a)^T\theta}}{\sum_be^{\phi(s,b)^T\theta}}$$</p>
<p>则通过求导很容易求出对应的分值函数为：</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)=\phi(s,a)-\mathbb{E}_{\pi_\theta}[\phi(s,.)]$$</p>
<p>另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布 $\mathbb{N}(\phi(\mathrm{s})^{\mathbb{T}}\theta,\sigma^2)$中产生。高斯策略对应的分值函数求导可以得到为:</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)==\frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}$$</p>
<p>有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。</p>
<h1 id="5-蒙特卡罗策略梯度reinforce算法">5. 蒙特卡罗策略梯度reinforce算法</h1>
<p>这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数 $v(s)$ 来近似代替策略梯度公式里面的 $Q_π(s,a)$。算法的流程很简单，如下所示:</p>
<ul>
<li>输入：N个蒙特卡罗完整序列,训练步长 $α$</li>
<li>输出：策略函数的参数 $θ$
<ul>
<li>(1). for 每个蒙特卡罗序列:
<ul>
<li>a. 用蒙特卡罗法计算序列每个时间位置t的状态价值 $v_t$</li>
<li>b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数 $θ$：
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t$$</li>
</ul>
</li>
</ul>
</li>
<li>(2).返回策略函数的参数 $θ$</li>
</ul>
</li>
</ul>
<p>　　这里的策略函数可以是softmax策略，高斯策略或者其他策略。</p>
<h1 id="6-策略梯度实例">6. 策略梯度实例</h1>
<p>这里给出第5节的蒙特卡罗策略梯度reinforce算法的一个实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py</p>
<p>这里我们采用softmax策略作为我们的策略函数，同时，softmax的前置部分，也就是我们的策略模型用一个三层的softmax神经网络来表示。这样好处就是梯度的更新可以交给神经网络来做。</p>
<p>我们的softmax神经网络的结构如下，注意这个网络不是价值Q网络，而是策略网络：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_softmax_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_num&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_value&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># softmax layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#softmax output</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">all_act_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;act_prob&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意我们的损失函数是softmax交叉熵损失函数和状态价值函数的乘积，这样TensorFlow后面可以自动帮我们做梯度的迭代优化。</p>
<p>另一个要注意的点就是蒙特卡罗法里面价值函数的计算，一般是从后向前算，这样前面的价值的计算可以利用后面的价值作为中间结果，简化计算，对应代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">      <span class="n">running_add</span> <span class="o">=</span> <span class="n">running_add</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="n">discounted_ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分和之前的DQN的代码类似。</p>
<h1 id="7-策略梯度小结">7. 策略梯度小结</h1>
<p>策略梯度提供了和DQN之类的方法不同的新思路，但是我们上面的蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，我们需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，我们的参数更新的方向很可能不是策略梯度的最优方向。</p>
<p>因此，Policy Based的强化学习方法还需要改进，注意到我们之前有Value Based强化学习方法，那么两者能不能结合起来一起使用呢？下一篇我们讨论Policy Based+Value Based结合的策略梯度方法Actor-Critic。</p>
<p>　　　　</p>
]]></description></item><item><title>强化学习笔记 [14] | Actor-Critic</title><link>https://jianye0428.github.io/posts/rl_learning_note_14/</link><pubDate>Sun, 25 Feb 2024 15:35:58 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_14/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。</p>
<p>在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。</p>
<p>本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。</p>
<h1 id="1-actor-critic算法简介">1. Actor-Critic算法简介</h1>
<p>Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。</p>
<p>回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。</p>
<p>也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：</p>
<p>$$
\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)
$$</p>
<p>第二组是价值函数的近似，对于状态价值和动作价值函数分别是：</p>
<p>$$
\hat{v}(s,w)\approx v_\pi(s)
$$</p>
<p>$$
\hat{q}(s,a,w)\approx q_\pi(s,a)
$$</p>
<p>对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：</p>
<p>$$
\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t
$$</p>
<p>梯度更新部分中，$\nabla_\theta log\pi_\theta(s_t,a_t)$是我们的分值函数，不用动，要变成Actor的话改动的是$v_t$，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。</p>
<p>而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。</p>
<p>现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值$v_t$,而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$,在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{te}$</p>
<h1 id="2-actor-critic算法可选形式">2. Actor-Critic算法可选形式</h1>
<p>在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值 $v_t$实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：</p>
<ul>
<li>
<p>a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)V(s,w)$$</li>
</ul>
</li>
<li>
<p>b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)Q(s,a,w)$$</li>
</ul>
</li>
<li>
<p>c) 基于TD误差：在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了TD误差，它的表达式是 $\delta(t)=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 或者 $\delta(t)=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t)$, 这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)$$</li>
</ul>
</li>
<li>
<p>d) 基于优势函数：在<a href="https://www.cnblogs.com/pinard/p/9923859.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十二) Dueling DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到过优势函数A的定义：$A(S,A,w,\beta)=Q(S,A,w,\alpha,\beta)-V(S,w,\alpha)$, 即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,A,w,\beta)$$</li>
</ul>
</li>
<li>
<p>e) 基于 $TD(λ)$ 误差：一般都是基于后向 $TD(λ)$误差, 在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)E(t)$</li>
</ul>
</li>
</ul>
<p>对于Critic本身的模型参数 $w$ ，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如 $Q(s,a,w)=ϕ(s,a)^Tw$,则Critic本身的模型参数 $w$的更新公式可以表示为：</p>
<p>$$\begin{gathered}
\delta=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t) \\
w=w+\beta\delta\phi(s,a)
\end{gathered}$$</p>
<p>通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。</p>
<h1 id="3-actor-critic算法流程">3. Actor-Critic算法流程</h1>
<p>这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数　　</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，衰减因子 $γ$, 探索率 $ϵ$, Critic网络结构和Actor网络结构。</p>
<p>输出：Actor 网络参数 $θ$, Critic网络参数 $w$</p>
<ul>
<li>(1). 随机初始化所有的状态和动作对应的价值Q�. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor网络中使用 $ϕ(S)$ 作为输入，输出动作 $A$,基于动作 $A$得到新的状态 $S&rsquo;$,反馈 $R$。</li>
<li>c) 在Critic网络中分别使用 $ϕ(S)$，$ϕ(S&rsquo;)$ 作为输入，得到Q值输出 $V(S)$，$V(S&rsquo;)$</li>
<li>d) 计算TD误差 $\delta=R+\gamma V(S^{\prime})-V(S)$</li>
<li>e) 使用均方差损失函数 $\sum(R+\gamma V(S^{\prime})-V(S,w))^2$ 作Critic网络参数 $w$的梯度更新</li>
<li>f) 更新Actor网络参数 $θ$:
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(S_t,A)\delta $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对于Actor的分值函数 $∇_θlogπ_θ(S_t,A)$,可以选择softmax或者高斯分值函数。</p>
<p>上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。</p>
<h1 id="4-actor-critic算法实例">4. Actor-Critic算法实例</h1>
<p>下面我们用一个具体的例子来演示上面的Actor-Critic算法。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法流程可以参考上面的第三节，这里的分值函数我们使用的是softmax函数，和上一片的类似。完整的代码参见Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py</p>
<p>代码主要分为两部分，第一部分是Actor，第二部分是Critic。对于Actor部分，大家可以和上一篇策略梯度的代码对比，改动并不大，主要区别在于梯度更新部分，策略梯度使用是蒙特卡罗法计算出的价值 $v(t)$,则我们的actor使用的是TD误差。</p>
<p>在策略梯度部分，对应的位置如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而我们的Actor对应的位置的代码是：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">td_error</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处要注意的是，由于使用的是TD误差，而不是价值 $v(t)$,此处需要最大化<code>self.exp</code>,而不是最小化它，这点和策略梯度不同。对应的Actor代码为：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除此之外，Actor部分的代码和策略梯度的代码区别并不大。</p>
<p>对于Critic部分，我们使用了类似于DQN的三层神经网络。不过我们简化了这个网络的输出，只有一维输出值，而不是之前DQN使用的有多少个可选动作，就有多少维输出值。网络结构如下:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">],</span> <span class="s2">&#34;state&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layerq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layerq</span><span class="p">,</span> <span class="n">W2q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2q</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和之前的DQN相比，这里还有一个区别就是我们的critic没有使用DQN的经验回放，只是使用了反馈和当前网络在下一个状态的输出来拟合当前状态。</p>
<p>对于算法中Actor和Critic交互的逻辑，在main函数中：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># e-greedy action for train</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">critic</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>  <span class="c1"># gradient = grad[r + gamma * V(s_) - V(s)]</span>
</span></span><span class="line"><span class="cl">  <span class="n">actor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">td_error</span><span class="p">)</span>  <span class="c1"># true_gradient = grad[logPi(s,a) * td_error]</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大家对照第三节的算法流程和代码应该可以比较容易理清这个过程。但是这个程序很难收敛。因此大家跑了后发现分数总是很低的话是可以理解的。我们需要优化这个问题。</p>
<h1 id="5-actor-critic算法小结">5. Actor-Critic算法小结</h1>
<p>基本版的Actor-Critic算法虽然思路很好，但是由于难收敛的原因，还需要做改进。</p>
<p>目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法我们在从DQN到Nature DQN的过程中已经用过一次了。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p>
<p>在后面的文章中，我们会继续讨论DDPG和A3C。</p>
<p>　</p>
]]></description></item><item><title>强化学习笔记 [15] | A3C</title><link>https://jianye0428.github.io/posts/rl_learning_note_15/</link><pubDate>Sun, 25 Feb 2024 15:36:01 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_15/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C的算法原理和算法流程。</p>
<p>本文主要参考了A3C的<a href="http://proceedings.mlr.press/v48/mniha16.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，以及ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-a3c的引入">1. A3C的引入</h1>
<p>上一篇Actor-Critic算法的代码，其实很难收敛，无论怎么调参，最后的CartPole都很难稳定在200分，这是Actor-Critic算法的问题。但是我们还是有办法去有优化这个难以收敛的问题的。</p>
<p>回忆下之前的DQN算法，为了方便收敛使用了经验回放的技巧。那么我们的Actor-Critic是不是也可以使用经验回放的技巧呢？当然可以！不过A3C更进一步，还克服了一些经验回放的问题。经验回放有什么问题呢？ 回放池经验数据相关性太强，用于训练的时候效果很可能不佳。举个例子，我们学习下棋，总是和同一个人下，期望能提高棋艺。这当然没有问题，但是到一定程度就再难提高了，此时最好的方法是另寻高手切磋。</p>
<p>A3C的思路也是如此，它<font color=green>利用多线程的方法，同时在多个线程里面分别和环境进行交互学习，每个线程都把学习的成果汇总起来，整理保存在一个公共的地方</font>。并且，定期从公共的地方把大家的齐心学习的成果拿回来，指导自己和环境后面的学习交互。</p>
<p>通过这种方法，A3C避免了经验回放相关性过强的问题，同时做到了异步并发的学习模型。</p>
<h1 id="2-a3c的算法优化">2. A3C的算法优化</h1>
<p>现在我们来看看相比Actor-Critic，A3C到底做了哪些具体的优化。</p>
<p>相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p>我们首先来看这个异步训练框架，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">异步训练框架</div>
</center>
<br>
<p>图中上面的Global Network就是上一节说的共享的公共部分，主要是一个公共的神经网络模型，这个神经网络包括Actor网络和Critic网络两部分的功能。下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。</p>
<p>每个线程和环境交互到一定量的数据后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度却并不更新自己线程里的神经网络，而是去更新公共的神经网络。也就是n个线程会独立的使用累积的梯度分别更新公共部分的神经网络模型参数。每隔一段时间，线程会将自己的神经网络的参数更新为公共神经网络的参数，进而指导后面的环境交互。</p>
<p>可见，公共部分的网络模型就是我们要学习的模型，而线程里的网络模型主要是用于和环境交互使用的，这些线程里的模型可以帮助线程更好的和环境交互，拿到高质量的数据帮助模型更快收敛。</p>
<p>现在我们来看看第二个优化，网络结构的优化。之前在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们使用了两个不同的网络Actor和Critic。在A3C这里，我们把两个网络放到了一起，即输入状态 $S$,可以输出状态价值 $V$,和对应的策略 $π$, 当然，我们仍然可以把Actor和Critic看做独立的两块，分别处理，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">把Actor和Critic看做独立的两块，分别处理</div>
</center>
<br>
<p>第三个优化点是Critic评估点的优化，在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节中，我们讨论了不同的Critic评估点的选择，其中d部分讲到了使用优势函数 $A$ 来做Critic评估点，优势函数 $A$ 在时刻t不考虑参数的默认表达式为：</p>
<p>$$A(S,A,t)=Q(S,A)-V(S)$$</p>
<p>$Q(S,A)$的值一般可以通过单步采样近似估计，即：</p>
<p>$$Q(S,A)=R+\gamma V(S^{\prime})$$</p>
<p>这样优势函数去掉动作可以表达为：</p>
<p>$$A(S,t)=R+\gamma V(S^{\prime})-V(S)$$</p>
<p>其中 $V(S)$的值需要通过Critic网络来学习得到。</p>
<p>在A3C中，采样更进一步，使用了N步采样，以加速收敛。这样A3C中使用的优势函数表达为：</p>
<p>$$A(S,t)=R_t++\gamma R_{t+1}+\ldots\gamma^{n-1}R_{t+n-1}+\gamma^nV(S^{\prime})-V(S)$$</p>
<p>对于Actor和Critic的损失函数部分，和Actor-Critic基本相同。有一个小的优化点就是在Actor-Critic策略函数的损失函数中，加入了策略 $π$ 的熵项,系数为 $c$, 即策略参数的梯度更新和Actor-Critic相比变成了这样：</p>
<p>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,t)+c\nabla_\theta H(\pi(S_t,\theta))$$</p>
<p>以上就是A3C和Actor-Critic相比有优化的部分。下面我们来总价下A3C的算法流程。</p>
<h1 id="3-a3c算法流程">3. A3C算法流程</h1>
<p>这里我们对A3C算法流程做一个总结，由于A3C是异步多线程的，我们这里给出任意一个线程的算法流程。</p>
<ul>
<li>
<p>输入：公共部分的A3C神经网络结构，对应参数位 $θ$ , $w$，本线程的A3C神经网络结构，对应参数 $θ&rsquo;$, $w&rsquo;$, 全局共享的迭代轮数 $T$，全局最大迭代次数 $T_{max}$, 线程内单次迭代时间序列最大长度 $T_{local}$,状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，熵系数 $c$, 衰减因子 $γ$</p>
</li>
<li>
<p>输出：公共部分的A3C神经网络参数 $θ$, $w$</p>
<ul>
<li>(1). 更新时间序列 $t=1$</li>
<li>(2). 重置Actor和Critic的梯度更新量: $dθ←0$,$dw←0$</li>
<li>(3). 从公共部分的A3C神经网络同步参数到本线程的神经网络：$θ&rsquo;=θ,w&rsquo;=w$</li>
<li>(4). $t_{start}=t$，初始化状态 $s_t$</li>
<li>(5). 基于策略 $π(at|st;θ)$ 选择出动作 $a_t$</li>
<li>(6). 执行动作 $a_t$得到奖励 $r_t$ 和新状态 $s_{t+1}$</li>
<li>(7). $t←t+1$, $T←T+1$</li>
<li>(8). 如果 $s_t$是终止状态，或 $t − t_{start}==t_{local}$,则进入步骤(9)，否则回到步骤(5)</li>
<li>(9). 计算最后一个时间序列位置 $s_t$的 $Q(s,t)$:
<ul>
<li>$$\left.Q(s,t)=\left\{\begin{array}{ll}0&amp;terminal~state\\V(s_t,w^{\prime})&amp;none~terminal~state,bootstrapping\end{array}\right.\right.$$</li>
</ul>
</li>
<li>(10). for $i∈(t−1,t−2,&hellip;t_{start})$:
<ul>
<li>1). 计算每个时刻的$Q(s,i)$： $Q(s,i)=r_i+\gamma Q(s,i+1)$</li>
<li>2). 累计Actor的本地梯度更新：
<ul>
<li>$$d\theta\leftarrow d\theta+\nabla_{\theta^{\prime}}log\pi_{\theta^{\prime}}(s_i,a_i)(Q(s,i)-V(S_i,w^{\prime}))+c\nabla_{\theta^{\prime}}H(\pi(s_i,\theta^{\prime}))$$</li>
</ul>
</li>
<li>3). 累计Critic的本地梯度更新：
<ul>
<li>$$\begin{aligned}dw&amp;\leftarrow dw+\frac{\partial(Q(s,i)-V(S_i,w^{\prime}))^2}{\partial w^{\prime}}\end{aligned}$$</li>
</ul>
</li>
</ul>
</li>
<li>(11). 更新全局神经网络的模型参数：
<ul>
<li>$$\theta=\theta+\alpha d\theta,~w=w-\beta dw$$</li>
</ul>
</li>
<li>(12). 如果 $T&gt;T_{max}$,则算法结束，输出公共部分的A3C神经网络参数 $θ$, $w$,否则进入步骤(3)</li>
</ul>
</li>
</ul>
<p>以上就是A3C算法单个线程的算法流程。</p>
<h1 id="4-a3c算法实例">4. A3C算法实例</h1>
<p>下面我们基于上述算法流程给出A3C算法实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法代码大部分参考了莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/10_A3C/A3C_discrete_action.py"target="_blank" rel="external nofollow noopener noreferrer">A3C代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，增加了模型测试部分的代码并调整了部分模型参数。完整的代码参见我的Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py</p>
<p>整个算法的Actor和Critic的网络结构都定义在这里， 所有的线程中的网络结构，公共部分的网络结构都在这里定义。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;actor&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;la&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_a</span><span class="p">,</span> <span class="n">N_A</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ap&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;critic&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lc&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">)</span>  <span class="c1"># state value</span>
</span></span><span class="line"><span class="cl">  <span class="n">a_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/actor&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">c_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/critic&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">a_prob</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">a_params</span><span class="p">,</span> <span class="n">c_params</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所有线程初始化部分，以及本线程和公共的网络结构初始化部分如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;/cpu:0&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropA&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">GLOBAL_AC</span> <span class="o">=</span> <span class="n">ACNet</span><span class="p">(</span><span class="n">GLOBAL_NET_SCOPE</span><span class="p">)</span>  <span class="c1"># we only need its params</span>
</span></span><span class="line"><span class="cl">  <span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Create worker</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">i_name</span> <span class="o">=</span> <span class="s1">&#39;W_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>   <span class="c1"># worker name</span>
</span></span><span class="line"><span class="cl">    <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker</span><span class="p">(</span><span class="n">i_name</span><span class="p">,</span> <span class="n">GLOBAL_AC</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本线程神经网络将本地的梯度更新量用于更新公共网络参数的逻辑在update_global函数中，而从公共网络把参数拉回到本线程神经网络的逻辑在pull_global中。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">update_a_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_c_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>  <span class="c1"># local grads applies to global net</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">pull_global</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_a_params_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_c_params_op</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>详细的内容大家可以对照代码和算法流程一起看。在主函数里我新加了一个测试模型效果的过程，大家可以试试看看最后的模型效果如何。</p>
<h1 id="5-a3c小结">5. A3C小结</h1>
<p>A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，提供了一种通用的异步的并发的强化学习框架，也就是说，这个并发框架不光可以用于A3C，还可以用于其他的强化学习算法。这是A3C最大的贡献。目前，已经有基于GPU的A3C框架，这样A3C的框架训练速度就更快了。</p>
<p>除了A3C, DDPG算法也可以改善Actor-Critic难收敛的问题。它使用了Nature DQN，DDQN类似的思想，用两个Actor网络，两个Critic网络，一共4个神经网络来迭代更新模型参数。在下一篇我们讨论DDPG算法。</p>
]]></description></item><item><title>强化学习笔记 [16] | 深度确定性策略梯度(DDPG)</title><link>https://jianye0428.github.io/posts/rl_learning_note_16/</link><pubDate>Sun, 25 Feb 2024 19:53:12 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_16/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10334127.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十五) A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DDPG)。</p>
<p>本篇主要参考了DDPG的<a href="https://arxiv.org/pdf/1509.02971.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-从随机策略到确定性策略">1. 从随机策略到确定性策略</h1>
<p>从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。PG(Policy Gradient)我们在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里已经讨论过。那什么是确定性策略梯度(Deterministic Policy Gradient，以下简称DPG)呢？</p>
<p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成：</p>
<p>$$\pi_\theta(s)=a$$</p>
<h1 id="2-从dpg到ddpg">2. 从DPG到DDPG</h1>
<p>在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>其中状态的采样空间为$\rho^\pi$, $\nabla_\theta log\pi_\theta(s,a)$是分值函数，可见随机性策略梯度需要在整个动作的空间$\pi_\mathrm{\theta}$进行采样。</p>
<p>而DPG基于Q值的确定性策略梯度的梯度计算公式是：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi}[\nabla_\theta\pi_\theta(s)\nabla_aQ_\pi(s,a)|<em>{a=\pi</em>\theta(s)}]$$</p>
<p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p>
<p>而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？</p>
<h1 id="3-ddpg的原理">3. DDPG的原理</h1>
<p>DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。可以复习<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>DDQN的当前Q网络负责对当前状态 $S$ 使用 $ϵ$−贪婪法选择动作 $A$，执行动作 $A$,获得新状态 $S&rsquo;$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态 $S&rsquo;$使用贪婪法选择动作 $A&rsquo;$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。</p>
<p>DDQN的目标Q网络则负责基于经验回放池计算目标Q值, 提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。</p>
<p>现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要 $ϵ$−贪婪法这样的选择方法，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态 $S&rsquo;$ 使用贪婪法选择动作 $A&rsquo;$，这部分工作由于用来估计目标Q值，因此可以放到Actor目标网络完成。</p>
<p>基于经验回放池和目标Actor网络提供的 $S&rsquo;$, $A&rsquo;$ 计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。</p>
<p>此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。</p>
<p>有了上面的思路，我们总结下DDPG 4个网络的功能定位：</p>
<ul>
<li>
<p>(1). <strong>Actor当前网络</strong>: 负责策略网络参数 $θ$的迭代更新，负责根据当前状态 $S$选择当前动作 $A$，用于和环境交互生成 $S&rsquo;$, $R$。</p>
</li>
<li>
<p>(2). <strong>Actor目标网络</strong>: 负责根据经验回放池中采样的下一状态 $S&rsquo;$ 选择最优下一动作$A&rsquo;$。网络参数 $θ&rsquo;$定期从 $θ$复制。</p>
</li>
<li>
<p>(3). <strong>Critic当前网络</strong>: 负责价值网络参数 $w$的迭代更新，负责计算负责计算当前Q值 $Q(S,A,w)$。目标Q值$y_i=R+γQ&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$</p>
</li>
<li>
<p>(4). <strong>Critic目标网络</strong>: 负责计算目标Q值中的 $Q&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$部分。网络参数 $w&rsquo;$ 定期从 $w$复制。</p>
</li>
</ul>
<p>DDPG除了这4个网络结构，还用到了经验回放，这部分用于计算目标Q值，和DQN没有什么区别，这里就不展开了。</p>
<p>此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w$′=$w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：</p>
<p>$$\begin{gathered}
w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</p>
<p>其中 $τ$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。</p>
<p>同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作 $A$会增加一定的噪声 $N$, 即最终和环境交互的动作 $A$ 的表达式是：</p>
<p>$$A=\pi_\theta(S)+\mathcal{N}$$</p>
<p>最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：</p>
<p>$$J(w)=\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>而对于 Actor当前网络，其损失函数就和之前讲的PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：</p>
<p>$$\nabla_J(\theta)=\frac1m\sum_{j=1}^m[\nabla_aQ_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>\theta(s)}\nabla_\theta\pi_{\theta(s)}|_{s=s_i}]$$</p>
<p>这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作 $a_1$和$a_2$，从Critic当前网络得到了两个反馈的 $Q$ 值，分别是 $Q_1$,$Q_2$，假设 $Q_1&gt;Q_2$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加 $a_1$的概率，降低$a_2$的概率，也就是说，Actor想要尽可能的得到更大的Q值。所以我们的Actor的损失可以简单的理解为得到的反馈Q值越大损失越小，得到的反馈Q值越小损失越大，因此只要对状态估计网络返回的Q值取个负号即可，即：</p>
<p>$$J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,w)$$</p>
<h1 id="4-ddpg算法流程">4. DDPG算法流程</h1>
<p>这里我们总结下DDPG的算法流程</p>
<p>输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为 $θ$,$θ&rsquo;$,$w$,$w&rsquo;$,衰减因子 $γ$, 软更新系数 $τ$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$。最大迭代次数 $T$。随机噪音函数 $\mathcal{N}$</p>
<p>输出：最优Actor当前网络参数 $θ$,Critic当前网络参数 $w$</p>
<ul>
<li>(1). 随机初始化$θ$,$w$, $w$′=$w$,$θ$′=$θ$。清空经验回放的集合$D$</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor当前网络基于状态 $S$ 得到动作 $A=π_θ(ϕ(S))+\mathcal{N}$</li>
<li>c) 执行动作$A$,得到新状态$S$′,奖励$R$,是否终止状态%is_end$</li>
<li>d) 将 ${ϕ(S), A, R, ϕ(S&rsquo;), is_end}$ 这个五元组存入经验回放集合$D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本${\phi(S_j),A_j,R_j,\phi(S_j^{\prime}),is_end_j},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\pi_{\theta^{\prime}}(\phi(S_j^{\prime})),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Critic当前网络的所有参数 $w$</li>
<li>h) 使用 $\begin{aligned}J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,\theta)\end{aligned}$，通过神经网络的梯度反向传播来更新Actor当前网络的所有参数 $θ$</li>
<li>i) 如果 i%C=1, 则更新Critic目标网络和Actor目标网络参数：
<ul>
<li>$$\begin{gathered} w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</li>
</ul>
</li>
<li>j) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤(b)</li>
</ul>
</li>
</ul>
<p>以上就是DDPG算法的主流程，要注意的是上面2.f中的 $\pi_{\theta^{\prime}}(\phi(S_j^{\prime}))$ 是通过Actor目标网络得到，而 $Q^{\prime}(\phi(S_i^{\prime}),\pi_{\theta^{\prime}}(\phi(S_i^{\prime})),w^{\prime})$ 则是通过Critic目标网络得到的。</p>
<h1 id="5-ddpg实例">5. DDPG实例</h1>
<p>这里我们给出DDPG第一个算法实例，代码主要参考自莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG_update.py"target="_blank" rel="external nofollow noopener noreferrer">Github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。增加了测试模型效果的部分，优化了少量参数。代码详见：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py</p>
<p>这里我们没有用之前的CartPole游戏，因为它不是连续动作。我们使用了Pendulum-v0这个游戏。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a href="https://github.com/openai/gym/wiki/Pendulum-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。输入状态是角度的sin，cos值，以及角速度。一共三个值。动作是一个连续的力矩值。</p>
<p>两个Actor网络和两个Critic网络的定义参见：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_bound</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;scaled_a&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_c</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_l1</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_s</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_s&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_a&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">w1_s</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">w1_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>  <span class="c1"># Q(s,a)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor当前网络和Critic当前网络损失函数的定义参见：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">q_target</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">td_error</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ce_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">a_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>    <span class="c1"># maximize the q</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">atrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">a_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ae_params</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor目标网络和Critic目标网络参数软更新，Actor当前网络和Critic当前网络反向传播更新部分的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># soft target replacement</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">soft_replace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">MEMORY_CAPACITY</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">bt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">ba</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">br</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs_</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">:</span> <span class="n">ba</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">:</span> <span class="n">br</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_</span><span class="p">:</span> <span class="n">bs_</span><span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余的可以对照算法和代码一起学习，应该比较容易理解。</p>
<h1 id="6-ddpg总结">6. DDPG总结</h1>
<p>DDPG参考了DDQN的算法思想吗，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p>
<p>到此，我们的Policy Based RL系列也讨论完了，而在更早我们讨论了Value Based RL系列，至此，我们还剩下Model Based RL没有讨论。后续我们讨论Model Based RL的相关算法。</p>
]]></description></item><item><title>强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架</title><link>https://jianye0428.github.io/posts/rl_learning_note_17/</link><pubDate>Sun, 25 Feb 2024 19:53:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_17/</guid><description><![CDATA[<h1 id="强化学习十七-基于模型的强化学习与dyna算法框架httpswwwcnblogscompinardp10384424html"><a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h1>
<p>在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。</p>
<p>本篇主要参考了UCL强化学习课程的第8讲和Dyna-2的<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-基于模型的强化学习简介">1. 基于模型的强化学习简介</h1>
<p>基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率 $P^a_{ss&rsquo;}$。</p>
<p>而基于模型的强化学习则会尝试从环境的模型去学习，一般是下面两个相互独立的模型：</p>
<ul>
<li>一个是状态转化预测模型，输入当前状态 $s$和动作 $a$，预测下一个状态 $s&rsquo;$。</li>
<li>另一个是奖励预测模型，输入当前状态 $s$和动作 $a$，预测环境的奖励 $r$。</li>
</ul>
<p>即模型可以描述为下面两个式子：</p>
<p>$$\begin{gathered}S_{t+1}\sim P(S_{t+1}|S_t,A_t)\\R_{t+1}\sim R(R_{t+1}|S_t,A_t)\end{gathered}$$</p>
<p>如果模型 $P$, $R$ 可以准确的描述真正的环境的转化模型，那么我们就可以基于模型来预测，当有一个新的状态 $S$ 和动作 $A$到来时，我们可以直接基于模型预测得到新的状态和动作奖励，不需要和环境交互。当然如果我们的模型不好，那么基于模型预测的新状态和动作奖励可能错的离谱。</p>
<p>从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的主要区别：即基于模型的强化学习是从模型中学习，而不基于模型的强化学习是从和环境交互的经历去学习。</p>
<p>下面这张图描述了基于模型的强化学习的思路：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Model-based RL</div>
</center>
<br>
<h1 id="2-基于模型的强化学习算法训练流程">2. 基于模型的强化学习算法训练流程</h1>
<p>这里我们看看基于模型的强化学习算法训练流程，其流程和我们监督学习算法是非常类似的。</p>
<p>假设训练数据是若干组这样的经历：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,R_2,\ldots,S_T$$</p>
<p>对于每组经历，我们可以将其转化为 $T−1$ 组训练样本，即：</p>
<p>$$\begin{gathered}
S_1,A_1\to S_2,S_1,A_1\to R_2 \\
S_2,A_2\to S_3,S_2,A_2\to R_3 \\
&hellip;&hellip; \\
S_{T-1},A_{T-1}\rightarrow S_T,~S_{T_1},A_{T-1}\rightarrow R_T
\end{gathered}$$</p>
<p>右边的训练样本一起组成了一个分类模型或密度估计模型，输入状态和动作，输出下一个状态。 右边的训练样本一起组成了一个回归模型训练集，输入状态和动作，输出动作奖励值。</p>
<p>至此我们的强化学习求解过程和传统的监督学习算法没有太多区别了，可以使用传统的监督学习算法来求解这两个模型。</p>
<p>当然还可以更简单，即通过对训练样本进行查表法进行统计，直接得到 $P(S_{t+1}|S_t,A_t)$ 的概率和 $R(R_{t+1}|S_t,A_t)$ 的平均值，这样就可以直接预测。比使用模型更简单。</p>
<p>此外，还有其他的方法可以用来得到 $P(S_{t+1}|S_t,A_t)$和 $R(R_{t+1}|S_t,A_t)$，这个我们后面再讲。</p>
<p>虽然基于模型的强化学习思路很清晰，而且还有不要和环境持续交互优化的优点，但是用于实际产品还是有很多差距的。主要是我们的模型绝大多数时候不能准确的描述真正的环境的转化模型，那么使用基于模型的强化学习算法得到的解大多数时候也不是很实用。那么是不是基于模型的强化学习就不能用了呢？也不是，我们可以将基于模型的强化学习和不基于模型的强化学习集合起来，取长补短，这样做最常见的就是Dyna算法框架。</p>
<h1 id="3-dyna算法框架">3. Dyna算法框架</h1>
<p>Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。如果用和第一节类似的图，可以表示如下图，和第一节的图相比，多了一个“Direct RL“的箭头，这正是不基于模型的强化学习的思路。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna算法示意图</div>
</center>
<br>
<p>Dyna算法框架和不同的具体的不基于模型的强化学习一起，可以得到具体的不同算法。如果我们使用基于价值函数的Q-Learning，那么我们就得到了Dyna-Q算法。我们基于Dyna-Q来看看Dyna算法框架的一般流程.</p>
<h1 id="4-dyna-q算法流程">4. Dyna-Q算法流程</h1>
<p>这里我们给出基于价值函数的Dyna-Q算法的概要流程。假设模型使用的是查表法。</p>
<ul>
<li>(1). 初始化任意一个状态 $s$,和任意一个动作 $a$ 对应的状态价值 $Q(s,a)$, 初始化奖励模型 $R(s,a)$和状态模型 $P(s,a)$</li>
<li>(2). for $i=1$ to 最大迭代次数T：
<ul>
<li>(a) $S \leftarrow \text{current state}$</li>
<li>(b) $A \leftarrow \text{ϵ−greedy(S,Q)}$</li>
<li>(c) 执行动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>(d) 使用Q-Learning更新价值函数：$Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
<li>(e) 使用 $S,A,S^{\prime}$ 更新状态模型 $P(s,a)$，使用 $S,A,R$ 更新状态模型 $R(s,a)$</li>
<li>(f) $\text{for} \space \space j=1 \space \space \text{to} \text{最大次数}n$：
<ul>
<li>(i) 随机选择一个之前出现过的状态 $S$ , 在状态 $S$ 上出现过的动作中随机选择一个动作 $A$</li>
<li>(ii) 基于模型 $P(S,A)$ 得到 $S&rsquo;$, 基于模型 $R(S,A)$ 得到 $R$</li>
<li>(iii) 使用Q-Learning更新价值函数: $Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>从上面的流程可以看出，Dyna框架在每个迭代轮中，会先和环境交互，并更新价值函数和（或）策略函数，接着进行n次模型的预测，同样更新价值函数和（或）策略函数。这样同时利用上了和环境交互的经历以及模型的预测。</p>
<h1 id="5-dyna-2算法框架">5. Dyna-2算法框架</h1>
<p>在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为<strong>永久性记忆</strong>（permanent memory）和<strong>瞬时记忆</strong>（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。</p>
<p>永久性记忆的Q函数定义为：</p>
<p>$$Q(S,A)=\phi(S,A)^T\theta $$</p>
<p>瞬时记忆的Q函数定义为：</p>
<p>$$Q^{\prime}(S,A)=\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>组合起来后记忆的Q函数定义为：</p>
<p>$$\overline{Q}(S,A)=\phi(S,A)^T\theta+\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似于 $SARSA(λ)$</p>
<p>以下是Dyna-2的算法流程：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna-2 算法流程</div>
</center>
<br>
<h1 id="6-基于模型的强化学习总结">6. 基于模型的强化学习总结</h1>
<p>基于模型的强化学习一般不单独使用，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。对于模型部分，我们可以用查表法和监督学习法等方法，预测或者采样得到模拟的经历。而对于非模型部分，使用前面的Q-Learning系列的价值函数近似，或者基于Actor-Critic的策略函数的近似都是可以的。</p>
<p>除了Dyna算法框架，我们还可以使用基于模拟的搜索(simulation-based search)来结合基于模型的强化学习和不基于模型的强化学习,并求解问题。这部分我们在后面再讨论。</p>
]]></description></item><item><title>强化学习笔记 [18] | 基于模拟的搜索与蒙特卡罗树搜索(MCTS)</title><link>https://jianye0428.github.io/posts/rl_learning_note_18/</link><pubDate>Sun, 25 Feb 2024 19:53:18 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_18/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。</p>
<p>本篇主要参考了UCL强化学习课程的第八讲，第九讲部分。</p>
<h1 id="1-基于模拟的搜索概述">1. 基于模拟的搜索概述</h1>
<p>什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这是数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。</p>
<p>那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点 $S_t$ 开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以 $S_t$ 为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到 $S_t$状态最应该采用的动作 $A_t$。前向搜索的sub-MDP如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">forward search sub-MDP</div>
</center>
<br>
<p>前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。</p>
<h1 id="2-简单蒙特卡罗搜索">2. 简单蒙特卡罗搜索</h1>
<p>首先我们看看基于模拟的搜索中比较简单的一种方法：简单蒙特卡罗搜索。</p>
<p>简单蒙特卡罗搜索基于一个强化学习模型 $M_v$ 和一个模拟策略 $π$.在此基础上，对于当前我们要选择动作的状态 $S_t$, 对每一个可能采样的动作 $a∈A$,都进行 $K$ 轮采样，这样每个动作 $a$ 都会得到 $K$ 组经历完整的状态序列(episode)。即：</p>
<p>$$\{S_t,a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>现在对于每个 $(S_t,a)$ 组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1K\sum_{k=1}^KG_t\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别,那么简单蒙特卡罗搜索也太慢了。同时，由于使用蒙特卡罗法计算其动作价值函数，模拟采样得到的一些中间状态和对应行为的价值就被忽略了，这部分数据能不能利用起来呢？</p>
<p>下面我们看看蒙特卡罗树搜索(Monte-Carlo Tree Search，以下简称MCTS)怎么优化这个问题的解决方案。</p>
<h1 id="3-mcts的原理">3. MCTS的原理</h1>
<p>MCTS摒弃了简单蒙特卡罗搜索里面对当前状态 $S_t$ 每个动作都要进行K次模拟采样的做法，而是总共对当前状态 $S_t$进行K次采样，这样采样到的动作只是动作全集 $A$ 中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。</p>
<p>在MCTS中，基于一个强化学习模型 $M_v$和一个模拟策略$π$，当前状态 $S_t$ 对应的完整的状态序列(episode)是这样的:</p>
<p>$$\{S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算 $Q(st,a)$和最大 $Q(s_t,a)$对应的动作。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1{N(S_t,a)}\sum_{k=1}^K\sum_{u=t}^T1(S_{uk}=S_t,A_{uk}=a)G_u\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>MCTS搜索的策略分为两个阶段：第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使 $ϵ−$贪婪策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。</p>
<p>这里讲到的是最经典的强化学习终MCTS的用户，每一步都有延时奖励，但是在棋类之类的零和问题中，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。</p>
<h1 id="4-上限置信区间算法uct">4. 上限置信区间算法UCT</h1>
<p>在讨论棋类游戏的MCTS搜索之前，我们先熟悉下上限置信区间算法(Upper Confidence Bound Applied to Trees, 以下简称UCT)。它是一种策略算法，我们之前最常用的是 $ϵ−$贪婪策略。但是在棋类问题中，UCT更常使用。</p>
<p>在棋类游戏中，经常有这样的问题，我们发现在某种棋的状态下，有2个可选动作，第一个动作历史棋局中是0胜1负，第二个动作历史棋局中是8胜10负，那么我们应该选择哪个动作好呢？如果按 $ϵ−$贪婪策略，则第二个动作非常容易被选择到。但是其实虽然第一个动作胜利0%，但是很可能是因为这个动作的历史棋局少，数据不够导致的，很可能该动作也是一个不错的动作。那么我们如何在最优策略和探索度达到一个选择平衡呢？ $ϵ−$贪婪策略可以用，但是UCT是一个更不错的选择。</p>
<p>UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度吗，一个常用的公式如下：</p>
<p>$$\text{score}=\left.\frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}\right.$$</p>
<p>其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$ 是所有模拟次数，c 是探索常数，理论值为$√2$，可根据经验调整，$c$ 越大就越偏向于广度搜索，$c$ 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。</p>
<p>比如对于下面的棋局，对于根节点来说，有3个选择，第一个选择7胜3负，第二个选择5胜3负，第三个选择0胜3负。</p>
<p>如果我们取c=10,则第一个节点的分数为：$$score(7,10)=7/10+C\cdot\sqrt{\frac{\log(21)}{10}}\approx6.2$$</p>
<p>第二个节点的分数为：$$score(5,8)=5/8+C\cdot\sqrt{\frac{\log(21)}8}\approx6.8$$</p>
<p>第三个节点的分数为：$$score(0,3)=0/3+C\cdot\sqrt{\frac{\log(21)}3}\approx10$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>可见，由于我们把探索率 $c$ 设置的比较大，第三个节点是被UCT选中要执行的动作节点。当然如果我们把c设置的比较小的话，第一个或者第二个可能就变成最大的分数了。</p>
<h1 id="5-棋类游戏mcts搜索">5. 棋类游戏MCTS搜索</h1>
<p>在像中国象棋，围棋这样的零和问题中，一个动作只有在棋局结束才能拿到真正的奖励，因此我们对MCTS的搜索步骤和树结构上需要根据问题的不同做一些细化。</p>
<p>对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步，如下图(图来自维基百科)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。</p>
<p>第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。</p>
<p>第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。</p>
<p>第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。</p>
<p>以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。</p>
<h1 id="6-mcts小结">6. MCTS小结</h1>
<p>MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。</p>
]]></description></item><item><title>强化学习笔记 [19] | AlphaGo Zero强化学习原理</title><link>https://jianye0428.github.io/posts/rl_learning_note_19/</link><pubDate>Sun, 25 Feb 2024 19:53:22 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_19/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。</p>
<p>本篇主要参考了AlphaGo Zero的<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, <a href="https://www.hhyz.me/2018/08/08/2018-08-08-AlphaGO-Zero/"target="_blank" rel="external nofollow noopener noreferrer">AlphaGo Zero综述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和AlphaGo Zero Cheat Sheet。</p>
<h1 id="1-alphago-zero模型基础">1. AlphaGo Zero模型基础</h1>
<p>AlphaGo Zero不需要学习人类的棋谱，通过自我对弈完成棋力提高。主要使用了两个模型，第一个就是我们上一节介绍MCTS树结构，另一个是一个神经网络。MCTS上一篇已经有基本介绍了，对于神经网络，它的输入是当前的棋局状态，输出两部分，第一部分输出是在当前棋局状态下各个可能的落子动作对应的获胜概率p，可以简单理解为Actor-Critic策略函数部分。另一部分输出为获胜或者失败的评估[-1,1]，可以简单理解为Actor-Critic价值函数部分。</p>
<p>AlphaGo Zero的行棋主要是由MCTS指导完成的，但是在MCTS搜索的过程中，由于有一些不在树中的状态需要仿真，做局面评估，因此需要一个简单的策略来帮助MCTS评估改进策略，这个策略改进部分由前面提到的神经网络完成。</p>
<p>这两部分的关系如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">AlphaGo Zero 中的MCTS和NN</div>
</center>
<br>
<p>具体AlphaGo Zero的MCTS如何搜索，神经网络如何训练，如何指导MCTS搜索我们在后面再讲。</p>
<h1 id="2-alphago-zero的训练过程简介">2. AlphaGo Zero的训练过程简介</h1>
<p>在讨论AlphaGo Zero的MCTS如何搜索，神经网络如何训练等细节之前，我们先看看AlphaGo Zero的训练过程是什么样的。</p>
<p>AlphaGo Zero训练过程主要分为三个阶段：自我对战学习阶段，训练神经网络阶段和评估网络阶段。</p>
<p>自我对战学习阶段主要是AlphaGo Zero自我对弈，产生大量棋局样本的过程，由于AlphaGo Zero并不使用围棋大师的棋局来学习，因此需要自我对弈得到训练数据用于后续神经网络的训练。在自我对战学习阶段，每一步的落子是由MCTS搜索来完成的。在MCTS搜索的过程中，遇到不在树中的状态，则使用神经网络的结果来更新MCTS树结构上保存的内容。在每一次迭代过程中，在每个棋局当前状态 $s$ 下，每一次移动使用1600次MCTS搜索模拟。最终MCTS给出最优的落子策略 $π$ ,这个策略 $π$ 和神经网络的输出 $p$ 是不一样的。当每一局对战结束后，我们可以得到最终的胜负奖励 $z$ ,1或者-1. 这样我们可以得到非常多的样本 $(s,π,z)$,这些数据可以训练神经网络阶段。</p>
<p>在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合(s,π,z)(�,�,�),训练我们神经网络的模型参数。训练的目的是对于每个输入 $s$, 神经网络输出的 $p,v$和我们训练样本中的 $π$, $z$差距尽可能的少。这个损失函数 $L$ 其实是很简单的：</p>
<p>$$L=(z-v)^2-\pi^Tlog(p)+c||\theta||^2$$</p>
<p>损失函数由三部分组成，第一部分是均方误差损失函数，用于评估神经网络预测的胜负结果和真实结果之间的差异。第二部分是交叉熵损失函数，用于评估神经网络的输出策略和我们MCTS输出的策略的差异。第三部分是L2正则化项。</p>
<p>通过训练神经网络，我们可以优化神经网络的参数 $θ$,用于后续指导我们的MCTS搜索过程。</p>
<p>当神经网络训练完毕后，我们就进行了评估阶段，这个阶段主要用于确认神经网络的参数是否得到了优化，这个过程中，自我对战的双方各自使用自己的神经网络指导MCTS搜索，并对战若干局，检验AlphaGo Zero在新神经网络参数下棋力是否得到了提高。除了神经网络的参数不同，这个过程和第一阶段的自我对战学习阶段过程是类似的。</p>
<h1 id="3-alphago-zero的神经网络结构">3. AlphaGo Zero的神经网络结构</h1>
<p>在第二节我们已经讨论了AlphaGo Zero的主要训练过程，但是还有两块没有讲清楚，一是AlphaGo Zero的MCTS搜索过程是怎么样的，二是AlphaGo Zero的神经网络的结构具体是什么样的。这一节我们来看看AlphaGo Zero的神经网络的细节。</p>
<p>首先我们看看AlphaGo Zero的输入，当前的棋局状态。由于围棋是19x19的361个点组成的棋局，每个点的状态有二种：如果当前是黑方行棋，则当前有黑棋的点取值1，有白棋或者没有棋子的位置取值为0，反过来，如果当前是白方行棋，则当前有白棋的点取值1，有黑棋或者没有棋子的位置取值为0。同时，为了提供更多的信息，输入的棋局状态不光只有当前的棋局状态，包括了黑棋白棋各自前8步对应的棋局状态。除了这16个棋局状态，还有一个单独的棋局状态用于标识当前行棋方，如果是当前黑棋行棋，则棋局状态上标全1，白棋则棋局状态上标全0。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Game State</div>
</center>
<br>
<p>最终神经网络的输入是一个19x19x17的张量。里面包含黑棋和白棋的最近8步行棋状态和当前行棋方的信息。</p>
<p>接着我们看看神经网络的输出，神经网络的输出包括策略部分和价值部分。对于策略部分，它预测当前各个行棋点落子的概率。由于围棋有361个落子点，加上还可以Pass一手，因此一共有362个策略端概率输出。对于价值端，输出就简单了，就是当前局面胜负的评估值，在[-1,1]之间。</p>
<p>看完了神经网络的输入和输出，我们再看看神经网络的结构，主要是用CNN组成的深度残差网络。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>在19x19x17的张量做了一个基本的卷积后，使用了19层或者39层的深度残差网络，这个是ResNet的经典结构。理论上这里也可以使用DenseNet等其他流行的网络结构。神经网络的损失函数部分我们在第二节已经将了。整个神经网络就是为了当MCTS遇到没有见过的局面时，提供的当前状态下的局面评估和落子概率参考。这部分信息会被MCTS后续综合利用。</p>
<h1 id="4-alphago-zero的mcts搜索">4. AlphaGo Zero的MCTS搜索</h1>
<p>　　　　现在我们来再看看AlphaGo Zero的MCTS搜索过程，在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们已经介绍了MCTS的基本原理，和4个主要的搜索阶段：选择，扩展，仿真和回溯。和上一篇的内容相比，这里MCTS的不同主要体现在树结构上保存的信息不同，进而UCT的计算公式也稍有不同。最后MCTS搜索完毕后，AlphaGo Zero也有自己选择真正落子点的策略。</p>
<p>　　　　在上一篇里，我们的MCTS上保存的数据很简单，就是下的总盘数和赢的总盘数。在AlphaGo Zero这里，我们保存的信息会多一些。主要包括下面的4部分：</p>
<ul>
<li>$N(s,a)$:记录边的访问次数</li>
<li>$W(s,a)$: 合计行动价值</li>
<li>$Q(s,a)$:平均行动价值</li>
<li>$P(s,a)$:选择该条边的先验概率</li>
</ul>
<p>其中 $s$ 为当前棋局状态，$a$ 为某一落子选择对应的树分支。</p>
<p>有了MCTS上的数据结构，我们看看AlphaGo Zero的MCTS搜索的4个阶段流程：</p>
<p>首先是选择，在MCTS内部，出现过的局面，我们会使用UCT选择子分支。子分支的UCT原理和上一节一样。但是具体的公式稍有不同，如下：</p>
<p>$$\begin{gathered}
U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)} \\
a_t=\arg\max_a(Q(s_t,a)+U(s_t,a))
\end{gathered}$$</p>
<p>最终我们会选择 $Q+U$最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数,上一篇已讲过。</p>
<p>如果到了没有到终局的MCTS叶子节点，那么我们就需要进入MCTS的第二步，扩展阶段,以及后续的第三步仿真阶段。我们这里一起讲。对于叶子节点状态s�，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL��落子的概率p�和对应的价值v�,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为：</p>
<p>$$\{N(s_L,a)=0,W(s_L,a)=0,Q(s_L,a)=0,P(s_L,a)=P_a\}$$</p>
<p>这个过程如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>这样扩展后，之前的叶子节点 $s$，现在就是内部节点了。做完了扩展和仿真后，我们需要进行回溯，将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点 $L$ 依次向根节点回溯，并依次更新上层分支数据结构如下：</p>
<p>$$\begin{gathered}
N(s_t,a_t)=N(s_t,a_t)+1 \\
W(s_t,a_t)=W(s_t,a_t)+v \\
Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}
\end{gathered}$$</p>
<p>这个MCTS搜索过程在一次真正行棋前，一般会进行约1600次搜索，每次搜索都会进行上述4个阶段。</p>
<p>这上千次MCTS搜索完毕后，AlphaGo Zero就可以在MCTS的根节点 $s$ 基于以下公式选择行棋的MCTS分支了:</p>
<p>$$\pi(a|s)=\frac{N(s,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}$$</p>
<p>其中，$τ$ 为温度参数，控制探索的程度，$τ$ 越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。每一次完整的自我对弈的前30步，参数 $τ=1$，这是早期鼓励探索的设置。游戏剩下的步数，该参数将逐渐降低至0。如果是比赛，则直接为0.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>同时在随后的时间步中，这个MCTS搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 。</p>
<p>以上就是AlphaGo Zero MCTS搜索的过程。</p>
<h1 id="5-alphago-zero小结与强化学习系列小结">5. AlphaGo Zero小结与强化学习系列小结</h1>
<p>AlphaGo Zero巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。两者一主一辅，非常优雅的解决了这类状态完全可见，信息充分的棋类问题。</p>
<p>当然这类强化学习算法只对特定的这类完全状态可见，信息充分的问题有效，遇到信息不对称的强化学习问题，比如星际，魔兽之类的对战游戏问题，这个算法就不那么有效了。要推广AlphaGo Zero的算法到大多数普通强化学习问题还是很难的。因此后续强化学习算法应该还有很多发展的空间。</p>
<p>至此强化学习系列就写完了，之前预计的是写三个月，结果由于事情太多，居然花了大半年。但是总算还是完成了，没有烂尾。生活不易，继续努力！</p>
]]></description></item><item><title>强化学习笔记 [10] | Double DQN (DDQN)</title><link>https://jianye0428.github.io/posts/rl_learning_note_10/</link><pubDate>Fri, 23 Feb 2024 13:17:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_10/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9756075.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（九）Deep Q-Learning进阶之Nature DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称DDQN）。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和DDQN的论文(Deep Reinforcement Learning with Double Q-learning)。</p>
<h1 id="1-dqn的目标q值计算问题">1. DQN的目标Q值计算问题</h1>
<p>在DDQN之前，基本上所有的目标Q值都是通过<strong>贪婪法</strong>直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算如下式:</p>
<p>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</p>
<h1 id="2-ddqn的算法建模">2. DDQN的算法建模</h1>
<p>DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。</p>
<p>在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：</p>
<p>$$y_j=R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})$$</p>
<p>在DDQN(Double DQN)这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即:</p>
<p>$$a^{max}(S_j^{\prime},w)=\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w)$$</p>
<p>然后利用这个选择出来的动作 $\begin{aligned}&amp;a^{max}(S_j^{\prime},w)\end{aligned}$ 在目标网络里面去计算目标Q值。即：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),a^{max}(S_j^{\prime},w),w^{\prime})$$</p>
<p>综合起来写就是：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})$$</p>
<p>除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。</p>
<h1 id="3-ddqn算法流程">3. DDQN算法流程</h1>
<p>这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本 $m$,目标Q网络参数更新频 $C$。</li>
<li>输出：Q网络参数</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;的参数 $w′=w$ 。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S′),is_end} $,这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$, 计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数w�</li>
<li>h) 如果 $i%C=1$,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-ddqn算法实例">4. DDQN算法实例　</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注DDQN和上一节的Nature DQN的代码的不同之处。代码只有一个地方不一样，就是计算目标Q值的时候，如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">current_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_action_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">current_Q_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">target_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">target_Q_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">max_action_next</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">target_Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而之前的Nature DQN这里的目标Q值计算是如下这样的：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，两个算法的代码完全相同。</p>
<h1 id="5-ddqn小结">5. DDQN小结</h1>
<p>DDQN算法出来以后，取得了比较好的效果，因此得到了比较广泛的应用。不过我们的DQN仍然有其他可以优化的点，如上一篇最后讲到的: 随机采样的方法好吗？按道理经验回放里不同样本的重要性是不一样的，TD误差大的样本重要程度应该高。针对这个问题，我们在下一节的Prioritised Replay DQN中讨论。</p>
]]></description></item><item><title>强化学习笔记 [19] | AlphaGo Zero强化学习原理</title><link>https://jianye0428.github.io/posts/rl_learning_note_19/</link><pubDate>Sun, 25 Feb 2024 19:53:22 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_19/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。</p>
<p>本篇主要参考了AlphaGo Zero的<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, <a href="https://www.hhyz.me/2018/08/08/2018-08-08-AlphaGO-Zero/"target="_blank" rel="external nofollow noopener noreferrer">AlphaGo Zero综述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和AlphaGo Zero Cheat Sheet。</p>
<h1 id="1-alphago-zero模型基础">1. AlphaGo Zero模型基础</h1>
<p>AlphaGo Zero不需要学习人类的棋谱，通过自我对弈完成棋力提高。主要使用了两个模型，第一个就是我们上一节介绍MCTS树结构，另一个是一个神经网络。MCTS上一篇已经有基本介绍了，对于神经网络，它的输入是当前的棋局状态，输出两部分，第一部分输出是在当前棋局状态下各个可能的落子动作对应的获胜概率p，可以简单理解为Actor-Critic策略函数部分。另一部分输出为获胜或者失败的评估[-1,1]，可以简单理解为Actor-Critic价值函数部分。</p>
<p>AlphaGo Zero的行棋主要是由MCTS指导完成的，但是在MCTS搜索的过程中，由于有一些不在树中的状态需要仿真，做局面评估，因此需要一个简单的策略来帮助MCTS评估改进策略，这个策略改进部分由前面提到的神经网络完成。</p>
<p>这两部分的关系如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">AlphaGo Zero 中的MCTS和NN</div>
</center>
<br>
<p>具体AlphaGo Zero的MCTS如何搜索，神经网络如何训练，如何指导MCTS搜索我们在后面再讲。</p>
<h1 id="2-alphago-zero的训练过程简介">2. AlphaGo Zero的训练过程简介</h1>
<p>在讨论AlphaGo Zero的MCTS如何搜索，神经网络如何训练等细节之前，我们先看看AlphaGo Zero的训练过程是什么样的。</p>
<p>AlphaGo Zero训练过程主要分为三个阶段：自我对战学习阶段，训练神经网络阶段和评估网络阶段。</p>
<p>自我对战学习阶段主要是AlphaGo Zero自我对弈，产生大量棋局样本的过程，由于AlphaGo Zero并不使用围棋大师的棋局来学习，因此需要自我对弈得到训练数据用于后续神经网络的训练。在自我对战学习阶段，每一步的落子是由MCTS搜索来完成的。在MCTS搜索的过程中，遇到不在树中的状态，则使用神经网络的结果来更新MCTS树结构上保存的内容。在每一次迭代过程中，在每个棋局当前状态 $s$ 下，每一次移动使用1600次MCTS搜索模拟。最终MCTS给出最优的落子策略 $π$ ,这个策略 $π$ 和神经网络的输出 $p$ 是不一样的。当每一局对战结束后，我们可以得到最终的胜负奖励 $z$ ,1或者-1. 这样我们可以得到非常多的样本 $(s,π,z)$,这些数据可以训练神经网络阶段。</p>
<p>在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合(s,π,z)(�,�,�),训练我们神经网络的模型参数。训练的目的是对于每个输入 $s$, 神经网络输出的 $p,v$和我们训练样本中的 $π$, $z$差距尽可能的少。这个损失函数 $L$ 其实是很简单的：</p>
<p>$$L=(z-v)^2-\pi^Tlog(p)+c||\theta||^2$$</p>
<p>损失函数由三部分组成，第一部分是均方误差损失函数，用于评估神经网络预测的胜负结果和真实结果之间的差异。第二部分是交叉熵损失函数，用于评估神经网络的输出策略和我们MCTS输出的策略的差异。第三部分是L2正则化项。</p>
<p>通过训练神经网络，我们可以优化神经网络的参数 $θ$,用于后续指导我们的MCTS搜索过程。</p>
<p>当神经网络训练完毕后，我们就进行了评估阶段，这个阶段主要用于确认神经网络的参数是否得到了优化，这个过程中，自我对战的双方各自使用自己的神经网络指导MCTS搜索，并对战若干局，检验AlphaGo Zero在新神经网络参数下棋力是否得到了提高。除了神经网络的参数不同，这个过程和第一阶段的自我对战学习阶段过程是类似的。</p>
<h1 id="3-alphago-zero的神经网络结构">3. AlphaGo Zero的神经网络结构</h1>
<p>在第二节我们已经讨论了AlphaGo Zero的主要训练过程，但是还有两块没有讲清楚，一是AlphaGo Zero的MCTS搜索过程是怎么样的，二是AlphaGo Zero的神经网络的结构具体是什么样的。这一节我们来看看AlphaGo Zero的神经网络的细节。</p>
<p>首先我们看看AlphaGo Zero的输入，当前的棋局状态。由于围棋是19x19的361个点组成的棋局，每个点的状态有二种：如果当前是黑方行棋，则当前有黑棋的点取值1，有白棋或者没有棋子的位置取值为0，反过来，如果当前是白方行棋，则当前有白棋的点取值1，有黑棋或者没有棋子的位置取值为0。同时，为了提供更多的信息，输入的棋局状态不光只有当前的棋局状态，包括了黑棋白棋各自前8步对应的棋局状态。除了这16个棋局状态，还有一个单独的棋局状态用于标识当前行棋方，如果是当前黑棋行棋，则棋局状态上标全1，白棋则棋局状态上标全0。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Game State</div>
</center>
<br>
<p>最终神经网络的输入是一个19x19x17的张量。里面包含黑棋和白棋的最近8步行棋状态和当前行棋方的信息。</p>
<p>接着我们看看神经网络的输出，神经网络的输出包括策略部分和价值部分。对于策略部分，它预测当前各个行棋点落子的概率。由于围棋有361个落子点，加上还可以Pass一手，因此一共有362个策略端概率输出。对于价值端，输出就简单了，就是当前局面胜负的评估值，在[-1,1]之间。</p>
<p>看完了神经网络的输入和输出，我们再看看神经网络的结构，主要是用CNN组成的深度残差网络。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>在19x19x17的张量做了一个基本的卷积后，使用了19层或者39层的深度残差网络，这个是ResNet的经典结构。理论上这里也可以使用DenseNet等其他流行的网络结构。神经网络的损失函数部分我们在第二节已经将了。整个神经网络就是为了当MCTS遇到没有见过的局面时，提供的当前状态下的局面评估和落子概率参考。这部分信息会被MCTS后续综合利用。</p>
<h1 id="4-alphago-zero的mcts搜索">4. AlphaGo Zero的MCTS搜索</h1>
<p>　　　　现在我们来再看看AlphaGo Zero的MCTS搜索过程，在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们已经介绍了MCTS的基本原理，和4个主要的搜索阶段：选择，扩展，仿真和回溯。和上一篇的内容相比，这里MCTS的不同主要体现在树结构上保存的信息不同，进而UCT的计算公式也稍有不同。最后MCTS搜索完毕后，AlphaGo Zero也有自己选择真正落子点的策略。</p>
<p>　　　　在上一篇里，我们的MCTS上保存的数据很简单，就是下的总盘数和赢的总盘数。在AlphaGo Zero这里，我们保存的信息会多一些。主要包括下面的4部分：</p>
<ul>
<li>$N(s,a)$:记录边的访问次数</li>
<li>$W(s,a)$: 合计行动价值</li>
<li>$Q(s,a)$:平均行动价值</li>
<li>$P(s,a)$:选择该条边的先验概率</li>
</ul>
<p>其中 $s$ 为当前棋局状态，$a$ 为某一落子选择对应的树分支。</p>
<p>有了MCTS上的数据结构，我们看看AlphaGo Zero的MCTS搜索的4个阶段流程：</p>
<p>首先是选择，在MCTS内部，出现过的局面，我们会使用UCT选择子分支。子分支的UCT原理和上一节一样。但是具体的公式稍有不同，如下：</p>
<p>$$\begin{gathered}
U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)} \\
a_t=\arg\max_a(Q(s_t,a)+U(s_t,a))
\end{gathered}$$</p>
<p>最终我们会选择 $Q+U$最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数,上一篇已讲过。</p>
<p>如果到了没有到终局的MCTS叶子节点，那么我们就需要进入MCTS的第二步，扩展阶段,以及后续的第三步仿真阶段。我们这里一起讲。对于叶子节点状态s�，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL��落子的概率p�和对应的价值v�,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为：</p>
<p>$$\{N(s_L,a)=0,W(s_L,a)=0,Q(s_L,a)=0,P(s_L,a)=P_a\}$$</p>
<p>这个过程如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>这样扩展后，之前的叶子节点 $s$，现在就是内部节点了。做完了扩展和仿真后，我们需要进行回溯，将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点 $L$ 依次向根节点回溯，并依次更新上层分支数据结构如下：</p>
<p>$$\begin{gathered}
N(s_t,a_t)=N(s_t,a_t)+1 \\
W(s_t,a_t)=W(s_t,a_t)+v \\
Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}
\end{gathered}$$</p>
<p>这个MCTS搜索过程在一次真正行棋前，一般会进行约1600次搜索，每次搜索都会进行上述4个阶段。</p>
<p>这上千次MCTS搜索完毕后，AlphaGo Zero就可以在MCTS的根节点 $s$ 基于以下公式选择行棋的MCTS分支了:</p>
<p>$$\pi(a|s)=\frac{N(s,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}$$</p>
<p>其中，$τ$ 为温度参数，控制探索的程度，$τ$ 越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。每一次完整的自我对弈的前30步，参数 $τ=1$，这是早期鼓励探索的设置。游戏剩下的步数，该参数将逐渐降低至0。如果是比赛，则直接为0.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>同时在随后的时间步中，这个MCTS搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 。</p>
<p>以上就是AlphaGo Zero MCTS搜索的过程。</p>
<h1 id="5-alphago-zero小结与强化学习系列小结">5. AlphaGo Zero小结与强化学习系列小结</h1>
<p>AlphaGo Zero巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。两者一主一辅，非常优雅的解决了这类状态完全可见，信息充分的棋类问题。</p>
<p>当然这类强化学习算法只对特定的这类完全状态可见，信息充分的问题有效，遇到信息不对称的强化学习问题，比如星际，魔兽之类的对战游戏问题，这个算法就不那么有效了。要推广AlphaGo Zero的算法到大多数普通强化学习问题还是很难的。因此后续强化学习算法应该还有很多发展的空间。</p>
<p>至此强化学习系列就写完了，之前预计的是写三个月，结果由于事情太多，居然花了大半年。但是总算还是完成了，没有烂尾。生活不易，继续努力！</p>
]]></description></item><item><title>强化学习笔记 [18] | 基于模拟的搜索与蒙特卡罗树搜索(MCTS)</title><link>https://jianye0428.github.io/posts/rl_learning_note_18/</link><pubDate>Sun, 25 Feb 2024 19:53:18 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_18/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。</p>
<p>本篇主要参考了UCL强化学习课程的第八讲，第九讲部分。</p>
<h1 id="1-基于模拟的搜索概述">1. 基于模拟的搜索概述</h1>
<p>什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这是数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。</p>
<p>那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点 $S_t$ 开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以 $S_t$ 为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到 $S_t$状态最应该采用的动作 $A_t$。前向搜索的sub-MDP如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">forward search sub-MDP</div>
</center>
<br>
<p>前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。</p>
<h1 id="2-简单蒙特卡罗搜索">2. 简单蒙特卡罗搜索</h1>
<p>首先我们看看基于模拟的搜索中比较简单的一种方法：简单蒙特卡罗搜索。</p>
<p>简单蒙特卡罗搜索基于一个强化学习模型 $M_v$ 和一个模拟策略 $π$.在此基础上，对于当前我们要选择动作的状态 $S_t$, 对每一个可能采样的动作 $a∈A$,都进行 $K$ 轮采样，这样每个动作 $a$ 都会得到 $K$ 组经历完整的状态序列(episode)。即：</p>
<p>$$\{S_t,a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>现在对于每个 $(S_t,a)$ 组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1K\sum_{k=1}^KG_t\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别,那么简单蒙特卡罗搜索也太慢了。同时，由于使用蒙特卡罗法计算其动作价值函数，模拟采样得到的一些中间状态和对应行为的价值就被忽略了，这部分数据能不能利用起来呢？</p>
<p>下面我们看看蒙特卡罗树搜索(Monte-Carlo Tree Search，以下简称MCTS)怎么优化这个问题的解决方案。</p>
<h1 id="3-mcts的原理">3. MCTS的原理</h1>
<p>MCTS摒弃了简单蒙特卡罗搜索里面对当前状态 $S_t$ 每个动作都要进行K次模拟采样的做法，而是总共对当前状态 $S_t$进行K次采样，这样采样到的动作只是动作全集 $A$ 中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。</p>
<p>在MCTS中，基于一个强化学习模型 $M_v$和一个模拟策略$π$，当前状态 $S_t$ 对应的完整的状态序列(episode)是这样的:</p>
<p>$$\{S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算 $Q(st,a)$和最大 $Q(s_t,a)$对应的动作。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1{N(S_t,a)}\sum_{k=1}^K\sum_{u=t}^T1(S_{uk}=S_t,A_{uk}=a)G_u\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>MCTS搜索的策略分为两个阶段：第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使 $ϵ−$贪婪策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。</p>
<p>这里讲到的是最经典的强化学习终MCTS的用户，每一步都有延时奖励，但是在棋类之类的零和问题中，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。</p>
<h1 id="4-上限置信区间算法uct">4. 上限置信区间算法UCT</h1>
<p>在讨论棋类游戏的MCTS搜索之前，我们先熟悉下上限置信区间算法(Upper Confidence Bound Applied to Trees, 以下简称UCT)。它是一种策略算法，我们之前最常用的是 $ϵ−$贪婪策略。但是在棋类问题中，UCT更常使用。</p>
<p>在棋类游戏中，经常有这样的问题，我们发现在某种棋的状态下，有2个可选动作，第一个动作历史棋局中是0胜1负，第二个动作历史棋局中是8胜10负，那么我们应该选择哪个动作好呢？如果按 $ϵ−$贪婪策略，则第二个动作非常容易被选择到。但是其实虽然第一个动作胜利0%，但是很可能是因为这个动作的历史棋局少，数据不够导致的，很可能该动作也是一个不错的动作。那么我们如何在最优策略和探索度达到一个选择平衡呢？ $ϵ−$贪婪策略可以用，但是UCT是一个更不错的选择。</p>
<p>UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度吗，一个常用的公式如下：</p>
<p>$$\text{score}=\left.\frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}\right.$$</p>
<p>其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$ 是所有模拟次数，c 是探索常数，理论值为$√2$，可根据经验调整，$c$ 越大就越偏向于广度搜索，$c$ 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。</p>
<p>比如对于下面的棋局，对于根节点来说，有3个选择，第一个选择7胜3负，第二个选择5胜3负，第三个选择0胜3负。</p>
<p>如果我们取c=10,则第一个节点的分数为：$$score(7,10)=7/10+C\cdot\sqrt{\frac{\log(21)}{10}}\approx6.2$$</p>
<p>第二个节点的分数为：$$score(5,8)=5/8+C\cdot\sqrt{\frac{\log(21)}8}\approx6.8$$</p>
<p>第三个节点的分数为：$$score(0,3)=0/3+C\cdot\sqrt{\frac{\log(21)}3}\approx10$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>可见，由于我们把探索率 $c$ 设置的比较大，第三个节点是被UCT选中要执行的动作节点。当然如果我们把c设置的比较小的话，第一个或者第二个可能就变成最大的分数了。</p>
<h1 id="5-棋类游戏mcts搜索">5. 棋类游戏MCTS搜索</h1>
<p>在像中国象棋，围棋这样的零和问题中，一个动作只有在棋局结束才能拿到真正的奖励，因此我们对MCTS的搜索步骤和树结构上需要根据问题的不同做一些细化。</p>
<p>对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步，如下图(图来自维基百科)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。</p>
<p>第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。</p>
<p>第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。</p>
<p>第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。</p>
<p>以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。</p>
<h1 id="6-mcts小结">6. MCTS小结</h1>
<p>MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。</p>
]]></description></item><item><title>强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架</title><link>https://jianye0428.github.io/posts/rl_learning_note_17/</link><pubDate>Sun, 25 Feb 2024 19:53:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_17/</guid><description><![CDATA[<h1 id="强化学习十七-基于模型的强化学习与dyna算法框架httpswwwcnblogscompinardp10384424html"><a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h1>
<p>在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。</p>
<p>本篇主要参考了UCL强化学习课程的第8讲和Dyna-2的<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-基于模型的强化学习简介">1. 基于模型的强化学习简介</h1>
<p>基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率 $P^a_{ss&rsquo;}$。</p>
<p>而基于模型的强化学习则会尝试从环境的模型去学习，一般是下面两个相互独立的模型：</p>
<ul>
<li>一个是状态转化预测模型，输入当前状态 $s$和动作 $a$，预测下一个状态 $s&rsquo;$。</li>
<li>另一个是奖励预测模型，输入当前状态 $s$和动作 $a$，预测环境的奖励 $r$。</li>
</ul>
<p>即模型可以描述为下面两个式子：</p>
<p>$$\begin{gathered}S_{t+1}\sim P(S_{t+1}|S_t,A_t)\\R_{t+1}\sim R(R_{t+1}|S_t,A_t)\end{gathered}$$</p>
<p>如果模型 $P$, $R$ 可以准确的描述真正的环境的转化模型，那么我们就可以基于模型来预测，当有一个新的状态 $S$ 和动作 $A$到来时，我们可以直接基于模型预测得到新的状态和动作奖励，不需要和环境交互。当然如果我们的模型不好，那么基于模型预测的新状态和动作奖励可能错的离谱。</p>
<p>从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的主要区别：即基于模型的强化学习是从模型中学习，而不基于模型的强化学习是从和环境交互的经历去学习。</p>
<p>下面这张图描述了基于模型的强化学习的思路：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Model-based RL</div>
</center>
<br>
<h1 id="2-基于模型的强化学习算法训练流程">2. 基于模型的强化学习算法训练流程</h1>
<p>这里我们看看基于模型的强化学习算法训练流程，其流程和我们监督学习算法是非常类似的。</p>
<p>假设训练数据是若干组这样的经历：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,R_2,\ldots,S_T$$</p>
<p>对于每组经历，我们可以将其转化为 $T−1$ 组训练样本，即：</p>
<p>$$\begin{gathered}
S_1,A_1\to S_2,S_1,A_1\to R_2 \\
S_2,A_2\to S_3,S_2,A_2\to R_3 \\
&hellip;&hellip; \\
S_{T-1},A_{T-1}\rightarrow S_T,~S_{T_1},A_{T-1}\rightarrow R_T
\end{gathered}$$</p>
<p>右边的训练样本一起组成了一个分类模型或密度估计模型，输入状态和动作，输出下一个状态。 右边的训练样本一起组成了一个回归模型训练集，输入状态和动作，输出动作奖励值。</p>
<p>至此我们的强化学习求解过程和传统的监督学习算法没有太多区别了，可以使用传统的监督学习算法来求解这两个模型。</p>
<p>当然还可以更简单，即通过对训练样本进行查表法进行统计，直接得到 $P(S_{t+1}|S_t,A_t)$ 的概率和 $R(R_{t+1}|S_t,A_t)$ 的平均值，这样就可以直接预测。比使用模型更简单。</p>
<p>此外，还有其他的方法可以用来得到 $P(S_{t+1}|S_t,A_t)$和 $R(R_{t+1}|S_t,A_t)$，这个我们后面再讲。</p>
<p>虽然基于模型的强化学习思路很清晰，而且还有不要和环境持续交互优化的优点，但是用于实际产品还是有很多差距的。主要是我们的模型绝大多数时候不能准确的描述真正的环境的转化模型，那么使用基于模型的强化学习算法得到的解大多数时候也不是很实用。那么是不是基于模型的强化学习就不能用了呢？也不是，我们可以将基于模型的强化学习和不基于模型的强化学习集合起来，取长补短，这样做最常见的就是Dyna算法框架。</p>
<h1 id="3-dyna算法框架">3. Dyna算法框架</h1>
<p>Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。如果用和第一节类似的图，可以表示如下图，和第一节的图相比，多了一个“Direct RL“的箭头，这正是不基于模型的强化学习的思路。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna算法示意图</div>
</center>
<br>
<p>Dyna算法框架和不同的具体的不基于模型的强化学习一起，可以得到具体的不同算法。如果我们使用基于价值函数的Q-Learning，那么我们就得到了Dyna-Q算法。我们基于Dyna-Q来看看Dyna算法框架的一般流程.</p>
<h1 id="4-dyna-q算法流程">4. Dyna-Q算法流程</h1>
<p>这里我们给出基于价值函数的Dyna-Q算法的概要流程。假设模型使用的是查表法。</p>
<ul>
<li>(1). 初始化任意一个状态 $s$,和任意一个动作 $a$ 对应的状态价值 $Q(s,a)$, 初始化奖励模型 $R(s,a)$和状态模型 $P(s,a)$</li>
<li>(2). for $i=1$ to 最大迭代次数T：
<ul>
<li>(a) $S \leftarrow \text{current state}$</li>
<li>(b) $A \leftarrow \text{ϵ−greedy(S,Q)}$</li>
<li>(c) 执行动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>(d) 使用Q-Learning更新价值函数：$Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
<li>(e) 使用 $S,A,S^{\prime}$ 更新状态模型 $P(s,a)$，使用 $S,A,R$ 更新状态模型 $R(s,a)$</li>
<li>(f) $\text{for} \space \space j=1 \space \space \text{to} \text{最大次数}n$：
<ul>
<li>(i) 随机选择一个之前出现过的状态 $S$ , 在状态 $S$ 上出现过的动作中随机选择一个动作 $A$</li>
<li>(ii) 基于模型 $P(S,A)$ 得到 $S&rsquo;$, 基于模型 $R(S,A)$ 得到 $R$</li>
<li>(iii) 使用Q-Learning更新价值函数: $Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>从上面的流程可以看出，Dyna框架在每个迭代轮中，会先和环境交互，并更新价值函数和（或）策略函数，接着进行n次模型的预测，同样更新价值函数和（或）策略函数。这样同时利用上了和环境交互的经历以及模型的预测。</p>
<h1 id="5-dyna-2算法框架">5. Dyna-2算法框架</h1>
<p>在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为<strong>永久性记忆</strong>（permanent memory）和<strong>瞬时记忆</strong>（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。</p>
<p>永久性记忆的Q函数定义为：</p>
<p>$$Q(S,A)=\phi(S,A)^T\theta $$</p>
<p>瞬时记忆的Q函数定义为：</p>
<p>$$Q^{\prime}(S,A)=\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>组合起来后记忆的Q函数定义为：</p>
<p>$$\overline{Q}(S,A)=\phi(S,A)^T\theta+\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似于 $SARSA(λ)$</p>
<p>以下是Dyna-2的算法流程：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna-2 算法流程</div>
</center>
<br>
<h1 id="6-基于模型的强化学习总结">6. 基于模型的强化学习总结</h1>
<p>基于模型的强化学习一般不单独使用，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。对于模型部分，我们可以用查表法和监督学习法等方法，预测或者采样得到模拟的经历。而对于非模型部分，使用前面的Q-Learning系列的价值函数近似，或者基于Actor-Critic的策略函数的近似都是可以的。</p>
<p>除了Dyna算法框架，我们还可以使用基于模拟的搜索(simulation-based search)来结合基于模型的强化学习和不基于模型的强化学习,并求解问题。这部分我们在后面再讨论。</p>
]]></description></item><item><title>强化学习笔记 [16] | 深度确定性策略梯度(DDPG)</title><link>https://jianye0428.github.io/posts/rl_learning_note_16/</link><pubDate>Sun, 25 Feb 2024 19:53:12 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_16/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10334127.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十五) A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DDPG)。</p>
<p>本篇主要参考了DDPG的<a href="https://arxiv.org/pdf/1509.02971.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-从随机策略到确定性策略">1. 从随机策略到确定性策略</h1>
<p>从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。PG(Policy Gradient)我们在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里已经讨论过。那什么是确定性策略梯度(Deterministic Policy Gradient，以下简称DPG)呢？</p>
<p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成：</p>
<p>$$\pi_\theta(s)=a$$</p>
<h1 id="2-从dpg到ddpg">2. 从DPG到DDPG</h1>
<p>在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>其中状态的采样空间为$\rho^\pi$, $\nabla_\theta log\pi_\theta(s,a)$是分值函数，可见随机性策略梯度需要在整个动作的空间$\pi_\mathrm{\theta}$进行采样。</p>
<p>而DPG基于Q值的确定性策略梯度的梯度计算公式是：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi}[\nabla_\theta\pi_\theta(s)\nabla_aQ_\pi(s,a)|<em>{a=\pi</em>\theta(s)}]$$</p>
<p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p>
<p>而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？</p>
<h1 id="3-ddpg的原理">3. DDPG的原理</h1>
<p>DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。可以复习<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>DDQN的当前Q网络负责对当前状态 $S$ 使用 $ϵ$−贪婪法选择动作 $A$，执行动作 $A$,获得新状态 $S&rsquo;$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态 $S&rsquo;$使用贪婪法选择动作 $A&rsquo;$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。</p>
<p>DDQN的目标Q网络则负责基于经验回放池计算目标Q值, 提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。</p>
<p>现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要 $ϵ$−贪婪法这样的选择方法，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态 $S&rsquo;$ 使用贪婪法选择动作 $A&rsquo;$，这部分工作由于用来估计目标Q值，因此可以放到Actor目标网络完成。</p>
<p>基于经验回放池和目标Actor网络提供的 $S&rsquo;$, $A&rsquo;$ 计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。</p>
<p>此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。</p>
<p>有了上面的思路，我们总结下DDPG 4个网络的功能定位：</p>
<ul>
<li>
<p>(1). <strong>Actor当前网络</strong>: 负责策略网络参数 $θ$的迭代更新，负责根据当前状态 $S$选择当前动作 $A$，用于和环境交互生成 $S&rsquo;$, $R$。</p>
</li>
<li>
<p>(2). <strong>Actor目标网络</strong>: 负责根据经验回放池中采样的下一状态 $S&rsquo;$ 选择最优下一动作$A&rsquo;$。网络参数 $θ&rsquo;$定期从 $θ$复制。</p>
</li>
<li>
<p>(3). <strong>Critic当前网络</strong>: 负责价值网络参数 $w$的迭代更新，负责计算负责计算当前Q值 $Q(S,A,w)$。目标Q值$y_i=R+γQ&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$</p>
</li>
<li>
<p>(4). <strong>Critic目标网络</strong>: 负责计算目标Q值中的 $Q&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$部分。网络参数 $w&rsquo;$ 定期从 $w$复制。</p>
</li>
</ul>
<p>DDPG除了这4个网络结构，还用到了经验回放，这部分用于计算目标Q值，和DQN没有什么区别，这里就不展开了。</p>
<p>此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w$′=$w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：</p>
<p>$$\begin{gathered}
w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</p>
<p>其中 $τ$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。</p>
<p>同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作 $A$会增加一定的噪声 $N$, 即最终和环境交互的动作 $A$ 的表达式是：</p>
<p>$$A=\pi_\theta(S)+\mathcal{N}$$</p>
<p>最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：</p>
<p>$$J(w)=\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>而对于 Actor当前网络，其损失函数就和之前讲的PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：</p>
<p>$$\nabla_J(\theta)=\frac1m\sum_{j=1}^m[\nabla_aQ_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>\theta(s)}\nabla_\theta\pi_{\theta(s)}|_{s=s_i}]$$</p>
<p>这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作 $a_1$和$a_2$，从Critic当前网络得到了两个反馈的 $Q$ 值，分别是 $Q_1$,$Q_2$，假设 $Q_1&gt;Q_2$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加 $a_1$的概率，降低$a_2$的概率，也就是说，Actor想要尽可能的得到更大的Q值。所以我们的Actor的损失可以简单的理解为得到的反馈Q值越大损失越小，得到的反馈Q值越小损失越大，因此只要对状态估计网络返回的Q值取个负号即可，即：</p>
<p>$$J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,w)$$</p>
<h1 id="4-ddpg算法流程">4. DDPG算法流程</h1>
<p>这里我们总结下DDPG的算法流程</p>
<p>输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为 $θ$,$θ&rsquo;$,$w$,$w&rsquo;$,衰减因子 $γ$, 软更新系数 $τ$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$。最大迭代次数 $T$。随机噪音函数 $\mathcal{N}$</p>
<p>输出：最优Actor当前网络参数 $θ$,Critic当前网络参数 $w$</p>
<ul>
<li>(1). 随机初始化$θ$,$w$, $w$′=$w$,$θ$′=$θ$。清空经验回放的集合$D$</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor当前网络基于状态 $S$ 得到动作 $A=π_θ(ϕ(S))+\mathcal{N}$</li>
<li>c) 执行动作$A$,得到新状态$S$′,奖励$R$,是否终止状态%is_end$</li>
<li>d) 将 ${ϕ(S), A, R, ϕ(S&rsquo;), is_end}$ 这个五元组存入经验回放集合$D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本${\phi(S_j),A_j,R_j,\phi(S_j^{\prime}),is_end_j},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\pi_{\theta^{\prime}}(\phi(S_j^{\prime})),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Critic当前网络的所有参数 $w$</li>
<li>h) 使用 $\begin{aligned}J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,\theta)\end{aligned}$，通过神经网络的梯度反向传播来更新Actor当前网络的所有参数 $θ$</li>
<li>i) 如果 i%C=1, 则更新Critic目标网络和Actor目标网络参数：
<ul>
<li>$$\begin{gathered} w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</li>
</ul>
</li>
<li>j) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤(b)</li>
</ul>
</li>
</ul>
<p>以上就是DDPG算法的主流程，要注意的是上面2.f中的 $\pi_{\theta^{\prime}}(\phi(S_j^{\prime}))$ 是通过Actor目标网络得到，而 $Q^{\prime}(\phi(S_i^{\prime}),\pi_{\theta^{\prime}}(\phi(S_i^{\prime})),w^{\prime})$ 则是通过Critic目标网络得到的。</p>
<h1 id="5-ddpg实例">5. DDPG实例</h1>
<p>这里我们给出DDPG第一个算法实例，代码主要参考自莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG_update.py"target="_blank" rel="external nofollow noopener noreferrer">Github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。增加了测试模型效果的部分，优化了少量参数。代码详见：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py</p>
<p>这里我们没有用之前的CartPole游戏，因为它不是连续动作。我们使用了Pendulum-v0这个游戏。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a href="https://github.com/openai/gym/wiki/Pendulum-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。输入状态是角度的sin，cos值，以及角速度。一共三个值。动作是一个连续的力矩值。</p>
<p>两个Actor网络和两个Critic网络的定义参见：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_bound</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;scaled_a&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_c</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_l1</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_s</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_s&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_a&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">w1_s</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">w1_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>  <span class="c1"># Q(s,a)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor当前网络和Critic当前网络损失函数的定义参见：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">q_target</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">td_error</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ce_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">a_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>    <span class="c1"># maximize the q</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">atrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">a_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ae_params</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor目标网络和Critic目标网络参数软更新，Actor当前网络和Critic当前网络反向传播更新部分的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># soft target replacement</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">soft_replace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">MEMORY_CAPACITY</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">bt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">ba</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">br</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs_</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">:</span> <span class="n">ba</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">:</span> <span class="n">br</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_</span><span class="p">:</span> <span class="n">bs_</span><span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余的可以对照算法和代码一起学习，应该比较容易理解。</p>
<h1 id="6-ddpg总结">6. DDPG总结</h1>
<p>DDPG参考了DDQN的算法思想吗，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p>
<p>到此，我们的Policy Based RL系列也讨论完了，而在更早我们讨论了Value Based RL系列，至此，我们还剩下Model Based RL没有讨论。后续我们讨论Model Based RL的相关算法。</p>
]]></description></item><item><title>强化学习笔记 [15] | A3C</title><link>https://jianye0428.github.io/posts/rl_learning_note_15/</link><pubDate>Sun, 25 Feb 2024 15:36:01 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_15/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C的算法原理和算法流程。</p>
<p>本文主要参考了A3C的<a href="http://proceedings.mlr.press/v48/mniha16.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，以及ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-a3c的引入">1. A3C的引入</h1>
<p>上一篇Actor-Critic算法的代码，其实很难收敛，无论怎么调参，最后的CartPole都很难稳定在200分，这是Actor-Critic算法的问题。但是我们还是有办法去有优化这个难以收敛的问题的。</p>
<p>回忆下之前的DQN算法，为了方便收敛使用了经验回放的技巧。那么我们的Actor-Critic是不是也可以使用经验回放的技巧呢？当然可以！不过A3C更进一步，还克服了一些经验回放的问题。经验回放有什么问题呢？ 回放池经验数据相关性太强，用于训练的时候效果很可能不佳。举个例子，我们学习下棋，总是和同一个人下，期望能提高棋艺。这当然没有问题，但是到一定程度就再难提高了，此时最好的方法是另寻高手切磋。</p>
<p>A3C的思路也是如此，它<font color=green>利用多线程的方法，同时在多个线程里面分别和环境进行交互学习，每个线程都把学习的成果汇总起来，整理保存在一个公共的地方</font>。并且，定期从公共的地方把大家的齐心学习的成果拿回来，指导自己和环境后面的学习交互。</p>
<p>通过这种方法，A3C避免了经验回放相关性过强的问题，同时做到了异步并发的学习模型。</p>
<h1 id="2-a3c的算法优化">2. A3C的算法优化</h1>
<p>现在我们来看看相比Actor-Critic，A3C到底做了哪些具体的优化。</p>
<p>相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p>我们首先来看这个异步训练框架，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">异步训练框架</div>
</center>
<br>
<p>图中上面的Global Network就是上一节说的共享的公共部分，主要是一个公共的神经网络模型，这个神经网络包括Actor网络和Critic网络两部分的功能。下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。</p>
<p>每个线程和环境交互到一定量的数据后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度却并不更新自己线程里的神经网络，而是去更新公共的神经网络。也就是n个线程会独立的使用累积的梯度分别更新公共部分的神经网络模型参数。每隔一段时间，线程会将自己的神经网络的参数更新为公共神经网络的参数，进而指导后面的环境交互。</p>
<p>可见，公共部分的网络模型就是我们要学习的模型，而线程里的网络模型主要是用于和环境交互使用的，这些线程里的模型可以帮助线程更好的和环境交互，拿到高质量的数据帮助模型更快收敛。</p>
<p>现在我们来看看第二个优化，网络结构的优化。之前在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们使用了两个不同的网络Actor和Critic。在A3C这里，我们把两个网络放到了一起，即输入状态 $S$,可以输出状态价值 $V$,和对应的策略 $π$, 当然，我们仍然可以把Actor和Critic看做独立的两块，分别处理，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">把Actor和Critic看做独立的两块，分别处理</div>
</center>
<br>
<p>第三个优化点是Critic评估点的优化，在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节中，我们讨论了不同的Critic评估点的选择，其中d部分讲到了使用优势函数 $A$ 来做Critic评估点，优势函数 $A$ 在时刻t不考虑参数的默认表达式为：</p>
<p>$$A(S,A,t)=Q(S,A)-V(S)$$</p>
<p>$Q(S,A)$的值一般可以通过单步采样近似估计，即：</p>
<p>$$Q(S,A)=R+\gamma V(S^{\prime})$$</p>
<p>这样优势函数去掉动作可以表达为：</p>
<p>$$A(S,t)=R+\gamma V(S^{\prime})-V(S)$$</p>
<p>其中 $V(S)$的值需要通过Critic网络来学习得到。</p>
<p>在A3C中，采样更进一步，使用了N步采样，以加速收敛。这样A3C中使用的优势函数表达为：</p>
<p>$$A(S,t)=R_t++\gamma R_{t+1}+\ldots\gamma^{n-1}R_{t+n-1}+\gamma^nV(S^{\prime})-V(S)$$</p>
<p>对于Actor和Critic的损失函数部分，和Actor-Critic基本相同。有一个小的优化点就是在Actor-Critic策略函数的损失函数中，加入了策略 $π$ 的熵项,系数为 $c$, 即策略参数的梯度更新和Actor-Critic相比变成了这样：</p>
<p>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,t)+c\nabla_\theta H(\pi(S_t,\theta))$$</p>
<p>以上就是A3C和Actor-Critic相比有优化的部分。下面我们来总价下A3C的算法流程。</p>
<h1 id="3-a3c算法流程">3. A3C算法流程</h1>
<p>这里我们对A3C算法流程做一个总结，由于A3C是异步多线程的，我们这里给出任意一个线程的算法流程。</p>
<ul>
<li>
<p>输入：公共部分的A3C神经网络结构，对应参数位 $θ$ , $w$，本线程的A3C神经网络结构，对应参数 $θ&rsquo;$, $w&rsquo;$, 全局共享的迭代轮数 $T$，全局最大迭代次数 $T_{max}$, 线程内单次迭代时间序列最大长度 $T_{local}$,状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，熵系数 $c$, 衰减因子 $γ$</p>
</li>
<li>
<p>输出：公共部分的A3C神经网络参数 $θ$, $w$</p>
<ul>
<li>(1). 更新时间序列 $t=1$</li>
<li>(2). 重置Actor和Critic的梯度更新量: $dθ←0$,$dw←0$</li>
<li>(3). 从公共部分的A3C神经网络同步参数到本线程的神经网络：$θ&rsquo;=θ,w&rsquo;=w$</li>
<li>(4). $t_{start}=t$，初始化状态 $s_t$</li>
<li>(5). 基于策略 $π(at|st;θ)$ 选择出动作 $a_t$</li>
<li>(6). 执行动作 $a_t$得到奖励 $r_t$ 和新状态 $s_{t+1}$</li>
<li>(7). $t←t+1$, $T←T+1$</li>
<li>(8). 如果 $s_t$是终止状态，或 $t − t_{start}==t_{local}$,则进入步骤(9)，否则回到步骤(5)</li>
<li>(9). 计算最后一个时间序列位置 $s_t$的 $Q(s,t)$:
<ul>
<li>$$\left.Q(s,t)=\left\{\begin{array}{ll}0&amp;terminal~state\\V(s_t,w^{\prime})&amp;none~terminal~state,bootstrapping\end{array}\right.\right.$$</li>
</ul>
</li>
<li>(10). for $i∈(t−1,t−2,&hellip;t_{start})$:
<ul>
<li>1). 计算每个时刻的$Q(s,i)$： $Q(s,i)=r_i+\gamma Q(s,i+1)$</li>
<li>2). 累计Actor的本地梯度更新：
<ul>
<li>$$d\theta\leftarrow d\theta+\nabla_{\theta^{\prime}}log\pi_{\theta^{\prime}}(s_i,a_i)(Q(s,i)-V(S_i,w^{\prime}))+c\nabla_{\theta^{\prime}}H(\pi(s_i,\theta^{\prime}))$$</li>
</ul>
</li>
<li>3). 累计Critic的本地梯度更新：
<ul>
<li>$$\begin{aligned}dw&amp;\leftarrow dw+\frac{\partial(Q(s,i)-V(S_i,w^{\prime}))^2}{\partial w^{\prime}}\end{aligned}$$</li>
</ul>
</li>
</ul>
</li>
<li>(11). 更新全局神经网络的模型参数：
<ul>
<li>$$\theta=\theta+\alpha d\theta,~w=w-\beta dw$$</li>
</ul>
</li>
<li>(12). 如果 $T&gt;T_{max}$,则算法结束，输出公共部分的A3C神经网络参数 $θ$, $w$,否则进入步骤(3)</li>
</ul>
</li>
</ul>
<p>以上就是A3C算法单个线程的算法流程。</p>
<h1 id="4-a3c算法实例">4. A3C算法实例</h1>
<p>下面我们基于上述算法流程给出A3C算法实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法代码大部分参考了莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/10_A3C/A3C_discrete_action.py"target="_blank" rel="external nofollow noopener noreferrer">A3C代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，增加了模型测试部分的代码并调整了部分模型参数。完整的代码参见我的Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py</p>
<p>整个算法的Actor和Critic的网络结构都定义在这里， 所有的线程中的网络结构，公共部分的网络结构都在这里定义。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;actor&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;la&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_a</span><span class="p">,</span> <span class="n">N_A</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ap&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;critic&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lc&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">)</span>  <span class="c1"># state value</span>
</span></span><span class="line"><span class="cl">  <span class="n">a_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/actor&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">c_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/critic&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">a_prob</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">a_params</span><span class="p">,</span> <span class="n">c_params</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所有线程初始化部分，以及本线程和公共的网络结构初始化部分如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;/cpu:0&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropA&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">GLOBAL_AC</span> <span class="o">=</span> <span class="n">ACNet</span><span class="p">(</span><span class="n">GLOBAL_NET_SCOPE</span><span class="p">)</span>  <span class="c1"># we only need its params</span>
</span></span><span class="line"><span class="cl">  <span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Create worker</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">i_name</span> <span class="o">=</span> <span class="s1">&#39;W_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>   <span class="c1"># worker name</span>
</span></span><span class="line"><span class="cl">    <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker</span><span class="p">(</span><span class="n">i_name</span><span class="p">,</span> <span class="n">GLOBAL_AC</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本线程神经网络将本地的梯度更新量用于更新公共网络参数的逻辑在update_global函数中，而从公共网络把参数拉回到本线程神经网络的逻辑在pull_global中。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">update_a_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_c_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>  <span class="c1"># local grads applies to global net</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">pull_global</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_a_params_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_c_params_op</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>详细的内容大家可以对照代码和算法流程一起看。在主函数里我新加了一个测试模型效果的过程，大家可以试试看看最后的模型效果如何。</p>
<h1 id="5-a3c小结">5. A3C小结</h1>
<p>A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，提供了一种通用的异步的并发的强化学习框架，也就是说，这个并发框架不光可以用于A3C，还可以用于其他的强化学习算法。这是A3C最大的贡献。目前，已经有基于GPU的A3C框架，这样A3C的框架训练速度就更快了。</p>
<p>除了A3C, DDPG算法也可以改善Actor-Critic难收敛的问题。它使用了Nature DQN，DDQN类似的思想，用两个Actor网络，两个Critic网络，一共4个神经网络来迭代更新模型参数。在下一篇我们讨论DDPG算法。</p>
]]></description></item><item><title>强化学习笔记 [14] | Actor-Critic</title><link>https://jianye0428.github.io/posts/rl_learning_note_14/</link><pubDate>Sun, 25 Feb 2024 15:35:58 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_14/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。</p>
<p>在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。</p>
<p>本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。</p>
<h1 id="1-actor-critic算法简介">1. Actor-Critic算法简介</h1>
<p>Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。</p>
<p>回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。</p>
<p>也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：</p>
<p>$$
\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)
$$</p>
<p>第二组是价值函数的近似，对于状态价值和动作价值函数分别是：</p>
<p>$$
\hat{v}(s,w)\approx v_\pi(s)
$$</p>
<p>$$
\hat{q}(s,a,w)\approx q_\pi(s,a)
$$</p>
<p>对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：</p>
<p>$$
\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t
$$</p>
<p>梯度更新部分中，$\nabla_\theta log\pi_\theta(s_t,a_t)$是我们的分值函数，不用动，要变成Actor的话改动的是$v_t$，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。</p>
<p>而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。</p>
<p>现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值$v_t$,而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$,在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{te}$</p>
<h1 id="2-actor-critic算法可选形式">2. Actor-Critic算法可选形式</h1>
<p>在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值 $v_t$实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：</p>
<ul>
<li>
<p>a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)V(s,w)$$</li>
</ul>
</li>
<li>
<p>b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)Q(s,a,w)$$</li>
</ul>
</li>
<li>
<p>c) 基于TD误差：在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了TD误差，它的表达式是 $\delta(t)=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 或者 $\delta(t)=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t)$, 这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)$$</li>
</ul>
</li>
<li>
<p>d) 基于优势函数：在<a href="https://www.cnblogs.com/pinard/p/9923859.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十二) Dueling DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到过优势函数A的定义：$A(S,A,w,\beta)=Q(S,A,w,\alpha,\beta)-V(S,w,\alpha)$, 即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,A,w,\beta)$$</li>
</ul>
</li>
<li>
<p>e) 基于 $TD(λ)$ 误差：一般都是基于后向 $TD(λ)$误差, 在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)E(t)$</li>
</ul>
</li>
</ul>
<p>对于Critic本身的模型参数 $w$ ，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如 $Q(s,a,w)=ϕ(s,a)^Tw$,则Critic本身的模型参数 $w$的更新公式可以表示为：</p>
<p>$$\begin{gathered}
\delta=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t) \\
w=w+\beta\delta\phi(s,a)
\end{gathered}$$</p>
<p>通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。</p>
<h1 id="3-actor-critic算法流程">3. Actor-Critic算法流程</h1>
<p>这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数　　</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，衰减因子 $γ$, 探索率 $ϵ$, Critic网络结构和Actor网络结构。</p>
<p>输出：Actor 网络参数 $θ$, Critic网络参数 $w$</p>
<ul>
<li>(1). 随机初始化所有的状态和动作对应的价值Q�. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor网络中使用 $ϕ(S)$ 作为输入，输出动作 $A$,基于动作 $A$得到新的状态 $S&rsquo;$,反馈 $R$。</li>
<li>c) 在Critic网络中分别使用 $ϕ(S)$，$ϕ(S&rsquo;)$ 作为输入，得到Q值输出 $V(S)$，$V(S&rsquo;)$</li>
<li>d) 计算TD误差 $\delta=R+\gamma V(S^{\prime})-V(S)$</li>
<li>e) 使用均方差损失函数 $\sum(R+\gamma V(S^{\prime})-V(S,w))^2$ 作Critic网络参数 $w$的梯度更新</li>
<li>f) 更新Actor网络参数 $θ$:
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(S_t,A)\delta $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对于Actor的分值函数 $∇_θlogπ_θ(S_t,A)$,可以选择softmax或者高斯分值函数。</p>
<p>上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。</p>
<h1 id="4-actor-critic算法实例">4. Actor-Critic算法实例</h1>
<p>下面我们用一个具体的例子来演示上面的Actor-Critic算法。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法流程可以参考上面的第三节，这里的分值函数我们使用的是softmax函数，和上一片的类似。完整的代码参见Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py</p>
<p>代码主要分为两部分，第一部分是Actor，第二部分是Critic。对于Actor部分，大家可以和上一篇策略梯度的代码对比，改动并不大，主要区别在于梯度更新部分，策略梯度使用是蒙特卡罗法计算出的价值 $v(t)$,则我们的actor使用的是TD误差。</p>
<p>在策略梯度部分，对应的位置如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而我们的Actor对应的位置的代码是：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">td_error</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处要注意的是，由于使用的是TD误差，而不是价值 $v(t)$,此处需要最大化<code>self.exp</code>,而不是最小化它，这点和策略梯度不同。对应的Actor代码为：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除此之外，Actor部分的代码和策略梯度的代码区别并不大。</p>
<p>对于Critic部分，我们使用了类似于DQN的三层神经网络。不过我们简化了这个网络的输出，只有一维输出值，而不是之前DQN使用的有多少个可选动作，就有多少维输出值。网络结构如下:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">],</span> <span class="s2">&#34;state&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layerq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layerq</span><span class="p">,</span> <span class="n">W2q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2q</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和之前的DQN相比，这里还有一个区别就是我们的critic没有使用DQN的经验回放，只是使用了反馈和当前网络在下一个状态的输出来拟合当前状态。</p>
<p>对于算法中Actor和Critic交互的逻辑，在main函数中：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># e-greedy action for train</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">critic</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>  <span class="c1"># gradient = grad[r + gamma * V(s_) - V(s)]</span>
</span></span><span class="line"><span class="cl">  <span class="n">actor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">td_error</span><span class="p">)</span>  <span class="c1"># true_gradient = grad[logPi(s,a) * td_error]</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大家对照第三节的算法流程和代码应该可以比较容易理清这个过程。但是这个程序很难收敛。因此大家跑了后发现分数总是很低的话是可以理解的。我们需要优化这个问题。</p>
<h1 id="5-actor-critic算法小结">5. Actor-Critic算法小结</h1>
<p>基本版的Actor-Critic算法虽然思路很好，但是由于难收敛的原因，还需要做改进。</p>
<p>目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法我们在从DQN到Nature DQN的过程中已经用过一次了。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p>
<p>在后面的文章中，我们会继续讨论DDPG和A3C。</p>
<p>　</p>
]]></description></item><item><title>强化学习笔记 [13] | 策略梯度(Policy Gradient)</title><link>https://jianye0428.github.io/posts/rl_learning_note_13/</link><pubDate>Sun, 25 Feb 2024 15:35:55 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_13/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradient)，它是Policy Based强化学习方法，基于策略来学习。</p>
<p>本文参考了Sutton的强化学习书第13章和策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-value-based强化学习方法的不足">1. Value Based强化学习方法的不足</h1>
<p>DQN系列强化学习算法主要的 <strong><font color=red>问题</font></strong> 主要有三点。</p>
<ul>
<li>
<p>第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。</p>
</li>
<li>
<p>第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。</p>
</li>
<li>
<p>第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。</p>
</li>
</ul>
<p>由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述类别问题的方法，比如Policy Based强化学习方法。</p>
<h1 id="2-policy-based强化学习方法引入">2. Policy Based强化学习方法引入</h1>
<p>回想我们在Value Based强化学习方法里，我们对价值函数进行了近似表示，引入了一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到近似的动作价值，即：</p>
<p>$$\hat{q}\left(s,a,w\right)\approx q_\pi(s,a)$$</p>
<p>在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略 $π$可以被被描述为一个包含参数 $θ$ 的函数,即：</p>
<p>$$\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)$$</p>
<p>将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？</p>
<h1 id="3-策略梯度的优化目标">3. 策略梯度的优化目标</h1>
<p>我们要用梯度上升来寻找最优的梯度，首先就要找到一个可以优化的函数目标。</p>
<p>最简单的优化目标就是初始状态收获的期望，即优化目标为：</p>
<p>$$J_1(\theta)=V_{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}(G_1)$$</p>
<p>但是有的问题是没有明确的初始状态的，那么我们的优化目标可以定义平均价值，即：
$$J_{avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$$</p>
<p>其中，$d_πθ(s)$ 是基于策略 $π_θ$生成的马尔科夫链关于状态的静态分布。</p>
<p>或者定义为每一时间步的平均奖励，即：</p>
<p>$$J_{avR}(\theta)==\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$$</p>
<p>无论我们是采用 $J_1$, $J_{av}V$, 还是 $J_{av}R$ 来表示优化目标，最终对 $θ$求导的梯度都可以表示为：</p>
<p>$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>具体的证明过程这里就不再列了，如果大家感兴趣，可以去看策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的附录1，里面有详细的证明。</p>
<p>当然我们还可以采用很多其他可能的优化目标来做梯度上升，此时我们的梯度式子里面的 $\nabla_\theta log\pi_\theta(s,a)$ 部分并不改变，变化的只是后面的 $Q_\pi(s,a)$ 部分。对于 $\nabla_\theta log\pi_\theta(s,a)$,我们一般称为<strong>分值函数</strong>(score function)。</p>
<p>现在梯度的式子已经有了，后面剩下的就是策略函数 $\pi_\theta(s,a)$的设计了。</p>
<h1 id="4-策略函数的设计">4. 策略函数的设计</h1>
<p>现在我们回头看一下策略函数 $\pi_\theta(s,a)$ 的设计，在前面它一直是一个数学符号。</p>
<p>最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征 $ϕ(s,a)$ 与参数 $θ$的线性组合来权衡一个行为发生的几率,即:</p>
<p>$$\pi_\theta(s,a)=\frac{e^{\phi(s,a)^T\theta}}{\sum_be^{\phi(s,b)^T\theta}}$$</p>
<p>则通过求导很容易求出对应的分值函数为：</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)=\phi(s,a)-\mathbb{E}_{\pi_\theta}[\phi(s,.)]$$</p>
<p>另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布 $\mathbb{N}(\phi(\mathrm{s})^{\mathbb{T}}\theta,\sigma^2)$中产生。高斯策略对应的分值函数求导可以得到为:</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)==\frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}$$</p>
<p>有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。</p>
<h1 id="5-蒙特卡罗策略梯度reinforce算法">5. 蒙特卡罗策略梯度reinforce算法</h1>
<p>这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数 $v(s)$ 来近似代替策略梯度公式里面的 $Q_π(s,a)$。算法的流程很简单，如下所示:</p>
<ul>
<li>输入：N个蒙特卡罗完整序列,训练步长 $α$</li>
<li>输出：策略函数的参数 $θ$
<ul>
<li>(1). for 每个蒙特卡罗序列:
<ul>
<li>a. 用蒙特卡罗法计算序列每个时间位置t的状态价值 $v_t$</li>
<li>b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数 $θ$：
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t$$</li>
</ul>
</li>
</ul>
</li>
<li>(2).返回策略函数的参数 $θ$</li>
</ul>
</li>
</ul>
<p>　　这里的策略函数可以是softmax策略，高斯策略或者其他策略。</p>
<h1 id="6-策略梯度实例">6. 策略梯度实例</h1>
<p>这里给出第5节的蒙特卡罗策略梯度reinforce算法的一个实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py</p>
<p>这里我们采用softmax策略作为我们的策略函数，同时，softmax的前置部分，也就是我们的策略模型用一个三层的softmax神经网络来表示。这样好处就是梯度的更新可以交给神经网络来做。</p>
<p>我们的softmax神经网络的结构如下，注意这个网络不是价值Q网络，而是策略网络：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_softmax_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_num&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_value&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># softmax layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#softmax output</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">all_act_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;act_prob&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意我们的损失函数是softmax交叉熵损失函数和状态价值函数的乘积，这样TensorFlow后面可以自动帮我们做梯度的迭代优化。</p>
<p>另一个要注意的点就是蒙特卡罗法里面价值函数的计算，一般是从后向前算，这样前面的价值的计算可以利用后面的价值作为中间结果，简化计算，对应代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">      <span class="n">running_add</span> <span class="o">=</span> <span class="n">running_add</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="n">discounted_ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分和之前的DQN的代码类似。</p>
<h1 id="7-策略梯度小结">7. 策略梯度小结</h1>
<p>策略梯度提供了和DQN之类的方法不同的新思路，但是我们上面的蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，我们需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，我们的参数更新的方向很可能不是策略梯度的最优方向。</p>
<p>因此，Policy Based的强化学习方法还需要改进，注意到我们之前有Value Based强化学习方法，那么两者能不能结合起来一起使用呢？下一篇我们讨论Policy Based+Value Based结合的策略梯度方法Actor-Critic。</p>
<p>　　　　</p>
]]></description></item><item><title>强化学习笔记 [12] | Dueling DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_12/</link><pubDate>Sun, 25 Feb 2024 11:16:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_12/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9797695.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十一) Prioritized Replay DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。</p>
<h1 id="1-dueling-dqn的优化点考虑">1. Dueling DQN的优化点考虑</h1>
<p>在前面讲到的DDQN中，我们通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，我们通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，我们尝试通过<font color=red>优化神经网络的结构</font>来优化算法。</p>
<p>具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态 $S$有关，与具体要采用的动作 $A$无关，这部分我们叫做<strong>价值函数部分</strong>，记做 $V(S,w,α)$,第二部分同时与状态状态 $S$ 和动作 $A$有关，这部分叫做**优势函数(Advantage Function)**部分,记为 $A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+A(S,A,w,\beta)$$</p>
<p>其中，$w$ 是公共部分的网络参数，而 $α$ 是价值函数独有部分的网络参数，而 $β$ 是优势函数独有部分的网络参数。</p>
<h1 id="2-dueling-dqn网络结构">2. Dueling DQN网络结构</h1>
<p>由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。为了简化算法描述，这里不使用原论文的CNN网络结构，而是使用前面文中用到的最简单的三层神经网络来描述。是否使用CNN对Dueling DQN算法本身无影响。</p>
<p>在前面讲到的DDQN等DQN算法中，我使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络与Dueling DQN</div>
</center>
<br>
<p>而在Dueling DQN中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。</p>
<p>我们可以直接使用上一节的价值函数的组合公式得到我们的动作价值，但是这个式子无法辨识最终输出里面 $V(S,w,α)$ 和 $A(S,A,w,β)$各自的作用，为了可以体现这种可辨识性(identifiability),实际使用的组合公式如下：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+(A(S,A,w,\beta)-\frac1{\mathcal{A}}\sum_{a^{\prime}\in\mathcal{A}}A(S,a^{\prime},w,\beta))$$</p>
<p>其实就是对优势函数部分做了中心化的处理。以上就是Dueling DQN的主要算法思路。由于它仅仅涉及神经网络的中间结构的改进，现有的DQN算法可以在使用Duel DQN网络结构的基础上继续使用现有的算法。由于算法主流程和其他算法没有差异，这里就不单独讲Duel DQN的算法流程了。</p>
<h1 id="3-dueling-dqn实例">3. Dueling DQN实例</h1>
<p>下面我们用一个具体的例子来演示Dueling DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>这个实例代基于Nature DQN，并将网络结构改为上图中右边的Dueling DQN网络结构，完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Dueling DQN和Nature DQN的代码的不同之处。也就是网络结构定义部分，主要的代码如下，一共有两个相同结构的Q网络，每个Q网络都有状态函数和优势函数的定义，以及组合后的Q网络输出，如代码红色部分：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W21</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b21</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W21</span><span class="p">)</span> <span class="o">+</span> <span class="n">b21</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W22</span><span class="p">)</span> <span class="o">+</span> <span class="n">b22</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2v</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2a</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分代码和Nature DQN基本相同。当然，我们可以也在前面DDQN，Prioritized Replay DQN代码的基础上，把网络结构改成上面的定义，这样Dueling DQN也可以起作用。</p>
<h1 id="4-dqn总结">4. DQN总结</h1>
<p>DQN系列我花了5篇来讲解，一共5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。</p>
<p>当然DQN家族的算法远远不止这些，还有一些其他的DQN算法我没有详细介绍，比如使用一些较复杂的CNN和RNN网络来提高DQN的表达能力，又比如改进探索状态空间的方法等，主要是在DQN的基础上持续优化。</p>
<p>DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。虽然NAF DQN可以解决这个问题，但是方法过于复杂了。而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题，从下一篇我们开始讨论Policy-Based深度强化学习。</p>
]]></description></item><item><title>强化学习笔记 [11] | Prioritized Replay DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_11/</link><pubDate>Sun, 25 Feb 2024 11:16:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_11/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay DQN。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Prioritized Replay DQN的论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<h1 id="1-prioritized-replay-dqn之前算法的问题">1. Prioritized Replay DQN之前算法的问题</h1>
<p>在Prioritized Replay DQN之前，我们已经讨论了很多种DQN，比如Nature DQN， DDQN等，他们都是通过经验回放来采样，进而做目标Q值的计算的。在采样的时候，我们是一视同仁，在经验回放池里面的所有的样本都有相同的被采样到的概率。</p>
<p>但是注意到在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，由于TD误差小，对反向梯度的计算影响不大。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。</p>
<p>这样如果TD误差的绝对值 $|δ(t)|$较大的样本更容易被采样，则我们的算法会比较容易收敛。下面我们看看Prioritized Replay DQN的算法思路。</p>
<h1 id="2-prioritized-replay-dqn算法的建模">2. Prioritized Replay DQN算法的建模</h1>
<p>Prioritized Replay DQN根据每个样本的TD误差绝对值 $|δ(t)|$，给定该样本的优先级正比于 $|δ(t)|$，将这个优先级的值存入经验回放池。回忆下之前的DQN算法，我们仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。</p>
<p>由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。</p>
<p>具体的SumTree树结构如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">sum_tree 结构图</div>
</center>
<br>
<p>所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。</p>
<p>这样保存有什么好处呢？主要是方便采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样，采样到哪个区间，就是哪个样本。比如我们采样到了26， 在（25-29）这个区间，那么就是第四个叶子节点被采样到。而注意到第三个叶子节点优先级最高，是12，它的区间13-25也是最长的，会比其他节点更容易被采样到。</p>
<p>如果要采样两个样本，我们可以在[0,21],[21,42]两个区间做均匀采样，方法和上面采样一个样本类似。</p>
<p>类似的采样算法思想我们在<a href="https://www.cnblogs.com/pinard/p/7249903.html"target="_blank" rel="external nofollow noopener noreferrer">word2vec原理(三) 基于Negative Sampling的模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第四节中也有讲到。</p>
<p>除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：</p>
<p>$$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>现在我们新的考虑了样本优先级的损失函数是</p>
<p>$$\frac1m\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>其中 $w_j$是第j个样本的优先级权重，由TD误差 $|δ(t)|$归一化得到。</p>
<p>第三个要注意的点就是当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。</p>
<p>除了以上三个部分，Prioritized Replay DQN和DDQN的算法流程相同。</p>
<h1 id="3-prioritized-replay-dqn算法流程">3. Prioritized Replay DQN算法流程</h1>
<p>下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，采样权重系数 $β$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$, SumTree的叶子节点数 $S$。</li>
<li>输出：Q网络参数。</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级 $p_j$为1。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S&rsquo;),is_end}$这个五元组存入SumTree</li>
<li>e) $S=S'$</li>
<li>f) 从SumTree中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，每个样本被采样的概率基于 $P(j)=\frac{p_j}{\sum_i(p_i)}$，损失函数权重 $w_j=(N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left\\{\begin{matrix}R_j&amp;is_end_j\textit{is true}\\\\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{is false}\end{matrix}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\begin{aligned}\frac{1}{m}\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2\end{aligned}$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 重新计算所有样本的TD误差 $\delta_j=y_j-Q(\phi(S_j),A_j,w)$，更新SumTree中所有节点的优先级 $p_j=|\delta_j|$</li>
<li>i) 如果i%C=1,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>j) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-prioritized-replay-ddqn算法流程">4. Prioritized Replay DDQN算法流程</h1>
<p>下面我们给出Prioritized Replay DDQN算法的实例代码。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 代码中的SumTree的结构和经验回放池的结构参考了morvanzhou的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py"target="_blank" rel="external nofollow noopener noreferrer">github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>这里重点讲下和第三节中算法描述不同的地方，主要是 $w_j$的计算。注意到：</p>
<p>$$w_j=\frac{(N<em>P(j))^{-\beta}}{\max_i(w_i)}=\frac{(N</em>P(j))^{-\beta}}{\max_i((N*P(i))^{-\beta})}=\frac{(P(j))^{-\beta}}{\max_i((P(i))^{-\beta})}=(\frac{P_j}{\min_iP(i)})^{-\beta}$$</p>
<p>因此代码里面$w_j$，即ISWeights的计算代码是这样的：</p>
<p><a href="javascript:void%280%29;"></a></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">pri_seg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span> <span class="o">/</span> <span class="n">n</span>       <span class="c1"># priority segment</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_increment_per_sampling</span><span class="p">])</span>  <span class="c1"># max = 1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">min_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">capacity</span><span class="p">:])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>     <span class="c1"># for later calculate ISweight</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">min_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_prob</span> <span class="o">=</span> <span class="mf">0.00001</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">get_leaf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prob</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>
</span></span><span class="line"><span class="cl">    <span class="n">ISWeights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prob</span><span class="o">/</span><span class="n">min_prob</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">b_memory</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上述代码的采样在第二节已经讲到。根据树的优先级的和total_p和采样数n，将要采样的区间划分为n段，每段来进行均匀采样，根据采样到的值落到的区间，决定被采样到的叶子节点。当我们拿到第i段的均匀采样值v以后，就可以去SumTree中找对应的叶子节点拿样本数据，样本叶子节点序号以及样本优先级了。代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree structure and array storage:
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree index:
</span></span></span><span class="line"><span class="cl"><span class="s2">        0         -&gt; storing priority sum
</span></span></span><span class="line"><span class="cl"><span class="s2">      / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">    1     2
</span></span></span><span class="line"><span class="cl"><span class="s2">    / \   / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">  3   4 5   6    -&gt; storing priority for transitions
</span></span></span><span class="line"><span class="cl"><span class="s2">  Array type for storing:
</span></span></span><span class="line"><span class="cl"><span class="s2">  [0,1,2,3,4,5,6]
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">parent_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>     <span class="c1"># the while loop is faster than the method in the reference code</span>
</span></span><span class="line"><span class="cl">    <span class="n">cl_idx</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">parent_idx</span> <span class="o">+</span> <span class="mi">1</span>         <span class="c1"># this leaf&#39;s left and right kids</span>
</span></span><span class="line"><span class="cl">    <span class="n">cr_idx</span> <span class="o">=</span> <span class="n">cl_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cl_idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">):</span>        <span class="c1"># reach bottom, end search</span>
</span></span><span class="line"><span class="cl">      <span class="n">leaf_idx</span> <span class="o">=</span> <span class="n">parent_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>       <span class="c1"># downward search, always search for a higher priority node</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cl_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cr_idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">data_idx</span> <span class="o">=</span> <span class="n">leaf_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">leaf_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">leaf_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了采样部分，要注意的就是当梯度更新完毕后，我们要去更新SumTree的权重，代码如下，注意叶子节点的权重更新后，要向上回溯，更新所有祖先节点的权重。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">batch_update</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">)</span>  <span class="c1"># update priority</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">batch_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">abs_errors</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>  <span class="c1"># convert to abs and avoid 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">clipped_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">abs_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs_err_upper</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">clipped_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">ps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ti</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">change</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># then propagate the change through tree</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">tree_idx</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>    <span class="c1"># this method is faster than the recursive loop in the reference code</span>
</span></span><span class="line"><span class="cl">      <span class="n">tree_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">change</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，和DDQN比，TensorFlow的网络结构流程中多了一个TD误差的计算节点，以及损失函数多了一个ISWeights系数。此外，区别不大。</p>
<h1 id="5-prioritized-replay-dqn小结">5. Prioritized Replay DQN小结</h1>
<p>Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。</p>
<p>下一篇我们讨论DQN家族的另一个优化算法Duel DQN，它将价值Q分解为两部分，第一部分是仅仅受状态但不受动作影响的部分，第二部分才是同时受状态和动作影响的部分，算法的效果也很好。</p>
]]></description></item><item><title>强化学习笔记 [10] | Double DQN (DDQN)</title><link>https://jianye0428.github.io/posts/rl_learning_note_10/</link><pubDate>Fri, 23 Feb 2024 13:17:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_10/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9756075.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（九）Deep Q-Learning进阶之Nature DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称DDQN）。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和DDQN的论文(Deep Reinforcement Learning with Double Q-learning)。</p>
<h1 id="1-dqn的目标q值计算问题">1. DQN的目标Q值计算问题</h1>
<p>在DDQN之前，基本上所有的目标Q值都是通过<strong>贪婪法</strong>直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算如下式:</p>
<p>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</p>
<h1 id="2-ddqn的算法建模">2. DDQN的算法建模</h1>
<p>DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。</p>
<p>在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：</p>
<p>$$y_j=R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})$$</p>
<p>在DDQN(Double DQN)这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即:</p>
<p>$$a^{max}(S_j^{\prime},w)=\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w)$$</p>
<p>然后利用这个选择出来的动作 $\begin{aligned}&amp;a^{max}(S_j^{\prime},w)\end{aligned}$ 在目标网络里面去计算目标Q值。即：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),a^{max}(S_j^{\prime},w),w^{\prime})$$</p>
<p>综合起来写就是：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})$$</p>
<p>除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。</p>
<h1 id="3-ddqn算法流程">3. DDQN算法流程</h1>
<p>这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本 $m$,目标Q网络参数更新频 $C$。</li>
<li>输出：Q网络参数</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;的参数 $w′=w$ 。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S′),is_end} $,这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$, 计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数w�</li>
<li>h) 如果 $i%C=1$,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-ddqn算法实例">4. DDQN算法实例　</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注DDQN和上一节的Nature DQN的代码的不同之处。代码只有一个地方不一样，就是计算目标Q值的时候，如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">current_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_action_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">current_Q_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">target_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">target_Q_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">max_action_next</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">target_Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而之前的Nature DQN这里的目标Q值计算是如下这样的：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，两个算法的代码完全相同。</p>
<h1 id="5-ddqn小结">5. DDQN小结</h1>
<p>DDQN算法出来以后，取得了比较好的效果，因此得到了比较广泛的应用。不过我们的DQN仍然有其他可以优化的点，如上一篇最后讲到的: 随机采样的方法好吗？按道理经验回放里不同样本的重要性是不一样的，TD误差大的样本重要程度应该高。针对这个问题，我们在下一节的Prioritised Replay DQN中讨论。</p>
]]></description></item><item><title>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_9/</link><pubDate>Fri, 23 Feb 2024 13:17:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_9/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9714655.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（八）价值函数的近似表示与Deep Q-Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Nature DQN的论文。</p>
<h1 id="1-dqnnips-2013的问题">1. DQN(NIPS 2013)的问题</h1>
<p>在上一篇我们已经讨论了DQN(NIPS 2013)的算法原理和代码实现，虽然它可以训练像CartPole这样的简单游戏，但是有很多问题。这里我们先讨论第一个问题。</p>
<p>注意到DQN(NIPS 2013)里面，我们使用的目标 $Q$值的计算方式：</p>
<p>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>这里目标Q值的计算使用到了当前要训练的Q网络参数来计算$Q(\phi(S_j^{\prime}),A_j^{\prime},w)$，而实际上，我们又希望通过 $y_j$来后续更新 $Q$网络参数。这样两者循环依赖，迭代起来两者的相关性就太强了。不利于算法的收敛。</p>
<p>因此，一个改进版的DQN: Nature DQN尝试<strong>用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系</strong>。下面我们来看看Nature DQN是怎么做的。</p>
<h1 id="2-nature-dqn的建模">2. Nature DQN的建模</h1>
<p>Nature DQN的两个Q网络分别命名为当前Q网络和目标Q网络。</p>
<p>Nature DQN使用了两个Q网络，一个<strong>当前Q网络</strong>$Q$用来选择动作，更新模型参数，另一个<strong>目标Q网络</strong> $Q&rsquo;$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。</p>
<p>要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。</p>
<p>Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。</p>
<h1 id="3-nature-dqn的算法流程">3. Nature DQN的算法流程</h1>
<p>下面我们来总结下Nature DQN的算法流程， 基于DQN NIPS 2015：</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率$C$。</p>
<p>输出：$Q$网络参数</p>
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S′),is_end\\}$这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，计算当前目标Q值 $y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果 $i%C=1$, 则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$ 需要随着迭代的进行而变小。</p>
<h1 id="4-nature-dqn算法实例">4. Nature DQN算法实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Nature DQN和上一节的NIPS 2013 DQN的代码的不同之处。</p>
<p>首先是Q网络，上一篇的DQN是一个三层的神经网络，而这里我们有两个一样的三层神经网络，一个是当前Q网络，一个是目标Q网络，网络的定义部分如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2t</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对于定期将目标Q网络的参数更新的代码如下面两部分：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">t_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;target_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">e_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;current_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;soft_replacement&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">t_params</span><span class="p">,</span> <span class="n">e_params</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_target_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># update target Q netowrk</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">REPLACE_TARGET_FREQ</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#print(&#39;episode &#39;+str(episode) +&#39;, target Q network params replaced!&#39;)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此外，注意下我们计算目标Q值的部分，这里使用的目标Q网络的参数，而不是当前Q网络的参数：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分基本和上一篇DQN的代码相同。这里给出我跑的某一次的结果:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 9.6
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.0
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 34.8
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 177.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 198.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 193.2
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>注意，由于DQN不保证稳定的收敛，所以每次跑的结果会不同，如果你跑的结果后面仍然收敛的不好，可以把代码多跑几次，选择一个最好的训练结果。</p>
<h1 id="5-nature-dqn总结">5. Nature DQN总结</h1>
<p>Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：</p>
<ul>
<li>1） 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？</li>
<li>2） 随机采样的方法好吗？按道理不同样本的重要性是不一样的。</li>
<li>3） Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？</li>
</ul>
<p>第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN，这三个DQN的改进版我们在下一篇来讨论。</p>
]]></description></item><item><title>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_8/</link><pubDate>Fri, 23 Feb 2024 13:17:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_8/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在强化学习系列的<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">前七篇<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。</p>
<p>Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。</p>
<h1 id="1-为何需要价值函数的近似表示">1. 为何需要价值函数的近似表示</h1>
<p>在之前讲到了强化学习求解方法，无论是动态规划DP，蒙特卡罗方法MC，还是时序差分TD，使用的状态都是离散的有限个状态集合 $S$。此时问题的规模比较小，比较容易求解。但是假如我们遇到复杂的状态集合呢？甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。　　　　</p>
<p>比如经典的冰球世界(PuckWorld)强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间步时长的一个大小固定的力，借此来改变大圆的速度。环境会在每一个时间步内告诉个体当前的水平与垂直坐标、当前的速度在水平和垂直方向上的分量以及目标的水平和垂直坐标共6项数据，奖励值为个体与目标两者中心距离的负数，也就是距离越大奖励值越低且最高奖励值为0。</p>
<p>在这个问题中，状态是一个6维的向量，并且是连续值。没法直接用之前离散集合的方法来描述状态。当然，你可以说，我们可以把连续特征离散化。比如把这个冰球场100x100的框按1x1的格子划分成10000个格子，那么对于运动员的坐标和冰球的坐标就有$10^4∗10^4=10^8$次种，如果再加上个体速度的分量就更是天文数字了，此时之前讲过的强化学习方法都会因为问题的规模太大而无法使用。怎么办呢？必须要对问题的建模做修改了，而价值函数的近似表示就是一个可行的方法。</p>
<h1 id="2-价值函数的近似表示方法">2. 价值函数的近似表示方法</h1>
<p>由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数 $\hat{v}$, 这个函数由参数 $w$ 描述，并接受状态 $s$ 作为输入，计算后得到状态 $s$ 的价值，即我们期望：</p>
<p>$$\hat{v}(s,w)\approx v_\pi(s)$$</p>
<p>类似的，引入一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到动作价值，即我们期望：</p>
<p>$$\hat{q}(s,a,w)\approx q_\pi(s,a)$$</p>
<p>价值函数近似的方法很多，比如最简单的线性表示法，用 $ϕ(s)$表示状态 $s$ 的特征向量，则此时我们的状态价值函数可以近似表示为：</p>
<p>$$\hat{v}(s,w)=\phi(s)^Tw$$</p>
<p>当然，除了线性表示法，我们还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。因此后面我们的近似表达方法如果没有特别提到，都是指的神经网络的近似表示。</p>
<p>对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络拟合价值函数</div>
</center>
<br>
<p>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值 $\hat{v}(s,w)$。对于动作价值函数，有两种方法，一种是输入状态 $s$ 的特征向量和动作 $a$，输出对应的动作价值 $\hat{q}(s,a,w)$，另一种是只输入状态 $s$ 的特征向量，动作集合有多少个动作就有多少个输出 $\hat{q}(s,ai,w)$。这里隐含了我们的动作是有限个的离散动作。</p>
<p>对于我们前一篇讲到的Q-Learning算法，我们现在就价值函数的近似表示来将其改造，采用上面右边的第三幅图的动作价值函数建模思路来做，现在我们叫它Deep Q-Learning。</p>
<h1 id="3-deep-q-learning算法思路">3. Deep Q-Learning算法思路</h1>
<p>Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值s和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p>
<p>DQN的输入是我们的状态s对应的状态向量 $ϕ(s)$， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p>
<p>DQN主要使用的技巧是经验回放(experience replay), 即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是<strong>经验回放</strong>。</p>
<p>通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数 $w$，当 $w$ 收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。</p>
<p>下面我们总结下DQN的算法流程，基于NIPS 2013 DQN。　　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, Q网络结构, 批量梯度下降的样本数 $m$。</li>
<li>输出：Q网络参数
<ul>
<li>
<ol>
<li>随机初始化$Q$网络的所有参数 $w$，基于 $w$初始化所有的状态和动作对应的价值 $Q$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S&rsquo;),is_end\\}$这个五元组存入经验回放集合D</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\mathrm{~}is\mathrm{~}true\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\mathrm{~}is\mathrm{~}false\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{i=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的 $f$步和 $g$步的 $Q$值计算也都需要通过 $Q$网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-deep-q-learning实例">4. Deep Q-Learning实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。这里使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码参考了知乎上的一个<a href="https://zhuanlan.zhihu.com/p/21477488"target="_blank" rel="external nofollow noopener noreferrer">DQN实例<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，修改了代码中的一些错误，并用最新的Python3.6+Tensorflow1.8.0运行。要跑代码需要安装OpenAI的Gym库，使用<code>pip install gym</code>即可。</p>
<p>代码使用了一个三层的神经网络，输入层，一个隐藏层和一个输出层。下面我们看看关键部分的代码。</p>
<p>算法第2步的步骤b通过$ϵ−$贪婪法选择动作的代码如下，注意每次我们$ϵ−$贪婪法后都会减小$ϵ$值。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">egreedy_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:[</span><span class="n">state</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">})[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤c在状态S�执行当前动作A�的代码如下，这个交互是由Gym完成的。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Define reward for agent</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mf">0.1</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤d保存经验回放数据的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">perceive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span><span class="n">one_hot_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">REPLAY_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤f,g计算目标Q值，并更新Q网络的代码如下：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 1: obtain random minibatch from replay memory</span>
</span></span><span class="line"><span class="cl">  <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">y_input</span><span class="p">:</span><span class="n">y_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">action_input</span><span class="p">:</span><span class="n">action_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">state_batch</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们在每100轮迭代完后会去玩10次交互测试，计算10次的平均奖励。运行了代码后，我的3000轮迭代的输出如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 12.2
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.4
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 10.4
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.5
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 11.6
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 12.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 29.6
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 48.1
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 85.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 169.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>大概到第1000次迭代后，算法已经收敛，达到最高的200分。当然由于是$ϵ−$探索，每次前面的输出可能不同，但最后应该都可以收敛到200的分数。当然由于DQN不保证绝对的收敛，所以可能到了200分后还会有抖动。</p>
<h1 id="5-deep-q-learning小结">5. Deep Q-Learning小结　　　　</h1>
<p>DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。但是DQN有个问题，就是它并不一定能保证Q网络的收敛，也就是说，我们不一定可以得到收敛后的Q网络参数。这会导致我们训练出的模型效果很差。</p>
<p>针对这个问题，衍生出了DQN的很多变种，比如Nature DQN(NIPS 2015), Double DQN，Dueling DQN等。这些我们在下一篇讨论。</p>
]]></description></item><item><title>强化学习笔记 [7] | 时序差分离线控制算法Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_7/</link><pubDate>Fri, 23 Feb 2024 13:17:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_7/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。</p>
<p>Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-q-learning算法的引入">1. Q-Learning算法的引入　　　　</h1>
<p>Q-Learning算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解<u>不需要环境的状态转化模型</u>，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新策略，通过策略来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，比如我们上一篇讲到的SARSA, 而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。这一类的经典算法就是Q-Learning。</p>
<p>对于Q-Learning，我们会使用 $ϵ−$贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的 $ϵ−$贪婪法。这一点就是SARSA和Q-Learning本质的区别。</p>
<h1 id="2-q-learning算法概述">2. Q-Learning算法概述</h1>
<p>Q-Learning算法的拓扑图如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Q Learning 拓扑图</div>
</center>
<br>
<p>首先我们基于状态 $S$，用 $ϵ−$贪婪法选择到动作 $A$, 然后执行动作$A$，得到奖励 $R$，并进入状态 $S&rsquo;$，此时，如果是SARSA，会继续基于状态 $S&rsquo;$，用 $ϵ−$贪婪法选择 $A&rsquo;$,然后来更新价值函数。但是Q-Learning则不同。</p>
<p>对于Q-Learning，它基于状态 $S&rsquo;$，没有使用 $ϵ−$贪婪法选择 $A$，而是使用贪婪法选择 $A&rsquo;$，也就是说，选择使 $Q(S&rsquo;,a)$ 最大的 $a$ 作为 $A&rsquo;$来更新价值函数。用数学公式表示就是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</p>
<p>对应到上图中就是在图下方的三个黑圆圈动作中选择一个使 $Q(S&rsquo;,a)$最大的动作作为 $A&rsquo;$。</p>
<p>此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态 $S&rsquo;$，用 $ϵ−$贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的 $A&rsquo;$ 会作为下一阶段开始时候的执行动作。</p>
<p>下面我们对Q-Learning算法做一个总结。</p>
<h1 id="3-q-learning算法流程">3. Q-Learning算法流程</h1>
<p>下面我们总结下Q-Learning算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出: 所有的状态和动作对应的价值 $Q$
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。</li>
<li>b) 用 $ϵ−$贪婪法在当前状态 $S$ 选择出动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$和奖励 $R$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S'$</li>
<li>f) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-q-learning算法实例windy-gridworld">4. Q-Learning算法实例：Windy GridWorld</h1>
<p>我们还是使用和SARSA一样的例子来研究Q-Learning。如果对windy gridworld的问题还不熟悉，可以复习<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第4节的第二段。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>绝大部分代码和SARSA是类似的。这里我们可以重点比较和SARSA不同的部分。区别都在<code>episode()</code>这个函数里面。</p>
<p>首先是初始化的时候，我们只初始化状态 $S$,把 $A$ 的产生放到了while循环里面, 而回忆下SARSA会同时初始化状态 $S$ 和动作 $A$，再去执行循环。下面这段Q-Learning的代码对应我们算法的第二步步骤a和b：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># play for an episode</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">episode</span><span class="p">(</span><span class="n">q_value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># track the total time steps in this episode</span>
</span></span><span class="line"><span class="cl">  <span class="n">time</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="n">state</span> <span class="o">!=</span> <span class="n">GOAL</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>接着我们会去执行动作 $A$,得到 $S&rsquo;$， 由于奖励不是终止就是-1，不需要单独计算。,这部分和SARSA的代码相同。对应我们Q-Learning算法的第二步步骤c：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>后面我们用贪婪法选择出最大的 $Q(S&rsquo;,a)$,并更新价值函数，最后更新当前状态 $S$。对应我们Q-Learning算法的第二步步骤d,e。注意SARSA这里是使用ϵ−�−贪婪法，而不是贪婪法。同时SARSA会同时更新状态S�和动作A�,而Q-Learning只会更新当前状态S�。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span></span></span></code></pre></td></tr></table>
</div>
</div><p>跑完完整的代码，大家可以很容易得到这个问题的最优解，进而得到在每个格子里的最优贪婪策略。</p>
<h1 id="5-sarsa-vs-q-learning">5. SARSA vs Q-Learning</h1>
<p>现在SARSA和Q-Learning算法我们都讲完了，那么作为时序差分控制算法的两种经典方法吗，他们都有说明特点，各自适用于什么样的场景呢？</p>
<p>Q-Learning直接学习的是 <font color=red>最优策略</font>，而SARSA<font color=red>在学习最优策略的同时还在做探索</font>。这导致我们在学习最优策略的时候，如果用SARSA，为了保证收敛，需要制定一个策略，使 $ϵ−$贪婪法的超参数 $ϵ$在迭代的过程中逐渐变小。Q-Learning没有这个烦恼。</p>
<p>另外一个就是Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以<font color=red>受样本数据的影响较大</font>，因此受到训练数据方差的影响很大，甚至会影响Q函数的收敛。Q-Learning的深度强化学习版Deep Q-Learning也有这个问题。</p>
<p>在学习过程中，SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进，导致出现像Q-Learning可能遇到一些特殊的最优“陷阱”。比如经典的强化学习问题&quot;Cliff Walk&quot;。</p>
<p>在实际应用中，如果我们是在模拟环境中训练强化学习模型，推荐使用Q-Learning，如果是 <strong><font color=red>在线生产环境</font></strong> 中训练模型，则推荐使用 <strong><font color=red>SARSA</font></strong>。</p>
<h1 id="6-q-learning结语">6. Q-Learning结语　　　　　　　　</h1>
<p>对于Q-Learning和SARSA这样的时序差分算法，对于小型的强化学习问题是非常灵活有效的，但是在大数据时代，异常复杂的状态和可选动作，使Q-Learning和SARSA要维护的Q表异常的大，甚至远远超出内存，这限制了时序差分算法的应用场景。在深度学习兴起后，基于深度学习的强化学习开始占主导地位，因此从下一篇开始我们开始讨论深度强化学习的建模思路。</p>
]]></description></item><item><title>RL学习笔记 [5] | 用时序差分法（TD）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_5/</link><pubDate>Thu, 22 Feb 2024 17:25:21 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_5/</guid><description><![CDATA[<h1 id="0-引言">0 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化学习问题的方法：时序差分(Temporal-Difference, TD)。</p>
<p>时序差分这一篇对应Sutton书的第六章部分和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-时序差分td简介">1. 时序差分TD简介</h1>
<p>时序差分法和蒙特卡罗法类似，都是<strong>不基于模型的强化学习问题</strong>求解方法。所以在上一篇定义的不基于模型的强化学习控制问题和预测问题的定义，在这里仍然适用。</p>
<p>预测问题：即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
<p>控制问题：也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
<p>回顾蒙特卡罗法中计算状态收获的方法是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>而对于时序差分法来说，我们没有完整的状态序列，只有部分的状态序列，那么如何可以近似求出某个状态的收获呢？回顾<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中的贝尔曼方程：</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s)$$</p>
<p>这启发我们可以用 $R_{t+1}+\gamma v(S_{t+1})$ 来近似的代替收获 $G_t$,一般我们把 $R_{t+1}+\gamma V(S_{t+1})$ 称为TD目标值。$R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 称为TD误差，将用TD目标值近似代替收获 $G(t)$ 的过程称为引导(bootstrapping)。这样我们只需要两个连续的状态与对应的奖励，就可以尝试求解强化学习问题了。</p>
<p>现在我们有了自己的近似收获 $G_t$ 的表达式，那么就可以去求解时序差分的预测问题和控制问题了。</p>
<h1 id="2-时序差分td的预测问题求解">2. 时序差分TD的预测问题求解</h1>
<p>时序差分的预测问题求解和蒙特卡罗法类似，但是主要有两个不同点。一是收获 $G_t$ 的表达式不同，时序差分 $G(t)$ 的表达式为：</p>
<p>$$G(t)=R_{t+1}+\gamma V(S_{t+1})$$</p>
<p>二是迭代的式子系数稍有不同，回顾蒙特卡罗法的迭代式子是：</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>由于在时序差分我们没有完整的序列，也就没有对应的次数 $N(S_t)$ ,一般就用一个[0,1]的系数 $α$ 代替。这样时序差分的价值函数迭代式子是：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t)) \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t)) $$</p>
<p>这里我们用一个简单的例子来看看蒙特卡罗法和时序差分法求解预测问题的不同。</p>
<p>假设我们的强化学习问题有A,B两个状态，模型未知，不涉及策略和行为。只涉及状态转化和即时奖励。一共有8个完整的状态序列如下：</p>
<p>　　① A,0,B,0 ②B,1 ③B,1 ④ B,1 ⑤ B,1 ⑥B,1 ⑦B,1 ⑧B,0</p>
<p>只有第一个状态序列是有状态转移的，其余7个只有一个状态。设置衰减因子 $γ=1$。</p>
<p>首先我们按蒙特卡罗法来求解预测问题。由于只有第一个序列中包含状态A，因此A的价值仅能通过第一个序列来计算，也就等同于计算该序列中状态A的收获：</p>
<p>$$V(A)=G(A)=R_A+\gamma R_B=0$$</p>
<p>对于B，则需要对其在8个序列中的收获值来平均，其结果是6/8。</p>
<p><strong>再来看看时序差分法求解的过程</strong>。其收获是在计算状态序列中某状态价值时是应用其后续状态的预估价值来计算的，对于B来说，它总是终止状态，没有后续状态，因此它的价值直接用其在8个序列中的收获值来平均，其结果是6/8。</p>
<p>对于A，只在第一个序列出现，它的价值为：</p>
<p>$$V(A)=R_A+\gamma V(B)=\frac68$$</p>
<p>从上面的例子我们也可以看到蒙特卡罗法和时序差分法求解预测问题的区别。</p>
<p>一是时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。</p>
<p>二是时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。</p>
<p>三是虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。</p>
<p>从上面的描述可以看出时序差分法的优势比较大，因此现在主流的强化学习求解方法都是基于时序差分的。后面的文章也会主要基于时序差分法来扩展讨论。</p>
<h1 id="3-n步时序差分">3. n步时序差分</h1>
<p>在第二节的时序差分法中，我们使用了用 $R_{t+1}+\gamma v(S_{t+1})$ 来近似的代替收获 $G_t$。即向前一步来近似我们的收获 $G_{t}$,那么能不能向前两步呢？当然可以，这时我们的收获 $G_t$ 的近似表达式为：</p>
<p>$$G_t^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2})$$</p>
<p>从两步，到三步，再到n步，我们可以归纳出n步时序差分收获 $G^{(n)}_t$表达式为：$$G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})$$</p>
<p>当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了。</p>
<p>对于n步时序差分来说，和普通的时序差分的区别就在于收获的计算方式的差异。那么既然有这个n步的说法，那么n到底是多少步好呢？如何衡量n的好坏呢？我们在下一节讨论。</p>
<h1 id="4-tdλ">4. TD(λ)</h1>
<p>n步时序差分选择多少步数作为一个较优的计算参数是需要尝试的超参数调优问题。为了能在不增加计算复杂度的情况下综合考虑所有步数的预测，我们引入了一个新[0,1]的参数 $\lambda$,定义入—收获是 $n$ 从 $1$ 到 $\infty$ 所有步的收获乘以权重的和。每一步的权重是 $(1-\lambda)\lambda^{n-1}$,这样 $\lambda-$收获的计算公式表示为:</p>
<p>$$G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}$$</p>
<p>进而我们可以得到 $TD(λ)$ 的价值函数的迭代公式：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t^\lambda-V(S_t)) \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t^\lambda-Q(S_t,A_t)) $$</p>
<p>每一步收获的权重定义为 $(1−λ)λ^{n−1}$ 的原因是什么呢？其图像如下图所示，可以看到随着n的增大，其第n步收获的权重呈几何级数的衰减。当在T时刻到达终止状态时，未分配的权重全部给予终止状态的实际收获值。这样可以使一个完整的状态序列中所有的n步收获的权重加起来为1，离当前状态越远的收获其权重越小。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">TD(λ)</div>
</center>
<br>
<p>从前向来看 $TD(λ)$， 一个状态的价值 $V(St)$由 $G_t$得到，而Gt��又间接由所有后续状态价值计算得到，因此可以认为更新一个状态的价值需要知道所有后续状态的价值。也就是说，必须要经历完整的状态序列获得包括终止状态的每一个状态的即时奖励才能更新当前状态的价值。这和蒙特卡罗法的要求一样，因此TD(λ)��(�)有着和蒙特卡罗法一样的劣势。当 $λ=0$ 时,就是第二节讲到的普通的时序差分法，当 $λ=1$ 时,就是蒙特卡罗法。</p>
<p>从反向来看 $TD(λ)$，它可以分析我们状态对后续状态的影响。比如老鼠在依次连续接受了3 次响铃和1 次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为频率启发(frequency heuristic) 式；而把电击归因于最近少数几次状态的影响，则称为就近启发(recency heuristic) 式。</p>
<p>如果给每一个状态引入一个数值：效用(eligibility, E) 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发。而所有状态的效用值总称为效用迹(eligibility traces,ES)。定义为：</p>
<p>$$ E_0(s)=0 \\\\ \left.E_t(s)=\gamma\lambda E_{t-1}(s)+1(S_t=s)=\left\\{\begin{array}{ll}0&amp;t&lt;k\\\\(\gamma\lambda)^{t-k}&amp;t\geq k\end{array}\right.\right.,\quad s.t.\quad\lambda,\gamma\in[0,1],s\textit{ is visited once at time k}$$</p>
<p>此时我们$TD(λ)$的价值函数更新式子可以表示为：</p>
<p>$$\delta_t=R_{t+1}+\gamma v(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(s)$$</p>
<p>也许有人会问，这前向的式子和反向的式子看起来不同啊，是不是不同的逻辑呢？其实两者是等价的。现在我们从前向推导一下反向的更新式子。</p>
<p>$$\begin{aligned}
G_t^\lambda-V(S_t)&amp; =-V(S_t)+(1-\lambda)\lambda^0(R_{t+1}+\gamma V(S_{t+1})) &amp;&amp; \text{(1)}  \\\\
&amp;+(1-\lambda)\lambda^1(R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2}))&amp;&amp; (2)  \\\\
&amp;+(1-\lambda)\lambda^2(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3V(S_{t+3}))&amp;&amp; (3)  \\\\
&amp;+\ldots &amp;&amp; \text{(4)}  \\\\
&amp;=-V(S_t)+(\gamma\lambda)^0(R_{t+1}+\gamma V(S_{t+1})-\gamma\lambda V(S_{t+1}))&amp;&amp; (5)  \\\\
&amp;+(\gamma\lambda)^1(R_{t+2}+\gamma V(S_{t+2})-\gamma\lambda V(S_{t+2}))&amp;&amp; \text{(6)}  \\\\
&amp;+(\gamma\lambda)^2(R_{t+3}+\gamma V(S_{t+3})-\gamma\lambda V(S_{t+3}))&amp;&amp; \text{(7)}  \\\\
&amp;\begin{array}{c}+\ldots\end{array}&amp;&amp; \text{(8)}  \\\\
&amp;=(\gamma\lambda)^0(R_{t+1}+\gamma V(S_{t+1})-V(S_t))&amp;&amp; \left(9\right)  \\\\
&amp;+(\gamma\lambda)^1(R_{t+2}+\gamma V(S_{t+2})-V(S_{t+1}))&amp;&amp; \text{(10)}  \\\\
&amp;+(\gamma\lambda)^2(R_{t+3}+\gamma V(S_{t+3})-V(S_{t+2}))&amp;&amp; (11)  \\\\
&amp;\begin{array}{c}+\ldots\end{array}&amp;&amp; (12)  \\\\
&amp;=\delta_t+\gamma\lambda\delta_{t+1}+(\gamma\lambda)^2\delta_{t+2}+\ldots &amp;&amp; (13)
\end{aligned}$$</p>
<p>可以看出前向TD误差和反向的TD误差实际上一致的。</p>
<h1 id="5-时序差分的控制问题求解">5. 时序差分的控制问题求解</h1>
<p>现在我们回到普通的时序差分，来看看它控制问题的求解方法。回想上一篇蒙特卡罗法在线控制的方法，我们使用的是$ϵ−$贪婪法来做价值迭代。对于时序差分，我们也可以用$ϵ−$贪婪法来价值迭代，和蒙特卡罗法在线控制的区别主要只是在于收获的计算方式不同。时序差分的在线控制(on-policy)算法最常见的是SARSA算法，我们在下一篇单独讲解。</p>
<p>而除了在线控制，我们还可以做离线控制(off-policy)，离线控制和在线控制的区别主要在于在线控制一般只有一个策略(最常见的是$ϵ−$贪婪法)。而离线控制一般有两个策略，其中一个策略(最常见的是$ϵ−$贪婪法)用于选择新的动作，另一个策略(最常见的是贪婪法)用于更新价值函数。时序差分的离线控制算法最常见的是Q-Learning算法，我们在下下篇单独讲解。</p>
<h1 id="6-时序差分小结">6. 时序差分小结</h1>
<p>时序差分和蒙特卡罗法比它更加灵活，学习能力更强，因此是目前主流的强化学习求解问题的方法，现在绝大部分强化学习乃至深度强化学习的求解都是以时序差分的思想为基础的。因此后面我们会重点讨论。</p>
<p>下一篇我们会讨论时序差分的在线控制算法SARSA。</p>
]]></description></item><item><title>RL学习笔记 [6] | 时序差分在线控制算法SARSA</title><link>https://jianye0428.github.io/posts/rl_learning_note_6/</link><pubDate>Thu, 22 Feb 2024 16:29:33 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_6/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。</p>
<p>SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-sarsa算法的引入">1. SARSA算法的引入</h1>
<p>SARSA算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解不需要环境的状态转化模型，是<strong>不基于模型的强化学习问题</strong>求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作。而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。</p>
<p>我们的SARSA算法，属于在线控制这一类，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是 $ϵ−$贪婪法，在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们对于 $ϵ−$贪婪法有详细讲解，即通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 m 个可选行为中选择行为。用公式可以表示为：</p>
<p>$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</p>
<p>π(a|s)={ϵ/m+1−ϵifa∗=argmaxa∈AQ(s,a)ϵ/melse�(�|�)={�/�+1−����∗=arg⁡max�∈��(�,�)�/�����</p>
<h1 id="2-sarsa算法概述">2. SARSA算法概述</h1>
<p>作为SARSA算法的名字本身来说，它实际上是由 $S,A,R,S,A$ 几个字母组成的。而 $S,A,R$ 分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">SARSA Transition</div>
</center>
<br>
<p>在迭代的时候，我们首先基于 $ϵ−$贪婪法在当前状态 $S$ 选择一个动作 $A$ ，这样系统会转到一个新的状态 $S′$, 同时给我们一个即时奖励 $R$ , 在新的状态 $S′$，我们会基于 $ϵ−$贪婪法在状态 $S′$ 选择一个动作 $A′$，但是注意这时候我们并不执行这个动作 $A′$，只是用来更新的我们的价值函数，价值函数的更新公式是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</p>
<p>其中，$γ$ 是衰减因子，$α$ 是迭代步长。这里和蒙特卡罗法求解在线控制问题的迭代公式的区别主要是，收获 $G_t$的表达式不同，对于时序差分，收获 $G_t$的表达式是 $R+\gamma Q(S&rsquo;,A&rsquo;)$ 。这个价值函数更新的贝尔曼公式我们在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节有详细讲到。</p>
<p>除了收获 $G_t$的表达式不同，SARSA算法和蒙特卡罗在线控制算法基本类似。</p>
<h1 id="3-sarsa算法流程">3. SARSA算法流程</h1>
<p>下面我们总结下SARSA算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出：所有的状态和动作对应的价值 $Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。设置 $A$ 为 $ϵ−$贪婪法在当前状态$S$ 选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$ ,得到新状态 $S′$ 和 奖励 $R$</li>
<li>c) 用 $\epsilon-$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S′$, $A=A′$</li>
<li>f) 如果 $S′$ 是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>这里有一个要注意的是，步长 $α$一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数 $Q$ 可以收敛。当 $Q$ 收敛时，我们的策略 $ϵ−$贪婪法也就收敛了。</p>
<h1 id="4-sarsa算法实例windy-gridworld">4. SARSA算法实例：Windy GridWorld</h1>
<p>下面我们用一个著名的实例Windy GridWorld来研究SARSA算法。</p>
<p>如下图一个10×7的长方形格子世界，标记有一个起始位置 S 和一个终止目标位置 G，格子下方的数字表示对应的列中一定强度的风。当个体进入该列的某个格子时，会按图中箭头所示的方向自动移动数字表示的格数，借此来模拟世界中风的作用。同样格子世界是有边界的，个体任意时刻只能处在世界内部的一个格子中。个体并不清楚这个世界的构造以及有风，也就是说它不知道格子是长方形的，也不知道边界在哪里，也不知道自己在里面移动移步后下一个格子与之前格子的相对位置关系，当然它也不清楚起始位置、终止目标的具体位置。但是个体会记住曾经经过的格子，下次在进入这个格子时，它能准确的辨认出这个格子曾经什么时候来过。格子可以执行的行为是朝上、下、左、右移动一步，每移动一步只要不是进入目标位置都给予一个 -1 的惩罚，直至进入目标位置后获得奖励 0 同时永久停留在该位置。现在要求解的问题是个体应该遵循怎样的策略才能尽快的从起始位置到达目标位置。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Windy GridWorld</div>
</center>
<br>
<p>逻辑并不复杂，完整的代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">我的github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。这里我主要看一下关键部分的代码。</p>
<p>算法中第2步步骤a,初始化 $S$,使用 $ϵ−$贪婪法在当前状态 $S$ 选择的动作的过程：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤b,在状态S�执行当前动作A�,得到新状态S′�′的过程，由于奖励不是终止就是-1，不需要单独计算：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤c,用 $ϵ−$贪婪法在状态 $S&rsquo;$选择新的动作 $A′$的过程：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤d,e, 更新价值函数 $Q(S,A)$ 以及更新当前状态动作的过程：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">  <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl"><span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span></span></span></code></pre></td></tr></table>
</div>
</div><p>代码很简单，相信大家对照算法，跑跑代码，可以很容易得到这个问题的最优解，进而搞清楚SARSA算法的整个流程。</p>
<h1 id="5-sarsaλ">5. SARSA(λ)</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讲到了多步时序差分 $TD(λ)$ 的价值函数迭代方法，那么同样的，对应的多步时序差分在线控制算法，就是我们的 $SARSA(λ)$。</p>
<p>$TD(\lambda)$有前向和后向两种价值函数迭代方式，当然它们是等价的。在控制问题的求解时，基于反向认识的 $SARSA(\lambda)$算法将可以有效地在线学习，数据学习完即可丢弃。因此 $SARSA(\lambda)$算法默认都是基于反向来进行价值函数迭代。</p>
<p>在上一篇我们讲到了$TD(\lambda)$状态价值函数的反向迭代，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(S)\end{gathered}$$</p>
<p>对应的动作价值函数的迭代公式可以找样写出，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\\\Q(S_t,A_t)=Q(S_t,A_t)+\alpha\delta_tE_t(S,A)\end{gathered}$$</p>
<p>除了状态价值函数 $Q(S,A)$ 的更新方式，多步参数 $λ$ 以及反向认识引入的效用迹 $E(S,A)$ ，其余算法思想和 $SARSA$ 类似。这里我们总结下 $SARSA(λ)$的算法流程。　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率$ϵ$, 多步参数$λ$</li>
<li>输出：所有的状态和动作对应的价值$Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 对于终止状态其 $Q$值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化所有状态动作的效用迹 $E$ 为0，初始化S为当前状态序列的第一个状态。设置$A$为 $ϵ−$贪婪法在当前状态 $S$选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>c) 用$ϵ−$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新效用迹函数 $E(S,A)$和TD误差 $δ$:
<ul>
<li>$$\begin{gathered}E(S,A)=E(S,A)+1\\\\\delta=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\end{gathered}$$</li>
</ul>
</li>
<li>e) 对当前序列所有出现的状态s和对应动作 $a$, 更新价值函数 $Q(s,a)$和效用迹函数 $E(s,a)$:
<ul>
<li>$$\begin{gathered}Q(s,a)=Q(s,a)+\alpha\delta E(s,a)\\\\E(s,a)=\gamma\lambda E(s,a)\end{gathered}$$</li>
</ul>
</li>
<li>f) $S=S&rsquo;$, $A=A'$</li>
<li>g) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>对于步长$α$，和SARSA一样，一般也需要随着迭代的进行逐渐变小才能保证动作价值函数$Q$收敛。</p>
<h1 id="6-sarsa小结">6. SARSA小结</h1>
<p>SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和蒙特卡罗法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。</p>
<p>但是SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在 SARSA 算法中，$Q(S,A)$ 的值使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。</p>
<p>下一篇我们讨论SARSA的姊妹算法，时序差分离线控制算法Q-Learning。</p>
]]></description></item><item><title>RL学习笔记 [4] | 用蒙特卡罗法（MC）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_4/</link><pubDate>Thu, 22 Feb 2024 13:00:24 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_4/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（三）用动态规划（DP）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型 $P$ 都无法知道，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？本文要讨论的蒙特卡罗(Monte-Calo, MC)就是一种可行的方法。</p>
<p>蒙特卡罗法这一篇对应Sutton书的第五章和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-不基于模型的强化学习问题定义">1. 不基于模型的强化学习问题定义</h1>
<p>在动态规划法中，强化学习的两个问题是这样定义的:</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的6个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>可见, 模型状态转化概率矩阵 $P$ 始终是已知的，即MDP已知，对于这样的强化学习问题，我们一般称为<mark>基于模型的强化学习</mark>问题。</p>
<p>不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵 $P$ ，这时如果仍然需要我们求解强化学习问题，那么这就是<mark>不基于模型的强化学习</mark>问题了。它的两个问题一般的定义是：</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$ , 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>本文要讨论的蒙特卡罗法就是上述不基于模型的强化学习问题。</p>
<h1 id="2-蒙特卡罗法求解特点">2. 蒙特卡罗法求解特点</h1>
<p>蒙特卡罗这个词之前的博文也讨论过，尤其是在之前的<a href="https://www.cnblogs.com/pinard/p/MCMC%28%e4%b8%80%29%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%96%b9%e6%b3%95"target="_blank" rel="external nofollow noopener noreferrer">MCMC系列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中。它是一种通过采样近似求解问题的方法。这里的蒙特卡罗法虽然和MCMC不同，但是采样的思路还是一致的。那么如何采样呢？</p>
<p>蒙特卡罗法通过采样若干经历完整的状态序列(episode)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。</p>
<p>从特卡罗法法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。</p>
<h1 id="3-蒙特卡罗法求解强化学习预测问题">3. 蒙特卡罗法求解强化学习预测问题</h1>
<p>这里我们先来讨论蒙特卡罗法求解强化学习预测问题的方法，即策略评估。一个给定策略 $π$ 的完整有T个状态的状态序列如下：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</p>
<p>回忆下<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中对于价值函数 $v_π(s)$的定义:</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(G_t|S_t=s)=\mathbb{E}_\pi(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t=s)$$</p>
<p>可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解，也就是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>$$v_\pi(s)\approx average(G_t),s.t.S_t=s$$</p>
<p>可以看出，预测问题的求解思路还是很简单的。不过有几个点可以优化考虑。</p>
<ul>
<li>
<p>第一个点是: 同样一个状态可能在一个完整的状态序列中重复出现，那么该状态的收获该如何计算？有两种解决方法。第一种是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：首次访问(first visit) 和每次访问(every visit) 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p>
</li>
<li>
<p>第二个点是累进更新平均值(incremental mean)。在上面预测问题的求解公式里，我们有一个average的公式，意味着要保存所有该状态的收获值之和最后取平均。这样浪费了太多的存储空间。一个较好的方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。通过下面的公式就很容易理解这个过程：</p>
</li>
</ul>
<p>$$\mu_k=\frac1k\sum_{j=1}^kx_j=\frac1k(x_k+\sum_{j=1}^{k-1}x_j)=\frac1k(x_k+(k-1)\mu_{k-1})=\mu_{k-1}+\frac1k(x_k-\mu_{k-1})$$</p>
<p>这样上面的状态价值公式就可以改写成：</p>
<p>$$N(S_t)=N(S_t)+1$$</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>这样我们无论数据量是多还是少，算法需要的内存基本是固定的 。</p>
<p>有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数 $N(S_t)$,这时我们可以用一个系数 $α$ 来代替，即：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t))$$</p>
<p>对于动作价值函数 $Q(S_t,A_t)$,也是类似的，比如对上面最后一个式子，动作价值函数版本为：</p>
<p>$$Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t))$$</p>
<p>以上就是蒙特卡罗法求解预测问题的整个过程，下面我们来看控制问题求解。</p>
<h1 id="4-蒙特卡罗法求解强化学习控制问题">4. 蒙特卡罗法求解强化学习控制问题</h1>
<p>蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。回忆下动态规划价值迭代的的思路， 每轮迭代先做策略评估，计算出价值 $v_k(s)$ ，然后基于据一定的方法（比如贪婪法）更新当前策略 $π$。最后得到最优价值函数 $v∗$ 和最优策略 $π∗$。</p>
<p>和动态规划比，蒙特卡罗法不同之处体现在三点:</p>
<ul>
<li>一是预测问题策略评估的方法不同，这个第三节已经讲了。</li>
<li>第二是蒙特卡罗法一般是优化最优动作价值函数 $q∗$，而不是状态价值函数 $v∗$。</li>
<li>三是动态规划一般基于贪婪法更新策略。而蒙特卡罗法一般采用 $ϵ−$贪婪法更新。这个 $ϵ$ 就是我们在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中讲到的第8个模型要素 $ϵ$。$ϵ−$贪婪法通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 $m$ 个可选行为中选择行为。用公式可以表示为：
$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</li>
</ul>
<p>在实际求解控制问题时，为了使算法可以收敛，一般 $ϵ$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Mento Carlo 搜索过程示意</div>
</center>
<br>
<h1 id="5-蒙特卡罗法控制问题算法流程">5. 蒙特卡罗法控制问题算法流程</h1>
<p>在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。在线和离线的区别我们在后续的文章里面会讲。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。</p>
<p>在线蒙特卡罗法求解强化学习控制问题的算法流程如下:</p>
<ul>
<li>输入：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率$ϵ$</li>
<li>输出：最优的动作价值函数 $q∗$ 和最优策略 $π∗$</li>
<li>
<ol>
<li>初始化所有的动作价值 $Q(s,a)=0$ ， 状态次数 $N(s,a)=0$，采样次数 $k=0$，随机初始化一个策略 $π$</li>
</ol>
</li>
<li>
<ol start="2">
<li>$k=k+1$, 基于策略 $π$ 进行第k次蒙特卡罗采样，得到一个完整的状态序列:
$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</li>
</ol>
</li>
<li>
<ol start="3">
<li>对于该状态序列里出现的每一状态行为对 $(S_t,A_t)$，计算其收获 $G_t$, 更新其计数 $N(s,a)$ 和行为价值函数 $Q(s,a)$：
$$\begin{gathered}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T \\\\
N(S_t,A_t)=N(S_t,A_t)+1 \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\frac1{N(S_t,A_t)}(G_t-Q(S_t,A_t))
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="4">
<li>基于新计算出的动作价值，更新当前的 $ϵ−$贪婪策略：
$$\begin{gathered}
\epsilon=\frac1k \\\\
\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;ifa^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="5">
<li>如果所有的 $Q(s,a)$ 收敛，则对应的所有 $Q(s,a)$ 即为最优的动作价值函数 $q∗$。对应的策略 $π(a|s)$ 即为最优策略 $π∗$。否则转到第二步。</li>
</ol>
</li>
</ul>
<h1 id="6-蒙特卡罗法求解强化学习问题小结">6. 蒙特卡罗法求解强化学习问题小结</h1>
<p>蒙特卡罗法是我们第二个讲到的求解强化问题的方法，也是第一个不基于模型的强化问题求解方法。它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点，这就是它每次采样都需要一个完整的状态序列。如果我们没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了， 也就是说，我们还需要寻找其他的更灵活的不基于模型的强化问题求解方法。</p>
<p>下一篇我们讨论用时序差分方法来求解强化学习预测和控制问题的方法。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9492980.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [3] | 用动态规划(DP)求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_3/</link><pubDate>Thu, 22 Feb 2024 08:59:02 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_3/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。</p>
<p>动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲。</p>
<h1 id="1-动态规划和强化学习问题的联系">1. 动态规划和强化学习问题的联系</h1>
<p>对于动态规划，相信大家都很熟悉，很多使用算法的地方都会用到。就算是机器学习相关的算法，使用动态规划的也很多，比如之前讲到的<a href="https://www.cnblogs.com/pinard/p/6955871.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="https://www.cnblogs.com/pinard/p/6991852.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 都是动态规划的典型例子。</p>
<p>动态规划的关键点有两个：一是问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解。第二是可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题的状态。而强化学习的问题恰好是满足这两个条件的。</p>
<p>我们先看看强化学习的两个基本问题。</p>
<p>第一个问题是预测，即给定强化学习的6个要素：状态集 $S$, 动作集$A$, 模型状态转化概率矩阵$P$, 即时奖励$R$，衰减因子$γ$, 给定策略$π$， 求解该策略的状态价值函数$v(π)$</p>
<p>第二个问题是控制，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 模型状态转化概率矩阵$P$, 即时奖励$R$，衰减因子$γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
<p>那么如何找到动态规划和强化学习这两个问题的关系呢？</p>
<p>回忆一下上一篇<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中状态价值函数的贝尔曼方程：</p>
<p>$$v_\pi(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_\pi(s&rsquo;))$$</p>
<p>从这个式子我们可以看出，我们可以定义出子问题求解每个状态的状态价值函数，同时这个式子又是一个递推的式子, 意味着利用它，我们可以使用上一个迭代周期内的状态价值来计算更新当前迭代周期某状态 $s$ 的状态价值。可见，使用动态规划来求解强化学习问题是比较自然的。</p>
<h1 id="2-策略评估求解预测问题">2. 策略评估求解预测问题</h1>
<p>首先，我们来看如何使用动态规划来求解强化学习的预测问题，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p>
<p>策略评估的基本思路是从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。</p>
<p>假设我们在第k轮迭代已经计算出了所有的状态的状态价值，那么在第 $k+1$ 轮我们可以利用第k轮计算出的状态价值计算出第k+1+1轮的状态价值。这是通过贝尔曼方程来完成的，即：</p>
<p>$$v_{k+1}(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_k(s&rsquo;))$$</p>
<p>和上一节的式子唯一的区别是由于我们的策略 $π$ 已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变很小(收敛)，那么我们就得出了预测问题的解，即给定策略的状态价值函数 $v(π)$。</p>
<p>下面我们用一个具体的例子来说明策略评估的过程。</p>
<h1 id="3-策略评估求解实例">3. 策略评估求解实例</h1>
<p>这是一个经典的Grid World的例子。我们有一个4x4的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为γ=1=1。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率P=1=1。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Grid World</div>
</center>
<br>
<p>首先我们初始化所有格子的状态价值为0，如上图 $k=0$ 的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。在 $k=1$ 的时候，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：</p>
<p>$$v_1^{(21)}=\frac14[(-1+0)+(-1+0)+(-1+0)+(-1+0)]=-1$$</p>
<p>第二行第二个格子的价值是：</p>
<p>$$v_1^{(22)}=\frac14[(-1+0)+(-1+0)+(-1+0)+(-1+0)]=-1$$</p>
<p>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图 $k=1$ 的时候。现在我们第一轮迭代完了。开始动态规划迭代第二轮了。还是看第二行第一个格子的价值：</p>
<p>$$v_2^{(21)}=\frac14[(-1+0)+(-1-1)+(-1-1)+(-1-1)]=-1.75$$</p>
<p>第二行第二个格子的价值是：</p>
<p>$$v_2^{(22)}=\frac14[(-1-1)+(-1-1)+(-1-1)+(-1-1)]=-2$$</p>
<p>最终得到的结果是上图 $k=2$ 的时候。第三轮的迭代如下：</p>
<p>$$v_3^{(21)}=\frac14[(-1-1.7)+(-1-2)+(-1-2)+(-1+0)]=-2.425$$</p>
<p>$$v_3^{(22)}=\frac14[(-1-1.7)+(-1-1.7)+(-1-2)+(-1-2)]=-2.85$$</p>
<p>最终得到的结果是上图 $k=3$ 的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h1 id="4-策略迭代求解控制问题">4. 策略迭代求解控制问题</h1>
<p>上面我们讲了使用策略评估求解预测问题，现在我们再来看如何使用动态规划求解强化学习的第二个问题控制问题。一种可行的方法就是根据我们之前基于任意一个给定策略评估得到的状态价值来及时调整我们的动作策略，这个方法我们叫做策略迭代(Policy Iteration)。</p>
<p>如何调整呢？最简单的方法就是贪婪法。考虑一种如下的贪婪策略：个体在某个状态下选择的行为是其能够到达后续所有可能的状态中状态价值最大的那个状态。还是以第三节的例子为例，如上面的图右边。当我们计算出最终的状态价值后，我们发现，第二行第一个格子周围的价值分别是0,-18,-20，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-14,-14,-20,-20。那么我们整行动策略为向状态价值为-14的方向移动，也就是图中的向左向上。</p>
<p>如果用一副图来表示策略迭代的过程的话，如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Policy Iteration</div>
</center>
<br>
<p>在策略迭代过程中，我们循环进行两部分工作，第一步是使用当前策略 $π∗$ 评估计算当前策略的最终状态价值 $v∗$，第二步是根据状态价值 $v∗$ 根据一定的方法（比如贪婪法）更新策略 $π∗$，接着回到第一步，一直迭代下去，最终得到收敛的策略 $π∗$ 和状态价值 $v∗$。</p>
<h1 id="5-价值迭代求解控制问题">5. 价值迭代求解控制问题</h1>
<p>观察第三节的图发现，我们如果用贪婪法调整动作策略，那么当k=3=3的时候，我们就已经得到了最优的动作策略。而不用一直迭代到状态价值收敛才去调整策略。那么此时我们的策略迭代优化为价值迭代。</p>
<p>还是以第三节的例子为例，如上面的图右边。比如当k=2=2时，第二行第一个格子周围的价值分别是0,-2,-2，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-1.7,-1.7,-2, -2。那么我们整行动策略为向状态价值为-1.7的方向移动，也就是图中的向左向上。</p>
<p>和上一节相比，我们没有等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时我们的状态价值的更新方法也和策略迭代不同。现在的贝尔曼方程迭代式子如下：</p>
<p>$$v_{k+1}(s)=\max_{a\in A}(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_k(s&rsquo;))$$</p>
<p>可见由于策略调整，我们现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。</p>
<h1 id="6-异步动态规划算法">6. 异步动态规划算法</h1>
<p>在前几节我们讲的都是同步动态规划算法，即每轮迭代我会计算出所有的状态价值并保存起来，在下一轮中，我们使用这些保存起来的状态价值来计算新一轮的状态价值。</p>
<p>另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。</p>
<p>常见的异步动态规划算法有三种：</p>
<p>第一种是原位动态规划 (in-place dynamic programming)， 此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。</p>
<p>第二种是优先级动态规划 (prioritised sweeping)：该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。</p>
<p>第三种是实时动态规划 (real-time dynamic programming)：实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢。</p>
<h1 id="7-动态规划求解强化学习问题小结">7. 动态规划求解强化学习问题小结</h1>
<p>动态规划是我们讲到的第一个系统求解强化学习预测和控制问题的方法。它的算法思路比较简单，主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。</p>
<p>动态规划算法使用全宽度（full-width）的回溯机制来进行状态价值的更新，也就是说，无论是同步还是异步动态规划，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此我们还需要寻找其他的针对复杂问题的强化学习问题求解方法。</p>
<p>下一篇我们讨论用蒙特卡罗方法来求解强化学习预测和控制问题的方法。</p>
<p>ref:
<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9463815.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [2] | 马尔科夫决策过程(MDP)</title><link>https://jianye0428.github.io/posts/rl_learning_note_2/</link><pubDate>Wed, 21 Feb 2024 10:38:11 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_2/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。</p>
<p>MDP这一篇对应Sutton书的第三章和UCL强化学习课程的第二讲。</p>
<h1 id="1-强化学习引入mdp的原因">1. 强化学习引入MDP的原因</h1>
<p>对于马尔科夫性本身，我之前讲过的<a href="http://www.cnblogs.com/pinard/p/6945257.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（一）HMM模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="http://www.cnblogs.com/pinard/p/7048333.html"target="_blank" rel="external nofollow noopener noreferrer">条件随机场CRF(一)从随机场到线性链条件随机场<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>以及<a href="http://www.cnblogs.com/pinard/p/6632399.html"target="_blank" rel="external nofollow noopener noreferrer">MCMC(二)马尔科夫链<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>都有讲到。它本身是一个比较简单的假设，因此这里就不专门对“马尔可夫性”做专门的讲述了。</p>
<p>除了对于环境的状态转化模型这个因素做马尔科夫假设外，我们还对强化学习第四个要素个体的策略(policy) $π$ 也做了马尔科夫假设。即在状态 $s$ 时采取动作 $a$ 的概率仅与当前状态 $s$ 有关，与其他的要素无关。用公式表示就是</p>
<p>$$\pi(a\mid s)=P(A_{t}=a\mid S_{t}=s)$$</p>
<p>对于第五个要素，价值函数 $v_π(s)$ 也是一样, $v_π(s)$ 现在仅仅依赖于当前状态了，那么现在价值函数 $v_π(s)$ 表示为:</p>
<p>$$\nu_{\pi}(s)=\mathrm{E}_{\pi}(G_{t}|S_{t}=s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)$$</p>
<p>其中，$G_t$ 代表收获(return), 是一个MDP中从某一个状态 $S_t$ 开始采样直到终止状态时所有奖励的有衰减的之和。</p>
<h1 id="2-mdp的价值函数与贝尔曼方程">2. MDP的价值函数与贝尔曼方程</h1>
<p>对于MDP，我们在第一节里已经讲到了它的价值函数 $v_π(s)$ 的表达式。但是这个表达式没有考虑到所采用的动作$a$带来的价值影响，因此我们除了 $v_π(s)$ 这个状态价值函数外，还有一个动作价值函数 $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=\operatorname{E}_{\pi}(G_{t}|S_{t}=s,A_{t}=a)=\operatorname{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s,A_{t}=a)$$</p>
<p>根据价值函数的表达式，我们可以推导出价值函数基于状态的递推关系，比如对于状态价值函数 $v_π(s)$，可以发现：</p>
<p>$$\begin{aligned}
V_{\pi}(s)&amp; =\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)  \\\\
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)|S_{t}=s) \\\\
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma G_{t+1}|S_{t}=s) \\\\
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})|S_{t}=s)
\end{aligned}$$</p>
<p>也就是说，在 $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 是满足递推关系的，即：</p>
<p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})\mid S_{t}=s)$$
　　　　
这个递推式子我们一般将它叫做<strong>贝尔曼方程</strong>。这个式子告诉我们，一个状态的价值由该状态的奖励以及后续状态价值按一定的衰减比例联合组成。</p>
<p>同样的方法，我们可以得到动作价值函数 $q_π(s,a)$ 的贝尔曼方程：</p>
<p>$$q_{\pi}(s,a)=\mathrm{E}_{\pi}(R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})\mid S_{t}=s,A_{t}=a)$$</p>
<h1 id="3-状态价值函数与动作价值函数的递推关系">3. 状态价值函数与动作价值函数的递推关系</h1>
<p>根据动作价值函数 $q_π(s,a)$ 和状态价值函数 $v_π(s)$ 的定义，我们很容易得到他们之间的转化关系公式：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$</p>
<p>也就是说，状态价值函数是所有动作价值函数基于策略 $π$ 的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值。</p>
<p>反过来，利用上贝尔曼方程，我们也很容易从状态价值函数 $v_π(s)$ 表示动作价值函数 $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=R_{s}^{a}+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;})$$</p>
<p>通俗说就是状态动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。</p>
<p>这两个转化过程也可以从下图中直观的看出：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">状态价值函数</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">动作价值函数</div>
</center>
<br>
<p>把上面两个式子互相结合起来，我们可以得到：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;}))$$</p>
<p>$$q_\pi(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\sum_{a&rsquo; \in A}\pi(a&rsquo; \mid s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)$$</p>
<h1 id="4-最优价值函数">4. 最优价值函数</h1>
<p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略我们可以用 $π^*$表示。一旦找到这个最优策略$π^∗$，那么我们就解决了这个强化学习问题。一般来说，比较难去找到一个最优策略，但是可以通过比较若干不同策略的优劣来确定一个较好的策略，也就是局部最优解。</p>
<p>如何比较策略的优劣呢？一般是通过对应的价值函数来比较的，也就是说，寻找较优策略可以通过寻找较优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者，即：</p>
<p>$$\nu_{*}(s)=\max_{\pi}\nu_{\pi}(s)$$</p>
<p>同理也可以定义最优动作价值函数是所有策略下产生的众多动作状态价值函数中的最大者，即：</p>
<p>$$q_*(s,a)=\max_\pi q_\pi(s,a)$$</p>
<p>对于最优的策略，基于动作价值函数我们可以定义为：</p>
<p>$$\pi_<em>(a|s)=\begin{cases}1&amp;\mathrm{if~}a=\mathrm{arg~}\max_{a\in A}q</em>(s,a)\\\\0&amp;\mathrm{else}&amp;\end{cases}$$</p>
<p>只要我们找到了最大的状态价值函数或者动作价值函数，那么对应的策略 $π^*$ 就是我们强化学习问题的解。同时，利用状态价值函数和动作价值函数之间的关系，我们也可以得到:</p>
<p>$$v_<em>(s)=\max_aq_</em>(s,a)$$</p>
<p>反过来的最优价值函数关系也很容易得到：</p>
<p>$$q_{<em>}(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss}^{a}{}_{</em>}(\mathrm{s&rsquo;})$$</p>
<p>利用上面的两个式子也可以得到和第三节末尾类似的式子：</p>
<p>$$\nu_<em>(s)=\max_a(R_s^a+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^a\nu_</em>(s&rsquo;))$$</p>
<p>$$q_<em>(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\max_{a&rsquo;}q_</em>(s&rsquo;,a&rsquo;)$$</p>
<h1 id="5-mdp实例">5. MDP实例</h1>
<p>上面的公式有点多，需要一些时间慢慢消化，这里给出一个UCL讲义上实际的例子，首先看看具体我们如何利用给定策略来计算价值函数。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP 举例</div>
</center>
<br>
<p>例子是一个学生学习考试的MDP。里面左下那个圆圈位置是起点，方框那个位置是终点。上面的动作有study, pub, facebook, quit, sleep，每个状态动作对应的即时奖励R已经标出来了。我们的目标是找到最优的动作价值函数或者状态价值函数，进而找出最优的策略。</p>
<p>为了方便，我们假设衰减因子 $γ=1$, $π(a|s)=0.5$。</p>
<p>对于终点方框位置，由于其没有下一个状态，也没有当前状态的动作，因此其状态价值函数为0。对于其余四个状态，我们依次定义其价值为<em>v</em>1,<em>v</em>2,<em>v</em>3,<em>v</em>4， 分别对应左上，左下，中下，右下位置的圆圈。我们基于$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s&rsquo;))$计算所有的状态价值函数。可以列出一个方程组。</p>
<ul>
<li>
<p>对于<em>v</em>1位置，我们有：$v_1=0.5*(-1+v_1)+0.5*(0+v_2)$</p>
</li>
<li>
<p>对于<em>v</em>2位置，我们有：$v_2=0.5*(-1+v_1)+0.5*(-2+v_3)$</p>
</li>
<li>
<p>对于<em>v</em>3位置，我们有：$v_3=0.5*(0+0)+0.5*(-2+v_4)$</p>
</li>
<li>
<p>对于<em>v</em>4位置，我们有：$v_4=0.5*(10+0)+0.5*(1+0.2<em>v_2+0.4</em>v_3+0.4*v_4)$</p>
</li>
</ul>
<p>解出这个方程组可以得到 $v_1=−2.3$, $v_2=−1.3$, $v_3=2.7$, $v_4=7.4$, 即每个状态的价值函数如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>上面我们固定了策略$ π(a|s)$, 虽然求出了每个状态的状态价值函数，但是却并不一定是最优价值函数。那么如何求出最优价值函数呢？这里由于状态机简单，求出最优的状态价值函数 $v*(s)$ 或者动作价值函数 $q*(s,a)$ s比较容易。</p>
<p>我们这次以动作价值函数 $q*(s,a)$ 来为例求解。首先终点方框处的好求。</p>
<p>$$q*(s_3,sleep)=0,q*(s_4,study)=10$$</p>
<p>接着我们就可利用 $q*(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\max_{a&rsquo;}q*(s&rsquo;,a&rsquo;)$ 列方程组求出所有的 $q∗(s,a)$ 。有了所有的 $q*(s,a)$,利用 $v_{<em>}(s)=\max_{a}q</em>(s,a)$ 就可以求出所有的 $v∗(s)$。最终求出的所有 $v∗(s)$ 和 $q∗(s,a)$ 如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>从而我们的最优决策路径是走6-&gt;6-&gt;8-&gt;10-&gt;结束。　　　　</p>
<h1 id="6-mdp小结">6. MDP小结</h1>
<p>MDP是强化学习入门的关键一步，如果这部分研究的比较清楚，后面的学习就会容易很多。因此值得多些时间在这里。虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。</p>
<p>下一篇讨论用动态规划的方法来求解强化学习的问题。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9426283.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [1] | 模型基础</title><link>https://jianye0428.github.io/posts/rl_learning_note_1/</link><pubDate>Wed, 21 Feb 2024 10:38:07 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_1/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>　从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。</p>
<p>　第一篇会从强化学习的基本概念讲起，对应Sutton书的第一章和UCL课程的第一讲。</p>
<h1 id="1-强化学习在机器学习中的位置">1. 强化学习在机器学习中的位置</h1>
<p>强化学习的学习思路和人比较类似，是在实践中学习，比如学习走路，如果摔倒了，那么我们大脑后面会给一个负面的奖励值，说明走的姿势不好。然后我们从摔倒状态中爬起来，如果后面正常走了一步，那么大脑会给一个正面的奖励值，我们会知道这是一个好的走路姿势。那么这个过程和之前讲的机器学习方法有什么区别呢？</p>
<p>强化学习是和监督学习，非监督学习并列的第三种机器学习方法，从下图我们可以看出来。</p>
  <br>
  <center>
    
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">RL、SL、UL与ML的区别联系</div>
  </center>
  <br>
<p>与监督学习相比，强化学习最大的区别是它没有监督学习已经准备好的训练数据输出值。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的，比如上面的例子里走路摔倒了才得到大脑的奖励值。同时，强化学习的每一步与时间顺序前后关系紧密。而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。</p>
<p>再来看看强化学习和非监督学习的区别。也还是在奖励值这个地方。非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。</p>
<h1 id="2-强化学习的建模">2. 强化学习的建模</h1>
<p>我们现在来看看强化学习这样的问题我们怎么来建模，简单的来说，是下图这样的：</p>
  <br>
  <center>
    
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">大脑与环境的交互</div>
  </center>
  <br>
<p>上面的大脑代表我们的算法执行个体，我们可以操作个体来做决策，即选择一个合适的动作（Action）$A_t$。下面的地球代表我们要研究的环境,它有自己的状态模型，我们选择了动作 $A_t$ 后，环境的状态(State)会变，我们会发现环境状态已经变为 $S_{t+1}$,同时我们得到了我们采取动作 $A_t$ 的延时奖励(Reward) $R_{t+1}$。然后个体可以继续选择下一个合适的动作，然后环境的状态又会变，又有新的奖励值&hellip;这就是强化学习的思路。</p>
<p>那么我们可以整理下这个思路里面出现的强化学习要素。</p>
<ul>
<li>
<p>第一个是环境的状态 $S$, $t$ 时刻环境的状态 $S_t$ 是它的环境状态集中某一个状态。</p>
</li>
<li>
<p>第二个是个体的动作 $A$, $t$ 时刻个体采取的动作 $A_t$ 是它的动作集中某一个动作。</p>
</li>
<li>
<p>第三个是环境的奖励 $R$, $t$ 时刻个体在状态 $S_t$ 采取的动作 $A_t$ 对应的奖励 $R_{t+1}$ 会在 $t+1$ 时刻得到。</p>
</li>
<li>
<p>第四个是个体的策略(policy) $π$,它代表个体采取动作的依据，即个体会依据策略 $π$ 来选择动作。最常见的策略表达方式是一个条件概率分布 $π(a|s)$, 即在状态 $s$ 时采取动作 $a$ 的概率。即 $π(a|s)=P(A_t=a|S_t=s)$.此时概率大的动作被个体选择的概率较高。</p>
</li>
<li>
<p>第五个是个体在策略 $π$ 和状态 $s$ 时，采取行动后的价值(value)，一般用 $v_π(s)$ 表示。这个价值一般是一个期望函数。虽然当前动作会给一个延时奖励 $R_{t+1}$,但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表到了 $t+1$, $t+2$,&hellip;时刻的后续奖励也高。比如下象棋，我们可以某个动作可以吃掉对方的车，这个延时奖励是很高，但是接着后面我们输棋了。此时吃车的动作奖励值高但是价值并不高。因此我们的价值要综合考虑当前的延时奖励和后续的延时奖励。价值函数 $v_{\pi}(s)$ 一般可以表示为下式，不同的算法会有对应的一些价值函数变种，但思路相同。
$$v_{\pi}(s)=\mathbb{E}_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+&hellip;|S_t=s)$$</p>
</li>
<li>
<p>其中 $γ$ 是第六个模型要素，即奖励衰减因子，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。大多数时候，我们会取一个0到1之间的数字，即当前延时奖励的权重比后续奖励的权重大。</p>
</li>
<li>
<p>第七个是环境的状态转化模型，可以理解为一个概率状态机，它可以表示为一个概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率，表示为 $P^a_{ss&rsquo;}$。</p>
</li>
<li>
<p>第八个是探索率 $ϵ$，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。因此我们在训练选择最优动作时，会有一定的概率 $ϵ$ 不选择使当前轮迭代价值最大的动作，而选择其他的动作。</p>
</li>
</ul>
<p>以上8个就是强化学习模型的基本要素了。当然，在不同的强化学习模型中，会考虑一些其他的模型要素，或者不考虑上述要素的某几个，但是这8个是大多数强化学习模型的基本要素。</p>
<h1 id="3-强化学习的简单实例">3. 强化学习的简单实例</h1>
<p>这里给出一个简单的强化学习例子Tic-Tac-Toe。这是一个简单的游戏，在一个3x3的九宫格里，两个人轮流下，直到有个人的棋子满足三个一横一竖或者一斜，赢得比赛游戏结束，或者九宫格填满也没有人赢，则和棋。</p>
<p>这个例子的完整代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/introduction.py"target="_blank" rel="external nofollow noopener noreferrer">github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。例子只有一个文件，很简单，代码首先会用两个电脑选手训练模型，然后可以让人和机器对战。当然，由于这个模型很简单，所以只要你不乱走，最后的结果都是和棋，当然想赢电脑也是不可能的。</p>
<p>我们重点看看这个例子的模型，理解上面第二节的部分。如何训练强化学习模型可以先不管。代码部分大家可以自己去看，只有300多行。</p>
<ul>
<li>
<p>首先看第一个要素环境的状态 $S$。这是一个九宫格，每个格子有三种状态，即没有棋子(取值0)，有第一个选手的棋子(取值1)，有第二个选手的棋子(取值-1)。那么这个模型的状态一共有$3^9=19683$个。</p>
</li>
<li>
<p>接着我们看个体的动作 $A$，这里只有9个格子，每次也只能下一步，所以最多只有9个动作选项。实际上由于已经有棋子的格子是不能再下的，所以动作选项会更少。实际可以选择动作的就是那些取值为0的格子。</p>
</li>
<li>
<p>第三个是环境的奖励 $R$，这个一般是我们自己设计。由于我们的目的是赢棋，所以如果某个动作导致的改变到的状态可以使我们赢棋，结束游戏，那么奖励最高，反之则奖励最低。其余的双方下棋动作都有奖励，但奖励较少。特别的，对于先下的棋手，不会导致结束的动作奖励要比后下的棋手少。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># give reward to two players</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">giveReward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">currentState</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p1Symbol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">currentState</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p2Symbol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第四个是个体的策略(policy) $π$，这个一般是学习得到的，我们会在每轮以较大的概率选择当前价值最高的动作，同时以较小的概率去探索新动作，在这里AI的策略如下面代码所示。里面的exploreRate就是我们的第八个要素探索率 $ϵ$。即策略是以 $1−ϵ$ 的概率选择当前最大价值的动作，以 $ϵ$ 的概率随机选择新动作。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># determine next action</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">takeAction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">nextStates</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">nextPositions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BOARD_ROWS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BOARD_COLS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextPositions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">nextState</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span><span class="o">.</span><span class="n">getHash</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploreRate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nextPositions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Not sure if truncating is the best way to deal with exploratory step</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Maybe it&#39;s better to only skip this step rather than forget all the history</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span> <span class="o">=</span> <span class="n">nextPositions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">action</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="nb">hash</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nextStates</span><span class="p">,</span> <span class="n">nextPositions</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="nb">hash</span><span class="p">],</span> <span class="n">pos</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">values</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">action</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第五个是价值函数，代码里用value表示。价值函数的更新代码里只考虑了当前动作的现有价值和得到的奖励两部分，可以认为我们的第六个模型要素衰减因子 $γ$ 为0。具体的代码部分如下，价值更新部分的代码加粗。具体为什么会这样更新价值函数我们以后会讲。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># update estimation according to reward</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">feedReward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="o">.</span><span class="n">getHash</span><span class="p">()</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">latestState</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">stepSize</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">      <span class="n">target</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第七个是环境的状态转化模型, 这里由于每一个动作后，环境的下一个模型状态是确定的，也就是九宫格的每个格子是否有某个选手的棋子是确定的，因此转化的概率都是1，不存在某个动作后会以一定的概率到某几个新状态，比较简单。</p>
</li>
</ul>
<p>从这个例子，相信大家对于强化学习的建模会有一个初步的认识了。　　　　　　　　</p>
<p>以上就是强化学习的模型基础，下一篇会讨论马尔科夫决策过程。</p>
]]></description></item><item><title>车辆路径规划之Dubins曲线与RS曲线简述</title><link>https://jianye0428.github.io/posts/dubinsandrs/</link><pubDate>Tue, 20 Feb 2024 09:10:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/dubinsandrs/</guid><description><![CDATA[<h2 id="0-描述">0. 描述</h2>
<p>Dubins和RS曲线都是路径规划的经典算法，其中车辆运动学利用RS曲线居多，因此简单介绍Dubins并引出RS曲线。</p>
<p>花了点时间看了二者的论文，并阅读了一个开源的代码。</p>
<h2 id="1-dubins曲线">1. Dubins曲线</h2>
<p>Dubins曲线是在满足曲率约束和规定的始端和末端的切线（进入方向）的条件下，连接两个二维平面的最短路径。它满足给定的运动曲率约束，即转弯半径大于等于给定的半径。</p>
<p>假设顺时针圆周运动为R，逆时针圆周运动为L，直线运动为S。求出Dubins曲线，用任务来表达就是</p>
<p>给了车辆起始位置 $(x_{start},y_{start})$ ，车辆的朝向 $\theta_{start}$ ，再给一个车辆的目标位置 $(x_{end},y_{end})$ ，车辆的目标朝向为 $\theta_{end}$ 。车辆最多能实现的曲率为 $K_{max}$ 。车辆不可以做后退运动，只能向前开。</p>
<p>规划的曲线分三段，第一段是绕固定圆心的L或R，第二段是L/R/S，第三段是L/R。用这三段曲线可以使车辆从初始位姿连续的移动到目标位姿。</p>
<p>三段曲线可以组成的集合有6种={LSL、RSR、RSL、LSR、RLR、LRL}。这六种可能中最短的路径就是Dubins曲线。</p>
<p>网上有很多关于Dubins曲线的文章，都很简单。先把初始位姿和目标位姿做一下差值，得到一个更简单的坐标系。再通过画圆和直线，求解一些方程，就可以计算出不同选择路径的长度值。再从中取最小的，就可以得到Dubins曲线了。这里不多介绍。</p>
<p>有关Dubins的文章有很多，对于6种不同的路径，都有对应的公式去计算每段的长度值。</p>
<h2 id="2-reeds-shepp曲线">2. Reeds-Shepp曲线</h2>
<p><strong>1. 原理</strong></p>
<p><strong>“利用倒挡的RS曲线可以比Dubins曲线更优”</strong></p>
<p>论文《Optimal paths for a car that goes both forwards and backwards》提出了Reeds-Shepp曲线。这篇论文由Reeds和Shepp在1990年发表。他们提出了一种能够计算出车辆以固定转弯半径，由一个姿态向另一个姿态运动的最短路径的曲线，即Reeds-Shepp曲线，简称为RS曲线。</p>
<p>对于Dubins曲线，当我们需要车辆位置不变原地掉头时，有如下图的情况：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dubins曲线</div>
</center>
<br>
<p>图A和图B都可以达到目标。根据Dubins曲线的定义，我们知道图B是最优的路线，同时存在另外一条曲线图C，也能达到相同的效果。但这和我们平时开车显然不同，因为我们的车辆是可以挂倒挡的。我们选择图D代表的路径，可以更快达到目标。</p>
<p>相较于Dubins曲线，RS曲线有了进一步的约束条件：要求每个点的曲率半径都大于1，同时允许车辆可以后退。当车辆可以倒退时，路径有可能被缩短变得更优，而此时Dubins曲线将不能满足要求。作者对RS曲线的所有可能进行了简化表示，在文中做出了一系列定义，我按照文章的顺序进行讲解。</p>
<p><strong>2. 定义</strong></p>
<p>•C代表左转（L）或右转（R），S代表沿直线行驶。C^+和C^-的上标代表前进和后退（在车辆里就是换档）。因此，CC代表的就是LR或者RL。</p>
<p>• $C^+<em>{\pi/2}$ 代表前进方向弧长走 $\pi/2$，C^-</em>{-\pi/2}代表后退方向弧长走\pi/2。所以再通用一些，C^\pm_t表达式中的C可以代表L、R或者S，上标代表前进方向，下标t是和上标正负号相同的代表弧长的值。</p>
<p>•C_uC_u这两个需要连在一起出现，代表两段弧长相等。</p>
<p>在诸如C^+_tC^-_uC^-_vv^+_w这样的路径族（由四段组成的路径，同样的还有C^-_tC^+_uC^+_vv^-_w……等）中，自由参数t,u,v,w一共有4个参数，比条件的数量（目标位置和角度）多了1个，因此对于给定的终点条件通常存在多个解。我们对族中的路径进行优化，能够得到一个额外的方程，结果就是 v = u，或v = π/2。这样看参数仍然是三个。</p>
<p>作者在后面会证明，确实还有类似于C^+C^-C^+C^-C^+C^-这样的路径存在，这样的路径并不在作者提到的形式之中。但是这种路径往往在我们的形式中有相同的替代路径。对于不在作者集合中的路径，将不会出现在简单的前向场景中。</p>
<p>作者提出了两种描述方式，一种是用\pm代表的正负方向</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>另一种是用 | 代表的档位变化</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>以车辆为例，如果用真实的左打轮和右打轮，加上档位变化，一共会有48种不同的路径，如下图。由于有一些路径会有多个公式来表达，所以这48种路径最多会有68个公式。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>Dubins曲线和我们的证明方式不同。Dubins证明了任何终点条件下都会存在路径，即问题的下限被确定了。同时，任何路径长度小于 \pi/8的路径一定是一个CSC类型的路径。然后容易得到，每个路径必须由有限的C和S组成，最终路径简化为了CSC和CCC两种可能。作者没有能够用Dubins的方法来解决本文章提到的车辆反向运动问题。作者在第7节证明了在不增加路径长度的情况下，最多5段就能表示路径，证明方法与Dubins类似。</p>
<p>作者除了用数学公式来推导证明，还用计算机来做了验证：设置了一个包含很多条子路径的集合W，随机出起始条件和终止条件，如果在集合W中找到了两条路径就能连接起点终点，那么集合W显然就是不充分的，就要对集合W进行一些修剪。最终用这种方式得到了一个最小的有效集合W。一旦猜测出了W，作者再次使用计算机来帮助进行大量情况下的广泛代数运算，以验证上述方法可以给出严格的证明。最后，作者发现证明可以简化为上图，这样普通人可以在没有计算机检查细节的情况下轻松理解。但作者认为，如果没有使用计算机，他们永远不可能找到正确的子路径集。（这实际上表明作者是在计算机辅助下，成功地找到了RS曲线的解，并通过广泛的代数运算验证了他们的方法。最终，他们再反过来简化证明过程，最终让RS曲线更易于理解。）</p>
<h2 id="3-等效运算">3. 等效运算</h2>
<p>作者提到，并不是每次都需要计算48种路径，因为存在一些基本变换规则可以简化计算</p>
<ul>
<li>第一种等效运算：”timeflip”——时间变换</li>
</ul>
<p>路径 $l^+ r^- s^- l^-$ 和路径 l^-r^+s^+l^+ 之间的关系就是时间变换，可以看出其实就是前进和后退动作替换了一下</p>
<p>当我们想求沿路径 l^-r^+s^+l^+ 从 [0,0,0] 到 [x,y,\phi] 时 ，我们可以求沿路径 l^+r^-s^-l^-从 [0,0,0] 到 [-x,y,-\phi]，求出的路径 l^+r^-s^-l^-中每段走的弧长，就是待求路径 l^-r^+s^+l^+ 的弧长结果</p>
<ul>
<li>第二种等效运算：“reflect”——反射变换</li>
</ul>
<p>路径 l^+r^-s^-l^- 和路径 r^+l^-s^-r^- 之间的关系就是反射变换，可以看出其实就是向左和向右替换了一下</p>
<p>当我们想求沿路径 r^+l^-s^-r^- 从 [0,0,0] 到 [x,y,\phi] 时 ，我们可以求沿路径 l^+r^-s^-l^-从 [0,0,0] 到 [x,-y,-\phi]，求出的路径 l^+r^-s^-l^-中每段走的弧长，就是待求路径 r^+l^-s^-r^- 的弧长结果</p>
<ul>
<li>第三种等效运算：“backwards”——反向变换</li>
</ul>
<p>路径 l^+r^-s^-l^- 和路径 l^-s^-r^-l^+ 之间的关系就是反向变换，可以看出其实两条路径前后顺序颠倒了一下</p>
<p>当我们想求沿路径 l^-s^-r^-l^+ 从 [0,0,0] 到 [x,y,\phi] 时 ，我们可以求沿路径 l^+r^-s^-l^-从 [0,0,0] 到 [xcos\phi+ysin\phi,xsin\phi-ycos\phi,\phi]，求出的路径 l^+r^-s^-l^-中每段走的弧长，就是待求路径 l^-s^-r^-l^+ 的弧长结果</p>
<p>上面提到了三种变换关系来简化运算，简单理解一下就是：</p>
<p>从 [0,0,0] 到 [x,y,\phi] 需要求解的三个路径 l^-r^+s^+l^+、r^+l^-s^-r^- 、l^-s^-r^-l^+，通过分别改变终点的坐标，均可以通过求解路径 l^+r^-s^-l^- 得到每段轨迹的弧长或前进距离</p>
<h2 id="4-流程与代码">4. 流程与代码</h2>
<p>与Dubins曲线一致，RS曲线每种路径也有对应的公式可以计算。</p>
<p>第一步：起始点坐标变换，令起点坐标变为(0,0,0)，终点的坐标转换为起点坐标系下的坐标，从而简化后续计算</p>
<p>q0 = [sx, sy, syaw]  # 起点:x,y,yaw</p>
<p>q1 = [gx, gy, gyaw]  # 终点:x,y,yaw</p>
<p>dx = q1[0] - q0[0]</p>
<p>dy = q1[1] - q0[1]</p>
<p>dth = q1[2] - q0[2]</p>
<p>c = math.cos(q0[2])</p>
<p>s = math.sin(q0[2])</p>
<p>x = (c * dx + s * dy) * max_curvature</p>
<p>y = (-s * dx + c * dy) * max_curvature</p>
<p># 起点变成了(0,0,0),终点坐标变成了(x, y, dth)</p>
<p>第二步：计算路径</p>
<p>利用三种基本定理，计算全部的路径</p>
<p>第三步：选择路径</p>
<p>选择最优的路径，并对生成的路径进行差值，得到路径上的每一个路径点</p>
<h2 id="5-总结">5. 总结</h2>
<p>原理基本上很清楚，暂时对可选路径计算的优化没有深究，之后再说。</p>
<p>ref: <a href="https://mp.weixin.qq.com/s/RfAEnFtUW7KkG7cSPqmWUw"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/RfAEnFtUW7KkG7cSPqmWUw<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>