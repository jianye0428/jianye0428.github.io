<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>所有文章 - yejian's blog</title><link>https://jianye0428.github.io/posts/</link><description>所有文章 | yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sat, 04 May 2024 17:42:03 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>RL学习笔记 [2] | 马尔科夫决策过程(MDP)</title><link>https://jianye0428.github.io/posts/rl_learning_note_2/</link><pubDate>Wed, 21 Feb 2024 10:38:11 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_2/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。</p>
<p>MDP这一篇对应Sutton书的第三章和UCL强化学习课程的第二讲。</p>
<h1 id="1-强化学习引入mdp的原因">1. 强化学习引入MDP的原因</h1>
<p>对于马尔科夫性本身，我之前讲过的<a href="http://www.cnblogs.com/pinard/p/6945257.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（一）HMM模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="http://www.cnblogs.com/pinard/p/7048333.html"target="_blank" rel="external nofollow noopener noreferrer">条件随机场CRF(一)从随机场到线性链条件随机场<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>以及<a href="http://www.cnblogs.com/pinard/p/6632399.html"target="_blank" rel="external nofollow noopener noreferrer">MCMC(二)马尔科夫链<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>都有讲到。它本身是一个比较简单的假设，因此这里就不专门对“马尔可夫性”做专门的讲述了。</p>
<p>除了对于环境的状态转化模型这个因素做马尔科夫假设外，我们还对强化学习第四个要素个体的策略(policy) $π$ 也做了马尔科夫假设。即在状态 $s$ 时采取动作 $a$ 的概率仅与当前状态 $s$ 有关，与其他的要素无关。用公式表示就是</p>
<p>$$\pi(a\mid s)=P(A_{t}=a\mid S_{t}=s)$$</p>
<p>对于第五个要素，价值函数 $v_π(s)$ 也是一样, $v_π(s)$ 现在仅仅依赖于当前状态了，那么现在价值函数 $v_π(s)$ 表示为:</p>
<p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(G_{t}|S_{t}=s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)$$</p>
<p>其中，$G_t$ 代表收获(return), 是一个MDP中从某一个状态 $S_t$ 开始采样直到终止状态时所有奖励的有衰减的之和。</p>
<h1 id="2-mdp的价值函数与贝尔曼方程">2. MDP的价值函数与贝尔曼方程</h1>
<p>对于MDP，我们在第一节里已经讲到了它的价值函数 $v_π(s)$ 的表达式。但是这个表达式没有考虑到所采用的动作$a$带来的价值影响，因此我们除了 $v_π(s)$ 这个<strong>状态价值函数</strong>外，还有一个<strong>动作价值函数</strong> $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=\operatorname{E}_{\pi}(G_{t}|S_{t}=s,A_{t}=a)=\operatorname{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s,A_{t}=a)$$</p>
<p>根据价值函数的表达式，我们可以推导出价值函数基于状态的递推关系，比如对于状态价值函数 $v_π(s)$，可以发现：</p>
<p>$$\begin{aligned}
V_{\pi}(s)&amp; =\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)  \
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)|S_{t}=s) \
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma G_{t+1}|S_{t}=s) \
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s)
\end{aligned}$$</p>
<p>也就是说，在 $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 是满足递推关系的，即：</p>
<p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})\mid S_{t}=s)$$
　　　　
这个递推式子我们一般将它叫做<strong>贝尔曼方程</strong>。这个式子告诉我们，一个状态的价值由该状态的奖励以及后续状态价值按一定的衰减比例联合组成。</p>
<p>同样的方法，我们可以得到动作价值函数 $q_π(s,a)$ 的贝尔曼方程：</p>
<p>$$q_{\pi}(s,a)=\mathrm{E}_{\pi}(R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})\mid S_{t}=s,A_{t}=a)$$</p>
<h1 id="3-状态价值函数与动作价值函数的递推关系">3. 状态价值函数与动作价值函数的递推关系</h1>
<p>根据动作价值函数 $q_π(s,a)$ 和状态价值函数 $v_π(s)$ 的定义，我们很容易得到他们之间的转化关系公式：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$</p>
<p>也就是说，状态价值函数是所有动作价值函数基于策略 $π$ 的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值。</p>
<p>反过来，利用上贝尔曼方程，我们也很容易从状态价值函数 $v_π(s)$ 表示动作价值函数 $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=R_{s}^{a}+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s&rsquo;)$$</p>
<p>通俗说就是状态动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。</p>
<p>这两个转化过程也可以从下图中直观的看出：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">状态价值函数</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">动作价值函数</div>
</center>
<br>
<p>把上面两个式子互相结合起来，我们可以得到：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;}))$$</p>
<p>$$q_\pi(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\sum_{a&rsquo; \in A}\pi(a&rsquo; \mid s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)$$</p>
<h1 id="4-最优价值函数">4. 最优价值函数</h1>
<p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略我们可以用 $π^*$表示。一旦找到这个最优策略$π^∗$，那么我们就解决了这个强化学习问题。一般来说，比较难去找到一个最优策略，但是可以通过比较若干不同策略的优劣来确定一个较好的策略，也就是局部最优解。</p>
<p>如何比较策略的优劣呢？一般是通过对应的价值函数来比较的，也就是说，寻找较优策略可以通过寻找较优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者，即：</p>
<p>$$v^{\ast}(s)=\max_{\pi} v_{\pi}(s)$$</p>
<p>同理也可以定义最优动作价值函数是所有策略下产生的众多动作状态价值函数中的最大者，即：</p>
<p>$$q^{\ast}(s,a)=\max_\pi q_\pi(s,a)$$</p>
<p>对于最优的策略，基于动作价值函数我们可以定义为：</p>
<p>$$\pi ^{\ast} (a|s)=\begin{cases}1&amp;\mathrm{if~}a=\mathrm{arg~}\max_{a\in A}q*(s,a)\\\\0&amp;\mathrm{else}&amp;\end{cases}$$</p>
<p>只要我们找到了最大的状态价值函数或者动作价值函数，那么对应的策略 $π^*$ 就是我们强化学习问题的解。同时，利用状态价值函数和动作价值函数之间的关系，我们也可以得到:</p>
<p>$$v^{\ast}(s)=\max_aq ^{\ast} (s,a)$$</p>
<p>反过来的最优价值函数关系也很容易得到：</p>
<p>$$q_{<em>}(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss}^{a}{}_{</em>}(\mathrm{s&rsquo;})$$</p>
<p>利用上面的两个式子也可以得到和第三节末尾类似的式子：</p>
<p>$$v_<em>(s)=\max_a(R_s^a+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^a\nu_</em>(s&rsquo;))$$</p>
<p>$$q_<em>(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\max_{a&rsquo;}q_</em>(s&rsquo;,a&rsquo;)$$</p>
<h1 id="5-mdp实例">5. MDP实例</h1>
<p>上面的公式有点多，需要一些时间慢慢消化，这里给出一个UCL讲义上实际的例子，首先看看具体我们如何利用给定策略来计算价值函数。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP 举例</div>
</center>
<br>
<p>例子是一个学生学习考试的MDP。里面左下那个圆圈位置是起点，方框那个位置是终点。上面的动作有study, pub, facebook, quit, sleep，每个状态动作对应的即时奖励R已经标出来了。我们的目标是找到最优的动作价值函数或者状态价值函数，进而找出最优的策略。</p>
<p>为了方便，我们假设衰减因子 $γ=1$, $π(a|s)=0.5$。</p>
<p>对于终点方框位置，由于其没有下一个状态，也没有当前状态的动作，因此其状态价值函数为0。对于其余四个状态，我们依次定义其价值为<em>v</em>1,<em>v</em>2,<em>v</em>3,<em>v</em>4， 分别对应左上，左下，中下，右下位置的圆圈。我们基于$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}v_{\pi}(s&rsquo;))$计算所有的状态价值函数。可以列出一个方程组。</p>
<ul>
<li>
<p>对于<em>v</em>1位置，我们有：$v_1=0.5*(-1+v_1)+0.5*(0+v_2)$</p>
</li>
<li>
<p>对于<em>v</em>2位置，我们有：$v_2=0.5*(-1+v_1)+0.5*(-2+v_3)$</p>
</li>
<li>
<p>对于<em>v</em>3位置，我们有：$v_3=0.5*(0+0)+0.5*(-2+v_4)$</p>
</li>
<li>
<p>对于<em>v</em>4位置，我们有：$v_4=0.5*(10+0)+0.5*(1+0.2<em>v_2+0.4</em>v_3+0.4*v_4)$</p>
</li>
</ul>
<p>解出这个方程组可以得到 $v_1=−2.3$, $v_2=−1.3$, $v_3=2.7$, $v_4=7.4$, 即每个状态的价值函数如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>上面我们固定了策略$ π(a|s)$, 虽然求出了每个状态的状态价值函数，但是却并不一定是最优价值函数。那么如何求出最优价值函数呢？这里由于状态机简单，求出最优的状态价值函数 $v*(s)$ 或者动作价值函数 $q*(s,a)$ s比较容易。</p>
<p>我们这次以动作价值函数 $q*(s,a)$ 来为例求解。首先终点方框处的好求。</p>
<p>$$q*(s_3,sleep)=0,q*(s_4,study)=10$$</p>
<p>接着我们就可利用 $q*(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\max_{a&rsquo;}q*(s&rsquo;,a&rsquo;)$ 列方程组求出所有的 $q∗(s,a)$ 。有了所有的 $q^{\ast}(s,a)$,利用 $v_{<em>}(s)=\max_{a}q</em>(s,a)$ 就可以求出所有的 $v∗(s)$。最终求出的所有 $v∗(s)$ 和 $q∗(s,a)$ 如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>从而我们的最优决策路径是走6-&gt;6-&gt;8-&gt;10-&gt;结束。　　　　</p>
<h1 id="6-mdp小结">6. MDP小结</h1>
<p>MDP是强化学习入门的关键一步，如果这部分研究的比较清楚，后面的学习就会容易很多。因此值得多些时间在这里。虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。</p>
<p>下一篇讨论用动态规划的方法来求解强化学习的问题。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9426283.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>强化学习 | 深度解读Soft Actor-Critic 算法</title><link>https://jianye0428.github.io/posts/sac/</link><pubDate>Sat, 04 May 2024 17:42:03 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/sac/</guid><description><![CDATA[<h1 id="深度解读soft-actor-critic-算法">深度解读Soft Actor-Critic 算法</h1>
<h2 id="1-前言">1 前言</h2>
<p>机器人学习Robot Learning正在快速的发展，其中深度强化学习deep reinforcement learning（DRL），特别是面向连续控制continous control的DRL算法起着重要的作用。在这一领域中，目前可以说有三类行之有效的model free DRL算法：</p>
<ul>
<li>TRPO,PPO</li>
<li>DDPG及其拓展（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.08617"target="_blank" rel="external nofollow noopener noreferrer">D4PG<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>,TD3等）</li>
<li>Soft Q-Learning, Soft Actor-Critic</li>
</ul>
<p><strong><font color=red>PPO</font></strong> 算法是目前最主流的DRL算法，同时面向离散控制和连续控制，在<a href="https://en.wikipedia.org/wiki/OpenAI_Five"target="_blank" rel="external nofollow noopener noreferrer">OpenAI Five<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>上取得了巨大成功。但是PPO是一种on-policy的算法，也就是PPO面临着严重的sample inefficiency，需要巨量的采样才能学习，这对于真实的机器人训练来说，是无法接受的。</p>
<p><strong><font color=red>DDPG</font></strong> 及其拓展则是DeepMind开发的面向连续控制的off policy算法，相对PPO 更sample efficient。<strong>DDPG训练的是一种确定性策略deterministic policy，即每一个state下都只考虑最优的一个动作</strong>。DDPG的拓展版D4PG从paper中的结果看取得了非常好的效果，但是并没有开源，目前github上也没有人能够完全复现Deepmind的效果。</p>
<p><strong><font color=red>Soft Actor-Critic (SAC)</font></strong> 是面向Maximum Entropy Reinforcement learning 开发的一种off policy算法，和DDPG相比，Soft Actor-Critic使用的是随机策略stochastic policy，相比确定性策略具有一定的优势（具体后面分析）。Soft Actor-Critic在公开的benchmark中取得了非常好的效果，并且能直接应用到真实机器人上。最关键的是，Soft Actor-Critic是完全开源的，因此，深入理解Soft Actor-Critic 算法具有非常重要的意义，也是本篇blog的目的。</p>
<p>Soft Actor-Critic算法相关链接：</p>
<p>Paper：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.01290"target="_blank" rel="external nofollow noopener noreferrer">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.05905"target="_blank" rel="external nofollow noopener noreferrer">Soft Actor-Critic Algorithms and Applications<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08165"target="_blank" rel="external nofollow noopener noreferrer">Reinforcement Learning with Deep Energy-Based Policies<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (Soft Q-Learning)</li>
</ul>
<p>Codes:</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/rail-berkeley/softlearning"target="_blank" rel="external nofollow noopener noreferrer">rail-berkeley/softlearning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (原作者实现）</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/vitchyr/rlkit"target="_blank" rel="external nofollow noopener noreferrer">vitchyr/rlkit<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/openai/spinningup"target="_blank" rel="external nofollow noopener noreferrer">openai/spinningup<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/hill-a/stable-baselines"target="_blank" rel="external nofollow noopener noreferrer">hill-a/stable-baselines<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p>下面我们来详细解读一下SAC的算法及其具体实现。本文的阅读需要有基本的DRL算法基础知识。</p>
<h2 id="2-为什么研究-maximum-entropy-reinforcement-learning">2 为什么研究 Maximum Entropy Reinforcement Learning？</h2>
<p>对于一般的DRL，学习目标很直接，就是学习一个policy使得累加的reward期望值最大：</p>
<p>$$\pi^*=\arg\max_\pi\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[\sum_tR(s_t,a_t)]\tag{1}$$</p>
<p>而最大熵RL，除了上面的基本目标，还要求policy的每一次输出的action 熵entropy最大：</p>
<p>$$\pi^*=\arg\max_\pi\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[\sum_t\underbrace{R(s_t,a_t)}_{reward}+\alpha\underbrace{H(\pi(\cdot|s_t))}_{entropy}]\tag{2}$$</p>
<p>这样做的基本目的是什么呢？让策略随机化，即输出的每一个action的概率尽可能分散，而不是集中在一个action上。不了解entropy的同学可以看这里：<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%86%B5_%28%E4%BF%A1%E6%81%AF%E8%AE%BA%29"target="_blank" rel="external nofollow noopener noreferrer">wiki-信息熵<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>我们知道DDPG训练得到的是一个deterministic policy确定性策略，也就是说这个策略对于一种状态state只考虑一个最优的动作。所以，stochastic policy相对deterministic policy有什么优势呢？</p>
<p>Stochastic policy随机策略在实际机器人控制上往往是更好的做法。比如我们让机器人抓取一个水杯，机器人是有无数条路径去实现这个过程的，而并不是只有唯一的一种做法。因此，我们就需要drl算法能够给出一个随机策略，在每一个state上都能输出每一种action的概率，比如有3个action都是最优的，概率一样都最大，那么我们就可以从这些action中随机选择一个做出action输出。<strong>最大熵maximum entropy的核心思想就是不遗落到任意一个有用的action，有用的trajectory</strong>。对比DDPG的deterministic policy的做法，看到一个好的就捡起来，差一点的就不要了，而最大熵是都要捡起来，都要考虑。</p>
<p><strong>基于最大熵的RL算法有什么优势？</strong></p>
<p>以前用deterministic policy的算法，我们找到了一条最优路径，学习过程也就结束了。现在，我们还要求熵最大，就意味着神经网络需要去explore探索所有可能的最优路径，这可以产生以下多种优势：</p>
<ul>
<li>
<p>1）学到policy可以作为更复杂具体任务的初始化。因为通过最大熵，policy不仅仅学到一种解决任务的方法，而是所有all。因此这样的policy就更有利于去学习新的任务。比如我们一开始是学走，然后之后要学朝某一个特定方向走。</p>
</li>
<li>
<p>2）更强的exploration能力，这是显而易见的，能够更容易的在多模态reward （multimodal reward）下找到更好的模式。比如既要求机器人走的好，又要求机器人节约能源</p>
</li>
<li>
<p>3）更robust鲁棒，更强的generalization。因为要从不同的方式来探索各种最优的可能性，也因此面对干扰的时候能够更容易做出调整。（干扰会是神经网络学习过程中看到的一种state，既然已经探索到了，学到了就可以更好的做出反应，继续获取高reward）</p>
</li>
</ul>
<p>既然最大熵RL算法这么好，我们当然应该研究它了。而实际上，在之前的DRL算法<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.01783"target="_blank" rel="external nofollow noopener noreferrer">A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们其实已经用了一下最大熵：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在训练policy的时候，A3C加了entropy项，作为一个regularizer，让policy更随机。不过A3C这么做主要是为了更好做exploration，整体的训练目标依然只考虑reward。这和Soft Actor-Critic的设定还是不一样的，Soft Actor-Critic是真正最大熵DRL算法。</p>
<h2 id="3-maximum-entropy-reinforcement-learning的bellman方程">3 Maximum Entropy Reinforcement Learning的Bellman方程</h2>
<p>我们先回顾一下dynamic programming中Bellman backup equation，参考<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"target="_blank" rel="external nofollow noopener noreferrer">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>$$q_\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})q_\pi(s^{\prime},a^{\prime})\tag{3}$$</p>
<p>那么对于最大熵（MaxEnt)的目标，其实可以把熵也作为reward的一部分，我们在计算q值时（记住q是累加reward的期望，传统rl的目标等价于让q最大），就需要计算每一个state的熵entropy (entropy的公式如下图所示）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>因此我们就可以得到Soft Bellman Backup equation (Entropy项)额外乘上 $\alpha$ 系数：</p>
<p>$$q_\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})(q_\pi(s^{\prime},a^{\prime})-\alpha\log(\pi(a^{\prime}|s^{\prime}))\quad(4)$$</p>
<p>Recall一下<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf"target="_blank" rel="external nofollow noopener noreferrer">Dynamic Programming Backup<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
对应Q值的公式是
<p>$$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q(s_{t+1},a_{t+1})]\tag{5}$$</p>
<p>根据公式（4），我们可以得到Soft Bellman Backup的 更新公式：</p>
<p>$$Q_{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]\tag{6}$$</p>
<p>上面公式（6）是直接使用dynamic programming，将entropy嵌入计算得到的结果。我们可以反过来先直接把entropy作为reward的一部分：</p>
<p>$$r_{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))\tag{7}$$</p>
<p>我们将（7）带入到公式（5）：</p>
<p>$$\begin{aligned}
{Q_{soft}(s_{t},a_{t})} &amp;=r(s_t,a_t)+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})]\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho,a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho,a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\gamma\mathbb{E}_{s_{t+1}\sim\rho}\mathbb{E}_{a_{t+1}\sim\pi}[-\alpha\log\pi(a_{t+1}|s_{t+1})]\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho}[\mathbb{E}_{a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]]\\&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]\end{aligned}$$</p>
<p>可以得到一样的结果。</p>
<p>与此同时，我们知道:</p>
<p>$$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho}[V(s_{t+1})]\tag{9}$$</p>
<p>因此，我们有：</p>
<p>$$V_{soft}(s_t)=\mathbb{E}_{a_t\sim\pi}[Q_{soft}(s_t,a_t)-\alpha\log\pi(a_t|s_t)]\tag{10}$$</p>
<p>至此我们理清楚了SAC paper原文中的公式(2)和(3)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>并且（7）的做法直接证明了Lemma 1 Soft Policy Evaluation (<strong>这个lemma为下一部分的soft policy iteration提供支撑</strong>）:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>但是，我们注意到上面的整个推导过程都是围绕maximum entropy，和soft 好像没有什么直接关系。所以，</p>
<p><strong>为什么称为soft？哪里soft了？以及为什么soft Q function能够实现maximum entropy？</strong></p>
<p>理解清楚这个问题是理解明白soft q-learning及sac的关键！</p>
<p>SAC这篇paper直接跳过了soft Q-function的定义问题，因此，要搞清楚上面的问题，我们从Soft Q-Learning的paper来寻找答案。</p>
<p>参考<a href="https://link.zhihu.com/?target=https%3A//bair.berkeley.edu/blog/2017/10/06/soft-q-learning/"target="_blank" rel="external nofollow noopener noreferrer">Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上面的曲线很明显的说明了stochastic policy的重要性，面对多模的（multimodal）的Q function，传统的RL只能收敛到一个选择（左图），而更优的办法是右图，让policy也直接符合Q的分布。这里，最直接的一种办法就是定义这样的energy-based policy：</p>
<p>\pi(a_t|s_t)\propto exp(-\mathcal{E}(s_t,a_t)) （11）</p>
<p>其中 \mathcal{E} 是能量函数，上面的形式就是Boltzmann Distribution <a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E5%88%86%E5%B8%83"target="_blank" rel="external nofollow noopener noreferrer">玻尔兹曼分布<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 。下图的 -f(x)=\mathcal{E}</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><a href="https://deepgenerativemodels.github.io/assets/slides/cs236_lecture13.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://deepgenerativemodels.github.io/assets/slides/cs236_lecture13.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>为了连接soft Q function，我们可以设定</p>
<p>$$\mathcal{E}(s_t,a_t)=-\frac{1}{\alpha}Q_{soft}(s_t,a_t)\tag{12}$$</p>
<p>因此，我们有</p>
<p>$$\pi(a_t|s_t)\propto exp(Q_{soft}(s_t,a_t))\tag{13}$$</p>
<p>这样的policy能够为每一个action赋值一个特定的概率符合Q值的分布，也就满足了stochastic policy的需求。</p>
<p>下面我们要<strong>发现(13)的形式正好就是最大熵RL的optimal policy最优策略的形式，而这实现了soft q function和maximum entropy的连接。</strong></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>实际上我们理解Soft Q-Learning及Soft Actor Critic，要清楚上图三者的关系。在Soft Q-Learning那篇paper中，他是从Soft Value Function的定义出发去连接Energy-Based Policy 和Maximum Entropy Objective的关系。而在本blog中，我们从Maximum Entropy Objective出发，来连接其他两部分。</p>
<p>前面我们已经推导得到了公式（10），那么根据公式（10），我们可以直接推导得到policy的形式：</p>
<p>$$\begin{aligned}\pi(s_{t},a_{t})&amp;=\exp(\frac1\alpha(Q_{soft}(s_t,a_t)-V_{soft}(s_t)))\\&amp;&amp;\text{(14)}\\&amp;=\frac{\exp(\frac1\alpha Q_{soft}(s_t,a_t))}{\exp(\frac1\alpha V_{soft}(s_t))}\end{aligned}$$</p>
<p>（14）符合了（13）， $\frac{1}{\alpha}V_{soft}(s_t)$ 可以看做是对应的log partition function. 由此，就连接了Maximum Entropy Objective和Energy Based Policy的关系。</p>
<p>下面我们要连接Soft Value Function。从（14）的 $\frac{1}{\alpha}V_{soft}(s_t)$ 已经很明显了：</p>
<p>$$\exp(\frac1\alpha V_{soft}(s_t))=\int\exp(\frac1\alpha Q_{soft}(s_t,a))d\mathbf{a} (15)$$</p>
<p>因此，我们可以定义 $V_{soft}(s_t)$ :</p>
<p>$$V_{soft}(s_t)\triangleq\alpha\log\int\exp(\frac1\alpha Q_{soft}(s_t,a))d\mathbf{a}\tag{16}$$</p>
<p>这和soft 有什么关系呢？(16）其实是LogSumExp的积分形式，就是smooth maximum/soft maximum (软的最大）。参考<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/LogSumExp"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/LogSumExp<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>所以就可以定义</p>
<p>$$\mathrm{soft}\max_af(a):=\log\int\exp f(a)da\tag{17}$$</p>
<p>因此我们也就可以根据公式（9）定义soft的Q-function：</p>
<p>$$Q_{soft}(s_t,a_t)=\mathbb{E}\left[r_t+\gamma\text{ soft}\max_aQ(s_{t+1},a)\right]\text{(18)}$$</p>
<p>所以，为什么称为soft是从这里来的。</p>
<p>这里有一个常见的疑问就是这里的soft max和我们常见的softmax好像不一样啊。是的，我们在神经网络中常用的activation function softmax 实际上是soft argmax，根据一堆logits找到对应的软的最大值对应的index。具体参看：<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Softmax_function"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/Softmax_function<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>上面的推导还只是面向policy的value和Q，我们下面要说明optimal policy也必然是energy-based policy的形式。</p>
<p>这一部分的证明依靠 Policy improvement theorem：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>具体证明过程见soft q-learning原文的A.1。</p>
<p>有了Theorem 4，</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>我们就可以看到optimal policy必然是energy based policy，也因此，我们有了soft q learning paper中最开始的定义：</p>
<p>$$\pi_{MaxEnt}^<em>(a_t|s_t)=\exp(\frac{1}{\alpha}(Q_{soft}^</em>(s_t,a_t)-V_{soft}^*(s_t)))\text{(19)}$$</p>
<h2 id="4-policy-iteration">4 Policy Iteration</h2>
<p>理清楚了上面的基本定义和联系，我们就可以研究怎么更新policy了，也就是policy iteration。</p>
<p>回顾一下一般的<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"target="_blank" rel="external nofollow noopener noreferrer">Policy Iteration<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在两步中进行循环迭代（我们直接使用Q值来说明）：</p>
<ol>
<li>Policy evaluation：固定policy，使用Bellman方程更新Q值直到收敛：</li>
</ol>
<p>$$Q_\pi(s,a)=r(s,a)+\lambda\mathbb{E}_{s^{\prime},a^{\prime}}Q_\pi(s^{\prime},a^{\prime})\tag{20}$$</p>
<ol start="2">
<li>Policy improvement: 更新policy：</li>
</ol>
<p>$$\pi^{\prime}(s)=\arg\max_aQ_\pi(s,a)\tag{21}$$</p>
<p>基于同样的方法，我们有Soft Policy Iteration：</p>
<ol>
<li>Soft policy evaluation:固定policy，使用Bellman方程更新Q值直到收敛:</li>
</ol>
<p>$$\begin{aligned}&amp;Q_{soft}^\pi(s_t,a_t)=r(s_t,a_t)+\lambda\mathbb{E}_{s_{t+1},a_{t+1}}\left[Q_{soft}^\pi(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))\right]\tag{22}\end{aligned}$$</p>
<ol start="2">
<li>Soft policy improvement: 更新policy：</li>
</ol>
<p>$$\pi^{\prime}=\arg\min_{\pi_k\in\Pi}D_{KL}(\pi_k(\cdot|s_t)||\frac{\exp(\frac{1}{\alpha}Q_{soft}^{\pi}(s_t,\cdot))}{Z_{soft}^{\pi}(s_t)}) \tag{23}$$</p>
<p>(22)基于上一部分说的Lemma 1 Soft Policy Evaluation, 可收敛。</p>
<p>(23)则基于上一部分的Theorem 4 Policy Improvement Theorem。只是这里的做法不是直接赋值，而是通过KL divergence来趋近 $\exp(Q^{\pi}_{soft}(s_t,\cdot))$ 。在SAC的paper原文中，我们可以看到这么做的原因是为了限制policy在一定范围的policies $\Pi$ 中从而tractable，policy的分布可以是高斯分布。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>同样的，作者也专门证明了采用KL divergence的方法一样能够保证policy improvement，也就是Lemma 2：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>最后，就是证明上面的Soft Policy Iteration过程能保证policy收敛到最优，即Theorem 1：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>由此，基本的理论建设也就结束了，下面进入Soft Actor-Critic的算法设计。</p>
<h2 id="5-soft-actor-critic">5 Soft Actor-Critic</h2>
<p>SAC算法的构建首先是神经网络化，我们用神经网络来表示Q和Policy： $Q_{\theta}(s_t,a_t)$ 和 $\pi_{\phi}(a_t|s_t)$ 。Q网络比较简单，几层的MLP最后输出一个单值表示Q就可以了，Policy网络需要输出一个分布，一般是输出一个Gaussian 包含mean和covariance。下面就是构建神经网络的更新公式。</p>
<p>对于Q网络的更新，我们根据（10）可以得到：</p>
<p>$$\begin{aligned}
J_{Q}(\theta)&amp; =\mathbb{E}_{(s_t,a_t,s_{t+1})\sim\mathcal{D}}[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma V_{\bar{\theta}}(s_{t+1})))^2] \\
&amp;=\mathbb{E}_{(s_t,a_t,s_{t+1})\sim\mathcal{D},a_{t+1}\sim\pi_\phi}[\frac12(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma(Q_{\bar{\theta}}(s_{t+1},a_{t+1})-\alpha\log(\pi_\phi(a_{t+1}|s_{t+1})))))^2] \tag{24}
\end{aligned}$$</p>
<p>这里和DDPG一样，构造了一个target soft Q 网络带参数 $\overline{\theta}$ ，这个参数通过exponentially moving average Q网络的参数 $\theta$ 得到。(ps:在第一个版本的SAC中，他们单独定义了V网络进行更新，说是更稳定，到新版的SAC中，由于自动更新temperature $\alpha$ 就直接使用Q网络更新）</p>
<p>对于Policy 网络参数的更新，就是最小化KL divergence：</p>
<p>$$\begin{aligned}
J_{\pi}(\phi)&amp; =D_{\mathrm{KL}}\left(\pi_\phi(.\left|s_t\right)|\exp(\frac1\alpha Q_\theta(s_t,.)-\log Z(s_t))\right) \\
&amp;=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}\Big[\log\big(\frac{\pi_\phi(a_t|s_t)}{\exp(\frac{1}{\alpha}Q_\theta(s_t,a_t)-\log Z(s_t))}\big)\Big] \\
&amp;=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}[\log\pi_\phi(a_t|s_t)-\frac1\alpha Q_\theta(s_t,a_t)+\log Z(s_t)] \tag{25}
\end{aligned}$$</p>
<p>这里的action我们采用reparameterization trick来得到，即</p>
<p>$$a_t=f_\phi(\varepsilon_t;s_t)=f_\phi^\mu(s_t)+\varepsilon_t\odot f_\phi^\sigma(s_t)\tag{26}$$</p>
<p>f函数输出平均值和方差，然后 $\varepsilon$ 是noise，从标准正态分布采样。使用这个trick，整个过程就是完全可微的(loss 乘以 $\alpha$ 并去掉不影响梯度的常量log partition function Z(s_t)) ：</p>
<p>$$J_\pi(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\varepsilon\sim\mathcal{N}}[\alpha\log\pi_\phi(f_\phi(\varepsilon_t;s_t)|s_t)-Q_\theta(s_t,f_\phi(\varepsilon_t;s_t))] \tag{27}$$</p>
<p>这样基本的Soft Actor-Critic的更新方法也就得到了。</p>
<h2 id="6-temperature-hyperparameter-auto-adjustment">6 Temperature Hyperparameter Auto-Adjustment</h2>
<p>前面的SAC中，我们只是人为给定一个固定的temperature $\alpha$ 作为entropy的权重，但实际上由于reward的不断变化，采用固定的temperature并不合理，会让整个训练不稳定，因此，有必要能够自动调节这个temperature。当policy探索到新的区域时，最优的action还不清楚，应该调整temperature $\alpha$ 去探索更多的空间。当某一个区域已经探索得差不多，最优的action基本确定了，那么这个temperature就可以减小。</p>
<p>这里，SAC的作者构造了一个带约束的优化问题，让平均的entropy权重是有限制的，但是在不同的state下entropy的权重是可变的，即</p>
<p>$$\max_{\pi_0,\ldots,\pi_T}\mathbb{E}\bigg[\sum_{t=0}^Tr(s_t,a_t)\bigg]\mathrm{s.t.~}\forall t, \mathcal{H}(\pi_t)\geq\mathcal{H}_0\tag{28}$$</p>
<p>对于这部分内容，<a href="https://link.zhihu.com/?target=https%3A//lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html%23sac"target="_blank" rel="external nofollow noopener noreferrer">Policy Gradient Algorithms<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 这个openai小姐姐的blog介绍得极其清楚，大家可以参考，最后得到temperature的loss：</p>
<p>$$J(\alpha)=\mathbb{E}_{a_t\sim\pi_t}[-\alpha\log\pi_t(a_t\mid\pi_t)-\alpha\mathcal{H}_0]\tag{29}$$</p>
<p>由此，我们可以得到完整的Soft Actor-Critic算法：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>为了更快速稳定的训练，作者引入了两个Q网络，然后每次选择Q值小的一个作为target Q值。更新Q，Policy及 \alpha 使用上文的（24）（27）（29）三个公式。</p>
<h2 id="7-神经网络结构">7 神经网络结构</h2>
<p>虽然上面把算法流程确定了，但是如何构造policy的神经网络还是比较复杂的。下图是带V网络的神经网络结构图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><a href="https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html"target="_blank" rel="external nofollow noopener noreferrer">https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>我们主要来探究一下Policy网络的设计。</p>
<p>见上图右上角的Policy网络，前面的input embedder和Middleware不用说，就是几层的MLP。然后，接下来神经网络分成两个分支，分别输出平均值mean $\mu$ 和log 标准差 log std 。然后使用exp得到std。</p>
<p>$$\pi_\phi(s_t) = \mu_t,\log \sigma_t \tag{30}$$</p>
<p>$$\sigma_t = \exp(\log \sigma_t)$$</p>
<p>正常输出这样的高斯分布作为action 的分布distribution是OK的，但是在实际中，这个action需要限定在一定范围内。因此，这里作者使用了squashing function tanh，将action限制在（-1,1）之间，即</p>
<p>$$\mathbf{u}_t =\mu_t + \varepsilon_t \odot \sigma_t $$</p>
<p>$$a_t = \tanh (\mathbf{u}) \tag{31}$$</p>
<p>这里和上文的公式（26）对应，多了一个tanh。</p>
<p>那么这会导致分布的变化，从而影响log likelihood的计算，而这是我们计算SAC的loss必须的。作者在paper中给出了计算方法如下：</p>
<p>$$\log \pi(a|s)=\log \mu(\mathbf{u}|s)-\sum_{i=1}^{D}{\log(1-\tanh^2(u_i))} \tag{32}$$</p>
<p>其中 u_i 是 $\mathbf{u}$ 的第i个元素。这里的 $\mu(\mathbf{u}|s)$ 是没有加限制时的likelihood function也就是高斯分布的likelihood function似然函数。高斯分布的log likelihood直接使用pytorch的<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/_modules/torch/distributions/normal.html"target="_blank" rel="external nofollow noopener noreferrer">Normal<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> class就可以获得。</p>
<h2 id="8-其他细节">8 其他细节</h2>
<p>1）SAC里的target entropy 设计为</p>
<p>$$\mathcal{H}_0 = -\dim (\mathcal{A}) \tag{33}$$</p>
<p>即-动作数量。</p>
<p>2）SAC paper里完全没有说明的训练时的episode设置。SAC设置为每一个episode采样1000次然后训练1000次。</p>
<p>3）在代码中SAC使用 log alpha作为更新的参数，而不是直接使用alpha如公式（25），这和输出log std是一样的，使用log有很大的正负范围，更方便网络输出。否则alpha或者std都是正值。</p>
<p>4）SAC有一个很大的问题，它的policy的目的是趋近于玻尔兹曼分布，但是实际实现的时候，为了能够tractable，选择了输出一个高斯，也就是让高斯趋近于玻尔兹曼分布。这意味着SAC本质上还是unimodal的算法，而不是soft q-learning的multi-modal。这使得SAC的创新性打了很大的折扣。但是算法效果确实还是不错的。</p>
<h2 id="9-小结">9 小结</h2>
<p>本文从理论到具体实现层面剖析了Soft Actor-Critic这一目前极强的DRL算法，基本上理解了本文的分析，对于代码的实现也就可以了然一胸了。</p>
<p>由于本人水平有限，前面的理论分析恐有错误，望批评指正！</p>
<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/70360272"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/70360272<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>大模型学习笔记 | GPT 系列</title><link>https://jianye0428.github.io/posts/survey/</link><pubDate>Fri, 03 May 2024 16:33:37 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/survey/</guid><description><![CDATA[<h1 id="万字长文cver-转-llm-学习笔记之大模型gpt-系列">万字长文，CVer 转 LLM 学习笔记之大模型GPT 系列</h1>
<h2 id="导读">导读</h2>
<p>本文是作者对 GPT 系列文章的学习笔记，从个人角度梳理了 GPT 系列的迭代逻辑，从技术的逻辑连续性和关联性都有很好的讲解，篇幅较长，建议大家点赞收藏。</p>
<p>这个系列的笔记主要面向像我一样已经具备一定的深度学习基础，但是新接触 NLP 和大模型领域的读者，目的是能提纲挈领地快速把握这个领域的一系列关键工作节点。</p>
<p>这篇笔记涵盖的内容有：</p>
<ul>
<li>GPT-1 论文</li>
<li>GPT-2 论文</li>
<li>GPT-3 论文</li>
<li>InstructGPT 论文（GPT-3.5 背后的技术）</li>
<li>GPT-4 技术报告</li>
<li>GPT-4 微软评测报告</li>
<li>GPT-4V 微软评测报告</li>
</ul>
<p>作为一个从 CV 转到 LLM 的新人，难免犯一些常见或低级的错误，欢迎任何读者及时指出和斧正，也欢迎任何留言讨论。</p>
<h2 id="要点-tldl">要点 (TL;DL)</h2>
<ul>
<li><strong>注重能力，而非过程</strong>：预训练任务其实形式不重要，可以是 classification，可以是预测 next token，真正重要的是 model 和 data 的 scaling up，以此快速高效地得到一个有优异泛化能力的特征提取器:
<ul>
<li>data: 找到/设计一个能把海量的数据用起来的任务，能用的数据越多越好，训练越快越好</li>
<li>model: <strong>模型性能</strong>可以随着参数量巨量地提升而不会快速饱和，一个优秀的模型架构是 scaling up 的基础保障</li>
</ul>
</li>
<li><strong>三种范式</strong>：
<ul>
<li>我们可以将专属任务上微调得到的模型，看成一种用户输入 0 个特殊 token，解决 1 种任务的范式，所以<strong>第一范式</strong>下的每个模型只能用于解决一个专属任务。</li>
<li><strong>第二范式</strong>的模型是为每一种任务准备 1 个特殊 token，因此通过改变输入 token，就能用一个模型解决不同的任务。</li>
<li><strong>第三范式</strong>的模型把特殊 token 替换成了特殊 token 序列，而自然语言正好就是一种最符合人类习惯和直觉的特殊 token 序列。</li>
</ul>
</li>
<li><strong>RLHF</strong>：
<ul>
<li>SFT 模型(16 epoch)</li>
<li>RM</li>
<li>PPO 继续训练出来的最终模型</li>
<li>SFT 数据(13k 条): 人工设计的问题，人工标注答案</li>
<li>Feedback 数据(33k 条): 针对上面人工设计的问题，模型输出的几份答案的排序（打分）</li>
<li>PPO 使用的数据(31k 条): 人工设计的问题(上面的模型没见过的新问题)，用 RM 的评分来继续训练，这份数据不需要人工标注答案</li>
</ul>
</li>
<li><strong>对 GPT-4 的全面探索</strong>：
<ul>
<li><strong>心理学角度</strong>：人类思维是快思考与慢思考两个系统的混合体，而 GPT-4 目前更类似于单纯的快思考</li>
<li>GPT-4 还有哪些<strong>局限性</strong>，以及哪些可以<strong>改进的地方</strong>（见 GPT-4 微软报告）</li>
</ul>
</li>
<li><strong>GPT-4V 的全面探索</strong>：
<ul>
<li>支持哪些输入和工作模式</li>
<li>在不同领域和任务上的能力质量和通用性如何</li>
<li>有效使用和提示方法</li>
<li>未来方向</li>
</ul>
</li>
</ul>
<h2 id="1-improving-language-understanding-by-generative-pre-training-201806">1. Improving Language Understanding by Generative Pre-Training (2018.06)</h2>
<p>GPT 系列的第一篇论文，定下了纯 <strong><font color=red>Transformer-Decoder</font></strong> 路线。</p>
<p>深度学习的早期突破很多来自于 CV 领域，其中很重要的一个原因是 CV 有 ImageNet 这个百万量级的有标注数据集，在 ImageNet 上训练分类任务得到的模型 Backbone 天然就是一个优秀的图片特征提取器，基于这个特征提取器在任意的子任务上做 fine-tuning 效果都不会太差（至少能展现出一定的泛化能力）。而 NLP 领域缺少这样大的数据集，因此，一直以来 NLP 模型发力卡在了特征提取器的构筑上，<font color=red>GPT 提出用训练语言模型的方式来得到这个特征提取器，然后用它来做子任务上的微调。</font></p>
<p>语言模型由于做的是“预测下一个词”这样的一个任务，因此不依赖于人工标注，可以实现海量数据的预训练和泛化。</p>
<p>GPT 工作是在 BERT 之前的，很多的 setting 都被 BERT 直接沿用了，比如 12 层 Transformer，768 的维度，800M 的 BookCorpus 数据集 等。</p>
<p>文章剩余部分介绍了如何在 NLP 四大主流任务类型上运用 GPT，即如何把不同形式的任务都表示成一个序列+对应的标签的形式。</p>
<p>对笔者的启示:</p>
<ul>
<li><strong>注重能力，而非过程</strong>：预训练任务其实形式不重要，可以是 classification，可以是预测 next token，真正重要的是 model 和 data 的 scaling up，以此快速高效地得到一个有优异泛化能力的特征提取器:
<ul>
<li>data: 找到/设计一个能把海量的数据用起来的任务，能用的数据越多越好，训练越快越好</li>
<li>model: 模型性能可以随着参数量巨量地提升而不会快速饱和，一个优秀的模型架构是 scaling up 的基础保障</li>
</ul>
</li>
</ul>
<h2 id="2-language-models-are-unsupervised-multitask-learners-201902">2. Language Models are Unsupervised Multitask Learners (2019.02)</h2>
<p>GPT 系列的第二篇工作。</p>
<p>参数量提升到了 1.5B，也用了更大量的数据。GPT-2 最大的贡献是把研究的重点从单个任务上的针对性调参刷榜，转向了 <strong>zero-shot</strong>，即， <font color=red>训练好的模型不再需要微调就能去做不同任务了，尽管性能上距离每个任务的 SOTA 都还有距离，但方案的可行性已经验证了</font>。要实现这一点，原来为特殊任务准备特殊 token 的做法就不合适了，因为预训练阶段模型是没见过这些 token 的，毕竟语言模型预训练阶段只见过自然语言，所以非常自然地就引出了 prompt 的概念，用自然语言来替代原本的任务 token，实现不同任务的 zero-shot。</p>
<p>可以看到，在这个时候 GPT-2 就已经初具 ChatGPT 的雏形了，只不过用户的输入还不完全是任意自然语言，而是类似于这样的模板输入。：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">翻译任务：
</span></span><span class="line"><span class="cl">(translate to
</span></span><span class="line"><span class="cl">french, english text, french text)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">QA 任务：
</span></span><span class="line"><span class="cl">(answer the question, document,
</span></span><span class="line"><span class="cl">question, answer)</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-language-models-are-few-shot-learners-200514165">3. Language Models are Few-Shot Learners (2005.14165)</h2>
<p>GPT 系列的第三篇工作。</p>
<p>GPT-3 的参数量来到了 175B，训练数据从一开始 GPT-1 的几千本书的数据集，开始进入到了网站爬虫数据和清洗的模式。</p>
<p>其实在 GPT-2 中就已经提到了一个叫 Common Crawl 的公开网络数据，但是因为他们觉得这份数据实在太脏了所以放弃了，而现在为了训练更大的模型也不得不用起来，因此清洗数据是免不了的。</p>
<p>数据清洗经历了两个过程：</p>
<ol>
<li><strong>过滤</strong>：他们将原来 GPT-2 训练用的数据作为正样本，Common Crawl 作为负样本训练了一个二分类器，然后用这个分类器来做数据筛选，过滤掉一些特别显著的脏数据。</li>
<li><strong>去重</strong>：用经典的 LSH 算法进行去重</li>
</ol>
<p>另一方面，GPT-3 也正式提出了“in-context learning”的概念，在模型参数不进行更新的情况下，通过输入的上下文来帮助模型提升表现。</p>
<p>对于笔者而言，这张图相当形象:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这揭示了模型学习的另一个维度，提升模型表现并不只有 SGD 梯度更新这一个优化方向。结合 GPT-2 中 prompt 的由来，prompt 的前身是语言模型做多任务 zero-shot 时，针对不同任务给的特殊 token，因此一个更加富有信息量的“特殊 token 序列”能提升模型表现似乎是一件非常符合直觉的事情。</p>
<p>相比于过去使用一个特殊 token 来代表某一种特定的任务，GPT-3 的 few-shot prompt，或者说 in-context learning 形式，在笔者看来是一种推广，用户输入的自然语言和 few-shot 样例可以看成是一组特殊 token 的序列，因为自然语言的 token 具有语义和逻辑关联性，一个强大的预训练模型做到了“特殊 token”之间的泛化。</p>
<p>通过实验我们也可以观察到，随着给出的示例样本数变多，模型的表现也在提升：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>从笔者个人的角度来总结：</p>
<ul>
<li>我们可以将专属任务上微调得到的模型，看成一种用户输入 0 个特殊 token，解决 1 种任务的范式，所以<strong>第一范式</strong>下的每个模型只能用于解决一个专属任务。</li>
<li><strong>第二范式</strong>的模型是为每一种任务准备 1 个特殊 token，因此通过改变输入 token，就能用一个模型解决不同的任务。</li>
<li><strong>第三范式</strong>的模型把特殊 token 替换成了特殊 token 序列，而自然语言正好就是一种最符合人类习惯和直觉的特殊 token 序列。</li>
</ul>
<h2 id="4-training-language-models-to-follow-instructions-with-human-feedback-220302155">4. Training language models to follow instructions with human feedback (2203.02155)</h2>
<p>InstructGPT 被认为是 ChatGPT（GPT3.5） 背后的技术，核心点是把 RLHF，即<strong>基于人类反馈的强化学习</strong>，用到了语言模型上来<strong>进行人类喜好对齐</strong>。经过 RLHF 的 1.3B GPT 模型能在人类主观评分上超过 175B 的 GPT-3.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这篇工作里将模型输出与人类意愿不一致的这个现象称为“misaligned”，并分析原因在于语言模型的训练目标只是预测下一个 token，这跟我们希望模型“follow the user&rsquo;s instructions helpfully and safely”的目标之间显然是存在差距的。</p>
<p>用 Anthropic 的工作里的话来说，大模型应该遵循 3H 原则，即：</p>
<ul>
<li>helpful：帮助用户解决问题</li>
<li>honest：不能伪造信息或误导用户</li>
<li>harmless：不能对人或环境造成身体、心理或社会伤害</li>
</ul>
<p>语言模型之所以存在这个问题，原因也很简单，因为使用的是无监督学习，本身的学习目标里就没有人为控制，所以很直观地可以想到用**监督微调（SFT）**的方式来把缺失的人类监督信号加进来。</p>
<p>但是前面 GPT 三篇工作好不容易才把模型做到 175B 这么大，现在又重新开始标数据做监督学习显然是有点不聪明的，而且模型大了以后也更容易过拟合，对于人类偏好这一类的问题标注起来难度又很大，简单地全靠 SFT 肯定是行不通的。所以很自然地，OpenAI 想到了用他们家的拿手好戏强化学习，要知道 OpenAI 本身就是做强化学习起家的，本文使用的强化学习方法 PPO 也全是之前他们已经提出的算法，没有任何新的算法被提出，甚至论文里都没有对已有的算法进行太多的解释和铺垫，需要你感兴趣自己去翻他们的论文。</p>
<br>
<center>
  
  <br>
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>他们的方法整体可以概括如下：</p>
<ol>
<li>人工标一批 SFT 数据（包含问题和回答），对 GPT-3 模型（在强化学习里对应 Policy）进行微调</li>
<li>用 SFT 得到的模型，针对每个问题生成几份回答，然后人工给这些回答质量打分（排序）</li>
<li>用这份打分数据训练一个奖励模型（Reward Model, RM），让奖励模型代替人工打分</li>
<li>采用 PPO 算法，基于奖励模型的评分来继续训练 GPT-3 模型</li>
</ol>
<p>换言之，他们一共<strong>造了三份数据</strong>：</p>
<ul>
<li>SFT 数据（13k 条）：人工设计的问题，人工标注答案</li>
<li>Feedback 数据（33k 条）：针对上面人工设计的问题，模型输出的几份答案的排序（打分）</li>
<li>PPO 使用的数据（31k 条）：人工设计的问题（上面的模型没见过的新问题），用 RM 的评分来继续训练，这份数据不需要人工标注答案</li>
</ul>
<p><strong>训练三个模型</strong>：</p>
<ul>
<li>SFT 模型（16 epoch）</li>
<li>RM (Reward Model 奖励模型)</li>
<li>PPO 继续训练出来的最终模型</li>
</ul>
<p>他们甚至对问题类型进行了一些分类：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>模型训练部分，个人觉得值得注意的点有：</p>
<ol>
<li>SFT 阶段，在训了 1 epoch 后模型就已经过拟合了，但他们发现继续训练过拟合的模型依然可以提升 RM 性能，所以他们训练了 16 epoch</li>
<li>RM 的权重是直接用 SFT 模型初始化的，因为评分模型也需要语言能力，直接拷贝一份权重是比较省事的。RM 的输入是问题+几份答案，输出是排序。</li>
<li>RM 模型只有 6B，因为 175B 的 RM 很难训</li>
<li>强化学习阶段加了一个逐 token 的 KL 散度，用来让最终模型跟第一版 SFT 模型的要输出分布尽量保持一致，因为 RM 是在训练 SFT 模型的数据上训的，如果分布差异太大，RM 的评分就不准了</li>
</ol>
<p>强化学习的目标函数如下：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>简单翻译一下：</p>
<p>$$损失 = RM 评分 + 新旧模型输出token分布的KL散度 + 旧问题上SFT的损失$$</p>
<p>其中：</p>
<ul>
<li>RM 评分是 RM 对当前正在训练的模型在新问题（第三份数据）上的输出的评分</li>
<li>KL 散度是当前模型跟旧的 SFT 模型输出之间计算的</li>
<li>旧问题 SFT 损失是用第一份数据集继续按 SFT 方法训当前模型得到的</li>
</ul>
<p>另外，关于训练数据和评测方面的取舍也有所不同，训练中他们更看重 helpful，而评测阶段则更看重 honest 和 harmless。</p>
<h2 id="5-gpt-4-technical-report-230308774">5. GPT-4 Technical Report (2303.08774)</h2>
<p><strong>多模</strong></p>
<p>基于上面笔者三种模型范式的思路，多模态的模型可以看成是让特殊 token 的类型从文本 token 拓宽到了视觉 token，将模型解决的任务从 NLP 任务拓宽到了 CV 任务，而两种模态 token 对齐的技术也早被 OpenAI 研究过了，也就是大名鼎鼎的 CLIP。因此，GPT-4 具备多模能力本身并不是一件意外的事情。</p>
<p><strong>RLHF</strong></p>
<p>在笔者看来，RLHF 等技术更多地是在不限制输入 token 序列的情况下，去约束模型输出的技术，当然从某种意义上，也可以看成是在监督 prompts -&gt; task 的映射关系的技术（其实发展到现在，task 这个词已经不太准确了，可以意会一下）。</p>
<p>GPT-4 的报告中明确指出，RLHF 并不能提升模型解决任务的质量（不会增加知识），甚至很多时候调的不好还会损害各个任务上的指标。RLHF 更多地是在构建一些明确的 prompts -&gt; task 映射关系，因为自然语言是具有歧义性的，尤其是在输入信息较少的情况下，模型根据 prompts “理解”到的那个 task，并不一定是人类真正心里的那个意图，RLHF 实现了一种定制化的映射搭建，或者说，人类喜好对齐。</p>
<p><strong>Predictable scaling</strong></p>
<p>由于大模型实验的成本日渐高昂，我们不再能像小模型那样随便起实验调参了。因此 OpenAI 的大部分实验应该是在一个比 GPT-4 小很多倍的模型上进行的，然后通过这个小模型的训练 loss，来预测大模型最终训练出来的 loss</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>同样的， predictable scaling laws 也在很多 HumanEval 集上得到了观察。当然，在一部分的任务上也还无法被拟合的性能曲线，因此 OpenAI 说后续还会进一步优化他们模型性能预测的方法。</p>
<h2 id="6-sparks-of-artifificial-general-intelligence-early-experiments-with-gpt-4-230312712">6. Sparks of Artifificial General Intelligence: Early experiments with GPT-4 (2303.12712)</h2>
<p>智能（Intelligence）是一个多方面且难以捉摸的概念，长期以来缺乏一个共识性的定义。1994 年 52 名 心理学家组成的公式小组出版的关于智力科学的社论中，将只能定义为一种非常普遍的心理能力，包括<strong>推理、计划、解决问题、抽象思考、理解复杂想法、快速学习和从经验中学习的能力</strong>。</p>
<p>这篇是微软关于 GPT-4 的研究报告，长达 55 页，文中的实验都是在 <strong>早期的文本单一模态版的 GPT-4</strong> 上进行的（而不是后面更新的多模态版本）, 其目标是生成一些新颖而困难的任务和问题，来证明 GPT-4 的能力并不是单纯的记忆，并且它对<strong>概念、技能和领域有深刻而灵活的理解</strong>。另外还旨在探索 GPT-4 的响应和行为，以验证其<strong>一致性、连贯性和正确性</strong>，揭示其<strong>局限性和偏差</strong>。</p>
<p><strong>如何衡量 GPT-4 的智能</strong></p>
<p>传统机器学习的标准做法是准备一组标准评测数据集，确保它们独立于训练数据之外，覆盖一系列的任务和领域。</p>
<p>但这种方法并不太适用于 GPT-4，因为 GPT-4 是闭源模型，相关的训练数据集信息不公开，并且可以预见地非常庞大，因此我们不能保证目前公开的基准测试集不在它的训练数据里。也正因为此，本文采用的研究方法更接近于传统心理学，而不是机器学习方法。</p>
<p><strong>多模态与跨学科整合</strong></p>
<p>智能的一个重要衡量指标是综合不同领域信息的能力，以及跨学科地应用知识和技能的能力。这一节中作者举了四个例子来说明 GPT-4 具有很强的多模态与跨学科整合能力：</p>
<ol>
<li>写 JavaScript 代码来生成画家 Wassily Kandinsky 风格的作品。图一是该画家的原作，后面分别是 GPT-4 和 ChatGPT 写的代码画出的。</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="2">
<li>用莎士比亚的文风来证明素数无穷定理</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="3">
<li>以圣雄甘地的口吻写一封信给他的妻子，内容是支持“电子”成为美国总统候选人</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="4">
<li>写 Python 代码，以年龄、性别、体重、身高和血液测试结果向量作为输入，来预测用户是否有患糖尿病的风险</li>
</ol>
<p>这些对比可以体现 GPT-4 能创新性地整合不同领域的概念，并且显著强于 ChatGPT。除此之外作者也测试了 GPT-4 在音乐、绘图、空间理解方面的能力。</p>
<p><strong>代码</strong></p>
<p>除了常见的 leetcode 刷题，作者测试了 GPT-4 生成逆向工程代码、解释已有代码、用自然语言模拟代码执行过程、运行伪代码等能力。</p>
<p><strong>数学</strong></p>
<p>在一系列的分析实验后，作者从以下三方面总结了GPT-4的数学能力：</p>
<ol>
<li><strong>创造性推理</strong>：识别每个阶段可能相关的参数、中间步骤、计算或代数操作的能力。该组件通常基于启发式猜测或直觉，通常被认为是数学解决问题的最实质性和最深刻的方面</li>
<li><strong>技术熟练程度</strong>：执行遵循指定步骤集的常规计算或操作的能力</li>
<li><strong>批判性推理</strong>：批判性地检查论点的每个步骤的能力，将其分解为其子组件，解释它需要的内容，它与其余论点相关以及为什么是正确的</li>
</ol>
<p>这一节作者发现 GPT-4 的很多缺陷，如：</p>
<ul>
<li>在执行一些很常规且机械的计算时经常算错和混淆</li>
<li>GPT-4 由于是自回归模型，因此是实时线性输出的，而没有办法“打腹稿”</li>
</ul>
<p><strong>与世界的交互</strong></p>
<p>智能的另一重要方面是交互能力，即跟外界环境和智能体进行交互的能力，作者主要通过<strong>工具调用</strong>和<strong>具身交互</strong>两个维度来评估。</p>
<p>工具调用这里不多赘述了，具身交互方面测试了文字跑团游戏，以及交互式地指导人员找到天花板漏水的地方并进行修补，逐步根据人类的每一步反馈，给出行动建议和指示。</p>
<p><strong>与人类的交互</strong></p>
<p>GPT-4 在推理他人心理状态方面表现非常突出，特别是在模拟现实场景中，它的解释能力也很强，能对自己的判断和言论进行自我解释。</p>
<p><strong>判别能力</strong></p>
<p>判别能力主要指模型区别不同事物、概念和情景的能力。比如，区分两个食物哪个是可以安全食用，哪个是有毒的。</p>
<p>这一节的测试里，揭示出当前的评测指标存在的缺陷：对于语句相似度捕捉不够，依然严重依赖单词和短句的相似度，因此在很多时候参考答案很短，而 GPT-4 生成的答案很长，会被 ROUGE 这样的指标判定为答案不匹配，而人工检查后发现 GPT-4 的答案更加高质量和具有说服力。</p>
<p>另一方面作者也测试了用 GPT-4 作为评分员，对回答进行打分，实验现实尽管距离人类打分还存在一些差距，但在一些强约束的场景下已经很具有竞争力了。</p>
<p><strong>自回归结构的局限性</strong></p>
<p>自回归结构的输出是实时进行的，因此不存在“打草稿”的机会，因此无法“step-by-step”地处理问题，而引入思维链则可以显著地提升模型准确度。</p>
<p>对于一些依赖递归回溯的问题，比如一步一步输出汉诺塔问题的解法，GPT-4 表现非常差，在解决不能以连续方式处理的复杂或创造性问题时，都暴露出了严重的局限性。</p>
<p>比如要求 GPT-4 修改“9 * 4 + 6 * 6 = 72”这个等式左边的一个数字，来让等式计算结果变成 99，这就是一个无法简单“step-by-step”推理得到答案的问题，GPT-4 最后的准确率也非常低。</p>
<p>这一节的讨论中，作者指出，理解这些局限性的一个方法是类比诺奖作者卡尼曼提出的“快思考”和“慢思考”的概念。卡尼曼认为人类思维分成快、慢两个系统，快思考是一种自动的、直观的、不需要花费精力的思考方式，速度快但是容易出错和偏见。慢思考是一种受控的、理性的、耗费精力的思考方式，虽然速度慢但是准确可靠。</p>
<p>当前的 GPT-4 很大程度上可以看成是在执行快思考，但缺少慢思考能力。</p>
<p><strong>未来方向</strong></p>
<p>作者在这一节总结了未来 GPT-4 可以研究和改进的方向：</p>
<ul>
<li><strong>置信度校验</strong>：模型的输出缺乏置信度，既会编造训练集中没有的内容（open-domain 幻觉），也会生成与 Prompt 不一致的内容（close-domain 幻觉）</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li><strong>长期记忆</strong>：即长的上下文</li>
<li><strong>持续学习</strong>：当前微调和自我更新成本过高、缺乏有效且稳定的手段（保持已有能力不丢失和遗忘）</li>
<li><strong>个性化</strong>：根据应用和需求进行定制、扮演、调整风格等</li>
<li><strong>提前规划和概念性跳跃</strong>：推理过程过于线性，在需要思维跳跃性的任务上表现不佳</li>
<li><strong>透明度、可解释性和一致性</strong></li>
<li><strong>认知谬误和非理性</strong>：数据中存在的偏见、成见或错误引入了认知偏差和非理性</li>
<li><strong>对输入的敏感性</strong>：对于 Prompt 过于敏感，鲁棒性不够</li>
</ul>
<p>这些局限性均指向一个核心问题：<strong>哪些缺陷是自回归架构的先天缺陷，哪些是在已有架构上可以通过处理数据、增加外挂的组件和增大参数量解决的。</strong></p>
<h3 id="7-the-dawn-of-lmms-preliminary-explorations-with-gpt-4vision-230917421">7. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) (2309.17421)</h3>
<p>微软发布的 166 页的 GPT-4V 报告，主要围绕以下四个点进行展开研究：</p>
<ol>
<li>GPT-4V 支持哪些输入和工作模式？</li>
<li>GPT-4V 在不同领域和任务上的能力质量和通用性如何？</li>
<li>GPT-4V 有效使用和提示方法有哪些？</li>
<li>未来有哪些有前途的方向？</li>
</ol>
<p><strong>GPT-4V 的输入模式</strong></p>
<ul>
<li>纯文本</li>
<li>单个图像-文本对</li>
<li>图像文本交替输入</li>
</ul>
<p>前两种相对来说比较简单，第三种交替输入的情况，已经非常接近于人的聊天模式了。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>GPT-4V 的工作模式和提示技术</strong></p>
<p>实验证明，在 LLM 上研究出来上各种提示技术，在 GPT-4V 上也是好使的，比如思维链、few-shot 提示等。</p>
<p>这一节提到了一个“LLMs don&rsquo;t want to succeed”的理论，貌似是来自于 Andrej Karpathy 的某次演讲，里面展示了一种类似于催眠一样的提示技术，即，<strong>你想要你的 LLM 表现更出色，你就要用直接的提示词说“你是xxx方面的专家”，否则它之后表现出一般普通人水平的能力</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>完整的 PPT 可以看这里：https://karpathy.ai/stateofgpt.pdf</p>
<p>在论文中作者是举了一个数苹果的案例，让 GPT-4V 来数一下画面中有几个苹果。一开始 GPT-4V 并不能轻易得到正确答案，但经过一系列我们已知的 LLM Prompt 技巧加强后，GPT-4V 变得可靠，能够正确计数：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在日常的人和人交互中，在图片中画圈、画箭头来指向关键信息是一种很自然且常见的方式， 经实验 GPT-4V 在这方面的理解能力非常强大。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>作者实验了一些很有挑战性的 case，发现基本上难不倒它：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在 In-context few-shot learning 方面，作者也用一个很有代表性的例子说明了提供示例样本的重要性。作者给了一张仪表的图，让 GPT-4V 读出当前仪表指针指向的数值，一开始不论如何改良 prompt 都无法得到正确的结果。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>甚至在给出一个示例的情况下模型仍然表现不佳，但当示例增加到两个后，GPT-4V 就突然能成功读数了，可见<strong>提供上下文示例对于提升大模型性能至关重要</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>视觉语言能力</strong></p>
<p>在大部分已有的 CV 子任务上，GPT-4V 都表现出了不错的能力，常见的场景描述等更是表现出色，在相对小众的领域，如医学图像上，同样让人印象深刻，GPT-4V 可以根据 CT 图判断出智齿和骨折等。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>当然，在一些已经被做得非常深入的子任务上，GPT-4V 相较于 SOTA 模型还有不小的差距，但还是那句话，潜力大于绝对精度，目前 GPT-4V 已经展现出了让人鼓舞的性能，优化个别任务上的表现只是时间问题。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>GPT-4V 甚至能看懂梗图，解释其中的笑点：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>借助 GPT-4V 强大的推理能力和具备的常识，我们甚至可以“<strong>假如你是一名侦探，你可以从图中推理出哪些线索？</strong>”</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>时间序列和视频理解</strong></p>
<p>作者实验了多图像序列，GPT-4V 能够识别出这是一组动态图像序列，并且能结合起来判断画面中的人正在做俯卧撑：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>情商测试</strong></p>
<p>在这一节，GPT-4V 可以基于予以内容和图像样式解释视觉情感，如满意、愤怒、敬畏和恐惧等：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>甚至可以一张图片让 GPT-4V 用两种不同方式来描述，分别让人感到不安和感到舒适（新闻学让它玩明白了）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>新兴应用亮点</strong></p>
<ul>
<li>行业：
<ul>
<li>缺陷检测</li>
<li>安全检查</li>
<li>杂货结账</li>
</ul>
</li>
<li>医疗</li>
<li>汽车保险
<ul>
<li>损害评估</li>
<li>保险报告</li>
</ul>
</li>
<li>定制化
<ul>
<li>照片组织</li>
<li>密集标注与分割</li>
</ul>
</li>
<li>图像生成
<ul>
<li>生成图像的评估</li>
<li>图像编辑的提示生成</li>
</ul>
</li>
<li>具象化智能体
<ul>
<li>操作机器</li>
<li>导航</li>
</ul>
</li>
<li>GUI 导航（软件层面的交互和导航）</li>
</ul>
<p>整篇报告篇幅较多，并且举了大量详细的例子，在这里就不一一展开了，感兴趣的同学可以自行翻阅。</p>
<hr>
<p>至此，我总结了 GPT 系列工作里一些我关注到的点，从中可以感受到 OpenAI 的工作之间都有着很深的逻辑链条，很多推广都似乎是最符合直觉的。OpenAI 早期公开的论文里各种细节还是很丰富的，不仅细致地告诉你如何清洗和构造数据，甚至还教你如何找到一个合适的标注员给你标数据。</p>
<p>作为一个从 CV 转到 LLM 的新人，难免犯一些常见或低级的错误，欢迎任何读者及时指出和斧正，也欢迎任何留言讨论。</p>
<p>本篇笔记的写作参考了沐神的几期 B 站视频，以及知乎@苏打的文章，特此感谢</p>
]]></description></item><item><title>LLM预训练之RLHF（一）：RLHF及其变种</title><link>https://jianye0428.github.io/posts/pretrain_rlhf_one/</link><pubDate>Sat, 04 May 2024 14:23:08 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/pretrain_rlhf_one/</guid><description><![CDATA[<h2 id="0-引言">0. 引言</h2>
<p>在ChatGPT引领的大型语言模型时代，国内外的大模型呈现爆发式发展，尤其是以年初的LLaMA模型为首的开源大模型和最近百川智能的baichuan模型，但无一例外，都使用了「基于人类反馈的强化学习」（RLHF）来提升语言模型的性能，并在模型重注入了人类的偏好，以提高模型的有用性和安全性。不过RLHF也早已更新换代，我们以如下目录进行详细讲述RLHF及其变种：</p>
<ul>
<li>LLM的经典预训练Pipeline</li>
<li>Llama 2中的RLHF</li>
<li>RLHF替代方案</li>
</ul>
<h2 id="一llm的经典预训练pipeline">一、LLM的经典预训练Pipeline</h2>
<p>​  目前基于Transformer decoder的LLM，比如ChatGPT、LLaMA、baichuan等，通常都会有基于预训练的base模型和在base模型至少使用RLHF微调的Chat模型，Chat模型的训练一般都包括如下三个步骤：预训练，有监督微调和对齐。</p>
<p>​  在<strong>预训练</strong>阶段，模型会从大量无标注文本数据集中学习通用知识，然后使用「<strong>有监督微调」（SFT）<strong>优化模型以更好地遵守特定指令，最后使用</strong>对齐</strong>技术使LLM可以更有用且更安全地响应用户提示。</p>
<h3 id="11-预训练pre-training">1.1 预训练（Pre-training）</h3>
<p>预训练阶段通常需要包含数十亿到数万亿个token的庞大文本语料库，但训练目标是<strong>模型需要根据提供的文本来预测「下一个单词」</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>1.2 有监督微调（Supervised Finetuning）</strong></p>
<p>​SFT的训练过程类似Pre-training阶段，也是预测「下一个单词」，但是<strong>需要人工标注的指令数据集</strong>，其中模型的输入是一个指令（根据任务的不同，也可能包含一段输入文本），输出为模型的预期回复内容。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>数据形式类似于：</p>
<blockquote>
<p>Instruction: &ldquo;Write a limerick about a pelican.&rdquo;</p>
<p>指令：“写一首关于鹈鹕的打油诗。“</p>
<p>Output: &ldquo;There once was a pelican so fine&hellip;&rdquo;</p>
<p>输出：“从前有一只鹈鹕很好&hellip;“</p>
</blockquote>
<p>模型会把“Write a limerick about a pelican”作为输入，逐个token进行预测，输出“There once was a pelican so fine&hellip;”</p>
<p>虽然两个阶段都采用类似的训练目标，但有监督微调数据集通常比预训练数据小得多，指令数据集需要人类（或其他高质量的LLM）提供标注结果，所以无法大规模应用。</p>
<p><strong>1.3 对齐（Alignment）</strong></p>
<p>第三阶段依然是微调，不过其主要目标在于将语言模型与人类的偏好、价值观进行对齐，这也是RLHF机制发挥的地方。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="二reinforcement-learning-with-human-feedback-rlhf">二、Reinforcement Learning with Human Feedback (RLHF)</h2>
<p>上节，我们讨论了现代LLM的三个训练过程；本小节，我们重点讨论「上述两个微调阶段」（Supervised Tinetuning和Alignment）中使用的RLHF技术。</p>
<p>RLHF主要包括三步：</p>
<ol>
<li>在预训练好的模型上进行「有监督微调」（SFT）；</li>
<li>在有监督微调模型基础上创建一个reward model（RM）模型；</li>
<li>基于RM模型使用PPO算法微调SFT模型；</li>
</ol>
<h3 id="21-在预训练好的模型上进行有监督微调">2.1 在预训练好的模型上进行有监督微调**</h3>
<p>先收集一个Prompts集合，并要求标注人员写出高质量的回复，然后使用该数据集以监督的方式微调预训练的基础模型。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>​该步骤与上小节的Supervised Finetuning类似，但这是RLHF不可或缺的一个步骤。</p>
<h3 id="22-在有监督微调模型基础上创建一个rm模型">2.2 在有监督微调模型基础上创建一个RM模型</h3>
<p>对于每个Prompt，要求有监督微调后的LLM生成四到九个回复，再由标注人员根据个人偏好对所有回复进行排序。虽然排序过程很耗时，但工作量还是比第一步的有监督数据集构建要少一些。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在处理排序数据时，使用了一个奖励模型RM，RM来自RLHF第一步的「有监督微调语言模型」（SFT），SFT的输出通过一个回归层（单个输出节点）转换为奖励分数，即可称为<strong>RM模型</strong>。</p>
<h3 id="23-基于rm模型使用ppo算法微调sft模型">2.3 基于RM模型使用PPO算法微调SFT模型</h3>
<p>基于RM模型使用proximal policy optimization (PPO)算法微调SFT模型</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>PPO的具体技术细节可以参考InstructGPT或下面的论文列表。</p>
<ol>
<li>Asynchronous Methods for Deep Reinforcement Learning (2016) ，https://arxiv.org/abs/1602.01783</li>
<li>Proximal Policy Optimization Algorithms (2017)，https://arxiv.org/abs/1707.06347</li>
<li>Fine-Tuning Language Models from Human Preferences (2020)，https://arxiv.org/abs/1909.08593</li>
<li>Learning to Summarize from Human Feedback (2022) ，https://arxiv.org/abs/2009.01325</li>
</ol>
<h2 id="三llama-2的rlhf">三、LLaMA 2的RLHF**</h2>
<p>Meta AI在创建Llama-2-chat模型时也使用了RLHF技术，不过与ChatGPT相比还是有些细微区别。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>简单来说，Llama-2-chat在第一步RLHF微调上使用相同的指令数据，但在第二步使用了两个奖励模型；通过多个阶段的不断进化，奖励模型也会根据Llama-2-chat模型出现的错误进行更新；并且增加了拒绝采样（rejection sampling）步骤。</p>
<h3 id="31-margin-loss">3.1 Margin Loss</h3>
<p>​在标准InstructGPT中使用的RLHF PPO方法，研究人员需要收集同一个提示下的4-9个模型输出并进行排序，比如四个回复的排序结果为A&lt;C&lt; D&lt;B，那么就可以得到六个对比结果：A &lt; C，A &lt; D ，A &lt; B，C &lt; D，C &lt; B，D &lt; B。</p>
<p>​Llama 2的数据集也采用类似的方式，不过标注人员每次只能看到两个（而非4-9个）回复并进行对比，但新增了一个边际（margin）标签，对比结果可以为「显著更好」（significantly better）和「好的不明显」（negligibly better）。</p>
<p>在排序训练时中，Llama 2相比InstructGPT增加了边际损失：</p>
<p>$$\mathcal{L}<em>{\mathrm{ranking}}=-\log\left(\sigma\left(r</em>\theta\left(x,y_c\right)-r_\theta\left(x,y_r\right)-m(r)\right)\right)$$</p>
<p>其中，$r_θ(x，y)$是提示x和生成的回复y的标量分数输出; θ为模型权重; σ是将层输出转换为范围从0到1的分数的逻辑S形函数; $y_c$是由标注人员选择的更优回复; $y_r$是较差的回复。$m(r)$可以调节两个回复之间的差值，如果对比结果为「显著更好」，则会增加梯度值，加快更新速度。</p>
<h3 id="32-两个rm模型">3.2 两个RM模型</h3>
<p>​Llama 2中的两个奖励模型分别侧重「有用性」（helpfulness）和「安全性」（safety），用于模型优化的最终奖励函数会将两个分数进行线性组合。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="33-拒绝采样">3.3 拒绝采样</h3>
<p>​Llama 2的作者使用了一个训练流水线，<strong>同时使用PPO和拒绝采样算法</strong>，迭代地产生多个RLHF模型（从RLHF-V1到RLHF-V5），模型在拒绝采样时会得到K个输出，并使用最高奖励的输出更新梯度，而PPO每次只基于单样本进行更新。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在监督微调的初始阶段之后，模型只使用拒绝采样进行训练，然后再结合拒绝采样和PPO。</p>
<p>从实验结果来看，RLHF微调模型在无害性和有用性上都得到了改善，并且在最后阶段RLHF-v5使用PPO算法的性能最好。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="四rlhf的替代方案">四、RLHF的替代方案</h2>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>RLHF在InstructGPT和Llama 2论文中被证明是有效的，但是RLHF的过程是比较复杂的，下面将介绍一下最近RLHF的替代方案：</p>
<h3 id="41-constitutional-ai-harmlessness-from-ai-feedback-dec-2022-httpsarxivorgabs221208073">4.1 Constitutional AI: Harmlessness from AI Feedback (Dec 2022, <a href="https://arxiv.org/abs/2212.08073"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2212.08073<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>研究人员提出了一种 <strong><font color=red>基于人类提供的规则列表的自我训练机制</font></strong>。与前面提到的InstructGPT论文类似，也使用了强化学习方法。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上图中的「红队」（Red Team）指的是测试目标系统的防御能力，即外部或内部专家模拟潜在对手的过程，通过模仿现实世界攻击者的战术、技术和程序来挑战、测试并最终改进系统。</p>
<h3 id="42-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-feb-2023-httpsarxivorgabs230205206">4.2 The Wisdom of Hindsight Makes Language Models Better Instruction Followers (Feb 2023, <a href="https://arxiv.org/abs/2302.05206"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2302.05206<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>研究人员提出了一种**<font color=red>基于重新标记的监督微调方法HIR</font>**，该方法在12个BigBench任务上优于RLHF。</p>
<p>​HIR是如何工作的？简而言之，HIR方法包括两个步骤，即<strong>采样</strong>和<strong>训练</strong>。在采样步骤中，Prompt和指令输入给LLM来获取答案，根据对齐得分，在训练阶段适当的地方重新标注指令；然后，重新标记的指令和原始的Prompt用于微调LLM。使用这种重新标记的方法，研究人员有效地将失败案例（LLM创建的输出与原始指令不匹配的案例）转化为有用的训练数据，用于监督学习。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="43-direct-preference-optimization-your-language-model-is-secretly-a-reward-model-httpsarxivorgabs230518290-may-2023">4.3 Direct Preference Optimization: Your Language Model is Secretly a Reward Model (<a href="https://arxiv.org/abs/2305.18290"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2305.18290<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, May 2023)</h3>
<p><strong><font color=red>直接偏好优化（DPO）是具有PPO的RLHF的替代方案</font></strong>，其中研究人员表明，在RLHF中拟合奖励模型的交叉熵损失可以直接用于微调LLM。根据他们的基准，使用DPO更有效，而且在响应质量方面通常也优于RLHF/PPO。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="44-reinforced-self-training-rest-for-language-modeling-aug-2023-httpsarxivorgabs230808998">4.4 Reinforced Self-Training (ReST) for Language Modeling (Aug 2023, <a href="https://arxiv.org/abs/2308.08998"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2308.08998<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>ReST是人类反馈强化学习（RLHF）的一种替代方案，它<strong>使LLM与人类偏好保持一致</strong>。 <strong><font color=red>ReST使用采样方法创建改进的数据集</font></strong>，在质量越来越高的子集上迭代训练，以完善其奖励函数。根据作者的说法，与标准的在线RLHF方法（如具有近端策略优化的RLHF，PPO）相比，ReST通过离线生成训练数据集实现了更高的效率，但缺少与InstructGPT或Llama 2中使用的标准RLHF PPO方法的全面比较。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="45-rlaif-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback-sep-2023-httpsarxivorgabs230900267">4.5 RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (Sep 2023, <a href="https://arxiv.org/abs/2309.00267"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2309.00267<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>最近的人工智能反馈强化学习（RLAIF）研究表明，RLHF中奖励模型训练的评级不一定必须由人类提供，而是可以由LLM生成（此处：PaLM 2）。标注人员在一半的案例中更喜欢RLAIF模型，也就意味着两个模型的差距并不大，RLHF和RLAIF都大大优于纯通过监督指令微调训练的模型。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这项研究的结果非常有用和有趣，因为它基本上意味着我们可能能够使基于RLHF的训练更加高效和容易。然而，这些RLAIF模型在专注于信息内容的安全性和真实性的定性研究中的表现还有待观察，而人类偏好研究仅部分捕捉到了这一点。</p>
<p><strong>参考文献：</strong></p>
<p>[1] <a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives"target="_blank" rel="external nofollow noopener noreferrer">https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><br>
[2] <a href="https://mp.weixin.qq.com/s/3Ff6C5zT7fXggQ1FwxvWAQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/3Ff6C5zT7fXggQ1FwxvWAQ<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>一文详解 ChatGPT RLHF 背后的 PPO 强化学习训练</title><link>https://jianye0428.github.io/posts/chatgpt_rlhf/</link><pubDate>Sat, 04 May 2024 17:00:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/chatgpt_rlhf/</guid><description><![CDATA[<h2 id="0-引言">0. 引言</h2>
<p></p>
<p>最近火出圈的🚀 ChatGPT 中 RLHF 主要采用了就是 PPO 进行强化学习训练</p>
<blockquote>
<p>主要运用在微调阶段（微调整个 10B～100B+ 参数的成本其实也非常高 ）使用<strong>策略梯度</strong>强化学习 (Policy Gradient RL) 算法、近端策略优化 (PPO) 微调初始 LM 的部分或全部参数。</p>
</blockquote>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<blockquote>
<p>以下主要参考台大李宏毅的推导过程</p>
</blockquote>
<h2 id="01-vanilla-policy-gradient">01. Vanilla policy gradient</h2>
<ul>
<li>动作/环境/奖励之间的关系：</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>轨迹可表示为集合</p>
<p>$$\begin{aligned}p_{\theta}(\tau)&amp;=p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_1|s_1)p(s_3|s_2,a_2)\ldots\&amp;=p(s_1)\prod_{t=1}^Tp_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\end{aligned}$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>一个轨迹的奖励总和为：</p>
<p>$$R(\tau)=\sum_{t=1}^Tr_t$$</p>
<p>则奖励的期望为：</p>
<p>$$\bar{R}<em>\theta=\sum</em>\tau R(\tau)p_\theta(\tau)=E_{\tau\sim p_\theta(\tau)}[R(\tau)]$$</p>
<p>将 $R(\tau)$ 看成常量，对其求微分：</p>
<p>$$\begin{aligned}
\nabla\bar{R}<em>{\theta}&amp; =\sum</em>{\tau}R(\tau)\nabla p_{\theta}(\tau) \
&amp;=\sum_{\tau}R(\tau)p_{\theta}(\tau)\frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \
&amp;=\sum_{\tau}R(\tau)p_{\theta}(\tau)\nabla\log p_{\theta}(\tau)\quad\nabla f(x)=f(x)\nabla\log f(x) \
&amp;=E_{\tau\sim p_{\theta}(\tau)}[R(\tau)\nabla\log p_{\theta}(\tau)]&amp; \left(2\right) \
&amp;\approx\frac1N\sum_{n=1}^{N}R(\tau^{n})\nabla\log p_{\theta}(\tau^{n}) \
&amp;=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla\log p_\theta(a_t^n|s_t^n)
\end{aligned}$$</p>
<p>策略网络梯度更新：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>可以看成一个分类问题（游戏中通过键盘输入来互动，分类类别为所有可操作的键位）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li>理想情况下， 并不一直为正数，增加一个 baseline:</li>
</ul>
<p>$$\nabla\bar{R}<em>{\theta}=\frac{1}{N}\sum</em>{n=1}^{N}\sum_{t=1}^{{T_{n}}}(R(\tau^{n})-b)\nabla\log p_{\theta}(a_{t}^{n}|s_{t}^{n})b\approx E[R(\tau)]$$</p>
<blockquote>
<p>在电子游戏中，奖励值常常为正（通常为游戏分数）。这时需要增加一个偏置来保证同时有正样本和负样本</p>
</blockquote>
<ul>
<li>分配合适的学分</li>
</ul>
<p>一个高分的游戏轨迹中也可能存在错误的动作，同样的，一个低分的游戏轨迹也可能存在正确的动作，而上文中的计算将最后的奖励值（最后的游戏分数）都一视同仁视为该游戏轨迹每个动作的学分。</p>
<p>为了更准确地描述每个动作所得到的学分，将一个动作执行后对应的学分为后续的所有奖励值的总和</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>$$\begin{aligned}
\nabla\bar{R}<em>\theta&amp; =\frac1N\sum</em>{n=1}^N\sum_{t=1}^{T_n}(R(\tau^n)-b)\nabla\log p_\theta(a_t^n|s_t^n) \Downarrow\nabla\bar{R}<em>\theta \
&amp;= \frac1N\sum</em>{n=1}^N\sum_{t=1}^{T_n}(\sum_{t^{\prime}=t}^{T_n}r_{t^{\prime}}^n-b)\nabla\log p_\theta(a_t^n|s_t^n)
\end{aligned}$$</p>
<p>当某个动作执行以后，其对后续的奖励分数的影响在慢慢减少，再增加一个衰减因子：</p>
<p>$$\begin{aligned}
\nabla\bar{R}<em>\theta&amp; =\frac1N\sum</em>{n=1}^N\sum_{t=1}^{T_n}(\sum_{t^{\prime}=t}^{T_n}r_{t^{\prime}}^n)\nabla\log p_\theta(a_t^n|s_t^n)\Downarrow\nabla\bar{R}<em>\theta \
&amp; = \frac{1}{N}\sum</em>{n=1}^{N}\sum_{t=1}^{T_{n}}(\sum_{t^{\prime}=t}^{T_{n}}\gamma^{t^{\prime}-t}r_{t^{\prime}}^{n}-b)\nabla\log p_{\theta}(a_{t}^{n}|s_{t}^{n}),\gamma&lt;1
\end{aligned}$$</p>
<h2 id="02-从on-policy到off-policy">02. 从on-policy到off-policy</h2>
<p>两者区别:</p>
<ul>
<li>On-policy: 学习到的 agent 和与环境交互的 agent 是相同的，每一次梯度更新都需要重新采样</li>
<li>Off-policy: 学习到的 agent 和与环境交互的 agent 是不同的，每次梯度更新不需要重新采样</li>
</ul>
<p>重新看看 的表达式：
$$\nabla\bar{R}<em>\theta=E</em>{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)]$$</p>
<ul>
<li>使用策略网络 收集数据。当 更新后，则需要重新收集训练样本</li>
<li>目标：使用相同的样本（通过 采样）训练 。其中 为固定的，因此我们可以重复使用其样本数据</li>
</ul>
<h3 id="21-重要性采样importance-sampling">2.1 重要性采样（Importance Sampling）</h3>
<p>考虑一个场景，假如正在尝试计算函数 $f(x)$ 的期望值，其中 $x \sim f(x)$ 服从某种分布。则对 $E(f(x))$ 有以下估计：</p>
<p>$$E_{x\sim p}[f(x)]=\int f(x)p(x)dx\approx\frac{1}{n}\sum_{i}f(x_{i})$$</p>
<p>蒙特卡洛抽样方法是简单地从分布 $p(x)$ 中抽出 ，然后取所有样本的平均值来得到期望值的估计。那么问题来了，如果  $p(x)$  非常难取样怎么办？是否能够根据一些已知的、容易抽样的分布来估计期望值？</p>
<p>答案是肯定的。公式的一个简单转换就可以做到</p>
<p>$$E_{x\sim p}[f(x)]=\int f(x)p(x)dx=\int f(x)\frac{p(x)}{q(x)}q(x)dx=E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]$$</p>
<p>其中$x$从分布$q(x)$中采样，$q(x)$不应为 0。通过这种方式，估计期望能够从另一个分布$q(x)$中采样，$p(x)/q(x)$是称为采样率或采样权重，它作为校正权重以抵消来自不同分布的概率采样。</p>
<ul>
<li>重要性采样的缺陷</li>
</ul>
<p>虽然重要性采样保证了期望的一致，但是这里来计算一下方差是否一致</p>
<p>方差的计算：</p>
<p>$$Var[X]=E[X^2]-(E[X])^2$$</p>
<p>分别计算方差：</p>
<p>$$\begin{aligned}Var_{x\sim p}[f(x)]&amp;=E_{x\sim p}[f(x)^2]-(E_{x\sim p}[f(x)])^2\Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}]&amp;=E_{x\sim q}[(f(x)\frac{p(x)}{q(x)})^2]-(E_{x\sim q}[f(x)\frac{p(x)}{q(x)}])^2\&amp;=E_{x\sim p}[f(x)^2\frac{p(x)}{q(x)}]-(E_{x\sim p}[f(x)])^2\end{aligned}$$</p>
<p>可以发现两者虽然期望相等但方差并不一致</p>
<h3 id="22-从-on-policy-到-off-policy">2.2 从 on-policy 到 off-policy</h3>
<p>我们使用重要性采样将 on-policy 调整为 off-policy</p>
<p>$$\nabla\bar{R}<em>\theta=E</em>{\tau\sim p_{\theta^{\prime}}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta^{\prime}}(\tau)}R(\tau)\nabla\log p_\theta(\tau)]$$</p>
<ul>
<li>从 $\theta&rsquo;$ 采样得到数据集</li>
<li>使用该 数据集多次训练 $\theta$</li>
</ul>
<p>梯度更新过程：</p>
<p>$$\begin{aligned}
&amp;=E_{(s_t,a_t)\sim\pi_\theta}[A^\theta(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(s_t,a_t)}{p_{\theta^{\prime}}(s_t,a_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{\prime}}(a_t|s_t)}\frac{p_\theta(s_t)}{p_{\theta^{\prime}}(s_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)]&amp; \text{(4)} \
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{\prime}}(a_t|s_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)]
\end{aligned}$$</p>
<ul>
<li>其中 $A^\theta(s_t,a_t)$ 指的是 advantage 函数,其计算方式为加上衰减机制后的奖励值并减去基线。</li>
<li>由于 $\frac{p_\theta(s_t)}{p_{\theta&rsquo;}(s_t)}$ 的值难以计算，将其设置为 1，简化计算</li>
</ul>
<p>目标函数可以表示为：</p>
<p>由于 $\nabla f(x)=f(x)\nabla\log f(x)$ 再结合不定积分，目标函数可以表示为:</p>
<p>$$J^{\theta&rsquo;}(\theta)=E_{(s_t,a_t)\sim\pi_{\theta&rsquo;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&rsquo;}(a_t|s_t)}A^{\theta&rsquo;}(s_t,a_t)]$$</p>
<h2 id="03-ppotrpo">03. PPO/TRPO</h2>
<p>为了消除重要性采样的缺陷的影响，以下为两种方式</p>
<ul>
<li>PPO（Proximal Policy Optimization）
<ul>
<li>初始化策 略网络参数</li>
<li>在每次迭代过程中:</li>
<li>目标函数:</li>
<li>使用 与环境互动以收集 ，并计算出 advantage 值</li>
<li>更新 优化</li>
<li>算法:</li>
</ul>
</li>
</ul>
<p>$$\begin{aligned}
PPO algorithm: \
J_{PPO}^{\theta^k}(\theta) &amp; = J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)J^{\theta^k}(\theta) \
&amp; = E_{(s_{t},a_{t})\sim\pi_{\theta^{k}}}[\frac{p_{\theta}(a_{t}|s_{t})}{p_{\theta^{k}}(a_{t}|s_{t})}A^{\theta^{k}}(s_{t},a_{t})] \
&amp; \approx \sum_{(s_{t},a_{t})}\frac{p_{\theta}(a_{t}|s_{t})}{p_{\theta^{k}}(a_{t}|s_{t})}A^{\theta^{k}}(s_{t},a_{t})
\end{aligned}$$</p>
<p>自适应 KL 惩罚：如果 $KL(\theta,\theta^k)&gt;KL_{\max}$ ,增大 $\beta$; 如果 $KL(\theta,\theta^k) &lt;KL_{\min}$,减小 $\beta$。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li>TRPO（Trust Region Policy Optimizatio）</li>
</ul>
<p>$$J_{TRPO}^{\theta&rsquo;}(\theta)=E_{(s_t,a_t)\sim\pi_{\theta&rsquo;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&rsquo;}(a_t|s_t)}A^{\theta&rsquo;}(s_t,a_t)]KL(\theta,\theta&rsquo;)&lt;\delta $$</p>
<p>TRPO 和 PPO 在各个测试上性能差不多。但相比 PPO ，TRPO 计算要更复杂</p>
<p><strong>参考文献</strong>:</p>
<p>[1] <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html"target="_blank" rel="external nofollow noopener noreferrer">https://spinningup.openai.com/en/latest/algorithms/ppo.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[2] <a href="https://openai.com/research/openai-baselines-ppo"target="_blank" rel="external nofollow noopener noreferrer">https://openai.com/research/openai-baselines-ppo<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[3] <a href="https://huggingface.co/blog/deep-rl-ppo"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/blog/deep-rl-ppo<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[4] <a href="https://huggingface.co/blog/rlhf"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/blog/rlhf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[5] <a href="https://mp.weixin.qq.com/s/zhkNDNDEJV3BEdcgeuHkOA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/zhkNDNDEJV3BEdcgeuHkOA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>Transformer 详解</title><link>https://jianye0428.github.io/posts/transformerdetailedexplanation/</link><pubDate>Mon, 24 Jul 2023 17:37:50 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/transformerdetailedexplanation/</guid><description><![CDATA[<h2 id="transformer-详解">Transformer 详解</h2>
<p>Transformer 是谷歌大脑在 2017 年底发表的论文 <a href="https://arxiv.org/pdf/1706.03762.pdf"target="_blank" rel="external nofollow noopener noreferrer">attention is all you need<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 中所提出的 seq2seq 模型。现在已经取得了大范围的应用和扩展，而 BERT 就是从 Transformer 中衍生出来的预训练语言模型</p>
<p>这篇文章分为以下几个部分</p>
<ul>
<li>Transformer 直观认识</br></li>
<li>Positional Encoding</br></li>
<li>Self Attention Mechanism</br></li>
<li>残差连接和 Layer Normalization</br></li>
<li>Transformer Encoder 整体结构</br></li>
<li>Transformer Decoder 整体结构</br></li>
<li>总结</br></li>
<li>参考文章</br></li>
</ul>
<h3 id="0-transformer-直观认识">0. Transformer 直观认识</h3>
<p>Transformer 和 LSTM 的最大区别，就是 <font color=green>LSTM 的训练是迭代的、串行的，必须要等当前字处理完，才可以处理下一个字</font>。而 Transformer 的训练时<strong>并行</strong>的，即所有字是同时训练的，这样就大大增加了计算效率。<font color=green>Transformer 使用了位置嵌入 (Positional Encoding) 来理解语言的顺序</font>，使用自注意力机制(Self Attention Mechanism)和全连接层进行计算，这些后面会讲到。</p>
<p>Transformer 模型主要分为两大部分，分别是 Encoder 和 Decoder。<font color=red>Encoder 负责把输入(语言序列)映射成隐藏层(下图中第 2 步用九宫格代表的部分)，然后解码器再把隐藏层映射为自然语言序列</font>。例如下图机器翻译的例子(Decoder 输出的时候，是通过 N 层 Decoder Layer 才输出一个 token，并不是通过一层 Decoder Layer 就输出一个 token)。</p>
<p></p>
<p>本篇文章大部分内容在于解释 Encoder 部分，即 **<font color=red>把自然语言序列映射为隐藏层的数学表达</font>**的过程。理解了 Encoder 的结构，再理解 Decoder 就很简单了</p>
<p></p>
<p>上图为 Transformer Encoder Block 结构图，注意：下面的内容标题编号分别对应着图中 1,2,3,4 个方框的序号</p>
<h3 id="1-positional-encoding">1. Positional Encoding</h3>
<p>由于 Transformer 模型没有循环神经网络的迭代操作，所以我们必须提供每个字的位置信息给 Transformer，这样它才能<font color=red>识别出语言中的顺序关系</font>。</p>
<p>现在定义一个 <strong><font color=red>位置嵌入</font></strong> 的概念，也就是 Positional Encoding，位置嵌入的维度为 [max_sequence_length, embedding_dimension], 位置嵌入的维度与词向量的维度是相同的，都是 embedding_dimension。max_sequence_length 属于超参数，指的是限定每个句子最长由多少个词构成。</p>
<p>注意，我们一般以字为单位训练 Transformer 模型。首先初始化字编码的大小为 [vocab_size, embedding_dimension]，vocab_size 为字库中所有字的数量，embedding_dimension 为字向量的维度，对应到 PyTorch 中，其实就是 nn.Embedding(vocab_size, embedding_dimension)</p>
<p>在论文中使用了 sin 和 cos 函数的线性变换来提供给模型<strong>位置信息</strong>:</p>
<p>$$\left\{\begin{aligned}
PE(pos, 2i) = \sin (pos/10000^{2i/d_{model}}) \cr
PE(pos, 2i + 1) = \cos (pos/10000^{2i/d_{model}}) \cr
\end{aligned}\right.$$</p>
<p>上式中 $pos$ 指的是一句话中某个字的位置，取值范围是 $[0, \text{max_sequence_length}]$ ， $i$ 指的是字向量的维度序号，取值范围是 $[0, \text{embedding_dimension} / 2]$ ， $d_{model}$ 指的是 embedding_dimension​的值</p>
<p>上面有 sin 和 cos 一组公式，也就是对应着 embedding_dimension 维度的一组奇数和偶数的序号的维度，例如 0,1 一组，2,3 一组，分别用上面的 sin 和 cos 函数做处理，从而产生不同的周期性变化，而位置嵌入在 embedding_dimension​维度上随着维度序号增大，周期变化会越来越慢，最终产生一种包含位置信息的纹理，就像论文原文中第六页讲的，位置嵌入函数的周期从 $ 2\pi $ 到 $10000 * 2 \pi$ 变化，而每一个位置在 embedding_dimension ​维度上都会得到不同周期的 $ \sin $ 和 $ \cos $ 函数的取值组合，从而产生唯一的纹理位置信息，最终使得模型学到<strong>位置之间的依赖关系和自然语言的时序特性</strong>。</p>
<p>如果不理解这里为何这么设计，可以看这篇文章 <a href="https://wmathor.com/index.php/archives/1453/"target="_blank" rel="external nofollow noopener noreferrer">Transformer 中的 Positional Encoding<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>下面画一下位置嵌入，纵向观察，可见随着 embedding_dimension​序号增大，位置嵌入函数的周期变化越来越平缓</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_positional_encoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 初始化一个positional encoding</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># embed_dim: 字嵌入的维度</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># max_seq_len: 最大的序列长度</span>
</span></span><span class="line"><span class="cl">        <span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">/</span> <span class="n">embed_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">pos</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i 偶数</span>
</span></span><span class="line"><span class="cl">        <span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i+1 奇数</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">positional_encoding</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">get_positional_encoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Sinusoidal Function&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;hidden dimension&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;sequence length&#34;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;dimension 1&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;dimension 2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">positional_encoding</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;dimension 3&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Sequence length&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Period of Positional Encoding&#34;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<h3 id="2-self-attention-mechanism">2. Self Attention Mechanism</h3>
<p>对于输入的句子 $ X $，通过 WordEmbedding 得到该句子中每个字的字向量，同时通过 Positional Encoding 得到所有字的位置向量，将其相加(维度相同，可以直接相加)，得到该字真正的向量表示。第 $ t $ 个字的向量记作 $ x_t $。</p>
<p>接着我们定义三个矩阵 $ W_Q $, $ W_K $, $ W_V $，使用这三个矩阵分别对所有的字向量进行三次线性变换，于是所有的字向量又衍生出三个新的向量 $ q_t $, $ k_t $, $ v_t $。我们将所有的 $ q_t $ 向量拼成一个大矩阵，记作查询矩阵 $ Q $ ，将所有的 $ k_t $ 向量拼成一个大矩阵，记作键矩阵 $ K $  ，将所有的 $ v_t $ 向量拼成一个大矩阵，记作值矩阵 $ V $ (见下图)</p>
<p></p>
<p>为了获得第一个字的注意力权重，我们需要用第一个字的查询向量 $ q_1 $ 乘以键矩阵 $ K $(见下图)</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">                [0, 4, 2]
</span></span><span class="line"><span class="cl">    [1, 0, 2] x [1, 4, 3] = [2, 4, 4]
</span></span><span class="line"><span class="cl">                [1, 0, 1]</span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<p>之后还需要将得到的值经过 softmax，使得它们的和为 1(见下图)</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"> softmax([2, 4, 4]) = [0.0, 0.5, 0.5]</span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<p>有了权重之后，将权重其分别乘以对应字的值向量 $ v_t $(见下图)</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]
</span></span><span class="line"><span class="cl">    0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]
</span></span><span class="line"><span class="cl">    0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]</span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<p>最后将这些<strong>权重化后的值向量求和</strong>，得到第一个字的输出(见下图)</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">      [0.0, 0.0, 0.0]
</span></span><span class="line"><span class="cl">    + [1.0, 4.0, 0.0]
</span></span><span class="line"><span class="cl">    + [1.0, 3.0, 1.5]
</span></span><span class="line"><span class="cl">    -----------------
</span></span><span class="line"><span class="cl">    = [2.0, 7.0, 1.5]</span></span></code></pre></td></tr></table>
</div>
</div><p></p>
<p>对其它的输入向量也执行相同的操作，即可得到通过 self-attention 后的所有输出</p>
<p></p>
<p><strong>矩阵计算</strong></p>
<p>上面介绍的方法需要一个循环遍历所有的字$ x_t $，我们可以把上面的向量计算变成矩阵的形式，从而一次计算出所有时刻的输出</p>
<p>第一步就不是计算某个时刻的$ q_t $, $ k_t $, $ v_t $了，而是一次计算所有时刻的 $
Q $, $ K $, $ V $。计算过程如下图所示，这里的输入是一个矩阵 $ X $，矩阵第 $ t $ 行为第 $ t $ 个词的向量表示 $x_t$</p>
<p></p>
<p>接下来将 $ Q $ 和 $K_T$ 相乘，然后除以 $ \sqrt{d_k} $(这是论文中提到的一个 trick)，经过 softmax 以后再乘以 $ V $ 得到输出</p>
<p></p>
<p><strong>Multi-Head Attention</strong></p>
<p>这篇论文还提出了 Multi-Head Attention 的概念。其实很简单，前面定义的一组 $Q $, $ K $, $ V $, 可以让一个词 attend to 相关的词，我们可以定义多组 $Q $, $ K $, $ V $，让它们分别关注不同的上下文。计算 $Q $, $ K $, $ V $ 的过程还是一样，只不过线性变换的矩阵从一组 $ W^Q $, $ W^K $, $ W^V $ 变成了多组$ W^Q_0 $, $ W^K_0 $, $ W^V_0 $  ，$ W^Q_1 $, $ W^K_1 $, $ W^V_1 $ ，… 如下图所示:</p>
<p></p>
<p>对于输入矩阵 $ X $ ，每一组 $ Q $ 、$ K $ 和 $ V $ 都可以得到一个输出矩阵 $ Z $ 。如下图所示</p>
<p></p>
<p><strong>Padding Mask</strong>
</p>
<p>上面 Self Attention 的计算过程中，我们通常使用 mini-batch 来计算，也就是一次计算多句话，即 $ X $ 的维度是 <code>[batch_size, sequence_length]</code>，sequence_length​是句长，而一个 mini-batch 是由多个不等长的句子组成的，我们需要按照这个 mini-batch 中最大的句长对剩余的句子进行补齐，一般用 0 进行填充，这个过程叫做 padding</p>
<p>但这时在进行 softmax 就会产生问题。回顾 softmax 函数 $\sigma(z_i) = \frac{e^{z_i}}{\sum_K^{j=i} e^{z_j}}$，$e^0$ 是 1，是有值的，这样的话 softmax 中被 padding 的部分就参与了运算，相当于让无效的部分参与了运算，这可能会产生很大的隐患。因此需要做一个 mask 操作，让这些无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置，即</p>
<p>$$
\begin{aligned}Z_{illegal}&amp;=Z_{illegal}+bias_{illegal}\cr
bias_{illegal}&amp;\to-\infty\end{aligned}
$$</p>
<h3 id="3-残差连接和-layer-normalization">3. 残差连接和 Layer Normalization</h3>
<p><strong>残差连接</strong></p>
<p>我们在上一步得到了经过 self-attention 加权之后输出，也就是$\text{Self-Attention(Q, K, V)}$，然后把他们加起来做残差连接</p>
<p>$$X_{\text{embedding}} + \text{Self-Attention(Q, K, V)}$$</p>
<p><strong>Layer Normalization</strong></p>
<p>Layer Normalization 的作用是<strong>把神经网络中隐藏层归一为标准正态分布</strong>，也就是 $i.i.d$ 独立同分布，以起到<strong>加快训练速度，加速收敛</strong>的作用</p>
<p>$$\mu_j=\frac1m\sum_{i=1}^mx_{ij}$$</p>
<p>上式以矩阵的列(column)为单位求均值；</p>
<p>$$\sigma^2_{j} = \frac{1}{m}\sum^m_{i=1}(x_{ij} - \mu_j)^2$$</p>
<p>上式以矩阵的列(column)为单位求方差</p>
<p>$$LayerNorm(x) = \frac{x_{ij} - \mu_{j}}{\sqrt{\sigma^2 + \epsilon}}$$</p>
<p>然后用每一列的每一个元素减去这列的均值，再除以这列的标准差，从而得到归一化后的数值，加 $\epsilon$ 是为了防止分母为 0。</p>
<p></p>
<p>下图展示了更多细节：输入 $x_1, x_2$ 经 self-attention 层之后变成 $z_1, z_2$，然后和输入 $x_1, x_2$ 进行残差连接，经过 LayerNorm 后输出给全连接层。全连接层也有一个残差连接和一个 LayerNorm，最后再输出给下一个 Encoder(每个 Encoder Block 中的 FeedForward 层权重都是共享的)</p>
<p></p>
<h3 id="4-transformer-encoder-整体结构">4. Transformer Encoder 整体结构</h3>
<p>经过上面 3 个步骤，我们已经基本了解了 Encoder 的主要构成部分，下面我们用公式把一个 Encoder block 的计算过程整理一下：</p>
<p>(1). 字向量与位置编码</p>
<p>$$X = \text{Embedding-Lookup(X)} + \text{Positional-Encoding}$$</p>
<p>(2). 自注意力机制</p>
<p>$$Q = Linear_{q}(X) = XW_{Q}$$
$$K = Linear_{k}(X) = XW_{K}$$
$$V = Linear_{v}(X) = XW_{V}$$
$$X_{attention} = \text{Self-Attention(Q, K, V)}$$</p>
<p>(3). self-attention 残差连接与 Layer Normalization</p>
<p>$$X_{attention} = X + X_{attention}$$
$$X_{attention} = LayerNorm(attention)$$</p>
<p>(4). 下面进行 Encoder block 结构图中的第 4 部分，也就是 FeedForward，其实就是两层线性映射并用激活函数激活，比如说 $ReLU$</p>
<p>$$X_{hidden} = Linear(ReLU(Linear(X_{attention})))$$</p>
<p>(5). FeedForward 残差连接与 Layer Normalization</p>
<p>$$X_{hidden} = X_{attention} + X_{hidden}$$
$$X_{hidden} = LayerNorm(X_{hidden})$$</p>
<p>其中
$$X_{hidden} \in \mathbb{R}^{batch_size * seq_len * embed_dim}$$</p>
<h3 id="5-transformer-decoder-整体结构">5. Transformer Decoder 整体结构</h3>
<p>我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是：</p>
<ul>
<li>Masked Multi-Head Self-Attention</li>
<li>Multi-Head Encoder-Decoder Attention</li>
<li>FeedForward Network</li>
</ul>
<p>和 Encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 Layer Normalization。Decoder 的中间部件并不复杂，大部分在前面 Encoder 里我们已经介绍过了，但是 Decoder 由于其特殊的功能，因此在训练时会涉及到一些细节</p>
<p></p>
<p><strong>Masked Self-Attention</strong></p>
<p>具体来说，传统 Seq2Seq 中 Decoder 使用的是 RNN 模型，因此在训练过程中输入 $t$ 时刻的词，模型无论如何也看不到未来时刻的词，因为循环神经网络是时间驱动的，只有当 $t$ 时刻运算结束了，才能看到 $t + 1$ 时刻的词。而 Transformer Decoder 抛弃了 RNN，改为 Self-Attention，由此就产生了一个问题，<font color=red>在训练过程中，整个 ground truth 都暴露在 Decoder 中</font>，这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 Mask
</br>
举个例子，Decoder 的 ground truth 为 &ldquo;<start> I am fine&rdquo;，我们将这个句子输入到 Decoder 中，经过 WordEmbedding 和 Positional Encoding 之后，将得到的矩阵做三次线性变换 $(W_Q, W_K, W_V)$。然后进行 self-attention 操作，首先通过得到 Scaled Scores，接下来非常关键，我们要<strong>对 Scaled Scores 进行 Mask</strong>，举个例子，当我们输入 &ldquo;I&rdquo; 时，模型目前仅知道包括 &ldquo;I&rdquo; 在内之前所有字的信息，即 &ldquo;<start>&rdquo; 和 &ldquo;I&rdquo; 的信息，不应该让其知道 &ldquo;I&rdquo; 之后词的信息。道理很简单，我们做预测的时候是按照顺序一个字一个字的预测，怎么能这个字都没预测完，就已经知道后面字的信息了呢？Mask 非常简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可</p>
<p></p>
<p>之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重</p>
<p></p>
<p>Multi-Head Self-Attention 无非就是并行的对上述步骤多做几次，前面 Encoder 也介绍了，这里就不多赘述了</p>
<p><strong>Masked Encoder-Decoder Attention</strong></p>
<p>其实这一部分的计算流程和前面 Masked Self-Attention 很相似，结构一模一样，唯一不同的是这里的 K, V为 Encoder 的输出，Q 为 Decoder 中 Masked Self-Attention 的输出</p>
<p></p>
<h3 id="6-总结">6. 总结</h3>
<p>到此为止，Transformer 中 95% 的内容已经介绍完了，我们用一张图展示其完整结构。不得不说，Transformer 设计的十分巧夺天工。</p>
<p></p>
<p>下面有几个问题，是我从网上找的，感觉看完之后能对 Transformer 有一个更深的理解</p>
<p><font color=red>Transformer 为什么需要进行 Multi-head Attention？</font></p>
<ul>
<li>原论文中说到进行 Multi-head Attention 的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次 attention，多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用多个卷积核的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征信息</li>
</ul>
<p><font color=red>Transformer 相比于 RNN/LSTM，有什么优势？为什么？</font></p>
<ul>
<li>RNN 系列的模型，无法并行计算，因为 T 时刻的计算依赖 T-1 时刻的隐层计算结果，而 T-1 时刻的计算依赖 T-2 时刻的隐层计算结果</li>
<li>Transformer 的特征抽取能力比 RNN 系列的模型要好</li>
</ul>
<p><font color=red>为什么说 Transformer 可以代替 seq2seq？</font></p>
<ul>
<li>这里用代替这个词略显不妥当，seq2seq 虽已老，但始终还是有其用武之地，seq2seq 最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>，并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词 (token) 的隐藏状态。在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，而且这样一股脑的把该固定向量送入 Decoder 端，Decoder 端不能够关注到其想要关注的信息。</li>
<li>Transformer 不但对 seq2seq 模型这两点缺点有了实质性的改进 (多头交互式 attention 模块)，而且还引入了 self-attention 模块，让源序列和目标序列首先 “自关联” 起来，这样的话，源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力，并且 Transformer 并行计算的能力远远超过了 seq2seq 系列模型</li>
</ul>
<h3 id="7-参考文章">7. 参考文章</h3>
<ul>
<li><a href="http://mantchs.com/2019/09/26/NLP/Transformer/"target="_blank" rel="external nofollow noopener noreferrer">Transformer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/"target="_blank" rel="external nofollow noopener noreferrer">The Illustrated Transformer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="http://www.peterbloem.nl/blog/transformers"target="_blank" rel="external nofollow noopener noreferrer">TRANSFORMERS FROM SCRATCH<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4"target="_blank" rel="external nofollow noopener noreferrer">Seq2seq pay Attention to Self Attention: Part 2<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p>ref:</br>
[1]. <a href="https://www.bilibili.com/video/BV1mk4y1q7eK?p=1"target="_blank" rel="external nofollow noopener noreferrer">B站讲解视频<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2]. <a href="https://wmathor.com/index.php/archives/1438/"target="_blank" rel="external nofollow noopener noreferrer">https://wmathor.com/index.php/archives/1438/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[3]. <a href="https://wmathor.com/index.php/archives/1455/"target="_blank" rel="external nofollow noopener noreferrer">Transformer的pytorch实现<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
]]></description></item><item><title>Transformer | 如何理解attention中的Q,K,V？</title><link>https://jianye0428.github.io/posts/attentionaqkv/</link><pubDate>Sun, 20 Aug 2023 17:44:07 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/attentionaqkv/</guid><description><![CDATA[<h2 id="解答一">解答一</h2>
<p>我们直接用torch实现一个SelfAttention来说一说：</p>
<ol>
<li>首先定义三个线性变换矩阵，query, key, value：</li>
</ol>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BertSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意，这里的query, key, value只是一种操作(线性变换)的名称，实际的Q/K/V是它们三个的输出</p>
<ol start="2">
<li>
<p>假设三种操作的输入都是同一个矩阵(暂且先别管为什么输入是同一个矩阵)，这里暂且定为长度为L的句子，每个token的特征维度是768，那么输入就是(L, 768)，每一行就是一个字，像这样：
</br>
乘以上面三种操作就得到了Q/K/V，(L, 768)*(768,768) = (L,768)，维度其实没变，即此刻的Q/K/V分别为：

代码为:</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BertSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl"> <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span> <span class="c1"># hidden_states 维度是(L, 768)</span>
</span></span><span class="line"><span class="cl">     <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>然后来实现这个操作:
$$Attention(Q,K_i,V_i)\color{red}{\boxed{=softmax(\frac{Q^TK_i}{\sqrt{d_k}})V_i}}$$
① 首先是Q和K矩阵乘，(L, 768)*(L, 768)的转置=(L,L)，看图：

首先用Q的第一行，即“我”字的768特征和K中“我”字的768为特征点乘求和，得到输出(0，0)位置的数值，这个数值就代表了“我想吃酸菜鱼”中“我”字对“我”字的注意力权重，然后显而易见输出的第一行就是“我”字对“我想吃酸菜鱼”里面每个字的注意力权重；整个结果自然就是“我想吃酸菜鱼”里面每个字对其它字(包括自己)的注意力权重(就是一个数值)了~</p>
<p>② 然后是除以根号dim，这个dim就是768，至于为什么要除以这个数值？主要是为了缩小点积范围，确保softmax梯度稳定性，具体推导可以看这里：<a href="https://zhuanlan.zhihu.com/p/149903065"target="_blank" rel="external nofollow noopener noreferrer">莲生三十二：Self-attention中dot-product操作为什么要被缩放<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，然后就是为什么要softmax，一种解释是为了保证注意力权重的非负性，同时增加非线性，还有一些工作对去掉softmax进行了实验，如<a href="https://zhuanlan.zhihu.com/p/157490738"target="_blank" rel="external nofollow noopener noreferrer">PaperWeekly：线性Attention的探索：Attention必须有个Softmax吗？<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>③ 然后就是刚才的注意力权重和V矩阵乘了，如图：
</p>
<p>注意力权重 x VALUE矩阵 = 最终结果 </br>
首先是“我”这个字对“我想吃酸菜鱼”这句话里面每个字的注意力权重，和V中“我想吃酸菜鱼”里面每个字的第一维特征进行相乘再求和，这个过程其实就相当于用每个字的权重对每个字的特征进行加权求和，然后再用“我”这个字对对“我想吃酸菜鱼”这句话里面每个字的注意力权重和V中“我想吃酸菜鱼”里面每个字的第二维特征进行相乘再求和，依次类推~最终也就得到了(L,768)的结果矩阵，和输入保持一致~</p>
<p>整个过程在草稿纸上画一画简单的矩阵乘就出来了，一目了然~最后上代码：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BertSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl"> <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span> <span class="c1"># 输入768， 输出768</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">hidden_states</span><span class="p">):</span> <span class="c1"># hidden_states 维度是(L, 768)</span>
</span></span><span class="line"><span class="cl">     <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">     <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">attention_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="k">return</span> <span class="n">out</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>为什么叫<strong>自注意力网络</strong>？</br>
因为可以看到Q/K/V都是通过同一句话的输入算出来的，按照上面的流程也就是一句话内每个字对其它字(包括自己)的权重分配；那如果不是自注意力呢？简单来说，Q来自于句A，K、V来自于句B即可~</br></p>
</li>
<li>
<p>注意，K/V中，如果同时替换任意两个字的位置，对最终的结果是不会有影响的，至于为什么，可以自己在草稿纸上画一画矩阵乘；也就是说注意力机制是没有位置信息的，不像CNN/RNN/LSTM；这也是为什么要引入positional embeding的原因。</p>
</li>
</ol>
<h2 id="解答二">解答二</h2>
<p>其实直接用邱锡鹏老师PPT里的一张图就可以直观理解——假设D是输入序列的内容，完全忽略线性变换的话可以近似认为Q=K=V=D(所以叫做Self-Attention，因为这是输入的序列对它自己的注意力)，于是序列中的每一个元素经过Self-Attention之后的表示就可以这样展现：</p>
<p></p>
<p>也就是说，The这个词的表示，实际上是整个序列加权求和的结果——权重从哪来？点积之后Softmax得到——这里Softmax(QK)就是求权重的体现。我们知道，向量点积的值可以表征词与词之间的相似性，而此处的“整个序列”包括The这个词自己(再一次强调这是Self-Attention)，所以最后输出的词的表示，其“主要成分”就主要地包含它自身和跟它相似的词的表示，其他无关的词的表示对应的权重就会比较低。</p>
<h2 id="解答三">解答三</h2>
<p>首先附上链接：<a href="https://zhuanlan.zhihu.com/p/37601161"target="_blank" rel="external nofollow noopener noreferrer">张俊林：深度学习中的注意力模型(2017版)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 。这个几乎是我读到过的讲解Attention最为透彻的篇章之一了。</p>
<p>Q(Querry)代表查询值，对应Decoder的H(t-1)状态。这里要正确理解H(t-1)，想要解码出t时刻的输出，你送入Decoder的必然有前一时刻计算出的隐状态。好了，所谓查询，就是你要拿着这个Decoder中的H(t-1)去和Encoder中各个时刻的隐状态<a href="%e4%b9%9f%e5%b0%b1%e6%98%af%e5%90%84%e4%b8%aaKey">H(1), H(2), &hellip; , H(T)</a>去比，也就是二者计算相似度(对应于文献中的各种energy函数)。最后算出来的结果用Softmax归一化，这个算出来的权重就是带有注意力机制的权重，其实在翻译任务中，Key和Value是相等的。在Transformer的实现源码中，Key和Value的初始值也是相等的。有了这个权重之后，就可以用这个权重对Value进行加权求和，生成的这个新的向量就是带有注意力机制的语义向量 Context vector，而这个语义向量会权衡Target与Source的token与token的关系，从而实现解码输出时，与Source中“真正有决定意义”的token关联。</p>
<p>姑且画蛇添足的再说几句：
首先，Attention机制是由Encoder-Decoder架构而来，且最初是用于完成NLP领域中的翻译(Translation)任务。那么输入输出就是非常明显的 Source-Target的对应关系，经典的Seq2Seq结构是从Encoder生成出一个语义向量(Context vector)而不再变化，然后将这个语义向量送入Decoder配合解码输出。这种方法的最大问题就是这个语义向量，我们是希望它一成不变好呢？还是它最好能配合Decoder动态调整自己，来使Target中的某些token与Source中的真正“有决定意义”的token关联起来好呢？
这就是为什么会有Attention机制的原因。说到底，Attention机制就是想生成会动态变化的语义向量来配合解码输出。而新贵 Self-Attention则是为了解决Target与Source各自内部token与token的关系。在Transformer中，这两种注意力机制得到了有机的统一，释放出了异常惊人的潜力。</p>
<p>ref:</br>
[1]. <a href="https://mp.weixin.qq.com/s/v7N3lhMBSdoGCz4K3TmsmA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/v7N3lhMBSdoGCz4K3TmsmA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>VectorNet 论文解读</title><link>https://jianye0428.github.io/posts/vectornet/</link><pubDate>Sun, 16 Jul 2023 17:13:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/vectornet/</guid><description><![CDATA[<h2 id="novel-highlights">Novel Highlights</h2>
<p>(1) 使用矢量化的高精地图以及障碍物的历史轨迹，从而避免有损渲染以及ConvNet编码(计算开销比较大)。</p>
<p>(2) 设计子图网络以及全局图网络，建模低阶以及高阶交互</p>
<p>(3) auxiliary task 提高网络性能</p>
<p></p>
<h2 id="vecotornet-网络介绍">VecotorNet 网络介绍</h2>
<h3 id="轨迹和地图的向量表示-representing-trajectories-and-hd-maps">轨迹和地图的向量表示 Representing trajectories and HD maps</h3>
<p>lane可以表示为splines，人行道可以表示为一个很多个点组成的polygon，stop sign标记可以表示为单一个点。 对于agent来说，他们的轨迹也是一种splines。 这些元素都可以向量表示。</p>
<ul>
<li>对于地图的特征：选择一个start point和朝向，等间距均匀采样关键点，并于相邻的关键点相连为向量</li>
<li>对于agent轨迹，按照0.1s sample关键点，并将它们连接成向量。</li>
</ul>
<p>通过向量化的过程，可以得到折线polylines，这个polylines和轨迹、地图标注之间是一一对应的。如果给定的时空间隔足够小，得到的这些折线就与原始地图和轨迹十分接近。</p>
<p>我们将属于折线 $P_j$​ 的每一个向量$v_i$看作图中的一个节点，节点特征如下:</p>
<p>$$v_i = [d_i^s, d_i^e, a_i, j]$$</p>
<ul>
<li>其中前两个vector分别是vector的start point和end point的坐标，可以是(x,y)或者(x,y,z)三维的形式</li>
<li>第三个向量则是attribute属性的特征，比如object的类型，轨迹的时间戳，道路的特征，道路限速等</li>
<li>最后一个是障碍物id，表示 $v_i$ ​属于 $P_j$</li>
</ul>
<h3 id="polyline-子图构建">Polyline 子图构建</h3>
<p>对于一个Polyline P, 它的节点有 ${v_1,v_2,&hellip;,v_p}$， 可以定义一个子图网络：</p>
<p>$$v_i^{l+1} = \varphi_{rel}(g_{enc}(v_i^{(l)}), \varphi({g_{enc}(v_j^{(l)})}))$$</p>
<ul>
<li>
<p>$v_i^{(l)}$​ 代表第i个节点第L层的节点特征。</p>
</li>
<li>
<p>$g_{enc}(\cdot)$代表节点的变换，实践中采用MLP来实现。</p>
</li>
<li>
<p>$\varphi_{agg}(\cdot)$代表特征聚合，用来从相邻的节点来获取信息，实践中采用的是max_pooling。</p>
</li>
<li>
<p>$\varphi_{rel}(\cdot)$代表vi和周围节点的关系，实践中采用的是concate的操作。</p>
</li>
</ul>
<p></p>
<p>最后经过多层的堆叠，来获取整个Polyline级别的特征：</p>
<p>$$P = \varphi_{agg}(v_i^{L_p})$$</p>
<p>这里， $φ_{agg}(⋅)$也是max pooling操作.</p>
<h3 id="全局图的高阶交互-global-graph-for-high-order-interactions">全局图的高阶交互 Global graph for high-order interactions</h3>
<p>经过上面的子图进行低阶模型建模后，现在有了polyline级别节点的特征${p_1,p_2,&hellip;,p_P}$.</p>
<p>为了建立高阶的交互，需要建立一个global的交互图，详见论文图2的第3个子图。</p>
<p>$$P_i^{l+1} = GNN(p^l_i, A)$$</p>
<ul>
<li>
<p>$p_i^l$​代表polyline节点的集合</p>
</li>
<li>
<p>A代表邻接矩阵，实践中采用全链接</p>
</li>
<li>
<p>$GNN(⋅)$代表一层的GNN网络，实践中采用的是self attention layer：
$$GNN(P) = softmax(P_Q P_K^T)P_V$$</p>
<p>其中，P是node的feature matrix， $P_Q$,$P_k$,$P_v$ ​则是它的线性投影。</p>
</li>
</ul>
<p>经过了全局的网络之后，就生成了节点的特征$P^{L_t}_i$，其中Lt是全局GNN网络的层数。然后将$P^{(L_t)}_i$放入decoder进行轨迹的生成:</p>
<p>$$v_i^{future} = \varphi_{traj}(P_i^{L_t})$$</p>
<p>论文中，decoder $φ_{traj}(⋅)$ 使用的是MLP，当然也可以用MultiPath中anchor-based的方法或者variational RNNs 来进行多模态轨迹预测。</p>
<h3 id="辅助任务训练-auxiliary-graph-completion-task">辅助任务训练 auxiliary graph completion task</h3>
<p>为了让全局交互图能更好地捕捉不同轨迹和地图元素之间的交互信息，论文还提出了一个辅助的任务：在训练过程中，随机mask掉一些节点的特征，然后尝试去还原被掩盖的节点特征:</p>
<p>$$\hat{P}<em>_i = \varphi</em>_{node}(P_i^{L_t})$$</p>
<p>这里节点的decoder $φ_{node}(⋅)$ 也是一个MLP，只在训练的时候使用,在inference过程中不使用。</p>
<h3 id="损失函数-loss-function">损失函数 Loss Function</h3>
<p>多任务训练目标， multi-task training task:</p>
<p>$$\mathcal{L} = \mathcal{L_{traj}} + \alpha \mathcal{L_{node}}$$</p>
<ul>
<li>
<p>$L_{traj}​$: negative Gaussian log-likelihood loss</p>
</li>
<li>
<p>$L_{node}$​: 是预测的节点和被掩盖节点的huber损失函数</p>
</li>
</ul>
<p>其中，
negative Gaussian Log Likelihood 损失函数为:</p>
<p>$$L(x, y) = -\log P(y) = - \log P(y|\mu(x), \sum(x))$$</p>
<p>where,</p>
<p>$$p(y) = p(y∣μ,Σ)=1(2π)n/2∣Σ∣1/2exp−12(y−μ)⊤Σ−1(y−μ)$$</p>
<p>Huber 损失函数为:</p>
<p>$$ L(Y|f(x))= \begin{cases} \frac{1}{2} (Y-f(x))^2, &amp; |Y-f(x)|&lt;= \delta \\ \delta |Y-f(x)| - \frac{1}{2}\delta^2, &amp; |Y-f(x)| &gt; \delta \end{cases} $$</p>
<h2 id="整理">整理</h2>
<p><strong>VectorNet数据处理部分:</strong></p>
<ul>
<li>
<p>对actor的处理:</p>
<ul>
<li>输入: 取轨迹点，每两个轨迹点构建vector, 形式为(x1, x2, y1, y2), 其他特征(object type, timestamp, track_id)</li>
</ul>
</li>
<li>
<p>对lane node的处理:</p>
<ul>
<li>输入: 针对lane segment 的点，求polyline，原则上求lane segment的左右边界的点的向量(x_start, x_end, y_start, y_end, turn_direction, traffic_control, is_intersection, lane_id)</li>
</ul>
</li>
</ul>
<p><strong>网络部分:</strong></p>
<ul>
<li>
<p>构建subgraphnet: 针对每一个polyline，通过mlp和maxpool构建subgraphnet</p>
</li>
<li>
<p>构建globalgraphnet: 以每个polyline作为graph node，构建全局图网络，采用全链接，通过自注意力机制$GNN(P) = softmax(P_Q, P_K)^T(P_V)$</p>
</li>
</ul>
<p><strong>轨迹生成:</strong></p>
<p>将全局网络的节点特征，通过mlp进行轨迹生成。</p>
<p><code>ref link</code>:
[1] <a href="https://blog.csdn.net/qq_41897558/article/details/120087113"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_41897558/article/details/120087113<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2] <a href="https://zhuanlan.zhihu.com/p/355131328"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/355131328<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
<code>ref code</code>:
[1]https://github.com/xk-huang/yet-another-vectornet</br>
[2]https://github.com/DQSSSSS/VectorNet</br></p>
]]></description></item><item><title>LaneGCN 论文解读</title><link>https://jianye0428.github.io/posts/lanegcn/</link><pubDate>Sun, 16 Jul 2023 15:53:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/lanegcn/</guid><description><![CDATA[<p><code>paper link:</code> <a href="https://arxiv.org/abs/2007.13732"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2007.13732<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<code>PPT:</code> <a href="https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="architechture">Architechture</h2>
<p><strong><font color=red>Lane Graph + Actor Map:</font></strong></p>
<ul>
<li>
<p>construct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失</p>
</li>
<li>
<p>LaneGCN:</p>
<ul>
<li>
<p>extends <strong>graph convolutions with multiple adjacency matrices</strong> and along-lane dilation</p>
<ul>
<li>to capture complex topology and long range dependencies of the lane graph.</li>
</ul>
</li>
<li>
<p>exploit a <strong>fusion network</strong> consisting of four types of interactions: <code>actor-to-lane</code>, <code>lane-to-actor</code>, <code>actor-to-actor</code>, <code>lane-to-lane</code>.</p>
<ul>
<li>present both actors and lanes as nodes in the graph and use a 1D CNN and LaneGCN to extract the features for the actor and lane nodes respectively, and then exploit spatial attention and another LaneGCN to model four types of interactions.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p></p>
<p><strong><font color=red>Difference between VectorNet and LaneGCN:</font></strong></p>
<ul>
<li><u>VecotrNet</u> uses vanilla graph networks with undirected full connections; <u>LaneGCN</u> uses connected lane graph following the map topology and propose task specific multi-type and dilated graph operators.</li>
<li>VectorNet uses polyline-level nodes for interactions; LaneGCN uses polyline segments as map nodes to capture higher resolution.</li>
</ul>
<h2 id="lane-graph-representations-for-motion-forecasting">Lane Graph Representations for Motion Forecasting</h2>
<p></p>
<h3 id="font-colorredactornetfont-extracting-traffic-participant-representations"><font color=red>ActorNet</font>: Extracting Traffic Participant Representations</h3>
<p>Each Trajctory is represented as a sequence of displacement ${ \bigtriangleup{p_{-(T-1)},&hellip;,\bigtriangleup{p_{-1}}, \bigtriangleup{p_0}}}$, where $\bigtriangleup{p_t}$ is the 2D displacement from time step $t-1$ to t, and T is the trajectory size.</p>
<p>For trajectories with sizes smaller than $T$ , we pad them with zeros. We add a binary $1 × T$ mask to indicate if the element at each step is padded or not and concatenate it with the trajectory tensor, resulting in an input tensor of size $3 × T$.</p>
<p>1D CNN is used to process the trajectory input for its effectiveness in extracting multi-scale features
and efficiency in parallel computing. The output of ActorNet is a temporal feature map, whose element at $t = 0$ is used as the actor feature. The network has 3 groups/scales of 1D convolutions.</p>
<p>Each group consists of 2 residual blocks, with the stride of the first block as 2. We then use a <strong><font color=red>Feature Pyramid Network (FPN)</font></strong>  to fuse the multi-scale features, and apply another residual block to obtain the output tensor. For all layers, the convolution kernel size is 3 and the number of output channels is 128. <strong><font color=red>Layer Normalization</font></strong> and the <strong><font color=red>Rectified Linear Unit (ReLU)</font></strong> are used after each convolution.</p>
<p></p>
<h3 id="font-colorredmapnetfont-extracting-structured-map-representation"><font color=red>MapNet</font>: Extracting Structured Map Representation</h3>
<p><strong>General Architecture</strong>:</p>
<ul>
<li>part 1: building a lane graph from vectorized map data;</li>
<li>part 2: applying our novel LaneGCN to the lane graph to output the map features.</li>
</ul>
<p><strong>Map Data:</strong></p>
<p>In this paper, we adopt a simple form of <strong>vectorized map data</strong> as our representation of HD maps. Specifically, the map data is represented as a set of lanes and their connectivity. Each lane contains a centerline, i.e., <font color=green>a sequence of 2D BEV points</font>, which are arranged following the lane direction (see Fig. 3, top). For any two lanes which are directly reachable, 4 types of connections are given: <code>predecessor</code>, <code>successor</code>, <code>left neighbour</code> and <code>right neighbour</code>.</p>
<p><strong>Lane Graph Construction:</strong></p>
<p>first define a lane node as the straight line segment formed by any two consecutive points (grey circles in Fig. 3) of the centerline. The location of a lane node is the averaged coordinates of its two end points. Following the connections between lane centerlines, we also derive <font color=red>4 connectivity types</font> 4 connectivity types for the lane nodes, i.e., <code>predecessor</code>, <code>successor</code>, <code>left neighbour</code> and <code>right neighbour</code>.</p>
<p>We denote the lane nodes with $V ∈ \mathbb R^{N ×2}$ , where $N$ is the number of lane nodes and the $i$-th row of $V$ is the BEV coordinates of the $i$-th node. We represent the connectivity with 4 adjacency matrices ${\lbrace A_i \rbrace}_{i \in {pre,suc,left,right}}$ , with $A_i \in \mathbb R^{N ×N}$.</p>
<p>We denote $A_{i,j,k}$, as the element in the $j$-th row and $k$-th column of $A_i$. Then $A_{i,j,k} = 1$ if node $k$ is an $i$-type neighbor of node $j$.</p>
<p><strong>LaneConv Operator:</strong></p>
<p><font color=green><em>Node Feature:</em></font>
Each lane node corresponds to a straight line segment of a centerline. To encode all the lane node information, we need to take into account both the shape (size and orientation) and the location (the coordinates of the center) of the corresponding line segment. We parameterize the node feature as follows,</p>
<p>$$x_i = MLP_{shape} (v_{i}^{end} - v_{i}^{start}) + MLP_{loc}(v_i) $$</p>
<p>where $MLP$ indicates a multi-layer perceptron and the two subscripts refer to shape and location, respectively. $v_i$ is the location of the i-th lane node, i.e., the center between two end points, $v_i^{start}$ and $v_i^{end}$ are the BEV coordinates of the node $i&rsquo;s$ starting and ending points, and $x_i$ is the $i$-th row of the node feature matrix $X$, denoting the input feature of the $i$-th lane node.</p>
<p><font color=green><em>LaneConv:</em> </font>
To aggregate the topology information of the lane graph at a larger scale, we design the following LaneConv operator:</p>
<p>$$Y = XW_0 + \sum_{i\in{pre, suc, left, right}}A_iXW_i,\tag{2}$$</p>
<p>where $A_i$ and $W_i$ are the adjacency and the weight matrices corresponding to the $i$-th connection type respectively. Since we order the lane nodes from the start to the end of the lane, $A_{suc}$ and $A_{pre}$ are matrices obtained by shifting the identity matrix (diagnal 1) one step towards upper right (non-zero superdiagonal) and lower left (non-zero subdiagonal). $A_{suc}$ and $A_{pre}$ can propagate information from the forward and backward neighbours whereas $A_{left}$ and $A_{right}$ allow information to flow from the cross-lane neighbours. It is not hard to see that our LaneConv builds on top of the general graph convolution and encodes more geometric (e.g., connection type/direction) information. As shown in our experiments this improves over the vanilla graph convolution.</p>
<p><font color=green><em>Dilated LaneConv:</em></font></p>
<p>Functionality: The model needs to capture the long range dependency along the lane direction for accurate prediction.</p>
<p>the k-dilation LaneConv operator is defined as follows:</p>
<p>$$Y = XW_0 + A_{pre}^k XW_{pre,k} + A_{suc}^k X W_{suc,k} \tag{3}$$</p>
<p>where $A_{pre}^k$ is the $k$-th matrix power of $A_{pre}$. This allows us to directly propagate information along the lane for $k$ steps, with $k$ a hyperparameter. Since $A_{pre}^k$ is highly sparse, one can efficiently compute it using sparse matrix multiplication. Note that the dilated LaneConv is only used for predecessor and successor, as the long range dependency is mostly along the lane direction.</p>
<p><font color=green><em>LaneGCN:</em></font></p>
<p>With Eq.(2) and Eq.(3), we get a multi-scale LaneConv operator with C dilation size as follows:</p>
<p>$$Y = XW_0 + \sum_{i\in \lbrace left, right \rbrace} A_i X W_i + \sum_{c=1}^C (A_{pre}^{k_c}XW_{pre, k_c} + A_{suc}^{k_c}XW_{suc, k_c})， \tag{4}$$</p>
<p>where $k_c$ is the $c$-th dilation size. We denote $LaneConv(k_1 , · · · , k_C)$ this multi-scale layer.</p>
<p></p>
<h3 id="font-colorredfusion-netfont"><font color=red>Fusion Net</font></h3>
<p>Four types fusion modules:</p>
<ul>
<li>A2L: introduces real-time traffic information to lane nodes, such as blockage or usage of the lanes.</li>
<li>L2L: updates lane node features by propagating the traffic information over the lane graph. -&gt; LaneGCN</li>
<li>L2A: fuses updated map features with real-time traffic information back to the actors.</li>
<li>A2A: handles the interactions between actors and produces the output actor features, which are then used by the prediction header for motion forecasting.</li>
</ul>
<p>We implement L2L using another LaneGCN, which has the same architecture as the one used in our MapNet (see Section 3.2). In the following we describe the other three modules in detail. We exploit a spatial attention layer for A2L, L2A and A2A. The attention layer applies to each of the three modules in the same way. Taking A2L as an example, given an actor node i, we aggregate the features from its context lane nodes j as follows:</p>
<p>$$y_i = x_i W_0 + \sum_j \phi (concat(x_i, \Delta_{i,j}, x_j)W_1)W_2, \tag{5}$$</p>
<p>with $x_i$ the feature of the $i$-th node, $W$ a weight matrix, $\phi$ the compositon of layer notmalization and RelU, and $\Delta_{ij} = MLP(v_j - v_i)$, where $v$ denotes the node location.</p>
<h3 id="font-colorredprediction-headerfont"><font color=red>Prediction Header</font></h3>
<p>Take after-fusion actor features as input, a multi-modal prediction header outputs the final motion forecasting. For each actor, it predicts $K$ possible future trajectories and their confidence scores.</p>
<p>The header has two branches, <strong>a regression branch</strong> to predict
the trajectory of each mode and <strong>a classification branch</strong> to predict the confidence score of each mode.</p>
<p>For the m-th actor, we apply a residual block and a linear layer in the
regression branch to regress the K sequences of BEV coordinates:</p>
<p>$$O_{m,reg} = \lbrace (p_{m,1}^k, p_{m,2}^k, &hellip;, p_{m,T}^k) \rbrace _{k\in[0,K-1]}$$</p>
<p>where $p_{m,i}^k$ is the predicted $m$-th actor&rsquo;s BEV coordinates of the $k$-th mode at the $i$-th time step. For the classification branch, we apply an MLP to $p^k_{m,T} − p_{m,0}$ to get $K$ distance embeddings. We then concatenate each distance embedding with the actor feature, apply a residual block and a linear layer to output $K$ confidence scores, $O_{m,cls} = (c_{m,0}, c_{m,1}, &hellip;, c_{m,K−1})$.</p>
<h3 id="font-colorredlearningfont"><font color=red>Learning</font></h3>
<p>use the sum of classification and regreesion losses to train the model:</p>
<p>$$ L = L_{cls} + \alpha L_{reg},$$</p>
<p>where $\alpha = 1.0$.</p>
<p>For classification, we use the <strong>max-margin loss</strong>:</p>
<p>$$L_{cls} = \frac{1}{M(K-1)}\sum_{m=1}^M \sum_{k \neq \hat{k}} \max(0, c_{m,k} + \epsilon - c_{m, \hat{k}}) \tag{6}$$</p>
<p>where $\epsilon$ is the margin and $M$ is the total number of actors. For regression, we apply the smooth $l1$ loss on all predicted time steps:</p>
<p>$$L_{reg} = \frac{1}{MT} \sum_{m=1}^M \sum_{t=1}^T reg(p_{m,y}^{\hat{k}} - p_{m,t}^*) \tag{7}$$</p>
<p>where $p_t^*$ is the ground truth BEV coordinates at time step $t$, $reg(x) = \sum\limits_i d(x_i)$, $x_i$ is the $i$-th element of $x$, and $d(x_i)$ is the smooth $\ell1$ loss defined as:</p>
<p>$$d(x_i) = \begin{cases}
0.5x_i^2 &amp;\text{if} ||x|| &lt; 1, \
||x_i|| - 0.5 &amp; \text{otherwise,}
\end{cases} \tag{8}$$</p>
<p>where $||x_i||$ denotes the $\ell1$ norm of $x_i$.</p>
<h3 id="font-colorred-neural-network-layoutfont"><font color=red> Neural Network Layout</font></h3>
<p></p>
<h3 id="font-colorreddata-process-and-network-constructionfont"><font color=red>Data Process And Network Construction</font></h3>
<blockquote>
<p>以官方的2645.csv数据集为例子</p>
</blockquote>
<p><strong>agent node:</strong></p>
<ul>
<li><code>data['city']:</code>城市名称</li>
<li><code>data['trajs'] = [agt_traj] + ctx_trajs:</code>轨迹点，(agent + context vehicles)</li>
<li><code>data['steps'] = [agt_step] + ctx_steps:</code>在原始数据中的位置</li>
<li><code>data['feats'] = feats:</code> (13 X 20 X 3) 前20预测轨迹 + 一维是否存在点</li>
<li><code>data['ctrs'] = ctrs:</code> (13 X 2) 中心点</li>
<li><code>data['orig'] = orig:</code> AGENT 当前点坐标</li>
<li><code>data['theta'] = theta:</code> AGENT 偏转角</li>
<li><code>data['rot'] = rot:</code> (2 X 2) 旋转矩阵</li>
<li><code>data['gt_preds'] = gt_preds:</code>(13 X 30 X 2) 后30帧真实轨迹</li>
<li><code>data['has_preds'] = has_preds:</code> (13 X 30) 标识后30帧轨迹是否存在</li>
</ul>
<p><strong>lane node:</strong></p>
<ul>
<li><code>graph['ctrs'] = np.concatenate(ctrs, 0):</code> lane node的中心点坐标</li>
<li><code>graph['num_nodes'] = num_nodes:</code> lane node的数量</li>
<li><code>graph['feats'] = np.concatenate(feats, 0):</code> lane node 方向向量</li>
<li><code>graph['turn'] = np.concatenate(turn, 0):</code> lane node 转向标识</li>
<li><code>graph['control'] = np.concatenate(control, 0):</code> lane node 的 has_traffic_control 标识</li>
<li><code>graph['intersect'] = np.concatenate(intersect, 0):</code> lane node 的 is_intersection 标识</li>
<li><code>graph['pre'] = [pre]:</code> pre[&lsquo;u&rsquo;] 和 pre[&lsquo;v&rsquo;], v 是 u 的pre， 这里表述的是lane node之间的关系</li>
<li><code>graph['suc'] = [suc]:</code> suc[&lsquo;u&rsquo;] 和 suc[&lsquo;v&rsquo;], v 是 u 的suc， 这里表述的是lane node之间的关系</li>
<li><code>graph['lane_idcs'] = lane_idcs:</code> lane node index
<ul>
<li>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="o">...</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="o">...</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl"><span class="mi">83</span> <span class="mi">83</span> <span class="mi">83</span> <span class="o">...</span> <span class="mi">83</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li><code>graph['pre_pairs'] = pre_pairs:</code> pair 表述的是lane之间的关系</li>
<li><code>graph['suc_pairs'] = suc_pairs:</code> pair 表述的是lane之间的关系</li>
<li><code>graph['left_pairs'] = left_pairs:</code> pair 表述的是lane之间的关系</li>
<li><code>graph['right_pairs'] = right_pairs:</code> pair 表述的是lane之间的关系
<ul>
<li>对于<code>pre['u']</code>和<code>pre['v']</code>, v 是 u 的 pre</li>
<li>对于<code>suc['u']</code>和<code>suc['v']</code>, v 是 u 的 suc</li>
<li>对于<code>left['u']</code>和<code>left['v']</code>, v 是 u 的 left</li>
<li>对于<code>right['u']</code>和<code>right['v']</code>, v 是 u 的 right</li>
</ul>
</li>
</ul>
<p><strong>Net结构</strong></p>
<ul>
<li><strong>ActorNet</strong>
<code>input:</code> M x 3 x 20
<code>output:</code> M x 128 x 20</li>
</ul>
<p>解释:</p>
<ul>
<li>
<p><strong>MapNet</strong>: 把 v 按照 u 加到center上
<code>input:</code> N x 4
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>A2M</strong>
<code>input:</code> N x 128
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>M2M</strong>
<code>input:</code> N x 128
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>M2A</strong>
<code>input:</code> N x 128
<code>output:</code> M x 128</p>
</li>
<li>
<p><strong>A2A</strong>
<code>input:</code> N x 128
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>Prediction Header:</strong>
<code>input</code> M x 128</p>
<ul>
<li>MLP Regression</li>
<li>MLP Classification</li>
</ul>
</li>
</ul>
<p>ref link: <a href="https://zhuanlan.zhihu.com/p/447129428"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/447129428<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>C++ 基础知识[一]</title><link>https://jianye0428.github.io/posts/basics_one/</link><pubDate>Tue, 11 Jul 2023 19:37:05 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/basics_one/</guid><description><![CDATA[<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>Overview<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">c++ 八股文 第一部分</div>
    </div>
  </div>
<h2 id="1-基础知识一">1. 基础知识(一)</h2>
<h3 id="11-c语言的特点">1.1 C++语言的特点</h3>
<blockquote>
<p>①C++在C的基础上引入了<u><font color=red><strong>面向对象</strong></font></u>机制，同时也兼容C语言；</br>
②C++三大特性：<font color=red><em>封装</em>、<em>继承</em>、<em>多态</em></font>；</br>
③C++程序结构清晰、易于扩充、程序可读性好；</br>
④C++代码质量高，<strong>运行效率高</strong>、仅比汇编语言慢10%~20%；</br>
⑥C++<strong>可复用性高</strong>，C++引入了模板的概念，有专门的模板库(STL)；</br>
⑦C++是不断发展的语言，C++11中新引入了nullptr、auto变量、Lambda匿名函数、右值引用、智能指针。</br></p>
</blockquote>
<table><body text=red><tr><td style="text-align:left;font-weight:bold" bgcolor=yellow><font size="3" color="red">C++面向对象的三大特征</font></td></tr></body></table>
<blockquote>
<p><font color=red><strong>封装性：</strong></font> 将客观事物抽象成类，每个类对自身的<u>数据</u>和<u>方法</u>实行<font color=darkblue>访问控制</font>，包括(private，protected，public)。</br>
<font color=red><strong>继承性：</strong></font> 广义的继承有三种实现形式：<u>实现继承</u>(使用基类的属性和方法而无需额外编码的能力)、<u>可视继承</u>(子窗体使用父窗体的外观和实现代码)、<u>接口继承</u>(仅使用属性和方法，实现滞后到子类实现)。</br>
<font color=red><strong>多态性：</strong></font> 是将父类对象设置成为和一个或更多它的子对象相等的技术。用子类对象给父类对象赋值之后，父类对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。</br></p>
</blockquote>
<h3 id="12-c和c语言的区别">1.2 C++和C语言的区别</h3>
<ul>
<li>① C语言是C++的子集，C++可以很好<strong>兼容C语言</strong>。但是C++又有很多新特性，如引用、智能指针、auto变量等；</br></li>
<li>② C++是面对<strong>对象</strong>(object-oriented)的编程语言；C语言是面对<strong>过程</strong>(process-oriented)的编程语言；</br></li>
<li>③ C语言有一些不安全的语言特性，如指针使用的潜在危险、强制转换的不确定性、内存泄露等。而C++对此增加了不少新特性来改善安全性，如const常量、引用、cast转换、智能指针、try—catch等等；</br></li>
<li>④ C++可复用性高，C++引入了模板的概念，后面在此基础上，实现了方便开发的标准模板库STL。C++的STL库相对于C语言的函数库更灵活、更通用。</br></li>
</ul>
<h3 id="13-c中-struct-和-class-的区别">1.3 C++中 struct 和 class 的区别</h3>
<blockquote>
<p>① struct 一般用于描述一个<u>数据结构集合</u>，而 class 是强调<u>对一个对象数据的<strong>封装</strong></u>；</br>
② struct 中默认的访问控制权限是 public 的，而 class 中默认的访问控制权限是 private 的；</br>
③ 在<u>继承关系</u>中，struct 默认是<strong>公有继承</strong>，而 class 是<strong>私有继承</strong>；</br>
④ class关键字可以用于定<strong>义模板参数</strong>，就像typename，而 struct 不能用于定义模板参数。</br></p>
</blockquote>
<h3 id="14-include头文件的顺序以及双引号和尖括号的区别">1.4 include头文件的顺序以及双引号<code>&quot;&quot;</code>和尖括号<code>&lt;&gt;</code>的区别</h3>
<blockquote>
<p>区别：</br>
① 尖括号<code>&lt; &gt;</code>的头文件是<font color=red>系统文件</font>，双引号<code>&quot; &quot;</code>的头文件是自定义文件;</br>
② 编译器预处理阶段查找头文件的路径不一样；</br>
查找路径：</br>
① 使用尖括号<code>&lt;  &gt;</code>(系统文件)的头文件的查找路径：编译器设置的头文件路径$\rightarrow$系统变量;</br>
② 使用双引号<code>&quot;  &quot;</code>(自定义文件)的头文件的查找路径：当前头文件目录$\rightarrow$编译器设置的头文件路径$\rightarrow$系统变量。</br></p>
</blockquote>
<h3 id="15-c结构体和c结构体的区别">1.5 C++结构体和C结构体的区别</h3>
<blockquote>
<p>①C的结构体内不允许有函数存在，C++允许有内部成员函数，且允许该函数是虚函数；</br>
②C的结构体对内部成员变量的访问权限<strong>只能是public</strong>，而C++允许 <font color=red>public</font>，<font color=red>protected</font>，<font color=red>private</font>三种；</br>
③C 中使用结构体需要加上 struct 关键字，或者对结构体使用 typedef 取别名，而 C++ 中可以省略 struct 关键字直接使用；</br>
④C语言的结构体是<strong>不可以继承的</strong>，C++的结构体是可以从其他的结构体或者类继承过来的。</br></p>
</blockquote>
<h3 id="16-导入c函数的关键字是什么c编译时和c有什么不同">1.6 导入C函数的关键字是什么，C++编译时和C有什么不同？</h3>
<blockquote>
<p><strong>关键字：</strong> 在C++中，导入C函数的关键字是<code>extern</code>，表达形式为<code>extern &quot;C&quot;</code>， <code>extern &quot;C&quot;</code> 的主要作用就是为了能够正确实现C++代码调用其他C语言代码。<font color=red>加上<code>extern &quot;C&quot;</code>后，会指示编译器这部分代码按C语言的进行编译</font>，而不是C++的。</br></p>
</blockquote>
<blockquote>
<p><strong>编译区别：</strong> 由于C++支持函数重载，因此<u>编译器编译函数的过程中会将函数的参数类型也加到编译后的代码中</u>，而不仅仅是函数名；而C语言并不支持函数重载，因此编译C语言代码的函数时不会带上函数的参数类型，一般只包括函数名。</br>
总结: 区别在于<font color=red>在编译过程中是否带上函数的参数类型，C++带，C不带</font>。</p>
</blockquote>
<h3 id="17-简述c从代码到可执行二进制文件的过程">1.7 简述C++从代码到可执行二进制文件的过程</h3>
<blockquote>
<p><strong>预编译、编译、汇编、链接</strong> </br></p>
</blockquote>
<ul>
<li>①<strong>预编译</strong>：这个过程主要的处理操作如下：</br>
<ul>
<li>(1) 将所有的#define删除，并且展开所有的宏定义</br></li>
<li>(2) 处理所有的<u><font color=purple>条件预编译指令</font></u>，如#if、#ifdef</br></li>
<li>(3) 处理#include预编译指令，将被包含的文件插入到该预编译指令的位置。</br></li>
<li>(4) 过滤所有的注释</br></li>
<li>(5) 添加行号和文件名标识</br></li>
</ul>
</li>
<li>②<strong>编译</strong>：这个过程主要的处理操作如下：</br>
<ul>
<li>(1) 词法分析：将源代码的字符序列分割成一系列的记号。</br></li>
<li>(2) 语法分析：对记号进行语法分析，产生语法树。</br></li>
<li>(3) 语义分析：判断表达式是否有意义。</br></li>
<li>(4) 代码优化：</br></li>
<li>(5) 目标代码生成：<strong>生成汇编代码</strong>。</br></li>
<li>(6) 目标代码优化</br></li>
</ul>
</li>
<li>③<strong>汇编</strong>：这个过程主要是将汇编代码转变成机器可以执行的指令(汇编代码转为机器码)。</br></li>
<li>④<strong>链接</strong>：将不同的源文件产生的目标文件进行链接，从而形成一个可以执行的程序(链接目标文件，形成可执行程序)。</br>
​</li>
</ul>
<p><strong>链接分为<font color=red>静态链接</font>和<font color=red>动态链接</font>。</strong></br></p>
<ul>
<li>(1) <strong>静态链接</strong>，是在链接的时候就已经把要调用的函数或者过程链接到了生成的可执行文件中，就算你再去把静态库删除也不会影响可执行程序的执行；生成的静态链接库，Windows下以.lib为后缀，Linux下以.a为后缀。</br></li>
<li>(2) <strong>动态链接</strong>，是在链接的时候没有把调用的函数代码链接进去，而是<font color=red>在执行的过程中，再去找要链接的函数，生成的可执行文件中没有函数代码</font>，只包含函数的重定位信息，所以当你删除动态库时，可执行程序就不能运行。生成的动态链接库，Windows下以.dll为后缀，Linux下以.so为后缀。</br></li>
</ul>
<h3 id="18-static关键字的作用">1.8 static关键字的作用</h3>
<ul>
<li>①<strong>定义全局静态变量和局部静态变量</strong>：在变量前面加上static关键字。static的变量默认初始化为0。初始化的静态变量会在<font color=red><strong>数据段</strong></font>分配内存，未初始化的静态变量会在<font color=red><strong>BSS段</strong></font>分配内存。直到程序结束，静态变量始终会维持前值。只不过全局静态变量(在整个工程文件有效)和局部静态变量(在当前定义的文件内有效)的作用域不一样；(什么是数据段和BBS段内存分配?)</br></li>
<li>②<strong>定义静态函数</strong>：在函数返回类型前加上static关键字，函数即被定义为静态函数。静态函数只能在本源文件中使用；<code>static int func()</code></br></li>
<li>③在变量类型前加上static关键字，变量即被定义为静态变量。静态变量只能在本源文件中使用;</br></li>
<li>④<font color=red><strong>类内静态成员变量:</strong></font> 在c++中，static关键字可以用于定义<strong>类中的静态成员变量</strong>：使用静态数据成员，它既可以被当成全局变量那样去存储，但又被隐藏在类的内部。类中的static静态数据成员拥<strong>有一块单独的存储区</strong>，而<u>不管创建了多少个该类的对象。所有这些对象的静态数据成员都共享这一块静态存储空间，static修饰的变量要在<font color=purplr><a href="https://blog.csdn.net/sevenjoin/article/details/81772792"target="_blank" rel="external nofollow noopener noreferrer">类外初始化<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></font></u>。</br></li>
<li>⑤<font color=red><strong>类内静态成员函数:</strong></font>在c++中，static关键字可以用于定义<strong>类中的静态成员函数</strong>：与静态成员变量类似，类里面同样可以定义静态成员函数。只需要在函数前加上关键字static即可。如静态成员函数也是类的一部分，而不是对象的一部分。所有这些对象的静态数据成员都共享这一块静态存储空间，只能访问类的static成员变量，static修饰的变量要在类外初始化。</br></li>
</ul>
<h3 id="19-数组和指针的区别">1.9 数组和指针的区别</h3>
<ul>
<li><strong>概念：</strong></br>
<ul>
<li>(1)数组：数组是用于储存多个<strong>相同类型数据</strong>的集合。数组名是首元素的地址。</br></li>
<li>(2)指针：指针相当于一个变量，但是它和一般变量不一样，它存放的是其它变量在内存中的地址。指针名指向了内存的首地址。</br></li>
</ul>
</li>
<li><strong>区别：</strong></br>
<ul>
<li>赋值：同类型指针变量可以相互赋值；数组不行，只能一个一个元素的赋值或拷贝；</br></li>
</ul>
</li>
<li><strong>存储方式：</strong></br>
<ul>
<li>数组：数组在<strong>内存中是连续</strong>存放的，开辟一块连续的内存空间。数组是根据数组的下标进行访问的，数组的存储空间，不是在静态区就是在栈上。</br></li>
<li>指针：指针很灵活，它可以指向任意类型的数据。指针的类型说明了它所指向地址空间的内存。由于指针本身就是一个变量，再加上它所存放的也是变量，所以指针的存储空间不能确定。</br></li>
</ul>
</li>
</ul>
<h3 id="110-什么是函数指针如何定义函数指针有什么使用场景">1.10 什么是函数指针，如何定义函数指针，有什么使用场景</h3>
<ul>
<li>
<p><strong>概念：</strong> 函数指针就是指向函数的指针变量。每一个函数都有一个入口地址，该函数入口地址就是函数指针所指向的地址。</br>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">函数指针指向函数的入口地址！</div>
    </div>
  </div></p>
</li>
<li>
<p><strong>定义形式：</strong></br></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">func</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">);</span> <span class="c1">// 函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">f</span><span class="p">)(</span><span class="kt">int</span> <span class="n">a</span><span class="p">);</span> <span class="c1">// 函数指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">f</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">func</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>使用场景： 回调(callback)。我们调用别人提供的 API函数(Application Programming Interface,应用程序编程接口)，称为Call；如果别人的库里面调用我们的函数，就叫Callback。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//以库函数qsort排序函数为例，它的原型如下：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">qsort</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span><span class="c1">//void*类型，代表原始数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">size_t</span> <span class="n">nmemb</span><span class="p">,</span> <span class="c1">//第二个是size_t类型，代表数据数量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="c1">//第三个是size_t类型，代表单个数据占用空间大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">int</span><span class="p">(</span><span class="o">*</span><span class="n">compar</span><span class="p">)(</span><span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">,</span><span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="c1">//第四个参数是函数指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//第四个参数告诉qsort，应该使用哪个函数来比较元素，
</span></span></span><span class="line"><span class="cl"><span class="c1">//即只要我们告诉qsort比较大小的规则，它就可以帮我们对任意数据类型的数组进行排序。
</span></span></span><span class="line"><span class="cl"><span class="c1">//在库函数qsort调用我们自定义的比较函数，这就是回调的应用。
</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//示例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="n">num</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">cmp_int</span><span class="p">(</span><span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">_a</span> <span class="p">,</span> <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">_b</span><span class="p">){</span><span class="c1">//参数格式固定
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">_a</span><span class="p">;</span>    <span class="c1">//强制类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">_b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">*</span><span class="n">a</span> <span class="o">-</span> <span class="o">*</span><span class="n">b</span><span class="p">;</span>　　
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">qsort</span><span class="p">(</span><span class="n">num</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">num</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">cmp_int</span><span class="p">);</span> <span class="c1">//回调cmp_int函数
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="111-静态变量什么时候初始化">1.11 静态变量什么时候初始化</h3>
<blockquote>
<p>对于C语言的全局和静态变量，初始化发生在任何代码执行之前，属于<font color=red><strong>编译期</strong></font>初始化。</br>
而C++标准规定：全局或静态对象当且仅当对象<font color=red>首次用到时</font>才进行构造。</p>
</blockquote>
<h3 id="112-nullptr调用成员函数可以吗为什么">1.12 nullptr调用成员函数可以吗？为什么？</h3>
<p>可以。因为<font color=red>在编译时对象就绑定了函数地址</font>，和指针空不空没关系。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C++" data-lang="C++"><span class="line"><span class="cl"><span class="c1">//给出实例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">animal</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">sleep</span><span class="p">()</span> <span class="p">{</span> <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;animal sleep&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">breathe</span><span class="p">()</span> <span class="p">{</span> <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;animal breathe haha&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">fish</span> <span class="o">:</span><span class="k">public</span> <span class="n">animal</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">breathe</span><span class="p">(){</span> <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;fish bubble&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">animal</span> <span class="o">*</span><span class="n">pAn</span><span class="o">=</span><span class="k">nullptr</span><span class="p">;</span>    <span class="c1">//类指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">pAn</span><span class="o">-&gt;</span><span class="n">breathe</span><span class="p">();</span>   <span class="c1">// 输出：animal breathe haha
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">fish</span> <span class="o">*</span><span class="n">pFish</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">pFish</span><span class="o">-&gt;</span><span class="n">breathe</span><span class="p">();</span> <span class="c1">// 输出：fish bubble
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// 原因：因为在编译时对象就绑定了函数地址，和指针空不空没关系。
</span></span></span><span class="line"><span class="cl"><span class="c1">// pAn-&gt;breathe();编译的时候，函数的地址就和指针pAn绑定了；
</span></span></span><span class="line"><span class="cl"><span class="c1">// 调用breath(*this), this就等于pAn。由于函数中没有需要解引用this的地方，所以函数运行不会出错，
</span></span></span><span class="line"><span class="cl"><span class="c1">// 但是若用到this，因为this=nullptr，运行出错。
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="113-什么是野指针怎么产生的如何避免">1.13 什么是野指针，怎么产生的，如何避免？</h3>
<blockquote>
<p><strong>概念：</strong> 野指针就是指针指向的位置是不可知的(随机的、不正确的、没有明确限制的)；</p>
</blockquote>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">指向位置不可知称为野指针！</div>
    </div>
  </div>
<blockquote>
<p>产生原因：<u>释放内存后指针不及时置空(野指针)</u>，依然指向了该内存，那么可能出现非法访问的错误。这些我们都要注意避免。(内存泄露)</p>
</blockquote>
<blockquote>
<p>避免办法：</br>
(1)初始化置NULL</br>
(2)申请内存后判空</br>
(3)指针释放后置NULL</br>
(4)使用智能指针</br></p>
</blockquote>
<h3 id="114-静态局部变量全局变量局部变量的特点以及使用场景">1.14 静态局部变量，全局变量，局部变量的特点，以及使用场景</h3>
<ul>
<li>
<p>①首先从作用域考虑: </br></p>
<ul>
<li>C++里作用域可分为6种: 全局，局部，类，语句，命名空间和文件作用域。</br>
<ul>
<li>全局变量: 全局作用域，可以通过extern作用于其他非定义的源文件。</br></li>
<li>静态全局变量: 全局作用域+文件作用域，所以无法在其他文件中使用。</br></li>
<li>局部变量: 局部作用域，比如函数的参数，函数内的局部变量等等。</br></li>
<li>静态局部变量: 局部作用域，只被初始化一次，直到程序结束。</br></li>
</ul>
</li>
</ul>
</li>
<li>
<p>②从所在空间考虑：除了局部变量在栈上外，其他都在静态存储区。因为静态变量都在静态存储区，所以下次调用函数的时候还是能取到原来的值。</br></p>
</li>
<li>
<p>③生命周期： 局部变量在栈上，出了作用域就回收内存；而全局变量、静态全局变量、静态局部变量都在静态存储区，直到程序结束才会回收内存。</br></p>
</li>
<li>
<p>④使用场景：从它们各自特点就可以看出各自的应用场景，不再赘述。</br></p>
</li>
</ul>
<h3 id="115-c继承">1.15 C++继承</h3>
<blockquote>
<p>①<strong>公有继承public</strong>：基类的公有成员和保护成员作为派生类的成员时，它们都保持原有的状态，而基类的私有成员仍然是私有的，不能被这个派生类的子类所访问。</br>
②<strong>私有继承private</strong>：私有继承的特点是基类的公有成员和保护成员都作为派生类的私有成员，并且不能被这个派生类的子类所访问。</br>
③<strong>保护继承protect</strong>：保护继承的特点是基类的所有公有成员和保护成员都成为派生类的保护成员，并且只能被它的派生类成员函数或友元访问，基类的私有成员仍然是私有的</br></p>
</blockquote>
<h3 id="116-常量指针和指针常量">1.16 常量指针和指针常量</h3>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>常量指针和指针常量的区别<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">常量指针: 内存里的值不变</br>
指针常量: 指针指向的内存地址不变</br></div>
    </div>
  </div>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="mf">1.</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">a</span><span class="p">;</span>     <span class="c1">//指的是a是一个常量，不允许修改。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="mf">2.</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">;</span>    <span class="c1">//a指针所指向的内存里的值不变，即(*a)不变  常量指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="mf">3.</span> <span class="kt">int</span> <span class="k">const</span> <span class="o">*</span><span class="n">a</span><span class="p">;</span>    <span class="c1">//同const int *a;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="mf">4.</span> <span class="kt">int</span> <span class="o">*</span><span class="k">const</span> <span class="n">a</span><span class="p">;</span>    <span class="c1">//a指针所指向的内存地址不变，即a不变     指针常量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="mf">5.</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="k">const</span> <span class="n">a</span><span class="p">;</span>   <span class="c1">//都不变，即(*a)不变，a也不变
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="117-内联函数和函数的区别">1.17 内联函数和函数的区别</h3>
<ul>
<li>①内联函数比普通函数多了关键字inline；</br></li>
<li>②内联函数避免了<strong>函数调用的开销</strong>；普通函数有调用的开销；</br></li>
<li>③普通函数在被调用的时候，需要寻址(函数入口地址)；<u>内联函数不需要寻址</u>。</br></li>
<li>④内联函数有一定的限制，内联函数体要求代码简单，不能包含复杂的结构控制语句(内联函数内不允许用循环语句和开关语句。普通函数没有这个要求。</br></li>
</ul>
<h3 id="118-简述c有几种传值方式之间的区别是什么">1.18 简述C++有几种传值方式，之间的区别是什么？</h3>
<ul>
<li><strong>值传递、引用传递、指针传递</strong></br>
<ul>
<li>①值传递: 形参即使在函数体内值发生变化，也不会影响实参的值；</br></li>
<li>②引用传递: 形参在函数体内值发生变化，会影响实参的值；</br></li>
<li>③指针传递: 在指针指向没有发生改变的前提下，形参在函数体内值发生变化，会影响实参的值；</br></li>
</ul>
</li>
</ul>
<h3 id="119-内联函数和宏函数的区别">1.19 内联函数和宏函数的区别</h3>
<blockquote>
<p><strong>宏常量&amp;宏函数</strong></br></p>
</blockquote>
<p>定义:</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// a. 定义一个宏常量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#define MAX 1024 </span><span class="c1">// 宏常量  MAX称为符号常量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// b. 定义一个宏函数
</span></span></span><span class="line"><span class="cl"><span class="c1">// 宏函数:宏函数就是使用宏定义定义出来的函数,并不是真正意义上的函数。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#define GETSUM(x, y) ((x) + (y)) </span><span class="c1">// 宏函数
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>使用宏函数的注意事项: </br></p>
<ul>
<li>1.要保证运算的完整性；</br></li>
<li>2.宏函数的使用场景:频繁调用和短小的函数,封装成宏函数；</br></li>
<li>3.使用宏函数的优点:以空间换时间；</br></li>
</ul>
<p>宏定义和函数的区别:</br></p>
<ul>
<li>1.宏在 <font color=red>预处理阶段完成替换</font>，之后被替换的文本参与编译，相当于 <font color=red>直接插入代码</font>，运行时不存在函数调用，执行起来更快；函数调用在运行时需要跳转到具体调用函数;</br></li>
<li>2.宏定义属于在结构中插入代码，<font color=red>没有返回值</font>; 函数调用具有返回值;</br></li>
<li>3.宏定义参数没有类型，不进行类型检查；函数参数具有类型，需要检查类型;</br></li>
<li>4.宏定义不要在最后加分号；</br></li>
</ul>
<p>宏定义和typedef的区别:</br></p>
<ul>
<li>1.宏主要用于 <font color=red>定义常量及书写复杂的内容</font>; typedef主要用于 <font color=red>定义类型别名</font>;</br></li>
<li>2.宏替换发生在<strong>预编译阶段</strong>，属于文本插入替换；typedef是<strong>编译</strong>的一部分;</br></li>
<li>3.宏不检查类型；typedef会检查数据类型;</br></li>
<li>4.宏不是语句，不需要在最后加分号; typedef是语句，要加分号标识结束;</br></li>
<li>5.注意对指针的操作，<code>typedef char * p_char</code>和<code>#define p_char char *</code>区别巨大;</br></li>
</ul>
<p>宏函数和内联函数的区别:</br></p>
<ul>
<li>1.在使用时，宏只做简单字符串替换(编译前或者预编译阶段)。而内联函数可以进行参数类型检查(编译时)，且具有返回值；</br></li>
<li>2.内联函数在编译时直接将函数代码嵌入到目标代码中，省去函数调用的开销来提高执行效率，并且进行参数类型检查，具有返回值，可以实现重载；</br></li>
<li>3.宏定义时要注意书写(参数要括起来)否则容易出现歧义(保证运算的完整性)，内联函数不会产生歧义；</br></li>
<li>4.内联函数有类型检查、语法判断等功能，而宏没有；</br></li>
</ul>
<p>define宏定义和const的区别:</br></p>
<ul>
<li>
<p>处理阶段: define是在编译的<strong>预处理</strong>阶段起作用，而const是在<strong>编译、运行</strong>的时候起作用；</p>
</li>
<li>
<p>安全性：</br></p>
<ul>
<li>1.define只做替换，不做类型检查和计算，也不求解，容易产生错误，一般最好加上一个大括号包含住全部的内容，要不然很容易出错;</br></li>
<li>2.const常量有数据类型，编译器可以对其进行类型安全检查;</br></li>
</ul>
</li>
<li>
<p>内存占用：</br></p>
<ul>
<li>1.define只是将宏名称进行替换，在内存中会产生多份相同的备份。const在程序运行中只有一份备份，且可以执行<strong>常量折叠</strong>，能将复杂的的表达式计算出结果放入常量表;</br></li>
<li>2.宏定义的数据没有分配内存空间，只是插入替换掉；const定义的变量只是值不能改变，但要分配内存空间;</br></li>
</ul>
</li>
</ul>
<h3 id="120四种cast类型转换">1.20 四种cast类型转换</h3>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>cast类型转换的作用<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">作用：克服c语言中强制类型转化带来的风险，C++引入了四种更加安全的<strong>强制类型转换运算符</strong>(明确转换的目的，便于程序的维护和分析)</div>
    </div>
  </div>
<ol>
<li>
<p><code>const_cast</code>: 去除变量的const属性</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 1.去除const属性，将只读变为只读写
</span></span></span><span class="line"><span class="cl"><span class="c1">// 2.针对常量指针、常量引用和常量对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">char</span> <span class="o">*</span><span class="n">p1</span> <span class="o">=</span> <span class="k">const_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">p</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>static_cast</code>: 内置数据类型之间、基类-派生类之间的转换</p>
<ul>
<li>内置数据类型之间的转换，int转double，char转int</br></li>
<li>基类指针与派生类指针之间的转换，只能转换有继承或派生关系的类。用于类层次结构之间基类和派生类指针和引用之间的转换，进行向上转型是安全的，但是进行向下转型是不安全的，但是是可以转换的;
<ul>
<li>向上转型(派生类向基类转换 -&gt; 安全)：我们知道基类的引用和指针都可以指向派生类的对象，那么将派生类的指针或者引用强转为基类的指针或者引用，那么这就是向上转型，也就是向父类转;</br></li>
<li>向下转型(基类向派生类转换 -&gt; 不安全)：向下转型就和向上转型相反，它是将父类的指针或者引用，强制转换为子类的指针或者引用</br></li>
</ul>
</li>
<li>把void类型指针转换为目标类型的指针</br></li>
<li>任何类型的表达式转化为void类型</br></li>
</ul>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 整形转浮点型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">double</span> <span class="n">b</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//基类指针转派生类
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">A</span><span class="p">{};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">public</span> <span class="n">A</span><span class="p">{};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">*</span><span class="n">pA</span> <span class="o">=</span> <span class="k">new</span> <span class="n">A</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">*</span><span class="n">pB</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">B</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">pA</span><span class="p">);</span> <span class="c1">// 向下转换不安全
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>reinterpret_cast</code>:</p>
<ul>
<li>可以将一个类型的<strong>指针</strong>转换为其它任意类型的指针，也可以用在指针和整形数据之间的转换。它是很危险的，如果我们没有使用它的充分理由，那么就不要使用它</br></li>
<li>为运算对象的位模式提供较低层次上的重新解释</br></li>
<li>用于底层的强制转换，依赖于机器，一般使用较少</br></li>
</ul>
</li>
<li>
<p><code>dynamic_cast</code>: 运行时处理; 基类向派生类转换时比<code>static_cast</code>更安全</p>
<ul>
<li><code>dynamic_cast</code>是<font color=red>运行时处理</font>的，运行时<strong>进行类型检查</strong>，其他三种是编译时处理的</br></li>
<li>不能用于内置数据类型之间的转换</br></li>
<li><code>dynamic_cast</code>在进行<strong>向上转换</strong>时和<code>static_cast</code>效果是一样的，但是进行<strong>向下转换</strong>时会进行类型检查，比<code>static_cast</code>更加安全，下行转换是否成功取决于转换对象的实际类型与目标类型是否相同</br></li>
<li>要求基类必须具有虚函数，否则编译不通过</br></li>
<li>若转换成功，返回的是指向目标的指针或引用，不成功返回NULL</br></li>
</ul>
</li>
</ol>
<h2 id="2-基础知识二">2. 基础知识(二)</h2>
<h3 id="21-写出-int-bool-float-指针变量与-零值比较的if-语句">2.1 写出 int 、bool、 float 、指针变量与 “零值”比较的if 语句</h3>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//int与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span> <span class="n">n</span> <span class="o">!=</span> <span class="mi">0</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//bool与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="n">flag</span><span class="p">)</span> <span class="c1">// 表示flag为真
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">flag</span><span class="p">)</span> <span class="c1">// 表示flag为假
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">//float与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">EPSINON</span> <span class="o">=</span> <span class="mf">0.00001</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">((</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="o">-</span> <span class="n">EPSINON</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">EPSINON</span><span class="p">)</span> <span class="c1">//其中EPSINON是允许的误差(即精度)。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">//指针变量与零值比较
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="22-变量的声明和定义有什么区别">2.2 变量的声明和定义有什么区别</h3>
<ul>
<li>① 变量的<strong>定义</strong>为变量<u><em>分配地址和存储空间</em></u>， 变量的<strong>声明</strong>不分配地址。</br></li>
<li>② 一个变量可以在多个地方声明， 但是只在一个地方定义。<font color=red>声明多次，定义一次。</font></br></li>
<li>③ 加入extern 修饰的是变量的声明，说明此变量将<em>在文件外部或在文件后面部分</em>定义。</br></li>
<li>④ 说明：很多时候一个变量，只是声明，不分配内存空间，直到具体使用时才初始化，分配内存空间， 如外部变量。</br></li>
</ul>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="k">extern</span> <span class="kt">int</span> <span class="n">A</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="c1">//这是个声明而不是定义，声明A是一个已经定义了的外部变量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="c1">//注意：声明外部变量时可以把变量类型去掉如：extern A;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">dosth</span><span class="p">();</span> <span class="c1">//执行函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">A</span><span class="p">;</span> <span class="c1">//是定义，定义了A为整型的外部变量
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="23-简述-ifdefelseendif和ifndef的作用">2.3 简述 <code>#ifdef</code>、<code>#else</code>、<code>#endif</code>和<code>#ifndef</code>的作用</h3>
<p>利用 <code>#ifdef</code>、<code>#endif</code> <u>将某程序功能模块包括进去，以向特定用户提供该功能</u>。在不需要时, 用户可轻易将其屏蔽。</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="cp">#ifdef MATH
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="cp">#include “math.c”
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">//在子程序前加上标记，以便于追踪和调试。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="cp">#ifdef DEBUG
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="n">printf</span> <span class="p">(</span><span class="err">“</span><span class="n">Indebugging</span><span class="err">…</span><span class="o">!</span><span class="err">”</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="cp">#endif</span></span></span></code></pre></td></tr></table>
</div>
</div><p>应对硬件的限制。由于一些具体应用环境的硬件不一样，限于条件，本地缺乏这种设备，只能绕过硬件，直接写出预期结果。</br>
注意：虽然不用条件编译命令而直接用if语句也能达到要求，但那样做目标程序长(因为所有语句都编译)，运行时间长(因为在程序运行时间对if语句进行测试)。而采用<u><strong>条件编译</strong></u>，可以减少被编译的语句，从而减少目标程序的长度，减少运行时间。</p>
<h3 id="24-结构体可以直接赋值吗">2.4 结构体可以直接赋值吗?</h3>
<ul>
<li>①结构体声明时可以直接初始化，同一结构体的不同对象之间也可以直接赋值，但是当结构体中含有指针“成员”时一定要小心。</br></li>
<li>②注意：当有多个指针指向同一段内存时，某个指针释放这段内存可能会导致其他指针的非法操作。因此，在释放前一定要确保其他指针不再使用这段内存空间。</li>
</ul>
<h3 id="25-sizeof-和strlen-的区别">2.5 sizeof 和strlen 的区别</h3>
<ul>
<li>①sizeof是一个<strong>操作符</strong>，strlen是<strong>库函数</strong>。</br></li>
<li>②sizeof的参数可以是<strong>数据的类型</strong>，也可以是<strong>变量</strong>，而strlen只能以结尾为&rsquo;\0&rsquo;的字符串作参数。</br></li>
<li>③编译器在<strong>编译时</strong>就计算出了sizeof的结果，而strlen函数必须在<strong>运行时</strong>才能计算出来。并且sizeof计算的是数据类型占内存的大小，而strlen计算的是字符串实际的长度。</br></li>
<li>④数组(array)做sizeof的参数不退化，传递给strlen就退化为指针了</br></li>
</ul>
<h3 id="26-sizeof求类型大小">2.6 sizeof求类型大小</h3>
<p>ref: <a href="https://www.cnblogs.com/maji233/p/11439880.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/maji233/p/11439880.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<ul>
<li>①类的大小为类的非静态成员数据的类型大小之和，也就是说<font color=red>静态成员数据不作考虑</font>。
普通成员函数与sizeof无关。</br></li>
<li>②虚函数由于要维护虚函数表，所以要占据一个指针大小，也就是4字节。
类的总大小也遵守类似class字节对齐的，调整规则。</br></li>
</ul>
<p>ref:</br></p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">=&gt;(32 位)
</span></span><span class="line"><span class="cl">指针都是  4个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">char     1个字节
</span></span><span class="line"><span class="cl">short    2个字节
</span></span><span class="line"><span class="cl">int      4个字节
</span></span><span class="line"><span class="cl">long     4个字节
</span></span><span class="line"><span class="cl">long int 4个字节
</span></span><span class="line"><span class="cl">float    4个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">double    8个字节
</span></span><span class="line"><span class="cl">long double  8个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">=&gt;(64 位)
</span></span><span class="line"><span class="cl">指针都是一个字长, 8个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">char    1个字节
</span></span><span class="line"><span class="cl">short   2个字节
</span></span><span class="line"><span class="cl">int     4个字节
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">long    8个字节
</span></span><span class="line"><span class="cl">long int  8个字节
</span></span><span class="line"><span class="cl">double    8个字节
</span></span><span class="line"><span class="cl">long double 也可以变长了, 16个字节</span></span></code></pre></td></tr></table>
</div>
</div><p>例如有如下结构体：</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="k">struct</span> <span class="nc">Stu</span>  <span class="c1">//自定义的数据类型，允许用户存储不同的数据类型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">id</span><span class="p">;</span> <span class="c1">// 4个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">char</span> <span class="n">sex</span><span class="p">;</span> <span class="c1">// 1个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">float</span> <span class="n">hight</span><span class="p">;</span> <span class="c1">// 4个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">};</span></span></span></code></pre></td></tr></table>
</div>
</div><p>那么一个这样的结构体变量占多大内存呢？也就是 <code>cout&lt;&lt;sizeof(Stu)&lt;&lt;endl;</code>  会输出什么？
在了解字节对齐方式之前想当然的会以为：sizeof(Stu) = sizeof(int)+sizeof(char)+sizeof(float) = 9.
然而事实并非如此！</p>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>字节对齐原则<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">在系统默认的对齐方式下: 每个成员相对于这个结构体变量地址的偏移量正好是该成员类型所占字节的整数倍，且最终占用字节数为成员类型中最大占用字节数的整数倍。</div>
    </div>
  </div>
<p>在这个例子中，<code>int id</code>的偏移量为0(0=4x0)，<code>char sex</code>的偏移量为4(4=1x4)，<code>float height</code>的偏移量为8(8=2x4)，此时占用12字节，也同时满足12=3x4.所以sizeof(Stu)=12.</p>
<p><strong>总结：</strong></br></p>
<ul>
<li>①最终大小一定是最大数据类型的整数倍;</br></li>
<li>②静态变量不占空间;</br></li>
<li>③每种类型的偏移量为自身的n倍;</br></li>
<li>详细请查阅：<a href="https://blog.csdn.net/weixin_30412577/article/details/95141536?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task"target="_blank" rel="external nofollow noopener noreferrer">struct/class等内存字节对齐问题详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></li>
</ul>
<p>ref:</br>
<a href="https://www.cnblogs.com/always-chang/p/6084973.html"target="_blank" rel="external nofollow noopener noreferrer">struct地址偏移量计算<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="27-c语言的关键字static和c的关键字static有什么区别">2.7 C语言的关键字<code>static</code>和C++的关键字<code>static</code>有什么区别</h3>
<ul>
<li>①在 C语言 中，static 用来修饰局部静态变量和外部静态变量、函数。而 C++中除了上述功能外，还用来定义类的成员变量和函数。即静态成员变量和静态成员函数。</br></li>
<li>②注意：编程时，static 的记忆性和全局性的特点可以让在不同时期调用的函数进行通信，传递信息，而 C++的静态成员则可以在多个对象实例间进行通信，传递信息。</br></li>
</ul>
<h3 id="28-ｃ语言的malloc和ｃ中的new有什么区别">2.8 Ｃ语言的<code>malloc</code>和Ｃ＋＋中的<code>new</code>有什么区别</h3>
<ul>
<li>①new 、delete 是操作符，可以重载，只能在C++ 中使用。</br></li>
<li>②malloc、free 是函数，可以覆盖，C、C++ 中都可以使用。</br></li>
<li>③new 可以调用对象的构造函数，对应的delete 调用相应的析构函数。</br></li>
<li>④malloc 仅仅分配内存，free 仅仅回收内存，并不执行构造和析构函数。</br></li>
<li>⑤new 、delete 返回的是<strong>某种数据类型指针</strong>，malloc、free 返回的是**<code>void</code>指针**。</br>
注意：<code>malloc</code>申请的内存空间要用<code>free</code>释放，而<code>new</code>申请的内存空间要用<code>delete</code>释放，不要混用。</br></li>
</ul>
<p>ref: <a href="https://jianye0428.github.io/posts/basics_one/#211-new%E5%92%8Cmalloc%E7%9A%84%E5%8C%BA%E5%88%AB%E5%90%84%E8%87%AA%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-delete-%E5%92%8C-free%E7%B1%BB%E4%BC%BC"target="_blank" rel="external nofollow noopener noreferrer">2.11 new 和 malloc的区别<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="29-写一个-标准-宏min">2.9 写一个 “标准” 宏MIN</h3>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#define min(a,b) ((a)&lt;=(b)?(a):(b))</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="210-i和i的区别">2.10 ++i和i++的区别</h3>
<blockquote>
<p>++i先自增1，再返回；i++先返回i,再自增1</br>
前置版本将对象本身作为左值返回，后置版本将对象原始值的副本作为右值返回。</p>
</blockquote>
<h3 id="211-new和malloc的区别各自底层实现原理delete和free类似">2.11 <code>new</code>和<code>malloc</code>的区别，各自底层实现原理(<code>delete</code>和<code>free</code>类似)</h3>
<ul>
<li>①new(delete)是操作符，而malloc(free)是函数。</br></li>
<li>②new在调用的时候先分配内存，再调用构造函数，释放的时候调用析构函数；而malloc没有调用构造函数和析构函数。</br></li>
<li>③malloc需要给定申请内存的大小，返回的指针需要强转(返回void指针)；new会调用构造函数，不用指定内存的大小，返回指针不需要强转。</br></li>
<li>④new是操作符，可以被重载; malloc不行</br></li>
<li>⑤new分配内存, 更直接和安全。</br></li>
<li>⑥new发生错误抛出异常，malloc返回null</br></li>
</ul>
<h3 id="212-const-和-define-的区别">2.12 const 和 define 的区别</h3>
<p><strong>区别</strong></br></p>
<ul>
<li>(1)就<font color=red>起作用的阶段</font>而言：<code>#define</code>是在编译的<strong>预处理</strong>阶段起作用，而<code>const</code>是在 <strong>编译</strong>、<strong>运行</strong>的时候起作用。</br></li>
<li>(2)就<font color=red>起作用的方式</font>而言：<code>#define</code>只是<u>简单的字符串替换，没有类型检查</u>。而<code>const</code>有对应的数据类型，是要进行判断的，可以避免一些低级的错误。</br></li>
<li>(3)就<font color=red>存储方式</font>而言：<code>#define</code>只是进行展开，有多少地方使用，就替换多少次，它定义的宏常量在内存中有若干个备份;const定义的只读变量在程序运行过程中只有一份备份。</br></li>
<li>(4)从<font color=red>代码调试的方便程度</font>而言： <code>const</code>常量可以进行调试的，<code>define</code>是不能进行调试的，因为在预编译阶段就已经替换掉了。</br></li>
</ul>
<p><strong>const优点：</strong></p>
<blockquote>
<p>(1)const常量有数据类型，而宏常量没有数据类型。编译器可以对前者进行类型安全检查。而对后者只进行字符替换，没有类型安全检查，并且在字符替换可能会产生意料不到的错误。</br>
(2)有些集成化的调试工具可以对const常量进行调试，但是不能对宏常量进行调试。</br>
(3)const可节省空间，避免不必要的内存分配，提高效率</br></p>
</blockquote>
<h3 id="213c中函数指针和指针函数的区别">2.13 C++中函数指针和指针函数的区别</h3>
<ol>
<li>定义不同</li>
</ol>
<blockquote>
<p>指针函数: 本质是一个函数，其返回值为指针。</br>
函数指针: 本质是一个指针，其指向一个函数。</p>
</blockquote>
<ol start="2">
<li>写法不同</li>
</ol>
<blockquote>
<p>指针函数：int *fun(int x, int y);</br>
函数指针：int (*fun)(int x, int y);</p>
</blockquote>
<ol start="3">
<li>用法不同</li>
</ol>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">//指针函数示例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">_Data</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span><span class="n">Data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">//指针函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">Data</span><span class="o">*</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span><span class="kt">int</span> <span class="n">b</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="n">Data</span> <span class="o">*</span> <span class="n">data</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//调用指针函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">Data</span> <span class="o">*</span> <span class="n">myData</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//Data * myData = static_cast&lt;Data*&gt;(f(4,5));
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//函数指针示例
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="nf">add</span><span class="p">(</span><span class="kt">int</span> <span class="n">x</span><span class="p">,</span><span class="kt">int</span> <span class="n">y</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">//函数指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">int</span> <span class="p">(</span><span class="o">*</span><span class="n">fun</span><span class="p">)(</span><span class="kt">int</span> <span class="n">x</span><span class="p">,</span><span class="kt">int</span> <span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//赋值, 函数指针指向函数add
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">fun</span> <span class="o">=</span> <span class="n">add</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">//调用
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;(*fun)(1,2) = &#34;</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="o">*</span><span class="n">fun</span><span class="p">)(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">//输出结果
</span></span></span><span class="line"><span class="cl"><span class="c1">//(*fun)(1,2) =  3
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="214使用指针需要注意什么">2.14 使用指针需要注意什么？</h3>
<ul>
<li>①定义指针时，先初始化为NULL空指针。</br></li>
<li>②用malloc或new申请内存之后，应该立即检查指针值是否为NULL。防止使用指针值为NULL的内存。</br></li>
<li>③不要忘记为数组和动态内存赋初值。防止将未被初始化的内存作为右值使用。</br></li>
<li>④避免数字或指针的下标越界，特别要当心发生“多1”或者“少1”操作。</br></li>
<li>⑤动态内存的申请与释放必须配对，防止内存泄漏。</br></li>
<li>⑥用free或delete释放了内存之后，立即将指针设置为NULL，防止“野指针”。</br></li>
</ul>
<h3 id="215volatile有什么作用">2.15 volatile有什么作用</h3>
<ul>
<li>①volatile为状态寄存器一类的并行设备硬件寄存器。</br></li>
<li>②一个中断服务子程序会访问到的非自动变量。</br></li>
<li>③多线程间被几个任务共享的变量。
注意: 虽然volatile在嵌入式方面应用比较多，但是在PC软件的多线程中，volatile修饰的临界变量也是非常实用的。</li>
</ul>
<p>C++中volatile的作用:</br>
<font color=red>总结: 建议编译器不要对该变量进行优化，每次都从<strong>内存</strong>中读取该变量，而不是从缓存(寄存器)中读取变量。</font></p>
<ul>
<li>
<p>volatile是“易变/不稳定”的意思。volatile是C的一个较为少用的关键字，解决变量在“共享”环境下容易出现读取错误的问题。</br></p>
</li>
<li>
<p>定义为volatile的变量是说这变量可能会被意想不到地改变，即在你程序运行过程中一直会变，<font color=red>你希望这个值被正确地处理，每次从内存中去读这个值，而不是因编译器优化从缓存的地方读取</font>，比如读取缓存在寄存器中的数值，从而保证volatile变量被正确的读取。</br></p>
</li>
<li>
<p>在单任务的环境中，一个函数体内部，如果在两次读取变量的值之间的语句没有对变量的值进行修改，那么编译器就会设法对可执行代码进行优化。由于访问寄存器的速度要快过RAM(从RAM中读取变量的值到寄存器)，以后只要变量的值没有改变，就一直从寄存器中读取变量的值，而不对RAM进行访问。</br></p>
</li>
<li>
<p>而在多任务环境中，虽然在一个函数体内部，在两次读取变量之间没有对变量的值进行修改，但是该变量仍然有可能被其他的程序(如中断程序、另外的线程等)所修改。如果这时还是从寄存器而不是从RAM中读取，就会出现被修改了的变量值不能得到及时反应的问题。</br></p>
</li>
</ul>
<h3 id="216-一个参数可以既是const又是volatile吗">2.16 一个参数可以既是const又是volatile吗</h3>
<blockquote>
<p>可以。用const和volatile同时修饰变量，表示这个变量在程序内部是只读的，不能改变的，只在程序外部条件变化下改变，并且编译器不会优化这个变量。每次使用这个变量时，都要小心地去内存读取这个变量的值，而不是去寄存器读取它的备份。</br>
注意：在此一定要注意const的意思，const只是不允许程序中的代码改变某一变量，其在编译期发挥作用，它并<font color=red>没有实际地禁止某段内存的读写特性</font><br></p>
</blockquote>
<h3 id="217a和a有什么区别">2.17 <code>*a</code>和<code>&amp;a</code>有什么区别</h3>
<ul>
<li><code>&amp;a</code>：其含义就是“变量a的地址”。</br></li>
<li><code>*a</code>：用在不同的地方，含义也不一样。</br>
<ul>
<li>①在声明语句中，<code>*a</code>只说明a是一个指针变量，如<code>int *a</code>；</li>
<li>②在其他语句中，<code>*a</code>前面没有操作数且a是一个指针时，<code>*a</code>代表指针a指向的地址内存放的数据(<font color=red>解引用</font>)，如<code>b=*a</code>；</li>
<li>③<code>*a</code>前面有操作数且a是一个普通变量时，a代表乘以a，如c=ba</li>
</ul>
</li>
</ul>
<h3 id="218-用c-编写一个死循环程序">2.18 用C 编写一个死循环程序</h3>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl">    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：很多种途径都可实现同一种功能，但是不同的方法时间和空间占用度不同，特别是对于嵌入式软件，处理器速度比较慢，存储空间较小，所以时间和空间优势是选择各种方法的首要考虑条件。</p>
</blockquote>
<h3 id="219全局变量和局部变量有什么区别是怎么实现的操作系统和编译器是怎么知道的">2.19 全局变量和局部变量有什么区别？是怎么实现的？操作系统和编译器是怎么知道的？</h3>
<ul>
<li>①全局变量是整个程序都可访问的变量，谁都可以访问，生存期在整个程序从运行到结束(在程序结束时所占内存释放)；</br></li>
<li>②而局部变量存在于模块(子程序，函数)中，只有所在模块可以访问，其他模块不可直接访问，模块结束(函数调用完毕)，局部变量消失，所占据的内存释放。</br></li>
<li>③操作系统和编译器，可能是通过内存分配的位置来知道的，全局变量分配在全局数据段并且在程序开始运行的时候被加载.局部变量则分配在堆栈里面。</br></li>
</ul>
<h3 id="220-结构体内存对齐问题">2.20 结构体内存对齐问题</h3>
<p>请写出以下代码的输出结果：</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cm">/**************************************************************
</span></span></span><span class="line"><span class="cl"><span class="cm">*		结构体内存对⻬问题
</span></span></span><span class="line"><span class="cl"><span class="cm">*   从偏移为0的位置开始存储；
</span></span></span><span class="line"><span class="cl"><span class="cm">*	如果没有定义 #pragma pack(n)
</span></span></span><span class="line"><span class="cl"><span class="cm">*	sizeof 的最终结果必然是结构内部最⼤成员的整数倍，不够补⻬；
</span></span></span><span class="line"><span class="cl"><span class="cm">*	结构内部各个成员的⾸地址必然是⾃身⼤⼩的整数倍；
</span></span></span><span class="line"><span class="cl"><span class="cm">*
</span></span></span><span class="line"><span class="cl"><span class="cm">***************************************************************/</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">S1</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">i</span> <span class="p">;</span>  <span class="c1">//起始偏移0，sizeof(i)=4; 地址0、1、2、3分配给成员i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">char</span> <span class="n">j</span> <span class="p">;</span> <span class="c1">//起始偏移4，sizeof(j)=1;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">a</span> <span class="p">;</span>	 <span class="c1">//sizeof(a)=4,内存对齐到8个字节，从偏移量为8处存放a;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">double</span> <span class="n">b</span><span class="p">;</span><span class="c1">//sizeof(b)=8,内存对齐到16个字节，再存放b,结构体总大小24;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="c1">//结构体成员的首地址必须是自身大小的整数倍
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">S3</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">char</span> <span class="n">j</span><span class="p">;</span><span class="c1">//起始偏移0，sizeof(j)=1;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">float</span> <span class="n">i</span><span class="p">;</span><span class="c1">//sizeof(i)=4，内存对齐到4，起始偏移量为4,再存放i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">double</span> <span class="n">b</span><span class="p">;</span><span class="c1">//当前地址为8，是b大小的整数倍，无需对齐，直接存放成员b 8个字节
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">a</span><span class="p">;</span><span class="c1">//sizeof(a)=4,内存对齐到20，再存放a,总大小24字节；
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">printf</span><span class="p">(</span><span class="s">&#34;%d</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">S1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">	<span class="n">printf</span><span class="p">(</span><span class="s">&#34;%d</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">S3</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>输出:</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">24
</span></span><span class="line"><span class="cl">24</span></span></code></pre></td></tr></table>
</div>
</div><p>说明：</br></p>
<ul>
<li>①结构体作为一种复合数据类型，其构成元素既可以是基本数据类型的变量，也可以是一些复合型类型数据。对此，编译器会自动进行成员变量的对齐以提高运算效率。</br></li>
<li>②默认情况下，按自然对齐条件分配空间。各个成员按照它们被声明的顺序在内存中顺序存储，第一个成员的地址和整个结构的地址相同，向结构体成员中size最大的成员对齐。</br></li>
<li>③许多实际的计算机系统对基本类型数据在内存中存放的位置有限制，它们会要求这些数据的首地址的值是某个数k(通常它为4或8)的倍数，而这个k则被称为该数据类型的对齐模数。</br></li>
</ul>
<h2 id="3-基础知识三">3 基础知识(三)</h2>
<h3 id="31-简述cc程序编译的内存分配情况">3.1 简述C、C++程序编译的内存分配情况</h3>
<ul>
<li>
<p>①从<font color=red>静态存储区域</font>分配：</br>
内存在程序编译时就已经分配好，这块内存在程序的整个运行期间都存在。速度快、不容易出错， 因为有系统会善后。例如全局变量，static 变量，常量字符串等。</p>
</li>
<li>
<p>②在<font color=red>栈上</font>分配：</br>
在执行函数时，函数内局部变量的存储单元都在栈上创建，函数执行结束时, 这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。大小为2M。</p>
</li>
<li>
<p>③从<font color=red>堆上</font>分配：</br>
即动态内存分配。程序在运行的时候用 malloc 或 new 申请任意大小的内存，程序员自己负责在何时用 free 或 delete 释放内存。动态内存的生存期由程序员决定，使用非常灵活。如果在堆上分配了空间，就有责任回收它，否则运行的程序会出现内存泄漏，另外频繁地分配和释放不同大小的堆空间将会产生<mark>堆内碎块</mark>。</p>
</li>
</ul>
<p><strong>一个C、C++程序编译时内存分为5大存储区：堆区、栈区、全局区、文字常量区、程序代码区。</strong></p>
<h3 id="32简述strcpysprintf-与memcpy-的区别">3.2 简述strcpy、sprintf 与memcpy 的区别</h3>
<ul>
<li>① <font color=red>操作对象不同</font>，strcpy 的两个操作对象均为字符串，sprintf 的操作源对象可以是多种数据类型， 目的操作对象是字符串，memcpy 的两个对象就是两个任意可操作的内存地址，并不限于何种数据类型。</br></li>
<li>② <font color=red>执行效率不同</font>，memcpy 最高，strcpy 次之，sprintf 的效率最低。</br></li>
<li>③ <font color=red>实现功能不同</font>，strcpy 主要实现字符串变量间的拷贝，sprintf 主要实现其他数据类型格式到字符串的转化，memcpy 主要是内存块间的拷贝。</br>
注意：strcpy、sprintf 与memcpy 都可以实现拷贝的功能，但是针对的对象不同，根据实际需求，来选择合适的函数实现拷贝功能。</li>
</ul>
<h3 id="33-请解析void---0-的含义">3.3 请解析((void ()( ) )0)( )的含义</h3>
<blockquote>
<p><code>void (0)( )</code> ：是一个返回值为void，参数为空的函数指针0。</br>
<code>(void ()( ))0</code>：把0转变成一个返回值为void，参数为空的函数指针。</br>
<code>((void ()( ))0()</code>：在上句的基础上加括号表示整个是一个返回值为void，无参数，并且起始地址为0的函数的名字。</br>
<code>((void (*)( ))0)( )</code>：这就是上句的函数名所对应的函数的调用。</br></p>
</blockquote>
<h3 id="34-typedef-和define-有什么区别">3.4 typedef 和define 有什么区别</h3>
<ul>
<li>①<font color=red>用法不同</font>：</br>
<ul>
<li>typedef 用来定义一种数据类型的别名，增强程序的可读性。define 主要用来定义常量，以及书写复杂使用频繁的宏。</br></li>
</ul>
</li>
<li>②<font color=red>执行时间不同</font>：</br>
<ul>
<li>typedef 是编译过程的一部分，有类型检查的功能。define 是宏定义，是预编译的部分，其发生在编译之前，只是简单的进行字符串的替换，不进行类型的检查。</br></li>
</ul>
</li>
<li>③<font color=red>作用域不同</font>：</br>
<ul>
<li>typedef 有作用域限定：define 不受作用域约束，只要在define 声明后的引用都是正确的。</br></li>
</ul>
</li>
<li>④<font color=red>对指针的操作不同</font>：</br>
<ul>
<li>typedef 和 define 定义的指针时有很大的区别。</br></li>
</ul>
</li>
</ul>
<p>注意：typedef 定义是语句，因为句尾要加上分号。而define 不是语句，千万不能在句尾加分号。</br></p>
<h3 id="35指针常量与常量指针区别">3.5 指针常量与常量指针区别</h3>
<ul>
<li>指针常量是指定义了一个指针，这个指针的值只能在定义时初始化，其他地方不能改变。(地址不可变)</br></li>
<li>常量指针是指定义了一个指针，这个指针指向一个只读的对象，不能通过常量指针来改变这个对象的值。(值不可变)</br>
<ul>
<li>指针常量强调的是指针的不可改变性，而常量指针强调的是指针对其所指对象的不可改变性。</br></li>
</ul>
</li>
</ul>
<blockquote>
<p>注意：无论是指针常量还是常量指针，其最大的用途就是作为函数的形式参数，保证实参在被调用函数中的不可改变特性。</p>
</blockquote>
<h3 id="36简述队列和栈的异同">3.6 简述队列和栈的异同</h3>
<ul>
<li>队列和栈都是<font color=red>线性存储结构</font>，但是两者的插入和删除数据的操作不同，队列是“先进先出”，栈是 “后进先出”。</li>
</ul>
<blockquote>
<p>注意：区别栈区和堆区。堆区的存取是“顺序随意”，而栈区是“后进先出”。栈由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。堆一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS 回收。分配方式类似于链表。 它与本题中的堆和栈是两回事。堆栈只是一种数据结构，而堆区和栈区是程序的不同内存存储区域。</p>
</blockquote>
<h3 id="37设置地址为0x67a9-的整型变量的值为0xaa66">3.7 设置地址为0x67a9 的整型变量的值为0xaa66</h3>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="o">*</span><span class="n">ptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ptr</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="p">)</span><span class="mh">0x67a9</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="o">*</span><span class="n">ptr</span> <span class="o">=</span> <span class="mh">0xaa66</span><span class="p">;</span> <span class="c1">// 对指针取地址
</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：这道题就是强制类型转换的典型例子，无论在什么平台，地址长度和整型数据的长度是一样的， 即一个整型数据可以强制转换成地址指针类型，只要有意义即可。</p>
</blockquote>
<h3 id="38编码实现字符串转化为数字">3.8 编码实现字符串转化为数字</h3>
<blockquote>
<p>编码实现函数atoi()，设计一个程序，把一个字符串转化为一个整型数值。例如字符：“5486321 ”转化成整型：5486321。</p>
</blockquote>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">myAtoi</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span> <span class="n">str</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="kt">int</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//保存转换后的数值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="kt">int</span> <span class="n">isNegative</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//记录字符串中是否有负号
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">   <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span><span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="kt">char</span> <span class="o">*</span><span class="n">p</span> <span class="o">=</span> <span class="n">str</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">if</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="c1">//判断指针的合法性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">while</span><span class="p">(</span><span class="o">*</span><span class="n">p</span><span class="o">++</span> <span class="o">!=</span> <span class="sc">&#39;\0&#39;</span><span class="p">)</span> <span class="c1">//计算数字符串度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">n</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="n">p</span> <span class="o">=</span> <span class="n">str</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="sc">&#39;-&#39;</span><span class="p">)</span> <span class="c1">//判断数组是否有负号
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">isNegative</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="sc">&#39;0&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="n">p</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">       <span class="k">if</span><span class="p">(</span><span class="n">temp</span> <span class="o">&gt;</span> <span class="sc">&#39;9&#39;</span> <span class="o">||</span><span class="n">temp</span> <span class="o">&lt;</span> <span class="sc">&#39;0&#39;</span><span class="p">)</span> <span class="c1">//滤除非数字字符
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span><span class="p">(</span><span class="n">num</span> <span class="o">!=</span><span class="mi">0</span> <span class="o">||</span> <span class="n">temp</span> <span class="o">!=</span> <span class="sc">&#39;0&#39;</span><span class="p">)</span> <span class="c1">//滤除字符串开始的0 字符
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="n">temp</span> <span class="o">-=</span> <span class="mh">0x30</span><span class="p">;</span> <span class="c1">//将数字字符转换为数值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">num</span> <span class="o">+=</span> <span class="n">temp</span> <span class="o">*</span><span class="kt">int</span><span class="p">(</span> <span class="n">pow</span><span class="p">(</span><span class="mi">10</span> <span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span><span class="n">i</span><span class="p">)</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">       <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">if</span><span class="p">(</span><span class="n">isNegative</span><span class="p">)</span> <span class="c1">//如果字符串中有负号，将数值取反
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="p">(</span><span class="mi">0</span> <span class="o">-</span> <span class="n">num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">else</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">num</span><span class="p">;</span> <span class="c1">//返回转换后的数值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="39c语言的结构体struct和c的类class有什么区别">3.9 C语言的结构体(struct)和C++的类(class)有什么区别</h3>
<ul>
<li>①C语言的结构体是不能有成员函数的，而C++的类可以有。</br></li>
<li>②C语言的结构体中数据成员是没有private、public和protected访问限定的。而C++的类的成员有这些访问权限限定。</br></li>
<li>③C语言的结构体是没有继承关系的，而C++的类却有丰富的继承关系。</br></li>
<li>注意：虽然C的结构体和C++的类有很大的相似度，但是类是实现面向对象的基础。而结构体只可以简单地理解为类的前身。</br></li>
</ul>
<h3 id="310-简述指针常量与常量指针的区别">3.10 简述指针常量与常量指针的区别</h3>
<ul>
<li>①指针常量是指定义了一个指针，这个指针的值只能在定义时初始化，其他地方不能改变。常量指针是定义了一个指针，这个指针指向一个只读的对象，不能通过常量指针来改变这个对象的值。指针常量的值只能在定义时初始化，常量指针指向一个只读的对象</br></li>
<li>②指针常量强调的是指针的不可改变性，而常量指针强调的是指针对其所指对象的不可改变性。</br></li>
</ul>
<blockquote>
<p>注意：无论是指针常量还是常量指针，其最大的用途就是作为函数的形式参数，保证实参在被调用函数中的不可改变特性。</br></p>
</blockquote>
<h3 id="311-哪些情况会导致野指针以及如何避免">3.11 哪些情况会导致“野指针”以及如何避免</h3>
<ul>
<li>①指针变量声明时没有被初始化。解决办法：指针声明时初始化，可以是具体的地址值，也可让它指向NULL。</br></li>
<li>②指针p被free或者delete之后，没有置为NULL。解决办法：指针指向的内存空间被释放后指针应该指向NULL。</br></li>
<li>③指针操作超越了变量的作用范围。解决办法：在变量的作用域结束前释放掉变量的地址空间并且让指针指向NULL。</br></li>
</ul>
<h3 id="312句柄和指针的区别和联系是什么">3.12 句柄和指针的区别和联系是什么？</h3>
<p>句柄和指针其实是两个截然不同的概念。Windows系统用句柄标记系统资源，隐藏系统的信息。你只要知道有这个东西，然后去调用就行了，它是个32bit的uint。指针则标记某个物理内存地址，两者是不同的概念。</p>
<h3 id="313newdelete与mallocfree的区别是什么">3.13 new/delete与malloc/free的区别是什么</h3>
<blockquote>
<p>new能自动计算需要分配的内存空间，而malloc需要手工计算字节数。</p>
</blockquote>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="o">*</span><span class="n">p</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="o">*</span><span class="n">q</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span> <span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="mi">2</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>①new与delete直接带具体类型的指针，malloc和free返回void类型的指针。</br></li>
<li>②new操作符能保证类型安全，而malloc不能。例如<code>int *p = new float[2];</code>就会报错；而<code>int p = malloc(2sizeof(int))</code>编译时编译器就无法指出错误来。</br></li>
<li>③new一般分为两步：new操作和调用构造函数。new操作对应于malloc，但new操作可以重载，可以自定义内存分配策略，不做内存分配，甚至分配到非内存设备上，而malloc不行。</br></li>
<li>④new调用构造函数，malloc不能；delete调用析构函数，而free不能。</br></li>
<li>⑤malloc/free需要库文件stdlib.h的支持，new/delete则不需要！</br></li>
<li>⑥new/delete是C++的关键字,申请内存失败时会抛出异常，malloc/free是库函数，申请内存失败后返回null。</br></li>
<li>⑦new/delete是C++的内存分配和回收机制，malloc/free是C的内存分配和回收机制。</br></li>
</ul>
<blockquote>
<p>注意：delete和free被调用后，内存不会立即回收，指针也不会指向空，delete或free仅仅是告诉操作系统，这一块内存被释放了，可以用作其他用途。但是由于没有重新对这块内存进行写操作，所以内存中的变量数值并没有发生变化，出现野指针的情况。因此，释放完内存后，应该将指针指向NULL。</br>
<a href="https://blog.csdn.net/qq_44443986/article/details/114800593"target="_blank" rel="external nofollow noopener noreferrer">new delete 详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<h3 id="314说一说externc">3.14 说一说extern“C”</h3>
<blockquote>
<p><code>extern &quot;C&quot;</code>的主要作用就是为了能够正确实现在C++代码中调用C语言代码。加上<code>extern &quot;C&quot;</code>后，会指示编译器这部分代码按C语言(而不是C++)的方式进行编译。由于C++支持函数重载，因此编译器编译函数的过程中会将函数的参数类型也加到编译后的代码中，而不仅仅是函数名；而C语言并不支持函数重载，因此编译C语言代码的函数时不会带上函数的参数类型，一般只包括函数名。</br></p>
</blockquote>
<blockquote>
<p>这个功能十分有用，因为在C++出现以前，很多代码(包括很多底层的库)都是C语言写的，为了更好地支持原来的C代码和已经写好的C语言库，需要在C++中尽可能的支持C，而<code>extern &quot;C&quot;</code>就是其中的一个策略。</br></p>
</blockquote>
<blockquote>
<p>在多个人协同开发时，可能有的人比较擅长C语言，而有的人擅长C++，这样的情况下也会出现在C++的代码中调用C语言代码的情况。</br></p>
</blockquote>
<h3 id="315请你来说一下c中struct和class的区别">3.15 请你来说一下C++中struct和class的区别</h3>
<p>在C++中，class和struct做类型定义是只有两点区别：</p>
<ul>
<li>①<strong>默认继承权限</strong>不同，class继承默认是private继承，而struct默认是public继承</br></li>
<li>②class还可用于定义模板参数，像typename，但是关键字struct不能用于定义模板参数</br></li>
<li>③C++保留struct关键字，原因: 保证与C语言的向下兼容性，C++必须提供一个struct</br></li>
<li>④C++中的struct定义必须百分百地保证与C语言中的struct的向下兼容性，把C++中的最基本的对象单元规定为class而不是struct，就是为了避免各种兼容性要求的限制</br></li>
<li>⑤对struct定义的扩展使C语言的代码能够更容易的被移植到C++中</br></li>
</ul>
<h3 id="316c类内可以定义引用数据成员吗">3.16 C++类内可以定义引用数据成员吗？</h3>
<blockquote>
<p>可以，必须通过成员函数初始化列表初始化。</p>
</blockquote>
<h3 id="317c中类成员的访问权限">3.17 C++中类成员的访问权限</h3>
<ul>
<li>①C++通过 <code>public</code>、<code>protected</code>、<code>private</code> 三个关键字来控制成员变量和成员函数的访问权限，它们分别表示公有的、受保护的、私有的，被称为<strong>成员访问限定符</strong>。</br></li>
<li>②在类的内部(定义类的代码内部)，无论成员被声明为 public、protected 还是 private，都是可以互相访问的，没有访问权限的限制。</br></li>
<li>③在类的外部(定义类的代码之外)，只能通过对象访问成员，并且通过对象只能访问 public 属性的成员，不能访问 private、protected 属性的成员</br></li>
</ul>
<h3 id="318什么是右值引用跟左值又有什么区别">3.18 什么是右值引用，跟左值又有什么区别？</h3>
<p>左值和右值的概念:</p>
<blockquote>
<p>①左值: </br>
能取地址，或者具名对象，表达式结束后依然存在的持久对象；</br>
右值: 不能取地址，匿名对象，表达式结束后就不再存在的临时对象；</br>
②区别: </br>
左值能寻址，右值不能;</br>
左值能赋值，右值不能;</br>
左值可变，右值不能(仅对基础类型适用，用户自定义类型右值引用可以通过成员函数改变);</br></p>
</blockquote>
<h3 id="319面向对象的三大特征">3.19 面向对象的三大特征</h3>
<ul>
<li>封装性: 将客观事物抽象成类，每个类对自身的数据和方法实行<strong>访问权限保护</strong>(private ， protected ， public)。</br></li>
<li>继承性: 广义的继承有三种实现形式：实现继承(使用基类的属性和方法而无需额外编码的能力)、可视继承(子窗体使用父窗体的外观和实现代码)、接口继承(仅使用属性和方法,实现滞后到子类实现)。</br></li>
<li>多态性: 是将父类对象设置成为和一个或更多它的子对象相等的技术。用子类对象给父类对象赋值之后，父类对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。</br></li>
</ul>
<h3 id="320c的空类有哪些成员函数">3.20 C++的空类有哪些成员函数</h3>
<p><a href="https://blog.csdn.net/weixin_45805339/article/details/128089198"target="_blank" rel="external nofollow noopener noreferrer">C++空类成员函数<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<blockquote>
<p>缺省构造函数。</br>
缺省拷贝构造函数。</br>
缺省析构函数。</br>
缺省赋值运算符。</br>
缺省取址运算符。</br>
缺省取址运算符 const 。</br>
注意：有些书上只是简单的介绍了前四个函数。没有提及后面这两个函数。但后面这两个函数也是空类的默认函数。另外需要注意的是，只有当实际使用这些空类成员函数的时候，编译器才会去定义它们。</br></p>
</blockquote>
<h2 id="4-基础知识四">4. 基础知识(四)</h2>
<h3 id="41-说一说c中四种cast转换">4.1 说一说c++中四种cast转换</h3>
<p>C++中四种类型转换是：<code>static_cast</code>, <code>dynamic_cast</code>, <code>const_cast</code>, <code>reinterpret_cast</code></br></p>
<ul>
<li>
<p>1、const_cast</br>
用于将const变量转为非const, 去除const属性</br></p>
</li>
<li>
<p>2、static_cast</br>
用于各种隐式转换，比如非const转const，void*转指针等, static_cast能用于多态向上转化，如果向下转能成功但是不安全，结果未知；</br></p>
</li>
<li>
<p>3、dynamic_cast</br>
用于动态类型转换。只能用于<strong>含有虚函数的类</strong>，用于类层次间的向上和向下转化。只能转指针或引用。向下转化时，如果是非法的，对于指针返回NULL，对于引用抛异常。要深入了解内部转换的原理。</br></p>
</li>
<li>
<p>向上转换：指的是子类向基类的转换</br></p>
</li>
<li>
<p>向下转换：指的是基类向子类的转换</br>
它通过判断在执行到该语句的时候变量的运行时类型和要转换的类型是否相同来判断是否能够进行向下转换。</br></p>
</li>
<li>
<p>4、reinterpret_cast</br>
几乎什么都可以转，比如将int转指针，可能会出问题，尽量少用；</br></p>
</li>
<li>
<p>5、为什么不使用C的强制转换？</br></p>
<blockquote>
<p>C的强制转换表面上看起来功能强大什么都能转，但是转化不够明确，不能进行错误检查，容易出错。</br></p>
</blockquote>
</li>
</ul>
<h3 id="42-对c中的smart-pointer四个智能指针的理解shared_ptrunique_ptrweak_ptrauto_ptr">4.2 对c++中的smart pointer四个智能指针的理解：shared_ptr,unique_ptr,weak_ptr,auto_ptr</h3>
<ul>
<li>①C++里面的四个智能指针: <code>auto_ptr</code>, <code>shared_ptr</code>, <code>weak_ptr</code>, <code>unique_ptr</code> 其中后三个是c++11支持，并且第一个已经被C++11弃用。</br></li>
<li>②智能指针的作用是管理一个指针，因为存在以下这种情况：</br>
<ul>
<li>申请的空间在函数结束时忘记释放，造成<strong>内存泄漏</strong>。使用智能指针可以很大程度上的避免这个问题，因为智能指针就是一个类，当超出了类的作用域时，类会自动调用析构函数，析构函数会自动释放资源。所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。</br></li>
</ul>
</li>
<li>③auto_ptr(c++98的方案，cpp11已经抛弃)</li>
</ul>
<p><font color=red>采用所有权模式。</font></p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="nf">p3</span> <span class="p">(</span><span class="k">new</span> <span class="n">string</span> <span class="p">(</span><span class="err">“</span><span class="k">auto</span><span class="err">”</span><span class="p">));</span> <span class="c1">//#4
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">unique_ptr</span> <span class="n">p4</span><span class="err">；</span> <span class="c1">//#5
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">p4</span> <span class="o">=</span> <span class="n">p3</span><span class="p">;</span><span class="c1">//此时会报错！！
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>编译器认为<code>p4=p3</code>非法，避免了p3不再指向有效数据的问题。因此，unique_ptr比auto_ptr更安全。</br></p>
<p>另外unique_ptr还有更聪明的地方：当程序试图将一个 unique_ptr 赋值给另一个时，如果源 unique_ptr 是个临时右值，编译器允许这么做；如果源 unique_ptr 将存在一段时间，编译器将禁止这么做，比如:</p>
<div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="nf">pu1</span><span class="p">(</span><span class="k">new</span> <span class="n">string</span> <span class="p">(</span><span class="err">“</span><span class="n">hello</span> <span class="n">world</span><span class="err">”</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="n">pu2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">pu2</span> <span class="o">=</span> <span class="n">pu1</span><span class="p">;</span> <span class="c1">// #1 not allowed
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">unique_ptr</span> <span class="n">pu3</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">pu3</span> <span class="o">=</span> <span class="n">unique_ptr</span><span class="p">(</span><span class="k">new</span> <span class="n">string</span> <span class="p">(</span><span class="err">“</span><span class="n">You</span><span class="err">”</span><span class="p">));</span> <span class="c1">// #2 allowed
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其中#1留下悬挂的 unique_ptr(pu1)，这可能导致危害。而#2不会留下悬挂的unique_ptr，因为它调用 unique_ptr 的构造函数，该构造函数创建的临时对象在其所有权让给 pu3 后就会被销毁。这种随情况而已的行为表明，unique_ptr 优于允许两种赋值的auto_ptr 。</p>
<p>注：如果确实想执行类似与#1的操作，要安全的重用这种指针，可给它赋新值。C++有一个标准库函数std::move()，让你能够将一个unique_ptr赋给另一个。例如：</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">unique_ptr</span> <span class="n">ps1</span><span class="p">,</span> <span class="n">ps2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ps1</span> <span class="o">=</span> <span class="n">demo</span><span class="p">(</span><span class="err">“</span><span class="n">hello</span><span class="err">”</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">ps2</span> <span class="o">=</span> <span class="n">move</span><span class="p">(</span><span class="n">ps1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">ps1</span> <span class="o">=</span> <span class="n">demo</span><span class="p">(</span><span class="err">“</span><span class="n">alexia</span><span class="err">”</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">ps2</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">ps1</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>shared_ptr实现共享式拥有概念。</strong><u>多个智能指针可以指向相同对象，该对象和其相关资源会在“最后一个引用被销毁”时候释放。</u>从名字share就可以看出了资源可以被多个指针共享，它使用计数机制来表明资源被几个指针共享。可以通过成员函数use_count()来查看资源的所有者个数。除了可以通过new来构造，还可以通过传入auto_ptr, unique_ptr,weak_ptr来构造。当我们调用release()时，当前指针会释放资源所有权，计数减一。当计数等于0时，资源会被释放。</p>
<p>shared_ptr 是为了解决 auto_ptr 在对象所有权上的局限性(auto_ptr 是独占的), 在使用引用计数的机制上提供了可以共享所有权的智能指针。</p>
<p><strong>成员函数：</strong></p>
<ul>
<li><code>use_count</code> 返回引用计数的个数</br></li>
<li><code>unique</code> 返回是否是独占所有权( use_count 为 1)</br></li>
<li><code>swap</code> 交换两个 shared_ptr 对象(即交换所拥有的对象)</br></li>
<li><code>reset</code> 放弃内部对象的所有权或拥有对象的变更, 会引起原有对象的引用计数的减少</br></li>
<li><code>get</code> 返回内部对象(指针), 由于已经重载了()方法, 因此和直接使用对象是一样的.如 <code>shared_ptr sp(new int(1))</code>; <code>sp</code> 与 <code>sp.get()</code>是等价的</br></li>
</ul>
<p><strong>weak_ptr:</strong></p>
<ul>
<li>weak_ptr 是一种不控制对象生命周期的智能指针, 它指向一个 shared_ptr 管理的对象. 对该对象进行内存管理的是那个强引用的shared_ptr. weak_ptr只是提供了对管理对象的一个访问手段。</br></br></li>
<li>weak_ptr 设计的目的是为了配合 <code>shared_ptr</code> 而引入的一种智能指针来协助 <code>shared_ptr</code> 工作, 它只可以从一个 <code>shared_ptr</code> 或另一个 <code>weak_ptr</code> 对象构造, 它的构造和析构不会引起引用记数的增加或减少。</br></br></li>
<li>weak_ptr是用来<strong>解决shared_ptr相互引用时的死锁问题</strong>,如果说两个shared_ptr相互引用,那么这两个指针的引用计数永远不可能下降为0,资源永远不会释放。它是对对象的一种弱引用，不会增加对象的引用计数，和shared_ptr之间可以相互转化，shared_ptr可以直接赋值给它，它可以通过调用lock函数来获得shared_ptr。</br></li>
</ul>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">B</span><span class="o">&gt;</span> <span class="n">pb_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">A</span><span class="p">(){</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">&#34;A delete&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">A</span><span class="o">&gt;</span> <span class="n">pa_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">B</span><span class="p">(){</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">&#34;B delete&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">fun</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">B</span><span class="o">&gt;</span> <span class="n">pb</span><span class="p">(</span><span class="k">new</span> <span class="n">B</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">A</span><span class="o">&gt;</span> <span class="n">pa</span><span class="p">(</span><span class="k">new</span> <span class="n">A</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">pb</span><span class="o">-&gt;</span><span class="n">pa_</span> <span class="o">=</span> <span class="n">pa</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">pa</span><span class="o">-&gt;</span><span class="n">pb_</span> <span class="o">=</span> <span class="n">pb</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">pb</span><span class="p">.</span><span class="n">use_count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">pa</span><span class="p">.</span><span class="n">use_count</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">fun</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到fun函数中pa ，pb之间互相引用，两个资源的引用计数为2，当要跳出函数时，智能指针pa，pb析构时两个资源引用计数会减一，但是两者引用计数还是为1，导致跳出函数时资源没有被释放(A B的析构函数没有被调用)，如果把其中一个改为weak_ptr就可以了，我们把类A里面的shared_ptr pb_; 改为weak_ptr pb_; 运行结果如下，这样的话，资源B的引用开始就只有1，当pb析构时，B的计数变为0，B得到释放，B释放的同时也会使A的计数减一，同时pa析构时使A的计数减一，那么A的计数为0，A得到释放。</br></p>
<p>注意：不能通过weak_ptr直接访问对象的方法，比如B对象中有一个方法print(),我们不能这样访问，pa-&gt;pb_-&gt;print(); 英文pb_是一个weak_ptr，应该先把它转化为shared_ptr,如：shared_ptr p = pa-&gt;pb_.lock(); p-&gt;print();</br></p>
<h3 id="43-说说强制类型转换运算符">4.3 说说强制类型转换运算符</h3>
<p><strong>①static_cast 用于非多态类型的转换</strong></p>
<ul>
<li>用于非多态类型的转换</br></li>
<li>不执行运行时类型检查(转换安全性不如 dynamic_cast)</br></li>
<li>通常用于转换数值数据类型(如 float -&gt; int)</br></li>
<li>可以在整个类层次结构中移动指针，子类转化为父类安全(向上转换)，父类转化为子类不安全(因为子类可能有不在父类的字段或方法)</br></li>
</ul>
<p><strong>②dynamic_cast 用于多态类型的转换</strong></p>
<ul>
<li>用于多态类型的转换</br></li>
<li>执行运行时类型检查</br></li>
<li>只适用于指针或引用</br></li>
<li>对不明确的指针的转换将失败(返回 nullptr)，但不引发异常</br></li>
<li>可以在整个类层次结构中移动指针，包括向上转换、向下转换</br></li>
</ul>
<p><strong>③const_cast</strong></p>
<ul>
<li>用于删除 const、volatile 和 __unaligned 特性(如将 const int 类型转换为 int 类型 )</li>
</ul>
<p><strong>④reinterpret_cast</strong></p>
<ul>
<li>用于位的简单重新解释</br></li>
<li>滥用 reinterpret_cast 运算符容易带来风险。除非所需转换本身是低级别的，否则应使用其他强制转换运算符之一。</br></li>
<li>允许将任何指针转换为任何其他指针类型(如 char* 到 int* 或 One_class* 到 Unrelated_class* 之类的转换，但其本身并不安全)
也允许将任何整数类型转换为任何指针类型以及反向转换。</br></li>
<li>reinterpret_cast 运算符不能丢掉 const、volatile 或 __unaligned 特性。</br></li>
<li>reinterpret_cast 的一个实际用途是在哈希函数中，即，通过让两个不同的值几乎不以相同的索引结尾的方式将值映射到索引。</br></li>
</ul>
<p><strong>⑤bad_cast转换失败异常</strong></p>
<ul>
<li>由于强制转换为引用类型失败，dynamic_cast 运算符引发 bad_cast 异常。
bad_cast 使用:</li>
</ul>
<div class="highlight" id="id-26"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Circle</span><span class="o">&amp;</span> <span class="n">ref_circle</span> <span class="o">=</span> <span class="k">dynamic_cast</span><span class="o">&lt;</span><span class="n">Circle</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="n">ref_shape</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">catch</span> <span class="p">(</span><span class="n">bad_cast</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Caught: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">b</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="44-谈谈你对拷贝构造函数和赋值运算符的认识">4.4 谈谈你对拷贝构造函数和赋值运算符的认识</h3>
<p>拷贝构造函数和赋值运算符重载有以下两个不同之处：</br></p>
<blockquote>
<p>①拷贝构造函数生成新的类对象，而赋值运算符不能。</br>
②由于拷贝构造函数是直接构造一个新的类对象，所以在初始化这个对象之前不用检验原对象是否和新建对象相同，而赋值运算符则需要这个操作;</br>
③另外，赋值运算中，如果原来的对象中有内存分配要先把内存释放掉。</br>
注意：当有类中有指针类型的成员变量时，一定要重写拷贝构造函数和赋值运算符，不要使用默认的。</p>
</blockquote>
<h3 id="45-在c中使用malloc申请的内存能否通过delete释放使用new申请的内存能否用free">4.5 在C++中，使用malloc申请的内存能否通过delete释放？使用new申请的内存能否用free？</h3>
<blockquote>
<p>不能，malloc /free主要为了兼容C，new和delete 完全可以取代malloc /free的。</br>
①malloc /free的操作对象都是必须明确大小的。<font color=red>而且不能用在动态类上</font>。</br>
②new 和delete会自动进行类型检查和大小，malloc/free不能执行构造函数与析构函数，所以动态对象它是不行的。</br></p>
</blockquote>
<p>当然从理论上说使用malloc申请的内存是可以通过delete释放的。不过一般不这样写的。而且也不能保证每个C++的运行时都能正常。</p>
<h3 id="46-用c设计一个不能被继承的类">4.6 用C++设计一个不能被继承的类</h3>
<p>ref: <a href="https://blog.csdn.net/wei_cheng18/article/details/81043858"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/wei_cheng18/article/details/81043858<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<div class="highlight" id="id-27"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">friend</span> <span class="n">T</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">A</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">   <span class="o">~</span><span class="n">A</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">virtual</span> <span class="k">public</span> <span class="n">A</span><span class="o">&lt;</span><span class="n">B</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">B</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"> <span class="o">~</span><span class="n">B</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">C</span> <span class="o">:</span> <span class="k">virtual</span> <span class="k">public</span> <span class="n">B</span><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">C</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"> <span class="o">~</span><span class="n">C</span><span class="p">()</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">main</span><span class="p">(</span> <span class="kt">void</span> <span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">B</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">//C c;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意：<font color=red>构造函数是实现继承的关键</font>，每次子类对象构造时，首先调用的是父类的构造函数，然后才是自己的。<font color=red>实现不能被继承的类关键在于想父类的构造函数使用private控制</font></p>
<ul>
<li>
<p>这里需要说明的是：我们设计的不能被继承的类B对基类A的继承必须是虚继承，这样一来C类继承B类时会去直接调用A的构造函数，而不是像普通继承那样，先调用B的构造函数再调用A的构造函数；</p>
</li>
<li>
<p>C类直接调用A类的构造函数，由于A类的构造函数是私有的，而B是A的友元，C类不是A的友元，友元关系不会继承，因此会编译报错。</p>
</li>
</ul>
<h3 id="48-访问基类的私有虚函数">4.8 访问基类的私有虚函数</h3>
<p>写出以下程序的输出结果：</p>
<div class="highlight" id="id-28"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">class</span> <span class="nc">A</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">g</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;A::g&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">   <span class="k">virtual</span> <span class="kt">void</span> <span class="n">f</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;A::f&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">public</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="kt">void</span> <span class="nf">g</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;B::g&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">h</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;B::h&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="nf">void</span><span class="p">(</span> <span class="o">*</span><span class="n">Fun</span> <span class="p">)(</span> <span class="kt">void</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="n">B</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="n">Fun</span> <span class="n">pFun</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">pFun</span> <span class="o">=</span> <span class="p">(</span> <span class="n">Fun</span> <span class="p">)</span><span class="o">*</span><span class="p">(</span> <span class="p">(</span> <span class="kt">int</span><span class="o">*</span> <span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="kt">int</span><span class="o">*</span> <span class="p">)(</span> <span class="o">&amp;</span><span class="n">b</span> <span class="p">)</span> <span class="o">+</span> <span class="n">i</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="n">pFun</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>输出结果:</p>
<div class="highlight" id="id-29"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">B</span><span class="o">::</span><span class="n">g</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span><span class="o">::</span><span class="n">f</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span><span class="o">::</span><span class="n">h</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：考察了面试者对虚函数的理解程度。一个对虚函数不了解的人很难正确的做出本题。 在学习面向对象的多态性时一定要深刻理解虚函数表的工作原理。</p>
</blockquote>
<blockquote>
<p>虚函数: 通过基类访问派生类定义的函数，多态时使用，使用虚函数加上virtual关键字。</br>
虚函数就是在基类定义一个未实现的函数名，为了提高程序的可读性</br>
<a href="https://blog.csdn.net/weixin_45138932/article/details/125667041"target="_blank" rel="external nofollow noopener noreferrer">虚函数详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/qq_42048450/article/details/117282640?spm=1001.2014.3001.5502"target="_blank" rel="external nofollow noopener noreferrer">C++虚函数详解_疯狂的麦克斯_max的博客-CSDN博客_c++虚函数<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<p><a href="https://blog.csdn.net/weixin_43700340/article/details/89471069"target="_blank" rel="external nofollow noopener noreferrer">菱形继承1<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/Y673789476/article/details/128271855#t9"target="_blank" rel="external nofollow noopener noreferrer">菱形继承2<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="49-对虚函数和多态的理解">4.9 对虚函数和多态的理解</h3>
<ul>
<li>①多态的实现主要分为<strong>静态多态</strong>和<strong>动态多态</strong>
<ul>
<li>静态多态主要是<strong>重载</strong>，在编译的时候就已经确定；</li>
<li>动态多态是用虚函数机制实现的，在运行期间动态绑定。</li>
<li>举个例子: 一个父类类型的指针指向一个子类对象时候，使用父类的指针去调用子类中重写了的父类中的虚函数的时候，会调用子类重写过后的函数，在父类中声明为加了virtual关键字的函数，在子类中重写时候不需要加virtual也是虚函数。</br></li>
</ul>
</li>
<li>②虚函数的实现: 在有虚函数的类中，类的最开始部分是一个虚函数表的指针，这个指针指向一个虚函数表，表中放了虚函数的地址，实际的虚函数在代码段(.text)中。当子类继承了父类的时候也会继承其虚函数表，当子类重写父类中虚函数时候，会将其继承到的虚函数表中的地址替换为重新写的函数地址。使用了虚函数，会增加访问内存开销，降低效率。</br></li>
</ul>
<h3 id="410-简述类成员函数的重写overwrite重载overload和隐藏的区别">4.10 简述类成员函数的重写(overwrite)、重载(overload)和隐藏的区别</h3>
<p>(1) 重写和重载主要有以下几点不同。</p>
<ul>
<li>①范围的区别: 被重写的函数和重写的函数在两个类中，而重载和被重载的函数在同一个类中。</br></li>
<li>②参数的区别: 被重写函数和重写函数的参数列表一定相同，而被重载函数和重载函数的参数列表一定不同。</br></li>
<li>③virtual的区别：重写的基类中被重写的函数必须要有virtual修饰，而重载函数和被重载函数可以被virtual修饰，也可以没有。</br></li>
</ul>
<p>(2) 隐藏和重写、重载有以下几点不同。</p>
<ul>
<li>与重载的范围不同: 和重写一样，隐藏函数和被隐藏函数不在同一个类中。</br></li>
<li>参数的区别: 隐藏函数和被隐藏的函数的参数列表可以相同，也可不同，但是函数名肯定要相同。 当参数不相同时，无论基类中的参数是否被virtual 修饰，基类的函数都是被隐藏，而不是被重写。</br></li>
</ul>
<p>注意：虽然重载和覆盖都是实现多态的基础，但是两者实现的技术完全不相同，达到的目的也是完全不同的，覆盖是<strong>动态绑定</strong>的多态，而重载是<strong>静态绑定</strong>的多态。</p>
<h3 id="411-链表和数组有什么区别">4.11 链表和数组有什么区别</h3>
<p><strong>存储形式:</strong></p>
<blockquote>
<p>数组是一块连续的空间，声明时就要确定长度。</br>
链表是一块可不连续的动态空间，长度可变，每个结点要保存相邻结点指针。</br></p>
</blockquote>
<p><strong>数据查找:</strong></p>
<blockquote>
<p>数组的线性查找速度快，查找操作直接使用偏移地址。</br>
链表需要按顺序检索结点， 效率低。</br>
数据插入或删除: 链表可以快速插入和删除结点，而数组则可能需要大量数据移动。</br></p>
</blockquote>
<p><strong>越界问题：</strong></p>
<blockquote>
<p>链表不存在越界问题，数组有越界问题。</p>
</blockquote>
<p><strong>注意：</strong></p>
<blockquote>
<p>在选择数组或链表数据结构时，一定要根据实际需要进行选择。<font color=green>数组便于查询，链表便于插入删除</font>。数组节省空间但是长度固定，链表虽然变长但是占了更多的存储空间。</p>
</blockquote>
<h3 id="412-用两个栈实现一个队列的功能">4.12 用两个栈实现一个队列的功能</h3>
<div class="highlight" id="id-30"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">node</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">   <span class="kt">int</span> <span class="n">data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">   <span class="n">node</span> <span class="o">*</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span><span class="n">node</span><span class="p">,</span><span class="o">*</span><span class="n">LinkStack</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//创建空栈：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LinkStack</span> <span class="nf">CreateNULLStack</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="n">S</span> <span class="o">=</span> <span class="p">(</span><span class="n">LinkStack</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="k">sizeof</span><span class="p">(</span> <span class="n">node</span> <span class="p">)</span> <span class="p">);</span> <span class="c1">// 申请新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;Fail to malloc a new node.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">S</span><span class="o">-&gt;</span><span class="n">data</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">//初始化新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//栈的插入函数：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LinkStack</span> <span class="nf">Push</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">,</span> <span class="kt">int</span> <span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="p">)</span> <span class="c1">//检验栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;There no node in stack!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">LinkStack</span> <span class="n">p</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">LinkStack</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="k">sizeof</span><span class="p">(</span> <span class="n">node</span> <span class="p">)</span> <span class="p">);</span> <span class="c1">// 申请新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;Fail to malloc a new node.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">p</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="k">else</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">p</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">p</span><span class="o">-&gt;</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">;</span> <span class="c1">//初始化新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">p</span><span class="p">;</span> <span class="c1">//插入新结点
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//出栈函数：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">node</span> <span class="nf">Pop</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="n">node</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span><span class="p">.</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">==</span> <span class="n">S</span><span class="p">)</span> <span class="c1">//检验栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;There no node in stack!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">if</span><span class="p">(</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">printf</span><span class="p">(</span><span class="s">&#34;The stack is NULL,can&#39;t pop!</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">LinkStack</span> <span class="n">p</span> <span class="o">=</span> <span class="n">S</span> <span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span> <span class="c1">//节点出栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">free</span><span class="p">(</span> <span class="n">p</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"> <span class="n">p</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">//双栈实现队列的入队函数：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LinkStack</span> <span class="nf">StackToQueuPush</span><span class="p">(</span> <span class="n">LinkStack</span> <span class="o">&amp;</span><span class="n">S</span><span class="p">,</span> <span class="kt">int</span> <span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="n">node</span> <span class="n">n</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">LinkStack</span> <span class="n">S1</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> <span class="n">CreateNULLStack</span><span class="p">(</span> <span class="n">S1</span> <span class="p">);</span> <span class="c1">//创建空栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="k">while</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">!=</span> <span class="n">S</span><span class="o">-&gt;</span><span class="n">next</span> <span class="p">)</span> <span class="c1">//S 出栈入S1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">n</span> <span class="o">=</span> <span class="n">Pop</span><span class="p">(</span> <span class="n">S</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Push</span><span class="p">(</span> <span class="n">S1</span><span class="p">,</span> <span class="n">n</span><span class="p">.</span><span class="n">data</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="n">Push</span><span class="p">(</span> <span class="n">S1</span><span class="p">,</span> <span class="n">data</span> <span class="p">);</span> <span class="c1">//新结点入栈
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"> <span class="k">while</span><span class="p">(</span> <span class="nb">NULL</span> <span class="o">!=</span> <span class="n">S1</span><span class="o">-&gt;</span><span class="n">next</span> <span class="p">)</span> <span class="c1">//S1 出栈入S
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">n</span> <span class="o">=</span> <span class="n">Pop</span><span class="p">(</span> <span class="n">S1</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Push</span><span class="p">(</span> <span class="n">S</span><span class="p">,</span> <span class="n">n</span><span class="p">.</span><span class="n">data</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">S</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意：用两个栈能够实现一个队列的功能，那用两个队列能否实现一个栈的功能呢？结果是否定的，因为栈是先进后出，将两个栈连在一起，就是先进先出。而队列是现先进先出，无论多少个连在一起都是先进先出，而无法实现先进后出。</p>
<h3 id="413-共享数据的保护">4.13 共享数据的保护</h3>
<ul>
<li>
<p>①常引用: 使所引用的形参不能被更新</p>
<div class="highlight" id="id-31"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">display</span><span class="p">(</span><span class="k">const</span> <span class="kt">double</span><span class="o">&amp;</span> <span class="n">a</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>②常对象：在生存期内不能被更新，但 <strong><font color=red>必须被初始化</font></strong></p>
<div class="highlight" id="id-32"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">A</span> <span class="k">const</span> <span class="nf">a</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>③常成员函数：
不能修改对象中数据成员，也不能调用类中没有被const 修饰的成员函数(常对象唯一的对外接口).如果声明了一个常对象，则该对象只能调用他的常函数！-&gt;可以用于对重载函数的区分;</p>
<div class="highlight" id="id-33"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">print</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">print</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>④<code>extern int a</code>: 使其他文件也能访问该变量</p>
<ul>
<li>声明一个函数或定义函数时，冠以static的话，函数的作用域就被限制在了当前编译单元，当前编译单元内也必须包含函数的定义，也只在其编译单元可见，其他单元不能调用这个函数(每一个cpp文件就是一个编译单元)。</li>
</ul>
</li>
</ul>
<h3 id="414-程序内存分配方式以及它们的区别">4.14 程序内存分配方式以及它们的区别</h3>
<p>内存分配大致上可以分成5块:</p>
<ul>
<li><strong>栈区(stack)</strong></br>
<ul>
<li>栈，就是那些由编译器在需要时分配，在不需要的时候自动清除的变量的存储区。里面的变量通常是<u>局部变量</u>、<u>函数参数</u>等。(由编译器管理)</br></li>
</ul>
</li>
<li><strong>堆区(heap)</strong></br>
<ul>
<li>一般由程序员分配、释放，若程序员不释放，程序结束时可能由系统回收。注意，它与数据结构中的堆是两回事，分配方式类似于链表。</br></li>
</ul>
</li>
<li><strong>全局区(静态区)(static)</strong></br>
<ul>
<li>全局变量和静态变量被分配到同一块内存中。程序结束后由系统释放。</br></li>
</ul>
</li>
<li><strong>常量存储区</strong></br>
<ul>
<li>常量字符串就是放在这里的，不允许修改，程序结束后由系统释放。</br></li>
</ul>
</li>
<li><strong>程序代码区</strong></br>
<ul>
<li>存放函数体的二进制代码。</br></li>
</ul>
</li>
</ul>
<p>C++程序在执行时，将内存大方向划分为4个区域:</p>
<ul>
<li>
<p>程序运行前</p>
<ul>
<li><font color=red>代码区</font>：存放函数体的二进制代码，由操作系统进行管理的</br></li>
<li><font color=red>全局区</font>：存放全局变量和静态变量以及常量</br></li>
</ul>
</li>
<li>
<p>程序运行后</p>
<ul>
<li><font color=red>栈区</font>：由编译器自动分配释放, 存放函数的参数值,局部变量等</br></li>
<li><font color=red>堆区</font>：由程序员分配和释放,若程序员不释放,程序结束时由操作系统回收</br></li>
</ul>
</li>
</ul>
<p>内存四区意义：</p>
<blockquote>
<p>不同区域存放的数据，赋予不同的生命周期, 给我们更大的灵活编程</p>
</blockquote>
<h3 id="415-explicit">4.15 explicit</h3>
<p>函数声明时加上explicit可以<u>阻止函数参数被隐式转换</u>。</p>
<div class="highlight" id="id-34"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="n">Class</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="k">explicit</span> <span class="nf">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Void</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="n">A</span> <span class="n">a1</span><span class="o">=</span><span class="mi">12</span><span class="p">;</span>   <span class="c1">//不加explicit时会被隐式转换位 A a1=A(12);加了此时编译器会报错。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>被声明为explicit的构造函数通常比non-explicit 函数更受欢迎。</p>
<h3 id="416-mutable关键字">4.16 mutable关键字</h3>
<blockquote>
<p>mutable的中文意思是“可变的，易变的”，跟constant(既C++中的const)是反义词。在C++中，mutable也是为了突破const的限制而设置的。</br>
被mutable修饰的变量(mutable只能用于修饰类的非静态数据成员)，将永远处于可变的状态，即使在一个const函数中。</br>
我们知道，假如类的成员函数不会改变对象的状态，那么这个成员函数一般会声明为const。但是，有些时候，我们<font color=purple>需要在const的函数里面修改一些跟类状态无关的数据成员，那么这个数据成员就应该被mutalbe来修饰</font>。(使用mutable修饰的数据成员可以被const成员函数修改)。</br></p>
</blockquote>
<h3 id="417-用const修饰函数的返回值">4.17 用const修饰函数的返回值</h3>
<p>如果给以“指针传递”方式的函数返回值加const修饰，那么函数返回值(即指针)的内容不能被修改，该返回值只能被赋给加const修饰的同类型指针。例如函数：</p>
<div class="highlight" id="id-35"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span> <span class="nf">GetString</span><span class="p">(</span><span class="kt">void</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 如下语句将出现编译错误：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">char</span><span class="o">*</span><span class="n">str</span> <span class="o">=</span> <span class="n">GetString</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 正确的用法是
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">str</span> <span class="o">=</span><span class="n">GetString</span><span class="p">();</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="418-宏const和enum">4.18 宏、const和enum</h3>
<ul>
<li><code>#define</code>不被视为语言的一部分。对于单纯常量，最好用const对象或者enum替换<code>#define</code>。</br></li>
<li>对于类似函数的宏，尽量使用内联函数inline替换掉<code>#define</code></br></li>
<li>enum枚举类型是被当做 int 或者 unsigned int 类型来处理的。</br></li>
</ul>
<h3 id="419-static-对象和-non-local-static对象区别">4.19 static 对象和 non-local static对象区别</h3>
<blockquote>
<p>①C++中的static对象是指存储区不属于stack和heap、&ldquo;寿命&quot;从被构造出来直至程序结束为止的对象。</br>
②这些对象包括全局对象，定义于namespace作用域的对象，在class、function以及file作用域中被声明为static的对象。</br>
③其中，函数内的static对象称为local static对象，而其它static对象称为non-local static对象。</br></p>
</blockquote>
<p>local static 对象和non-local static对象在何时被初始化(构造)这个问题上存在细微的差别:</p>
<blockquote>
<p>①对于local static对象，在其所属的函数被调用之前，该对象并不存在，即只有在第一次调用对应函数时，local static对象才被构造出来。</br>
②而对于non-local static对象，在main()函数开始前就已经被构造出来，并在main()函数结束后被析构。</br></p>
</blockquote>
<p><font color=red>建议：</font></p>
<blockquote>
<p>1.对内置对象进行手工初始化，因为C++不保证初始化它们。</br>
2.构造函数最好使用成员初值列，而不要在构造函数本体中使用赋值操作。初值列中列出的成员变量，其排序次序应该和它们在class中的声明次序相同(初始化顺序与声明变量顺序一致)。</br>
3.为免除“跨编译单元的初始化次序问题”，尽量以local static对象替换non-local static对象。</br></p>
</blockquote>
<h3 id="420-全局变量和static变量的区别">4.20 全局变量和static变量的区别</h3>
<blockquote>
<p>①全局变量本身就是静态存储方式， 静态全局变量当然也是静态存储方式。 这两者在存储方式上并无不同。</br>
②这两者的区别在于非静态全局变量的作用域是整个源程序， 当一个源程序由多个源文件组成时，非静态的全局变量在各个源文件中都是有效的。</br>
③而静态全局变量则限制了其作用域， 即只在定义该变量的源文件内有效， 在同一源程序的其它源文件中不能使用它。</br>
④由于静态全局变量的作用域局限于一个源文件内，只能为该源文件内的函数公用， 因此可以避免在其它源文件中引起错误。</br></p>
</blockquote>
<h2 id="5-基础知识五">5. 基础知识(五)</h2>
<h3 id="51-为什么栈要比堆速度要快">5.1 为什么栈要比堆速度要快</h3>
<ul>
<li>①首先, 栈是本着LIFO原则的存储机制, 对栈数据的定位相对比较快速, 而堆则是随机分配的空间, 处理的数据比较多, 无论如何, 至少要两次定位.</li>
<li>②其次, 栈是由CPU提供指令支持的, 在指令的处理速度上, 对栈数据进行处理的速度自然要优于由操作系统支持的堆数据.</li>
<li>③再者, 栈是在一级缓存中做缓存的, 而堆则是在二级缓存中, 两者在硬件性能上差异巨大.</li>
<li>④最后, 各语言对栈的优化支持要优于对堆的支持, 比如swift语言中, 三个字及以内的struct结构, 可以在栈中内联, 从而达到更快的处理速度.</li>
</ul>
<h3 id="52-c-析构函数调用时间">5.2 c++ 析构函数调用时间</h3>
<ul>
<li>对象生命周期结束，被销毁时;</li>
<li>delete指向对象的指针时，或delete指向对象的基类类型指针，而其基类析构函数是虚函数时;</li>
<li>对象i是对象o的成员，o的析构函数被调用时，对象i的析构函数也被调用</li>
</ul>
<h3 id="53-静态绑定-动态绑定-也叫动态连编静态连编">5.3 静态绑定 动态绑定 (也叫动态连编，静态连编)</h3>
<blockquote>
<p>如果父类中存在有虚函数，那么编译器便会为之生成虚表(属于类)与虚指针(属于某个对象)，在程序运行时，根据虚指针的指向，来决定调用哪个虚函数，这称之与动态绑定，与之相对的是静态绑定，静态绑定在编译期就决定了。</p>
</blockquote>
<p>class和template都支持接口与多态:</p>
<ul>
<li>①对classes而言，接口是显式的，以函数签名为中心。多态则是通过virtual函数(虚函数)发生于运行期；</li>
<li>②对template参数而言，接口是隐式的，奠基于有效表达式。多态则是通过template具现化和函数重载解析发生于编译期。</li>
</ul>
<p><strong>泛型</strong></p>
<p>泛型是通过参数化类型来实现在同一份代码上操作多种数据类型。利用“参数化类型”将类型抽象化，从而实现灵活的复用。</p>
<h3 id="54-c语言的指针和c的引用有什么区别">5.4 C语言的指针和C++的引用有什么区别？</h3>
<ul>
<li>指针有自己的一块空间，指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元，即指针是一个实体。而引用只是一个别名;</br></li>
<li>使用sizeof看一个指针的大小是4，而引用则是被引用对象的大小;</br></li>
<li>指针可以被初始化为NULL，而引用必须被初始化且必须是一个已有对象的引用;</br></li>
<li>作为参数传递时，指针需要被解引用才可以对对象进行操作，而直接对引用的修改都会改变引用所指向的对象;</br></li>
</ul>
<h3 id="55-请你说说c语言是怎么进行函数调用的">5.5 请你说说C语言是怎么进行函数调用的</h3>
<blockquote>
<p>每一个函数调用都会分配函数栈，在栈内进行函数执行过程。调用前，先把返回地址压栈，然后把当前函数的esp指针压栈。(ESP(Extended Stack Pointer)为扩展栈指针寄存器，是指针寄存器的一种，用于存放函数栈顶指针)</br></p>
</blockquote>
<p>C语言参数压栈顺序？：从右到左</p>
<h3 id="56-c中拷贝赋值函数的形参能否进行值传递">5.6 C++中拷贝赋值函数的形参能否进行值传递？</h3>
<p>⭐ <font color=red>不能</font>。如果是这种情况下，调用拷贝构造函数的时候，首先要将实参传递给形参，这个传递的时候又要调用拷贝构造函数(aa = ex.aa; //此处调用拷贝构造函数)。如此循环，无法完成拷贝，栈也会满。</p>
<h3 id="57-include头文件的顺序以及双引号和尖括号的区别">5.7 include头文件的顺序以及双引号<code>&quot;&quot;</code>和尖括号<code>&lt;&gt;</code>的区别</h3>
<p>编译器预处理阶段查找头文件的路径不一样</p>
<ul>
<li>
<p>使用双引号<code>&quot;&quot;</code>包含的头文件，查找头文件路径的顺序为：</br></p>
<ul>
<li>①当前头文件目录</br></li>
<li>②编译器设置的头文件路径(编译器可使用-I显式指定搜索路径)</br></li>
<li>③系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径</br></li>
</ul>
</li>
<li>
<p>对于使用尖括号<code>&lt;&gt;</code>包含的头文件，查找头文件的路径顺序为：</br></p>
<ul>
<li>①编译器设置的头文件路径(编译器可使用-I显式指定搜索路径)</br></li>
<li>②系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径</br></li>
</ul>
</li>
</ul>
<h3 id="58-一个c源文件从文本到可执行文件经历的过程">5.8 一个C++源文件从文本到可执行文件经历的过程</h3>
<p>对于C/C++编写的程序，从源代码到可执行文件，一般经过下面四个步骤：</br></p>
<ul>
<li><strong>预编译</strong>，预编译的时候做一些简单的文本替换，比如宏替换，而不进行语法的检查；</li>
<li><strong>编译</strong>，在编译阶段，编译器将检查一些语法错误，但是，如果使用的函数事先没有定义这种情况，不再这一阶段检查，编译后，得到.s文件</li>
<li><strong>汇编</strong>，将C/C++代码变为汇编代码，得到.o或者.obj文件</li>
<li><strong>链接</strong>，将所用到的外部文件链接在一起，在这一阶段，就会检查使用的函数有没有定义</li>
</ul>
<p>链接过后，形成可执行文件.exe
详细请参阅: <a href="https://blog.csdn.net/daaikuaichuan/article/details/89060957"target="_blank" rel="external nofollow noopener noreferrer">一个C++源文件从文本到可执行文件经历的过程<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="59-内存泄漏原因和判断方法">5.9 内存泄漏原因和判断方法</h3>
<p>内存泄漏通常是因为调用了malloc/new等内存申请操作，但是缺少了对应的free/delete。
为了判断内存是否泄漏，我们一方面可以使用Linux环境下的内存泄漏检查工具Valgrind，另一方面我们写代码的时候，可以添加内存申请和释放的统计功能，统计当前申请和释放的内存是否一致，以此来判断内存是否有泄漏。</p>
<p>内存泄漏分类:</p>
<ul>
<li><strong>堆内存泄漏(heap leak)</strong>。堆内存是程序运行过程中根据需要通过malloc\realloc\new等从堆中分配的一块内存，在完成之后，必须要通过调用对应的free或者delete删除。如果程序的设计的错误导致这部分内存没有被释放，那么此后这块内存将不会被使用，就会产生Heap Leak。</li>
<li><strong>系统资源泄露(Resource Leak)</strong>。主要指程序使用系统分配的资源比如 Bitmap，handle，SOCKET等没有使用相应的函数释放掉，导致系统资源的浪费，严重可导致系统效能降低，系统运行不稳定。</li>
<li><strong>没有将基类的析构函数定义为虚函数</strong>。当基类指针指向子类对象时，如果基类的析构函数不是virtual，那么子类的析构函数将不会被调用，子类的资源没有正确的释放，从而造成内存泄漏。</li>
</ul>
<h3 id="510-段错误的产生原因">5.10 段错误的产生原因</h3>
<p><strong>段错误是什么?</strong></p>
<blockquote>
<p>一句话来说，段错误是指访问的内存超出了系统给这个程序所设定的内存空间，例如访问了不存在的内存地址、访问了系统保护的内存地址、访问了只读的内存地址等等情况。这里贴一个对于“段错误”的准确定义。</br></p>
</blockquote>
<p><strong>段错误产生的原因</strong></p>
<blockquote>
<p>访问不存在的内存地址</br>
访问系统保护的内存地址</br>
访问只读的内存地址</br>
栈溢出</br>
详细请参阅：<a href="https://www.cnblogs.com/lidabo/p/4545625.html"target="_blank" rel="external nofollow noopener noreferrer">Linux环境下段错误的产生原因及调试方法小结<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
</blockquote>
<h3 id="511-c-函数调用过程">5.11 C++ 函数调用过程</h3>
<p>总结起来整个过程就三步:</p>
<ul>
<li>
<ol>
<li>根据调用的函数名找到函数入口;</br></li>
</ol>
</li>
<li>
<ol start="2">
<li>在栈中申请调用函数中的参数及函数体内定义的变量的内存空间;</br></li>
</ol>
</li>
<li>
<ol start="3">
<li>函数执行完后，释放函数在栈中的申请的参数和变量的空间，最后返回值(如果有的话)。</br></li>
</ol>
</li>
</ul>
<p>详细请查阅：<a href="https://www.cnblogs.com/biyeymyhjob/archive/2012/07/20/2601204.html"target="_blank" rel="external nofollow noopener noreferrer">函数调用过程 / C/C++函数调用过程分析<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="512-如何调试c多线程程序">5.12 如何调试c++多线程程序？</h3>
<ol>
<li>打印日志，日志中加上线程ID；(简单粗暴)
gdb有thread相关命令，如infothread(简写infoth)显示线程消息，bxxthreadyy可以对某个thread设置断点，threadxx(简写成thrxx)切换到某个thread。再配合frame(简写f)相关的命令(比如up，down在不同frame间跳转)，基本可以处理若干个不同的线程间的debug……</br>
详细请查阅：<a href="https://www.cnblogs.com/LuckCoder/p/10948242.html"target="_blank" rel="external nofollow noopener noreferrer">C++(vs)多线程调试 (转)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ol>
<h3 id="513面向对象和面向过程的区别">5.13 面向对象和面向过程的区别</h3>
<ul>
<li>①面向对象方法中，把<u>数据</u>和<u>数据操作</u>放在一起，组成对象; 对同类的对象抽象出其共性组成类; 类通过简单的接口与外界发生联系，对象和对象之间通过消息进行通信。</br></li>
<li>②面向对象的三大特性是&quot;封装、“多态”、“继承”，五大原则是&quot;单一职责原则&rdquo;、“开放封闭原则”、“里氏替换原则”、“依赖倒置原则”、“接口分离原则”。</br></li>
<li>③而面向过程方法是以过程为中心的开发方法，它自顶向下顺序进行， 程序结构按照功能划分成若干个基本模块，这些模块形成树状结构。</br></li>
</ul>
<p><strong>(过程)优点：</strong></p>
<ul>
<li>性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗源;比如嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。</li>
<li>缺点：没有面向对象易维护、易复用、易扩展。</li>
</ul>
<p><strong>(对象)优点：</strong></p>
<ul>
<li>易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统。</li>
<li>缺点：性能比面向过程低。</li>
</ul>
<h3 id="514-关于引用赋值的多态">5.14 关于引用赋值的多态：</h3>
<div class="highlight" id="id-36"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">B</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">Class</span> <span class="nl">D</span> <span class="p">:</span> <span class="k">public</span> <span class="n">B</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">B</span><span class="o">&amp;</span> <span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">D</span><span class="o">&amp;</span> <span class="n">d</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span><span class="o">&amp;</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">d</span> <span class="p">;</span>  <span class="c1">//父类可以作为子类的引用，此时b1表现和指针形式一致(会调用B的非虚函数)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">D</span><span class="o">&amp;</span> <span class="n">d1</span> <span class="o">=</span> <span class="n">b</span><span class="err">；</span> <span class="c1">//错误，不能将子类作为父类的引用
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="515-模板的声明和实现不能分开的原因">5.15 模板的声明和实现不能分开的原因</h3>
<blockquote>
<p>链接的时候，需要实例化模板，这时候就需要找模板的具体实现了。假设在main函数中调用了一个模板函数，这时候就需要去实例化该类型的模板。注意main函数里面只包含了.h文件，也就是只有模板的声明，没有具体实现。就会报错。
而模板的实现.cpp里面，虽然有模板的具体实现，但是没有谁在该.cpp里面使用一个模板函数，就不会生成一个具体化的实例
详细请参阅：<a href="https://www.cnblogs.com/callme/articles/6142129.html"target="_blank" rel="external nofollow noopener noreferrer">C++ 模板类的声明与实现分离问题<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> / ​​​​​<a href="https://blog.csdn.net/weixin_40539125/article/details/83375452?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param"target="_blank" rel="external nofollow noopener noreferrer">​C++ 模板类的声明与实现分离问题(模板实例化)​​​​​​<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<h3 id="516-c类中引用成员和常量成员的初始化必须用初始化列表进行初始化">5.16 C++类中引用成员和常量成员的初始化(必须用初始化列表进行初始化)</h3>
<p>如果一个类是这样定义的：</p>
<div class="highlight" id="id-37"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">A</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">pram1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram2</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">      <span class="kt">int</span> <span class="n">a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="kt">int</span> <span class="o">&amp;</span><span class="n">b</span><span class="p">;</span> <span class="c1">// 引用成员
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">c</span><span class="p">;</span> <span class="c1">// 常量成员
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>假如在构造函数中对三个私有变量进行赋值则通常会这样写：</p>
<div class="highlight" id="id-38"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">A</span><span class="o">::</span><span class="n">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">pram1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram2</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">a</span><span class="o">=</span><span class="n">pram1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">b</span><span class="o">=</span><span class="n">pram2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">c</span><span class="o">=</span><span class="n">pram3</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>但是，这样是编译不过的。因为常量和引用初始化必须赋值。所以上面的构造函数的写法只是简单的赋值，并不是初始化。</p>
<p>正确写法应该是：</p>
<div class="highlight" id="id-39"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">  <span class="n">A</span><span class="o">::</span><span class="n">A</span><span class="p">(</span><span class="kt">int</span> <span class="n">pram1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram2</span><span class="p">,</span> <span class="kt">int</span> <span class="n">pram3</span><span class="p">)</span><span class="o">:</span><span class="n">b</span><span class="p">(</span><span class="n">pram2</span><span class="p">),</span><span class="n">c</span><span class="p">(</span><span class="n">pram3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="o">=</span><span class="n">pram1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>采用初始化列表实现了对常量和引用的初始化。采用括号赋值的方法，括号赋值只能用在变量的初始化而不能用在定义之后的赋值。
凡是有引用类型的成员变量或者常量类型的变量的类，<font color=red>不能有缺省构造函数</font>。默认构造函数没有对引用成员提供默认的初始化机制，也因此造成引用未初始化的编译错误。并且必须<strong>使用初始化列表进行初始化const对象、引用对象</strong>。</p>
</blockquote>
<h3 id="517-memset为int型数组初始化问题">5.17 memset为int型数组初始化问题</h3>
<p>头文件: <code>#include &lt;string.h&gt;</code>
<code>memset()</code> 函数用来将指定内存的前n个字节设置为特定的值，其原型为:</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>函数说明<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content"><div class="highlight" id="id-42"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="o">*</span> <span class="nf">memset</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span> <span class="n">ptr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">value</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">num</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><p>参数说明：</br></p>
<ul>
<li><code>ptr</code> 为要操作的内存的指针。</br></li>
<li><code>value</code> 为要设置的值。你既可以向 value 传递 int 类型的值，也可以传递 char 类型的值，int 和 char 可以根据 ASCII 码相互转换。</br></li>
<li><code>num</code> 为 ptr 的前 num 个字节，size_t 就是unsigned int。</br></li>
</ul>
<p>memset() 会将 ptr 所指的内存区域的前 num 个字节的值都设置为 value，然后返回指向 ptr 的指针。</p>
</div>
    </div>
  </div>
<p>无法下面这样初始化，这样的结果是a被赋值成168430090，168430090&hellip;..</p>
<div class="highlight" id="id-40"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span> <span class="c1">// array
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这是因为int由4个字节(说)表示，并且不能得到数组a中整数的期望值。</p>
<p>但我经常看到程序员使用memset将int数组元素设置为 0 或 -1。其他值不行！</p>
<div class="highlight" id="id-41"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">a</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">b</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="n">memset</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">b</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="c1">//假设a为int型数组：
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mh">0x7f</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="c1">//a数组每个空间将被初始化为0x7f7f7f7f,原因是C函数传参过程中的指针降级，导致sizeof(a)，返回的是一个 something*指针类型大小的的字节数，如果是32位，就是4字节。所以memset按字节赋值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">memset</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="mh">0xaf</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">a</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="c1">//a数组每个空间将被初始化为0xafafafaf
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="518-编译器对-inline-函数的处理步骤">5.18 编译器对 inline 函数的处理步骤</h3>
<p>编译器对 inline 函数的处理:</p>
<ul>
<li>将 inline 函数体复制到 inline 函数调用点处;</br></li>
<li>为所有 inline 函数中的局部变量分配内存空间;</br></li>
<li>将 inline 函数的输入参数和返回值映射到调用方法的局部变量空间中;</br></li>
<li>如果 inline 函数有多个返回点，将其转变为 inline 函数代码块末尾的分支(使用 GOTO);</br></li>
</ul>
<p>优点:</p>
<ul>
<li>内联函数同宏函数一样将在被调用处进行代码展开，省去了<u>参数压栈</u>、<u>栈帧开辟与回收</u>，<u>结果返回</u>等，从而提高程序运行速度。</br></li>
<li>内联函数相比宏函数来说，在代码展开时，会做安全检查或自动类型转换(同普通函数)，而宏定义则不会。</br></li>
<li>在类中声明同时定义的成员函数，自动转化为内联函数，因此内联函数可以访问类的成员变量，宏定义则不能。</br></li>
<li>内联函数在运行时可调试，而宏定义不可以。</br></li>
</ul>
<p>缺点:</p>
<ul>
<li>代码膨胀。内联是以代码膨胀(复制)为代价，消除函数调用带来的开销。如果执行函数体内代码的时间，相比于函数调用的开销较大，那么效率的收获会很少。另一方面，每一处内联函数的调用都要复制代码，将使程序的总代码量增大，消耗更多的内存空间。</br></li>
<li>inline 函数无法随着函数库升级而升级。inline函数的改变需要重新编译，不像 non-inline 可以直接链接。</br></li>
<li>是否内联，程序员不可控。内联函数只是对编译器的建议，是否对函数内联，决定权在于编译器。</br></li>
</ul>
<h3 id="519-虚函数virtual可以是内联函数inline吗">5.19 虚函数(virtual)可以是内联函数(inline)吗？</h3>
<ul>
<li>虚函数可以是内联函数，内联是可以修饰虚函数的，但是<strong>当虚函数表现多态性的时候不能内联</strong>。</br></li>
<li>内联是在<strong>编译期</strong>建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时(运行期)不可以内联。</br></li>
<li>inline virtual 唯一可以内联的时候是：编译器知道所调用的对象是哪个类(如 <code>Base::who()</code>)，这只有在编译期具有实际对象而不是对象的指针或引用时才会发生;</br></li>
</ul>
<h3 id="520静态库和动态库比较">5.20 静态库和动态库比较</h3>
<p>静态库 (.a、.lib):</p>
<ul>
<li>将静态库的内容添加到程序中，此时程序的空间，变成了源程序空间大小 + 静态库空间大小。</li>
</ul>
<p>动态库(共享库)(.so、.dll):</p>
<ul>
<li>常驻内存，当程序需要调用相关函数时，会从内存调用。</li>
</ul>
<p>区别:</p>
<ul>
<li>静态库：对空间要求较低，而时间要求较高的核心程序中。(.a、.lib) </br></li>
<li>动态库：对时间要求较低，对空间要求较高。(.so、.dll) </br></li>
</ul>
<p><a href="https://blog.csdn.net/m0_46245582/article/details/124027320"target="_blank" rel="external nofollow noopener noreferrer">hash<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="6-基础知识六">6 基础知识(六)</h2>
<h3 id="61-构造函数为什么不能定义为虚函数-析构函数般写成虚函数的原因-">6.1 构造函数为什么不能定义为虚函数？ ⽽析构函数⼀般写成虚函数的原因 ？</h3>
<p>构造函数不能声明为虚函数的原因是:</p>
<blockquote>
<ol>
<li>构造一个对象的时候，必须知道对象的实际类型，而虚函数行为是在运行期间确定实际类型的。而在构造一个对象时，由于对象还未构造成功。编译器无法知道对象的实际类型，是该类本身，还是该类的一个派生类，或是更深层次的派生类。无法确定。</br></li>
<li>虚函数的执行依赖于虚函数表。而<strong>虚函数表在构造函数中进行初始化工作</strong>，即初始化vptr，让他指向正确的虚函数表。而在构造对象期间，虚函数表还没有被初始化，将无法进行。</br></li>
</ol>
</blockquote>
<p>虚函数的意思就是开启<strong>动态绑定</strong>，程序会根据对象的动态类型来选择要调用的方法。然而在构造函数运行的时候，这个对象的动态类型还不完整，没有办法确定它到底是什么类型，故构造函数不能动态绑定。(动态绑定是根据对象的动态类型而不是函数名，在调用构造函数之前，这个对象根本就不存在，它怎么动态绑定？)
编译器在调用基类的构造函数的时候并不知道你要构造的是一个基类的对象还是一个派生类的对象。</p>
<blockquote>
<p>析构函数设为虚函数的作用:
解释：在类的继承中，如果有基类指针指向派生类，那么用基类指针delete时，如果不定义成虚函数，派生类中派生的那部分无法析构。(如果基类的析构函数不是虚函数，那么在delete 基类指针时，只调用基类的析构函数，不会调用派生类的析构函数，故派生类部分不会被析构。)</p>
</blockquote>
<p>ref:</br>
[1]. <a href="https://blog.csdn.net/Yangy_Jiaojiao/article/details/127588598"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Yangy_Jiaojiao/article/details/127588598<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2]. <a href="https://blog.csdn.net/Yangy_Jiaojiao/article/details/128145609"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/Yangy_Jiaojiao/article/details/128145609<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br></p>
<p>参考(待补充):</br>
[1]. <a href="https://zhuanlan.zhihu.com/p/401341063"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/401341063<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2]. <a href="https://zhuanlan.zhihu.com/p/602866792"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/602866792<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[3]. <a href="https://zhuanlan.zhihu.com/p/675399586"target="_blank" rel="external nofollow noopener noreferrer">万字长文超全 C++面经<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>ref: <a href="https://blog.csdn.net/m0_46245582/category_11569287.html"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/m0_46245582/category_11569287.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>强化学习 | 深度解读Soft Actor-Critic 算法</title><link>https://jianye0428.github.io/posts/sac/</link><pubDate>Sat, 04 May 2024 17:42:03 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/sac/</guid><description><![CDATA[<h1 id="深度解读soft-actor-critic-算法">深度解读Soft Actor-Critic 算法</h1>
<h2 id="1-前言">1 前言</h2>
<p>机器人学习Robot Learning正在快速的发展，其中深度强化学习deep reinforcement learning（DRL），特别是面向连续控制continous control的DRL算法起着重要的作用。在这一领域中，目前可以说有三类行之有效的model free DRL算法：</p>
<ul>
<li>TRPO,PPO</li>
<li>DDPG及其拓展（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.08617"target="_blank" rel="external nofollow noopener noreferrer">D4PG<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>,TD3等）</li>
<li>Soft Q-Learning, Soft Actor-Critic</li>
</ul>
<p><strong><font color=red>PPO</font></strong> 算法是目前最主流的DRL算法，同时面向离散控制和连续控制，在<a href="https://en.wikipedia.org/wiki/OpenAI_Five"target="_blank" rel="external nofollow noopener noreferrer">OpenAI Five<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>上取得了巨大成功。但是PPO是一种on-policy的算法，也就是PPO面临着严重的sample inefficiency，需要巨量的采样才能学习，这对于真实的机器人训练来说，是无法接受的。</p>
<p><strong><font color=red>DDPG</font></strong> 及其拓展则是DeepMind开发的面向连续控制的off policy算法，相对PPO 更sample efficient。<strong>DDPG训练的是一种确定性策略deterministic policy，即每一个state下都只考虑最优的一个动作</strong>。DDPG的拓展版D4PG从paper中的结果看取得了非常好的效果，但是并没有开源，目前github上也没有人能够完全复现Deepmind的效果。</p>
<p><strong><font color=red>Soft Actor-Critic (SAC)</font></strong> 是面向Maximum Entropy Reinforcement learning 开发的一种off policy算法，和DDPG相比，Soft Actor-Critic使用的是随机策略stochastic policy，相比确定性策略具有一定的优势（具体后面分析）。Soft Actor-Critic在公开的benchmark中取得了非常好的效果，并且能直接应用到真实机器人上。最关键的是，Soft Actor-Critic是完全开源的，因此，深入理解Soft Actor-Critic 算法具有非常重要的意义，也是本篇blog的目的。</p>
<p>Soft Actor-Critic算法相关链接：</p>
<p>Paper：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.01290"target="_blank" rel="external nofollow noopener noreferrer">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.05905"target="_blank" rel="external nofollow noopener noreferrer">Soft Actor-Critic Algorithms and Applications<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08165"target="_blank" rel="external nofollow noopener noreferrer">Reinforcement Learning with Deep Energy-Based Policies<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (Soft Q-Learning)</li>
</ul>
<p>Codes:</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/rail-berkeley/softlearning"target="_blank" rel="external nofollow noopener noreferrer">rail-berkeley/softlearning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (原作者实现）</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/vitchyr/rlkit"target="_blank" rel="external nofollow noopener noreferrer">vitchyr/rlkit<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/openai/spinningup"target="_blank" rel="external nofollow noopener noreferrer">openai/spinningup<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/hill-a/stable-baselines"target="_blank" rel="external nofollow noopener noreferrer">hill-a/stable-baselines<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p>下面我们来详细解读一下SAC的算法及其具体实现。本文的阅读需要有基本的DRL算法基础知识。</p>
<h2 id="2-为什么研究-maximum-entropy-reinforcement-learning">2 为什么研究 Maximum Entropy Reinforcement Learning？</h2>
<p>对于一般的DRL，学习目标很直接，就是学习一个policy使得累加的reward期望值最大：</p>
<p>$$\pi^*=\arg\max_\pi\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[\sum_tR(s_t,a_t)]\tag{1}$$</p>
<p>而最大熵RL，除了上面的基本目标，还要求policy的每一次输出的action 熵entropy最大：</p>
<p>$$\pi^*=\arg\max_\pi\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[\sum_t\underbrace{R(s_t,a_t)}_{reward}+\alpha\underbrace{H(\pi(\cdot|s_t))}_{entropy}]\tag{2}$$</p>
<p>这样做的基本目的是什么呢？让策略随机化，即输出的每一个action的概率尽可能分散，而不是集中在一个action上。不了解entropy的同学可以看这里：<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%86%B5_%28%E4%BF%A1%E6%81%AF%E8%AE%BA%29"target="_blank" rel="external nofollow noopener noreferrer">wiki-信息熵<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>我们知道DDPG训练得到的是一个deterministic policy确定性策略，也就是说这个策略对于一种状态state只考虑一个最优的动作。所以，stochastic policy相对deterministic policy有什么优势呢？</p>
<p>Stochastic policy随机策略在实际机器人控制上往往是更好的做法。比如我们让机器人抓取一个水杯，机器人是有无数条路径去实现这个过程的，而并不是只有唯一的一种做法。因此，我们就需要drl算法能够给出一个随机策略，在每一个state上都能输出每一种action的概率，比如有3个action都是最优的，概率一样都最大，那么我们就可以从这些action中随机选择一个做出action输出。<strong>最大熵maximum entropy的核心思想就是不遗落到任意一个有用的action，有用的trajectory</strong>。对比DDPG的deterministic policy的做法，看到一个好的就捡起来，差一点的就不要了，而最大熵是都要捡起来，都要考虑。</p>
<p><strong>基于最大熵的RL算法有什么优势？</strong></p>
<p>以前用deterministic policy的算法，我们找到了一条最优路径，学习过程也就结束了。现在，我们还要求熵最大，就意味着神经网络需要去explore探索所有可能的最优路径，这可以产生以下多种优势：</p>
<ul>
<li>
<p>1）学到policy可以作为更复杂具体任务的初始化。因为通过最大熵，policy不仅仅学到一种解决任务的方法，而是所有all。因此这样的policy就更有利于去学习新的任务。比如我们一开始是学走，然后之后要学朝某一个特定方向走。</p>
</li>
<li>
<p>2）更强的exploration能力，这是显而易见的，能够更容易的在多模态reward （multimodal reward）下找到更好的模式。比如既要求机器人走的好，又要求机器人节约能源</p>
</li>
<li>
<p>3）更robust鲁棒，更强的generalization。因为要从不同的方式来探索各种最优的可能性，也因此面对干扰的时候能够更容易做出调整。（干扰会是神经网络学习过程中看到的一种state，既然已经探索到了，学到了就可以更好的做出反应，继续获取高reward）</p>
</li>
</ul>
<p>既然最大熵RL算法这么好，我们当然应该研究它了。而实际上，在之前的DRL算法<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.01783"target="_blank" rel="external nofollow noopener noreferrer">A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们其实已经用了一下最大熵：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在训练policy的时候，A3C加了entropy项，作为一个regularizer，让policy更随机。不过A3C这么做主要是为了更好做exploration，整体的训练目标依然只考虑reward。这和Soft Actor-Critic的设定还是不一样的，Soft Actor-Critic是真正最大熵DRL算法。</p>
<h2 id="3-maximum-entropy-reinforcement-learning的bellman方程">3 Maximum Entropy Reinforcement Learning的Bellman方程</h2>
<p>我们先回顾一下dynamic programming中Bellman backup equation，参考<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"target="_blank" rel="external nofollow noopener noreferrer">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>$$q_\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})q_\pi(s^{\prime},a^{\prime})\tag{3}$$</p>
<p>那么对于最大熵（MaxEnt)的目标，其实可以把熵也作为reward的一部分，我们在计算q值时（记住q是累加reward的期望，传统rl的目标等价于让q最大），就需要计算每一个state的熵entropy (entropy的公式如下图所示）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>因此我们就可以得到Soft Bellman Backup equation (Entropy项)额外乘上 $\alpha$ 系数：</p>
<p>$$q_\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})(q_\pi(s^{\prime},a^{\prime})-\alpha\log(\pi(a^{\prime}|s^{\prime}))\quad(4)$$</p>
<p>Recall一下<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf"target="_blank" rel="external nofollow noopener noreferrer">Dynamic Programming Backup<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
对应Q值的公式是
<p>$$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q(s_{t+1},a_{t+1})]\tag{5}$$</p>
<p>根据公式（4），我们可以得到Soft Bellman Backup的 更新公式：</p>
<p>$$Q_{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]\tag{6}$$</p>
<p>上面公式（6）是直接使用dynamic programming，将entropy嵌入计算得到的结果。我们可以反过来先直接把entropy作为reward的一部分：</p>
<p>$$r_{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))\tag{7}$$</p>
<p>我们将（7）带入到公式（5）：</p>
<p>$$\begin{aligned}
{Q_{soft}(s_{t},a_{t})} &amp;=r(s_t,a_t)+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})]\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho,a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho,a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\gamma\mathbb{E}_{s_{t+1}\sim\rho}\mathbb{E}_{a_{t+1}\sim\pi}[-\alpha\log\pi(a_{t+1}|s_{t+1})]\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho}[\mathbb{E}_{a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]]\\&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]\end{aligned}$$</p>
<p>可以得到一样的结果。</p>
<p>与此同时，我们知道:</p>
<p>$$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho}[V(s_{t+1})]\tag{9}$$</p>
<p>因此，我们有：</p>
<p>$$V_{soft}(s_t)=\mathbb{E}_{a_t\sim\pi}[Q_{soft}(s_t,a_t)-\alpha\log\pi(a_t|s_t)]\tag{10}$$</p>
<p>至此我们理清楚了SAC paper原文中的公式(2)和(3)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>并且（7）的做法直接证明了Lemma 1 Soft Policy Evaluation (<strong>这个lemma为下一部分的soft policy iteration提供支撑</strong>）:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>但是，我们注意到上面的整个推导过程都是围绕maximum entropy，和soft 好像没有什么直接关系。所以，</p>
<p><strong>为什么称为soft？哪里soft了？以及为什么soft Q function能够实现maximum entropy？</strong></p>
<p>理解清楚这个问题是理解明白soft q-learning及sac的关键！</p>
<p>SAC这篇paper直接跳过了soft Q-function的定义问题，因此，要搞清楚上面的问题，我们从Soft Q-Learning的paper来寻找答案。</p>
<p>参考<a href="https://link.zhihu.com/?target=https%3A//bair.berkeley.edu/blog/2017/10/06/soft-q-learning/"target="_blank" rel="external nofollow noopener noreferrer">Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上面的曲线很明显的说明了stochastic policy的重要性，面对多模的（multimodal）的Q function，传统的RL只能收敛到一个选择（左图），而更优的办法是右图，让policy也直接符合Q的分布。这里，最直接的一种办法就是定义这样的energy-based policy：</p>
<p>\pi(a_t|s_t)\propto exp(-\mathcal{E}(s_t,a_t)) （11）</p>
<p>其中 \mathcal{E} 是能量函数，上面的形式就是Boltzmann Distribution <a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E5%88%86%E5%B8%83"target="_blank" rel="external nofollow noopener noreferrer">玻尔兹曼分布<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 。下图的 -f(x)=\mathcal{E}</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><a href="https://deepgenerativemodels.github.io/assets/slides/cs236_lecture13.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://deepgenerativemodels.github.io/assets/slides/cs236_lecture13.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>为了连接soft Q function，我们可以设定</p>
<p>$$\mathcal{E}(s_t,a_t)=-\frac{1}{\alpha}Q_{soft}(s_t,a_t)\tag{12}$$</p>
<p>因此，我们有</p>
<p>$$\pi(a_t|s_t)\propto exp(Q_{soft}(s_t,a_t))\tag{13}$$</p>
<p>这样的policy能够为每一个action赋值一个特定的概率符合Q值的分布，也就满足了stochastic policy的需求。</p>
<p>下面我们要<strong>发现(13)的形式正好就是最大熵RL的optimal policy最优策略的形式，而这实现了soft q function和maximum entropy的连接。</strong></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>实际上我们理解Soft Q-Learning及Soft Actor Critic，要清楚上图三者的关系。在Soft Q-Learning那篇paper中，他是从Soft Value Function的定义出发去连接Energy-Based Policy 和Maximum Entropy Objective的关系。而在本blog中，我们从Maximum Entropy Objective出发，来连接其他两部分。</p>
<p>前面我们已经推导得到了公式（10），那么根据公式（10），我们可以直接推导得到policy的形式：</p>
<p>$$\begin{aligned}\pi(s_{t},a_{t})&amp;=\exp(\frac1\alpha(Q_{soft}(s_t,a_t)-V_{soft}(s_t)))\\&amp;&amp;\text{(14)}\\&amp;=\frac{\exp(\frac1\alpha Q_{soft}(s_t,a_t))}{\exp(\frac1\alpha V_{soft}(s_t))}\end{aligned}$$</p>
<p>（14）符合了（13）， $\frac{1}{\alpha}V_{soft}(s_t)$ 可以看做是对应的log partition function. 由此，就连接了Maximum Entropy Objective和Energy Based Policy的关系。</p>
<p>下面我们要连接Soft Value Function。从（14）的 $\frac{1}{\alpha}V_{soft}(s_t)$ 已经很明显了：</p>
<p>$$\exp(\frac1\alpha V_{soft}(s_t))=\int\exp(\frac1\alpha Q_{soft}(s_t,a))d\mathbf{a} (15)$$</p>
<p>因此，我们可以定义 $V_{soft}(s_t)$ :</p>
<p>$$V_{soft}(s_t)\triangleq\alpha\log\int\exp(\frac1\alpha Q_{soft}(s_t,a))d\mathbf{a}\tag{16}$$</p>
<p>这和soft 有什么关系呢？(16）其实是LogSumExp的积分形式，就是smooth maximum/soft maximum (软的最大）。参考<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/LogSumExp"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/LogSumExp<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>所以就可以定义</p>
<p>$$\mathrm{soft}\max_af(a):=\log\int\exp f(a)da\tag{17}$$</p>
<p>因此我们也就可以根据公式（9）定义soft的Q-function：</p>
<p>$$Q_{soft}(s_t,a_t)=\mathbb{E}\left[r_t+\gamma\text{ soft}\max_aQ(s_{t+1},a)\right]\text{(18)}$$</p>
<p>所以，为什么称为soft是从这里来的。</p>
<p>这里有一个常见的疑问就是这里的soft max和我们常见的softmax好像不一样啊。是的，我们在神经网络中常用的activation function softmax 实际上是soft argmax，根据一堆logits找到对应的软的最大值对应的index。具体参看：<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Softmax_function"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/Softmax_function<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>上面的推导还只是面向policy的value和Q，我们下面要说明optimal policy也必然是energy-based policy的形式。</p>
<p>这一部分的证明依靠 Policy improvement theorem：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>具体证明过程见soft q-learning原文的A.1。</p>
<p>有了Theorem 4，</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>我们就可以看到optimal policy必然是energy based policy，也因此，我们有了soft q learning paper中最开始的定义：</p>
<p>$$\pi_{MaxEnt}^<em>(a_t|s_t)=\exp(\frac{1}{\alpha}(Q_{soft}^</em>(s_t,a_t)-V_{soft}^*(s_t)))\text{(19)}$$</p>
<h2 id="4-policy-iteration">4 Policy Iteration</h2>
<p>理清楚了上面的基本定义和联系，我们就可以研究怎么更新policy了，也就是policy iteration。</p>
<p>回顾一下一般的<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"target="_blank" rel="external nofollow noopener noreferrer">Policy Iteration<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在两步中进行循环迭代（我们直接使用Q值来说明）：</p>
<ol>
<li>Policy evaluation：固定policy，使用Bellman方程更新Q值直到收敛：</li>
</ol>
<p>$$Q_\pi(s,a)=r(s,a)+\lambda\mathbb{E}_{s^{\prime},a^{\prime}}Q_\pi(s^{\prime},a^{\prime})\tag{20}$$</p>
<ol start="2">
<li>Policy improvement: 更新policy：</li>
</ol>
<p>$$\pi^{\prime}(s)=\arg\max_aQ_\pi(s,a)\tag{21}$$</p>
<p>基于同样的方法，我们有Soft Policy Iteration：</p>
<ol>
<li>Soft policy evaluation:固定policy，使用Bellman方程更新Q值直到收敛:</li>
</ol>
<p>$$\begin{aligned}&amp;Q_{soft}^\pi(s_t,a_t)=r(s_t,a_t)+\lambda\mathbb{E}_{s_{t+1},a_{t+1}}\left[Q_{soft}^\pi(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))\right]\tag{22}\end{aligned}$$</p>
<ol start="2">
<li>Soft policy improvement: 更新policy：</li>
</ol>
<p>$$\pi^{\prime}=\arg\min_{\pi_k\in\Pi}D_{KL}(\pi_k(\cdot|s_t)||\frac{\exp(\frac{1}{\alpha}Q_{soft}^{\pi}(s_t,\cdot))}{Z_{soft}^{\pi}(s_t)}) \tag{23}$$</p>
<p>(22)基于上一部分说的Lemma 1 Soft Policy Evaluation, 可收敛。</p>
<p>(23)则基于上一部分的Theorem 4 Policy Improvement Theorem。只是这里的做法不是直接赋值，而是通过KL divergence来趋近 $\exp(Q^{\pi}_{soft}(s_t,\cdot))$ 。在SAC的paper原文中，我们可以看到这么做的原因是为了限制policy在一定范围的policies $\Pi$ 中从而tractable，policy的分布可以是高斯分布。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>同样的，作者也专门证明了采用KL divergence的方法一样能够保证policy improvement，也就是Lemma 2：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>最后，就是证明上面的Soft Policy Iteration过程能保证policy收敛到最优，即Theorem 1：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>由此，基本的理论建设也就结束了，下面进入Soft Actor-Critic的算法设计。</p>
<h2 id="5-soft-actor-critic">5 Soft Actor-Critic</h2>
<p>SAC算法的构建首先是神经网络化，我们用神经网络来表示Q和Policy： $Q_{\theta}(s_t,a_t)$ 和 $\pi_{\phi}(a_t|s_t)$ 。Q网络比较简单，几层的MLP最后输出一个单值表示Q就可以了，Policy网络需要输出一个分布，一般是输出一个Gaussian 包含mean和covariance。下面就是构建神经网络的更新公式。</p>
<p>对于Q网络的更新，我们根据（10）可以得到：</p>
<p>$$\begin{aligned}
J_{Q}(\theta)&amp; =\mathbb{E}_{(s_t,a_t,s_{t+1})\sim\mathcal{D}}[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma V_{\bar{\theta}}(s_{t+1})))^2] \\
&amp;=\mathbb{E}_{(s_t,a_t,s_{t+1})\sim\mathcal{D},a_{t+1}\sim\pi_\phi}[\frac12(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma(Q_{\bar{\theta}}(s_{t+1},a_{t+1})-\alpha\log(\pi_\phi(a_{t+1}|s_{t+1})))))^2] \tag{24}
\end{aligned}$$</p>
<p>这里和DDPG一样，构造了一个target soft Q 网络带参数 $\overline{\theta}$ ，这个参数通过exponentially moving average Q网络的参数 $\theta$ 得到。(ps:在第一个版本的SAC中，他们单独定义了V网络进行更新，说是更稳定，到新版的SAC中，由于自动更新temperature $\alpha$ 就直接使用Q网络更新）</p>
<p>对于Policy 网络参数的更新，就是最小化KL divergence：</p>
<p>$$\begin{aligned}
J_{\pi}(\phi)&amp; =D_{\mathrm{KL}}\left(\pi_\phi(.\left|s_t\right)|\exp(\frac1\alpha Q_\theta(s_t,.)-\log Z(s_t))\right) \\
&amp;=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}\Big[\log\big(\frac{\pi_\phi(a_t|s_t)}{\exp(\frac{1}{\alpha}Q_\theta(s_t,a_t)-\log Z(s_t))}\big)\Big] \\
&amp;=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}[\log\pi_\phi(a_t|s_t)-\frac1\alpha Q_\theta(s_t,a_t)+\log Z(s_t)] \tag{25}
\end{aligned}$$</p>
<p>这里的action我们采用reparameterization trick来得到，即</p>
<p>$$a_t=f_\phi(\varepsilon_t;s_t)=f_\phi^\mu(s_t)+\varepsilon_t\odot f_\phi^\sigma(s_t)\tag{26}$$</p>
<p>f函数输出平均值和方差，然后 $\varepsilon$ 是noise，从标准正态分布采样。使用这个trick，整个过程就是完全可微的(loss 乘以 $\alpha$ 并去掉不影响梯度的常量log partition function Z(s_t)) ：</p>
<p>$$J_\pi(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\varepsilon\sim\mathcal{N}}[\alpha\log\pi_\phi(f_\phi(\varepsilon_t;s_t)|s_t)-Q_\theta(s_t,f_\phi(\varepsilon_t;s_t))] \tag{27}$$</p>
<p>这样基本的Soft Actor-Critic的更新方法也就得到了。</p>
<h2 id="6-temperature-hyperparameter-auto-adjustment">6 Temperature Hyperparameter Auto-Adjustment</h2>
<p>前面的SAC中，我们只是人为给定一个固定的temperature $\alpha$ 作为entropy的权重，但实际上由于reward的不断变化，采用固定的temperature并不合理，会让整个训练不稳定，因此，有必要能够自动调节这个temperature。当policy探索到新的区域时，最优的action还不清楚，应该调整temperature $\alpha$ 去探索更多的空间。当某一个区域已经探索得差不多，最优的action基本确定了，那么这个temperature就可以减小。</p>
<p>这里，SAC的作者构造了一个带约束的优化问题，让平均的entropy权重是有限制的，但是在不同的state下entropy的权重是可变的，即</p>
<p>$$\max_{\pi_0,\ldots,\pi_T}\mathbb{E}\bigg[\sum_{t=0}^Tr(s_t,a_t)\bigg]\mathrm{s.t.~}\forall t, \mathcal{H}(\pi_t)\geq\mathcal{H}_0\tag{28}$$</p>
<p>对于这部分内容，<a href="https://link.zhihu.com/?target=https%3A//lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html%23sac"target="_blank" rel="external nofollow noopener noreferrer">Policy Gradient Algorithms<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 这个openai小姐姐的blog介绍得极其清楚，大家可以参考，最后得到temperature的loss：</p>
<p>$$J(\alpha)=\mathbb{E}_{a_t\sim\pi_t}[-\alpha\log\pi_t(a_t\mid\pi_t)-\alpha\mathcal{H}_0]\tag{29}$$</p>
<p>由此，我们可以得到完整的Soft Actor-Critic算法：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>为了更快速稳定的训练，作者引入了两个Q网络，然后每次选择Q值小的一个作为target Q值。更新Q，Policy及 \alpha 使用上文的（24）（27）（29）三个公式。</p>
<h2 id="7-神经网络结构">7 神经网络结构</h2>
<p>虽然上面把算法流程确定了，但是如何构造policy的神经网络还是比较复杂的。下图是带V网络的神经网络结构图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><a href="https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html"target="_blank" rel="external nofollow noopener noreferrer">https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>我们主要来探究一下Policy网络的设计。</p>
<p>见上图右上角的Policy网络，前面的input embedder和Middleware不用说，就是几层的MLP。然后，接下来神经网络分成两个分支，分别输出平均值mean $\mu$ 和log 标准差 log std 。然后使用exp得到std。</p>
<p>$$\pi_\phi(s_t) = \mu_t,\log \sigma_t \tag{30}$$</p>
<p>$$\sigma_t = \exp(\log \sigma_t)$$</p>
<p>正常输出这样的高斯分布作为action 的分布distribution是OK的，但是在实际中，这个action需要限定在一定范围内。因此，这里作者使用了squashing function tanh，将action限制在（-1,1）之间，即</p>
<p>$$\mathbf{u}_t =\mu_t + \varepsilon_t \odot \sigma_t $$</p>
<p>$$a_t = \tanh (\mathbf{u}) \tag{31}$$</p>
<p>这里和上文的公式（26）对应，多了一个tanh。</p>
<p>那么这会导致分布的变化，从而影响log likelihood的计算，而这是我们计算SAC的loss必须的。作者在paper中给出了计算方法如下：</p>
<p>$$\log \pi(a|s)=\log \mu(\mathbf{u}|s)-\sum_{i=1}^{D}{\log(1-\tanh^2(u_i))} \tag{32}$$</p>
<p>其中 u_i 是 $\mathbf{u}$ 的第i个元素。这里的 $\mu(\mathbf{u}|s)$ 是没有加限制时的likelihood function也就是高斯分布的likelihood function似然函数。高斯分布的log likelihood直接使用pytorch的<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/_modules/torch/distributions/normal.html"target="_blank" rel="external nofollow noopener noreferrer">Normal<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> class就可以获得。</p>
<h2 id="8-其他细节">8 其他细节</h2>
<p>1）SAC里的target entropy 设计为</p>
<p>$$\mathcal{H}_0 = -\dim (\mathcal{A}) \tag{33}$$</p>
<p>即-动作数量。</p>
<p>2）SAC paper里完全没有说明的训练时的episode设置。SAC设置为每一个episode采样1000次然后训练1000次。</p>
<p>3）在代码中SAC使用 log alpha作为更新的参数，而不是直接使用alpha如公式（25），这和输出log std是一样的，使用log有很大的正负范围，更方便网络输出。否则alpha或者std都是正值。</p>
<p>4）SAC有一个很大的问题，它的policy的目的是趋近于玻尔兹曼分布，但是实际实现的时候，为了能够tractable，选择了输出一个高斯，也就是让高斯趋近于玻尔兹曼分布。这意味着SAC本质上还是unimodal的算法，而不是soft q-learning的multi-modal。这使得SAC的创新性打了很大的折扣。但是算法效果确实还是不错的。</p>
<h2 id="9-小结">9 小结</h2>
<p>本文从理论到具体实现层面剖析了Soft Actor-Critic这一目前极强的DRL算法，基本上理解了本文的分析，对于代码的实现也就可以了然一胸了。</p>
<p>由于本人水平有限，前面的理论分析恐有错误，望批评指正！</p>
<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/70360272"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/70360272<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>一文详解 ChatGPT RLHF 背后的 PPO 强化学习训练</title><link>https://jianye0428.github.io/posts/chatgpt_rlhf/</link><pubDate>Sat, 04 May 2024 17:00:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/chatgpt_rlhf/</guid><description><![CDATA[<h2 id="0-引言">0. 引言</h2>
<p></p>
<p>最近火出圈的🚀 ChatGPT 中 RLHF 主要采用了就是 PPO 进行强化学习训练</p>
<blockquote>
<p>主要运用在微调阶段（微调整个 10B～100B+ 参数的成本其实也非常高 ）使用<strong>策略梯度</strong>强化学习 (Policy Gradient RL) 算法、近端策略优化 (PPO) 微调初始 LM 的部分或全部参数。</p>
</blockquote>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<blockquote>
<p>以下主要参考台大李宏毅的推导过程</p>
</blockquote>
<h2 id="01-vanilla-policy-gradient">01. Vanilla policy gradient</h2>
<ul>
<li>动作/环境/奖励之间的关系：</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>轨迹可表示为集合</p>
<p>$$\begin{aligned}p_{\theta}(\tau)&amp;=p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_1|s_1)p(s_3|s_2,a_2)\ldots\&amp;=p(s_1)\prod_{t=1}^Tp_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\end{aligned}$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>一个轨迹的奖励总和为：</p>
<p>$$R(\tau)=\sum_{t=1}^Tr_t$$</p>
<p>则奖励的期望为：</p>
<p>$$\bar{R}<em>\theta=\sum</em>\tau R(\tau)p_\theta(\tau)=E_{\tau\sim p_\theta(\tau)}[R(\tau)]$$</p>
<p>将 $R(\tau)$ 看成常量，对其求微分：</p>
<p>$$\begin{aligned}
\nabla\bar{R}<em>{\theta}&amp; =\sum</em>{\tau}R(\tau)\nabla p_{\theta}(\tau) \
&amp;=\sum_{\tau}R(\tau)p_{\theta}(\tau)\frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \
&amp;=\sum_{\tau}R(\tau)p_{\theta}(\tau)\nabla\log p_{\theta}(\tau)\quad\nabla f(x)=f(x)\nabla\log f(x) \
&amp;=E_{\tau\sim p_{\theta}(\tau)}[R(\tau)\nabla\log p_{\theta}(\tau)]&amp; \left(2\right) \
&amp;\approx\frac1N\sum_{n=1}^{N}R(\tau^{n})\nabla\log p_{\theta}(\tau^{n}) \
&amp;=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla\log p_\theta(a_t^n|s_t^n)
\end{aligned}$$</p>
<p>策略网络梯度更新：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>可以看成一个分类问题（游戏中通过键盘输入来互动，分类类别为所有可操作的键位）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li>理想情况下， 并不一直为正数，增加一个 baseline:</li>
</ul>
<p>$$\nabla\bar{R}<em>{\theta}=\frac{1}{N}\sum</em>{n=1}^{N}\sum_{t=1}^{{T_{n}}}(R(\tau^{n})-b)\nabla\log p_{\theta}(a_{t}^{n}|s_{t}^{n})b\approx E[R(\tau)]$$</p>
<blockquote>
<p>在电子游戏中，奖励值常常为正（通常为游戏分数）。这时需要增加一个偏置来保证同时有正样本和负样本</p>
</blockquote>
<ul>
<li>分配合适的学分</li>
</ul>
<p>一个高分的游戏轨迹中也可能存在错误的动作，同样的，一个低分的游戏轨迹也可能存在正确的动作，而上文中的计算将最后的奖励值（最后的游戏分数）都一视同仁视为该游戏轨迹每个动作的学分。</p>
<p>为了更准确地描述每个动作所得到的学分，将一个动作执行后对应的学分为后续的所有奖励值的总和</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>$$\begin{aligned}
\nabla\bar{R}<em>\theta&amp; =\frac1N\sum</em>{n=1}^N\sum_{t=1}^{T_n}(R(\tau^n)-b)\nabla\log p_\theta(a_t^n|s_t^n) \Downarrow\nabla\bar{R}<em>\theta \
&amp;= \frac1N\sum</em>{n=1}^N\sum_{t=1}^{T_n}(\sum_{t^{\prime}=t}^{T_n}r_{t^{\prime}}^n-b)\nabla\log p_\theta(a_t^n|s_t^n)
\end{aligned}$$</p>
<p>当某个动作执行以后，其对后续的奖励分数的影响在慢慢减少，再增加一个衰减因子：</p>
<p>$$\begin{aligned}
\nabla\bar{R}<em>\theta&amp; =\frac1N\sum</em>{n=1}^N\sum_{t=1}^{T_n}(\sum_{t^{\prime}=t}^{T_n}r_{t^{\prime}}^n)\nabla\log p_\theta(a_t^n|s_t^n)\Downarrow\nabla\bar{R}<em>\theta \
&amp; = \frac{1}{N}\sum</em>{n=1}^{N}\sum_{t=1}^{T_{n}}(\sum_{t^{\prime}=t}^{T_{n}}\gamma^{t^{\prime}-t}r_{t^{\prime}}^{n}-b)\nabla\log p_{\theta}(a_{t}^{n}|s_{t}^{n}),\gamma&lt;1
\end{aligned}$$</p>
<h2 id="02-从on-policy到off-policy">02. 从on-policy到off-policy</h2>
<p>两者区别:</p>
<ul>
<li>On-policy: 学习到的 agent 和与环境交互的 agent 是相同的，每一次梯度更新都需要重新采样</li>
<li>Off-policy: 学习到的 agent 和与环境交互的 agent 是不同的，每次梯度更新不需要重新采样</li>
</ul>
<p>重新看看 的表达式：
$$\nabla\bar{R}<em>\theta=E</em>{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)]$$</p>
<ul>
<li>使用策略网络 收集数据。当 更新后，则需要重新收集训练样本</li>
<li>目标：使用相同的样本（通过 采样）训练 。其中 为固定的，因此我们可以重复使用其样本数据</li>
</ul>
<h3 id="21-重要性采样importance-sampling">2.1 重要性采样（Importance Sampling）</h3>
<p>考虑一个场景，假如正在尝试计算函数 $f(x)$ 的期望值，其中 $x \sim f(x)$ 服从某种分布。则对 $E(f(x))$ 有以下估计：</p>
<p>$$E_{x\sim p}[f(x)]=\int f(x)p(x)dx\approx\frac{1}{n}\sum_{i}f(x_{i})$$</p>
<p>蒙特卡洛抽样方法是简单地从分布 $p(x)$ 中抽出 ，然后取所有样本的平均值来得到期望值的估计。那么问题来了，如果  $p(x)$  非常难取样怎么办？是否能够根据一些已知的、容易抽样的分布来估计期望值？</p>
<p>答案是肯定的。公式的一个简单转换就可以做到</p>
<p>$$E_{x\sim p}[f(x)]=\int f(x)p(x)dx=\int f(x)\frac{p(x)}{q(x)}q(x)dx=E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]$$</p>
<p>其中$x$从分布$q(x)$中采样，$q(x)$不应为 0。通过这种方式，估计期望能够从另一个分布$q(x)$中采样，$p(x)/q(x)$是称为采样率或采样权重，它作为校正权重以抵消来自不同分布的概率采样。</p>
<ul>
<li>重要性采样的缺陷</li>
</ul>
<p>虽然重要性采样保证了期望的一致，但是这里来计算一下方差是否一致</p>
<p>方差的计算：</p>
<p>$$Var[X]=E[X^2]-(E[X])^2$$</p>
<p>分别计算方差：</p>
<p>$$\begin{aligned}Var_{x\sim p}[f(x)]&amp;=E_{x\sim p}[f(x)^2]-(E_{x\sim p}[f(x)])^2\Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}]&amp;=E_{x\sim q}[(f(x)\frac{p(x)}{q(x)})^2]-(E_{x\sim q}[f(x)\frac{p(x)}{q(x)}])^2\&amp;=E_{x\sim p}[f(x)^2\frac{p(x)}{q(x)}]-(E_{x\sim p}[f(x)])^2\end{aligned}$$</p>
<p>可以发现两者虽然期望相等但方差并不一致</p>
<h3 id="22-从-on-policy-到-off-policy">2.2 从 on-policy 到 off-policy</h3>
<p>我们使用重要性采样将 on-policy 调整为 off-policy</p>
<p>$$\nabla\bar{R}<em>\theta=E</em>{\tau\sim p_{\theta^{\prime}}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta^{\prime}}(\tau)}R(\tau)\nabla\log p_\theta(\tau)]$$</p>
<ul>
<li>从 $\theta&rsquo;$ 采样得到数据集</li>
<li>使用该 数据集多次训练 $\theta$</li>
</ul>
<p>梯度更新过程：</p>
<p>$$\begin{aligned}
&amp;=E_{(s_t,a_t)\sim\pi_\theta}[A^\theta(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(s_t,a_t)}{p_{\theta^{\prime}}(s_t,a_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{\prime}}(a_t|s_t)}\frac{p_\theta(s_t)}{p_{\theta^{\prime}}(s_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)]&amp; \text{(4)} \
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{\prime}}(a_t|s_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)]
\end{aligned}$$</p>
<ul>
<li>其中 $A^\theta(s_t,a_t)$ 指的是 advantage 函数,其计算方式为加上衰减机制后的奖励值并减去基线。</li>
<li>由于 $\frac{p_\theta(s_t)}{p_{\theta&rsquo;}(s_t)}$ 的值难以计算，将其设置为 1，简化计算</li>
</ul>
<p>目标函数可以表示为：</p>
<p>由于 $\nabla f(x)=f(x)\nabla\log f(x)$ 再结合不定积分，目标函数可以表示为:</p>
<p>$$J^{\theta&rsquo;}(\theta)=E_{(s_t,a_t)\sim\pi_{\theta&rsquo;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&rsquo;}(a_t|s_t)}A^{\theta&rsquo;}(s_t,a_t)]$$</p>
<h2 id="03-ppotrpo">03. PPO/TRPO</h2>
<p>为了消除重要性采样的缺陷的影响，以下为两种方式</p>
<ul>
<li>PPO（Proximal Policy Optimization）
<ul>
<li>初始化策 略网络参数</li>
<li>在每次迭代过程中:</li>
<li>目标函数:</li>
<li>使用 与环境互动以收集 ，并计算出 advantage 值</li>
<li>更新 优化</li>
<li>算法:</li>
</ul>
</li>
</ul>
<p>$$\begin{aligned}
PPO algorithm: \
J_{PPO}^{\theta^k}(\theta) &amp; = J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)J^{\theta^k}(\theta) \
&amp; = E_{(s_{t},a_{t})\sim\pi_{\theta^{k}}}[\frac{p_{\theta}(a_{t}|s_{t})}{p_{\theta^{k}}(a_{t}|s_{t})}A^{\theta^{k}}(s_{t},a_{t})] \
&amp; \approx \sum_{(s_{t},a_{t})}\frac{p_{\theta}(a_{t}|s_{t})}{p_{\theta^{k}}(a_{t}|s_{t})}A^{\theta^{k}}(s_{t},a_{t})
\end{aligned}$$</p>
<p>自适应 KL 惩罚：如果 $KL(\theta,\theta^k)&gt;KL_{\max}$ ,增大 $\beta$; 如果 $KL(\theta,\theta^k) &lt;KL_{\min}$,减小 $\beta$。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li>TRPO（Trust Region Policy Optimizatio）</li>
</ul>
<p>$$J_{TRPO}^{\theta&rsquo;}(\theta)=E_{(s_t,a_t)\sim\pi_{\theta&rsquo;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&rsquo;}(a_t|s_t)}A^{\theta&rsquo;}(s_t,a_t)]KL(\theta,\theta&rsquo;)&lt;\delta $$</p>
<p>TRPO 和 PPO 在各个测试上性能差不多。但相比 PPO ，TRPO 计算要更复杂</p>
<p><strong>参考文献</strong>:</p>
<p>[1] <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html"target="_blank" rel="external nofollow noopener noreferrer">https://spinningup.openai.com/en/latest/algorithms/ppo.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[2] <a href="https://openai.com/research/openai-baselines-ppo"target="_blank" rel="external nofollow noopener noreferrer">https://openai.com/research/openai-baselines-ppo<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[3] <a href="https://huggingface.co/blog/deep-rl-ppo"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/blog/deep-rl-ppo<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[4] <a href="https://huggingface.co/blog/rlhf"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/blog/rlhf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[5] <a href="https://mp.weixin.qq.com/s/zhkNDNDEJV3BEdcgeuHkOA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/zhkNDNDEJV3BEdcgeuHkOA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>LLM预训练之RLHF（一）：RLHF及其变种</title><link>https://jianye0428.github.io/posts/pretrain_rlhf_one/</link><pubDate>Sat, 04 May 2024 14:23:08 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/pretrain_rlhf_one/</guid><description><![CDATA[<h2 id="0-引言">0. 引言</h2>
<p>在ChatGPT引领的大型语言模型时代，国内外的大模型呈现爆发式发展，尤其是以年初的LLaMA模型为首的开源大模型和最近百川智能的baichuan模型，但无一例外，都使用了「基于人类反馈的强化学习」（RLHF）来提升语言模型的性能，并在模型重注入了人类的偏好，以提高模型的有用性和安全性。不过RLHF也早已更新换代，我们以如下目录进行详细讲述RLHF及其变种：</p>
<ul>
<li>LLM的经典预训练Pipeline</li>
<li>Llama 2中的RLHF</li>
<li>RLHF替代方案</li>
</ul>
<h2 id="一llm的经典预训练pipeline">一、LLM的经典预训练Pipeline</h2>
<p>​  目前基于Transformer decoder的LLM，比如ChatGPT、LLaMA、baichuan等，通常都会有基于预训练的base模型和在base模型至少使用RLHF微调的Chat模型，Chat模型的训练一般都包括如下三个步骤：预训练，有监督微调和对齐。</p>
<p>​  在<strong>预训练</strong>阶段，模型会从大量无标注文本数据集中学习通用知识，然后使用「<strong>有监督微调」（SFT）<strong>优化模型以更好地遵守特定指令，最后使用</strong>对齐</strong>技术使LLM可以更有用且更安全地响应用户提示。</p>
<h3 id="11-预训练pre-training">1.1 预训练（Pre-training）</h3>
<p>预训练阶段通常需要包含数十亿到数万亿个token的庞大文本语料库，但训练目标是<strong>模型需要根据提供的文本来预测「下一个单词」</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>1.2 有监督微调（Supervised Finetuning）</strong></p>
<p>​SFT的训练过程类似Pre-training阶段，也是预测「下一个单词」，但是<strong>需要人工标注的指令数据集</strong>，其中模型的输入是一个指令（根据任务的不同，也可能包含一段输入文本），输出为模型的预期回复内容。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>数据形式类似于：</p>
<blockquote>
<p>Instruction: &ldquo;Write a limerick about a pelican.&rdquo;</p>
<p>指令：“写一首关于鹈鹕的打油诗。“</p>
<p>Output: &ldquo;There once was a pelican so fine&hellip;&rdquo;</p>
<p>输出：“从前有一只鹈鹕很好&hellip;“</p>
</blockquote>
<p>模型会把“Write a limerick about a pelican”作为输入，逐个token进行预测，输出“There once was a pelican so fine&hellip;”</p>
<p>虽然两个阶段都采用类似的训练目标，但有监督微调数据集通常比预训练数据小得多，指令数据集需要人类（或其他高质量的LLM）提供标注结果，所以无法大规模应用。</p>
<p><strong>1.3 对齐（Alignment）</strong></p>
<p>第三阶段依然是微调，不过其主要目标在于将语言模型与人类的偏好、价值观进行对齐，这也是RLHF机制发挥的地方。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="二reinforcement-learning-with-human-feedback-rlhf">二、Reinforcement Learning with Human Feedback (RLHF)</h2>
<p>上节，我们讨论了现代LLM的三个训练过程；本小节，我们重点讨论「上述两个微调阶段」（Supervised Tinetuning和Alignment）中使用的RLHF技术。</p>
<p>RLHF主要包括三步：</p>
<ol>
<li>在预训练好的模型上进行「有监督微调」（SFT）；</li>
<li>在有监督微调模型基础上创建一个reward model（RM）模型；</li>
<li>基于RM模型使用PPO算法微调SFT模型；</li>
</ol>
<h3 id="21-在预训练好的模型上进行有监督微调">2.1 在预训练好的模型上进行有监督微调**</h3>
<p>先收集一个Prompts集合，并要求标注人员写出高质量的回复，然后使用该数据集以监督的方式微调预训练的基础模型。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>​该步骤与上小节的Supervised Finetuning类似，但这是RLHF不可或缺的一个步骤。</p>
<h3 id="22-在有监督微调模型基础上创建一个rm模型">2.2 在有监督微调模型基础上创建一个RM模型</h3>
<p>对于每个Prompt，要求有监督微调后的LLM生成四到九个回复，再由标注人员根据个人偏好对所有回复进行排序。虽然排序过程很耗时，但工作量还是比第一步的有监督数据集构建要少一些。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在处理排序数据时，使用了一个奖励模型RM，RM来自RLHF第一步的「有监督微调语言模型」（SFT），SFT的输出通过一个回归层（单个输出节点）转换为奖励分数，即可称为<strong>RM模型</strong>。</p>
<h3 id="23-基于rm模型使用ppo算法微调sft模型">2.3 基于RM模型使用PPO算法微调SFT模型</h3>
<p>基于RM模型使用proximal policy optimization (PPO)算法微调SFT模型</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>PPO的具体技术细节可以参考InstructGPT或下面的论文列表。</p>
<ol>
<li>Asynchronous Methods for Deep Reinforcement Learning (2016) ，https://arxiv.org/abs/1602.01783</li>
<li>Proximal Policy Optimization Algorithms (2017)，https://arxiv.org/abs/1707.06347</li>
<li>Fine-Tuning Language Models from Human Preferences (2020)，https://arxiv.org/abs/1909.08593</li>
<li>Learning to Summarize from Human Feedback (2022) ，https://arxiv.org/abs/2009.01325</li>
</ol>
<h2 id="三llama-2的rlhf">三、LLaMA 2的RLHF**</h2>
<p>Meta AI在创建Llama-2-chat模型时也使用了RLHF技术，不过与ChatGPT相比还是有些细微区别。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>简单来说，Llama-2-chat在第一步RLHF微调上使用相同的指令数据，但在第二步使用了两个奖励模型；通过多个阶段的不断进化，奖励模型也会根据Llama-2-chat模型出现的错误进行更新；并且增加了拒绝采样（rejection sampling）步骤。</p>
<h3 id="31-margin-loss">3.1 Margin Loss</h3>
<p>​在标准InstructGPT中使用的RLHF PPO方法，研究人员需要收集同一个提示下的4-9个模型输出并进行排序，比如四个回复的排序结果为A&lt;C&lt; D&lt;B，那么就可以得到六个对比结果：A &lt; C，A &lt; D ，A &lt; B，C &lt; D，C &lt; B，D &lt; B。</p>
<p>​Llama 2的数据集也采用类似的方式，不过标注人员每次只能看到两个（而非4-9个）回复并进行对比，但新增了一个边际（margin）标签，对比结果可以为「显著更好」（significantly better）和「好的不明显」（negligibly better）。</p>
<p>在排序训练时中，Llama 2相比InstructGPT增加了边际损失：</p>
<p>$$\mathcal{L}<em>{\mathrm{ranking}}=-\log\left(\sigma\left(r</em>\theta\left(x,y_c\right)-r_\theta\left(x,y_r\right)-m(r)\right)\right)$$</p>
<p>其中，$r_θ(x，y)$是提示x和生成的回复y的标量分数输出; θ为模型权重; σ是将层输出转换为范围从0到1的分数的逻辑S形函数; $y_c$是由标注人员选择的更优回复; $y_r$是较差的回复。$m(r)$可以调节两个回复之间的差值，如果对比结果为「显著更好」，则会增加梯度值，加快更新速度。</p>
<h3 id="32-两个rm模型">3.2 两个RM模型</h3>
<p>​Llama 2中的两个奖励模型分别侧重「有用性」（helpfulness）和「安全性」（safety），用于模型优化的最终奖励函数会将两个分数进行线性组合。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="33-拒绝采样">3.3 拒绝采样</h3>
<p>​Llama 2的作者使用了一个训练流水线，<strong>同时使用PPO和拒绝采样算法</strong>，迭代地产生多个RLHF模型（从RLHF-V1到RLHF-V5），模型在拒绝采样时会得到K个输出，并使用最高奖励的输出更新梯度，而PPO每次只基于单样本进行更新。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在监督微调的初始阶段之后，模型只使用拒绝采样进行训练，然后再结合拒绝采样和PPO。</p>
<p>从实验结果来看，RLHF微调模型在无害性和有用性上都得到了改善，并且在最后阶段RLHF-v5使用PPO算法的性能最好。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="四rlhf的替代方案">四、RLHF的替代方案</h2>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>RLHF在InstructGPT和Llama 2论文中被证明是有效的，但是RLHF的过程是比较复杂的，下面将介绍一下最近RLHF的替代方案：</p>
<h3 id="41-constitutional-ai-harmlessness-from-ai-feedback-dec-2022-httpsarxivorgabs221208073">4.1 Constitutional AI: Harmlessness from AI Feedback (Dec 2022, <a href="https://arxiv.org/abs/2212.08073"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2212.08073<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>研究人员提出了一种 <strong><font color=red>基于人类提供的规则列表的自我训练机制</font></strong>。与前面提到的InstructGPT论文类似，也使用了强化学习方法。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上图中的「红队」（Red Team）指的是测试目标系统的防御能力，即外部或内部专家模拟潜在对手的过程，通过模仿现实世界攻击者的战术、技术和程序来挑战、测试并最终改进系统。</p>
<h3 id="42-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-feb-2023-httpsarxivorgabs230205206">4.2 The Wisdom of Hindsight Makes Language Models Better Instruction Followers (Feb 2023, <a href="https://arxiv.org/abs/2302.05206"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2302.05206<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>研究人员提出了一种**<font color=red>基于重新标记的监督微调方法HIR</font>**，该方法在12个BigBench任务上优于RLHF。</p>
<p>​HIR是如何工作的？简而言之，HIR方法包括两个步骤，即<strong>采样</strong>和<strong>训练</strong>。在采样步骤中，Prompt和指令输入给LLM来获取答案，根据对齐得分，在训练阶段适当的地方重新标注指令；然后，重新标记的指令和原始的Prompt用于微调LLM。使用这种重新标记的方法，研究人员有效地将失败案例（LLM创建的输出与原始指令不匹配的案例）转化为有用的训练数据，用于监督学习。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="43-direct-preference-optimization-your-language-model-is-secretly-a-reward-model-httpsarxivorgabs230518290-may-2023">4.3 Direct Preference Optimization: Your Language Model is Secretly a Reward Model (<a href="https://arxiv.org/abs/2305.18290"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2305.18290<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, May 2023)</h3>
<p><strong><font color=red>直接偏好优化（DPO）是具有PPO的RLHF的替代方案</font></strong>，其中研究人员表明，在RLHF中拟合奖励模型的交叉熵损失可以直接用于微调LLM。根据他们的基准，使用DPO更有效，而且在响应质量方面通常也优于RLHF/PPO。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="44-reinforced-self-training-rest-for-language-modeling-aug-2023-httpsarxivorgabs230808998">4.4 Reinforced Self-Training (ReST) for Language Modeling (Aug 2023, <a href="https://arxiv.org/abs/2308.08998"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2308.08998<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>ReST是人类反馈强化学习（RLHF）的一种替代方案，它<strong>使LLM与人类偏好保持一致</strong>。 <strong><font color=red>ReST使用采样方法创建改进的数据集</font></strong>，在质量越来越高的子集上迭代训练，以完善其奖励函数。根据作者的说法，与标准的在线RLHF方法（如具有近端策略优化的RLHF，PPO）相比，ReST通过离线生成训练数据集实现了更高的效率，但缺少与InstructGPT或Llama 2中使用的标准RLHF PPO方法的全面比较。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="45-rlaif-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback-sep-2023-httpsarxivorgabs230900267">4.5 RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (Sep 2023, <a href="https://arxiv.org/abs/2309.00267"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2309.00267<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>最近的人工智能反馈强化学习（RLAIF）研究表明，RLHF中奖励模型训练的评级不一定必须由人类提供，而是可以由LLM生成（此处：PaLM 2）。标注人员在一半的案例中更喜欢RLAIF模型，也就意味着两个模型的差距并不大，RLHF和RLAIF都大大优于纯通过监督指令微调训练的模型。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这项研究的结果非常有用和有趣，因为它基本上意味着我们可能能够使基于RLHF的训练更加高效和容易。然而，这些RLAIF模型在专注于信息内容的安全性和真实性的定性研究中的表现还有待观察，而人类偏好研究仅部分捕捉到了这一点。</p>
<p><strong>参考文献：</strong></p>
<p>[1] <a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives"target="_blank" rel="external nofollow noopener noreferrer">https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><br>
[2] <a href="https://mp.weixin.qq.com/s/3Ff6C5zT7fXggQ1FwxvWAQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/3Ff6C5zT7fXggQ1FwxvWAQ<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>大模型学习笔记 | GPT 系列</title><link>https://jianye0428.github.io/posts/survey/</link><pubDate>Fri, 03 May 2024 16:33:37 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/survey/</guid><description><![CDATA[<h1 id="万字长文cver-转-llm-学习笔记之大模型gpt-系列">万字长文，CVer 转 LLM 学习笔记之大模型GPT 系列</h1>
<h2 id="导读">导读</h2>
<p>本文是作者对 GPT 系列文章的学习笔记，从个人角度梳理了 GPT 系列的迭代逻辑，从技术的逻辑连续性和关联性都有很好的讲解，篇幅较长，建议大家点赞收藏。</p>
<p>这个系列的笔记主要面向像我一样已经具备一定的深度学习基础，但是新接触 NLP 和大模型领域的读者，目的是能提纲挈领地快速把握这个领域的一系列关键工作节点。</p>
<p>这篇笔记涵盖的内容有：</p>
<ul>
<li>GPT-1 论文</li>
<li>GPT-2 论文</li>
<li>GPT-3 论文</li>
<li>InstructGPT 论文（GPT-3.5 背后的技术）</li>
<li>GPT-4 技术报告</li>
<li>GPT-4 微软评测报告</li>
<li>GPT-4V 微软评测报告</li>
</ul>
<p>作为一个从 CV 转到 LLM 的新人，难免犯一些常见或低级的错误，欢迎任何读者及时指出和斧正，也欢迎任何留言讨论。</p>
<h2 id="要点-tldl">要点 (TL;DL)</h2>
<ul>
<li><strong>注重能力，而非过程</strong>：预训练任务其实形式不重要，可以是 classification，可以是预测 next token，真正重要的是 model 和 data 的 scaling up，以此快速高效地得到一个有优异泛化能力的特征提取器:
<ul>
<li>data: 找到/设计一个能把海量的数据用起来的任务，能用的数据越多越好，训练越快越好</li>
<li>model: <strong>模型性能</strong>可以随着参数量巨量地提升而不会快速饱和，一个优秀的模型架构是 scaling up 的基础保障</li>
</ul>
</li>
<li><strong>三种范式</strong>：
<ul>
<li>我们可以将专属任务上微调得到的模型，看成一种用户输入 0 个特殊 token，解决 1 种任务的范式，所以<strong>第一范式</strong>下的每个模型只能用于解决一个专属任务。</li>
<li><strong>第二范式</strong>的模型是为每一种任务准备 1 个特殊 token，因此通过改变输入 token，就能用一个模型解决不同的任务。</li>
<li><strong>第三范式</strong>的模型把特殊 token 替换成了特殊 token 序列，而自然语言正好就是一种最符合人类习惯和直觉的特殊 token 序列。</li>
</ul>
</li>
<li><strong>RLHF</strong>：
<ul>
<li>SFT 模型(16 epoch)</li>
<li>RM</li>
<li>PPO 继续训练出来的最终模型</li>
<li>SFT 数据(13k 条): 人工设计的问题，人工标注答案</li>
<li>Feedback 数据(33k 条): 针对上面人工设计的问题，模型输出的几份答案的排序（打分）</li>
<li>PPO 使用的数据(31k 条): 人工设计的问题(上面的模型没见过的新问题)，用 RM 的评分来继续训练，这份数据不需要人工标注答案</li>
</ul>
</li>
<li><strong>对 GPT-4 的全面探索</strong>：
<ul>
<li><strong>心理学角度</strong>：人类思维是快思考与慢思考两个系统的混合体，而 GPT-4 目前更类似于单纯的快思考</li>
<li>GPT-4 还有哪些<strong>局限性</strong>，以及哪些可以<strong>改进的地方</strong>（见 GPT-4 微软报告）</li>
</ul>
</li>
<li><strong>GPT-4V 的全面探索</strong>：
<ul>
<li>支持哪些输入和工作模式</li>
<li>在不同领域和任务上的能力质量和通用性如何</li>
<li>有效使用和提示方法</li>
<li>未来方向</li>
</ul>
</li>
</ul>
<h2 id="1-improving-language-understanding-by-generative-pre-training-201806">1. Improving Language Understanding by Generative Pre-Training (2018.06)</h2>
<p>GPT 系列的第一篇论文，定下了纯 <strong><font color=red>Transformer-Decoder</font></strong> 路线。</p>
<p>深度学习的早期突破很多来自于 CV 领域，其中很重要的一个原因是 CV 有 ImageNet 这个百万量级的有标注数据集，在 ImageNet 上训练分类任务得到的模型 Backbone 天然就是一个优秀的图片特征提取器，基于这个特征提取器在任意的子任务上做 fine-tuning 效果都不会太差（至少能展现出一定的泛化能力）。而 NLP 领域缺少这样大的数据集，因此，一直以来 NLP 模型发力卡在了特征提取器的构筑上，<font color=red>GPT 提出用训练语言模型的方式来得到这个特征提取器，然后用它来做子任务上的微调。</font></p>
<p>语言模型由于做的是“预测下一个词”这样的一个任务，因此不依赖于人工标注，可以实现海量数据的预训练和泛化。</p>
<p>GPT 工作是在 BERT 之前的，很多的 setting 都被 BERT 直接沿用了，比如 12 层 Transformer，768 的维度，800M 的 BookCorpus 数据集 等。</p>
<p>文章剩余部分介绍了如何在 NLP 四大主流任务类型上运用 GPT，即如何把不同形式的任务都表示成一个序列+对应的标签的形式。</p>
<p>对笔者的启示:</p>
<ul>
<li><strong>注重能力，而非过程</strong>：预训练任务其实形式不重要，可以是 classification，可以是预测 next token，真正重要的是 model 和 data 的 scaling up，以此快速高效地得到一个有优异泛化能力的特征提取器:
<ul>
<li>data: 找到/设计一个能把海量的数据用起来的任务，能用的数据越多越好，训练越快越好</li>
<li>model: 模型性能可以随着参数量巨量地提升而不会快速饱和，一个优秀的模型架构是 scaling up 的基础保障</li>
</ul>
</li>
</ul>
<h2 id="2-language-models-are-unsupervised-multitask-learners-201902">2. Language Models are Unsupervised Multitask Learners (2019.02)</h2>
<p>GPT 系列的第二篇工作。</p>
<p>参数量提升到了 1.5B，也用了更大量的数据。GPT-2 最大的贡献是把研究的重点从单个任务上的针对性调参刷榜，转向了 <strong>zero-shot</strong>，即， <font color=red>训练好的模型不再需要微调就能去做不同任务了，尽管性能上距离每个任务的 SOTA 都还有距离，但方案的可行性已经验证了</font>。要实现这一点，原来为特殊任务准备特殊 token 的做法就不合适了，因为预训练阶段模型是没见过这些 token 的，毕竟语言模型预训练阶段只见过自然语言，所以非常自然地就引出了 prompt 的概念，用自然语言来替代原本的任务 token，实现不同任务的 zero-shot。</p>
<p>可以看到，在这个时候 GPT-2 就已经初具 ChatGPT 的雏形了，只不过用户的输入还不完全是任意自然语言，而是类似于这样的模板输入。：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">翻译任务：
</span></span><span class="line"><span class="cl">(translate to
</span></span><span class="line"><span class="cl">french, english text, french text)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">QA 任务：
</span></span><span class="line"><span class="cl">(answer the question, document,
</span></span><span class="line"><span class="cl">question, answer)</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-language-models-are-few-shot-learners-200514165">3. Language Models are Few-Shot Learners (2005.14165)</h2>
<p>GPT 系列的第三篇工作。</p>
<p>GPT-3 的参数量来到了 175B，训练数据从一开始 GPT-1 的几千本书的数据集，开始进入到了网站爬虫数据和清洗的模式。</p>
<p>其实在 GPT-2 中就已经提到了一个叫 Common Crawl 的公开网络数据，但是因为他们觉得这份数据实在太脏了所以放弃了，而现在为了训练更大的模型也不得不用起来，因此清洗数据是免不了的。</p>
<p>数据清洗经历了两个过程：</p>
<ol>
<li><strong>过滤</strong>：他们将原来 GPT-2 训练用的数据作为正样本，Common Crawl 作为负样本训练了一个二分类器，然后用这个分类器来做数据筛选，过滤掉一些特别显著的脏数据。</li>
<li><strong>去重</strong>：用经典的 LSH 算法进行去重</li>
</ol>
<p>另一方面，GPT-3 也正式提出了“in-context learning”的概念，在模型参数不进行更新的情况下，通过输入的上下文来帮助模型提升表现。</p>
<p>对于笔者而言，这张图相当形象:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这揭示了模型学习的另一个维度，提升模型表现并不只有 SGD 梯度更新这一个优化方向。结合 GPT-2 中 prompt 的由来，prompt 的前身是语言模型做多任务 zero-shot 时，针对不同任务给的特殊 token，因此一个更加富有信息量的“特殊 token 序列”能提升模型表现似乎是一件非常符合直觉的事情。</p>
<p>相比于过去使用一个特殊 token 来代表某一种特定的任务，GPT-3 的 few-shot prompt，或者说 in-context learning 形式，在笔者看来是一种推广，用户输入的自然语言和 few-shot 样例可以看成是一组特殊 token 的序列，因为自然语言的 token 具有语义和逻辑关联性，一个强大的预训练模型做到了“特殊 token”之间的泛化。</p>
<p>通过实验我们也可以观察到，随着给出的示例样本数变多，模型的表现也在提升：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>从笔者个人的角度来总结：</p>
<ul>
<li>我们可以将专属任务上微调得到的模型，看成一种用户输入 0 个特殊 token，解决 1 种任务的范式，所以<strong>第一范式</strong>下的每个模型只能用于解决一个专属任务。</li>
<li><strong>第二范式</strong>的模型是为每一种任务准备 1 个特殊 token，因此通过改变输入 token，就能用一个模型解决不同的任务。</li>
<li><strong>第三范式</strong>的模型把特殊 token 替换成了特殊 token 序列，而自然语言正好就是一种最符合人类习惯和直觉的特殊 token 序列。</li>
</ul>
<h2 id="4-training-language-models-to-follow-instructions-with-human-feedback-220302155">4. Training language models to follow instructions with human feedback (2203.02155)</h2>
<p>InstructGPT 被认为是 ChatGPT（GPT3.5） 背后的技术，核心点是把 RLHF，即<strong>基于人类反馈的强化学习</strong>，用到了语言模型上来<strong>进行人类喜好对齐</strong>。经过 RLHF 的 1.3B GPT 模型能在人类主观评分上超过 175B 的 GPT-3.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这篇工作里将模型输出与人类意愿不一致的这个现象称为“misaligned”，并分析原因在于语言模型的训练目标只是预测下一个 token，这跟我们希望模型“follow the user&rsquo;s instructions helpfully and safely”的目标之间显然是存在差距的。</p>
<p>用 Anthropic 的工作里的话来说，大模型应该遵循 3H 原则，即：</p>
<ul>
<li>helpful：帮助用户解决问题</li>
<li>honest：不能伪造信息或误导用户</li>
<li>harmless：不能对人或环境造成身体、心理或社会伤害</li>
</ul>
<p>语言模型之所以存在这个问题，原因也很简单，因为使用的是无监督学习，本身的学习目标里就没有人为控制，所以很直观地可以想到用**监督微调（SFT）**的方式来把缺失的人类监督信号加进来。</p>
<p>但是前面 GPT 三篇工作好不容易才把模型做到 175B 这么大，现在又重新开始标数据做监督学习显然是有点不聪明的，而且模型大了以后也更容易过拟合，对于人类偏好这一类的问题标注起来难度又很大，简单地全靠 SFT 肯定是行不通的。所以很自然地，OpenAI 想到了用他们家的拿手好戏强化学习，要知道 OpenAI 本身就是做强化学习起家的，本文使用的强化学习方法 PPO 也全是之前他们已经提出的算法，没有任何新的算法被提出，甚至论文里都没有对已有的算法进行太多的解释和铺垫，需要你感兴趣自己去翻他们的论文。</p>
<br>
<center>
  
  <br>
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>他们的方法整体可以概括如下：</p>
<ol>
<li>人工标一批 SFT 数据（包含问题和回答），对 GPT-3 模型（在强化学习里对应 Policy）进行微调</li>
<li>用 SFT 得到的模型，针对每个问题生成几份回答，然后人工给这些回答质量打分（排序）</li>
<li>用这份打分数据训练一个奖励模型（Reward Model, RM），让奖励模型代替人工打分</li>
<li>采用 PPO 算法，基于奖励模型的评分来继续训练 GPT-3 模型</li>
</ol>
<p>换言之，他们一共<strong>造了三份数据</strong>：</p>
<ul>
<li>SFT 数据（13k 条）：人工设计的问题，人工标注答案</li>
<li>Feedback 数据（33k 条）：针对上面人工设计的问题，模型输出的几份答案的排序（打分）</li>
<li>PPO 使用的数据（31k 条）：人工设计的问题（上面的模型没见过的新问题），用 RM 的评分来继续训练，这份数据不需要人工标注答案</li>
</ul>
<p><strong>训练三个模型</strong>：</p>
<ul>
<li>SFT 模型（16 epoch）</li>
<li>RM (Reward Model 奖励模型)</li>
<li>PPO 继续训练出来的最终模型</li>
</ul>
<p>他们甚至对问题类型进行了一些分类：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>模型训练部分，个人觉得值得注意的点有：</p>
<ol>
<li>SFT 阶段，在训了 1 epoch 后模型就已经过拟合了，但他们发现继续训练过拟合的模型依然可以提升 RM 性能，所以他们训练了 16 epoch</li>
<li>RM 的权重是直接用 SFT 模型初始化的，因为评分模型也需要语言能力，直接拷贝一份权重是比较省事的。RM 的输入是问题+几份答案，输出是排序。</li>
<li>RM 模型只有 6B，因为 175B 的 RM 很难训</li>
<li>强化学习阶段加了一个逐 token 的 KL 散度，用来让最终模型跟第一版 SFT 模型的要输出分布尽量保持一致，因为 RM 是在训练 SFT 模型的数据上训的，如果分布差异太大，RM 的评分就不准了</li>
</ol>
<p>强化学习的目标函数如下：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>简单翻译一下：</p>
<p>$$损失 = RM 评分 + 新旧模型输出token分布的KL散度 + 旧问题上SFT的损失$$</p>
<p>其中：</p>
<ul>
<li>RM 评分是 RM 对当前正在训练的模型在新问题（第三份数据）上的输出的评分</li>
<li>KL 散度是当前模型跟旧的 SFT 模型输出之间计算的</li>
<li>旧问题 SFT 损失是用第一份数据集继续按 SFT 方法训当前模型得到的</li>
</ul>
<p>另外，关于训练数据和评测方面的取舍也有所不同，训练中他们更看重 helpful，而评测阶段则更看重 honest 和 harmless。</p>
<h2 id="5-gpt-4-technical-report-230308774">5. GPT-4 Technical Report (2303.08774)</h2>
<p><strong>多模</strong></p>
<p>基于上面笔者三种模型范式的思路，多模态的模型可以看成是让特殊 token 的类型从文本 token 拓宽到了视觉 token，将模型解决的任务从 NLP 任务拓宽到了 CV 任务，而两种模态 token 对齐的技术也早被 OpenAI 研究过了，也就是大名鼎鼎的 CLIP。因此，GPT-4 具备多模能力本身并不是一件意外的事情。</p>
<p><strong>RLHF</strong></p>
<p>在笔者看来，RLHF 等技术更多地是在不限制输入 token 序列的情况下，去约束模型输出的技术，当然从某种意义上，也可以看成是在监督 prompts -&gt; task 的映射关系的技术（其实发展到现在，task 这个词已经不太准确了，可以意会一下）。</p>
<p>GPT-4 的报告中明确指出，RLHF 并不能提升模型解决任务的质量（不会增加知识），甚至很多时候调的不好还会损害各个任务上的指标。RLHF 更多地是在构建一些明确的 prompts -&gt; task 映射关系，因为自然语言是具有歧义性的，尤其是在输入信息较少的情况下，模型根据 prompts “理解”到的那个 task，并不一定是人类真正心里的那个意图，RLHF 实现了一种定制化的映射搭建，或者说，人类喜好对齐。</p>
<p><strong>Predictable scaling</strong></p>
<p>由于大模型实验的成本日渐高昂，我们不再能像小模型那样随便起实验调参了。因此 OpenAI 的大部分实验应该是在一个比 GPT-4 小很多倍的模型上进行的，然后通过这个小模型的训练 loss，来预测大模型最终训练出来的 loss</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>同样的， predictable scaling laws 也在很多 HumanEval 集上得到了观察。当然，在一部分的任务上也还无法被拟合的性能曲线，因此 OpenAI 说后续还会进一步优化他们模型性能预测的方法。</p>
<h2 id="6-sparks-of-artifificial-general-intelligence-early-experiments-with-gpt-4-230312712">6. Sparks of Artifificial General Intelligence: Early experiments with GPT-4 (2303.12712)</h2>
<p>智能（Intelligence）是一个多方面且难以捉摸的概念，长期以来缺乏一个共识性的定义。1994 年 52 名 心理学家组成的公式小组出版的关于智力科学的社论中，将只能定义为一种非常普遍的心理能力，包括<strong>推理、计划、解决问题、抽象思考、理解复杂想法、快速学习和从经验中学习的能力</strong>。</p>
<p>这篇是微软关于 GPT-4 的研究报告，长达 55 页，文中的实验都是在 <strong>早期的文本单一模态版的 GPT-4</strong> 上进行的（而不是后面更新的多模态版本）, 其目标是生成一些新颖而困难的任务和问题，来证明 GPT-4 的能力并不是单纯的记忆，并且它对<strong>概念、技能和领域有深刻而灵活的理解</strong>。另外还旨在探索 GPT-4 的响应和行为，以验证其<strong>一致性、连贯性和正确性</strong>，揭示其<strong>局限性和偏差</strong>。</p>
<p><strong>如何衡量 GPT-4 的智能</strong></p>
<p>传统机器学习的标准做法是准备一组标准评测数据集，确保它们独立于训练数据之外，覆盖一系列的任务和领域。</p>
<p>但这种方法并不太适用于 GPT-4，因为 GPT-4 是闭源模型，相关的训练数据集信息不公开，并且可以预见地非常庞大，因此我们不能保证目前公开的基准测试集不在它的训练数据里。也正因为此，本文采用的研究方法更接近于传统心理学，而不是机器学习方法。</p>
<p><strong>多模态与跨学科整合</strong></p>
<p>智能的一个重要衡量指标是综合不同领域信息的能力，以及跨学科地应用知识和技能的能力。这一节中作者举了四个例子来说明 GPT-4 具有很强的多模态与跨学科整合能力：</p>
<ol>
<li>写 JavaScript 代码来生成画家 Wassily Kandinsky 风格的作品。图一是该画家的原作，后面分别是 GPT-4 和 ChatGPT 写的代码画出的。</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="2">
<li>用莎士比亚的文风来证明素数无穷定理</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="3">
<li>以圣雄甘地的口吻写一封信给他的妻子，内容是支持“电子”成为美国总统候选人</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="4">
<li>写 Python 代码，以年龄、性别、体重、身高和血液测试结果向量作为输入，来预测用户是否有患糖尿病的风险</li>
</ol>
<p>这些对比可以体现 GPT-4 能创新性地整合不同领域的概念，并且显著强于 ChatGPT。除此之外作者也测试了 GPT-4 在音乐、绘图、空间理解方面的能力。</p>
<p><strong>代码</strong></p>
<p>除了常见的 leetcode 刷题，作者测试了 GPT-4 生成逆向工程代码、解释已有代码、用自然语言模拟代码执行过程、运行伪代码等能力。</p>
<p><strong>数学</strong></p>
<p>在一系列的分析实验后，作者从以下三方面总结了GPT-4的数学能力：</p>
<ol>
<li><strong>创造性推理</strong>：识别每个阶段可能相关的参数、中间步骤、计算或代数操作的能力。该组件通常基于启发式猜测或直觉，通常被认为是数学解决问题的最实质性和最深刻的方面</li>
<li><strong>技术熟练程度</strong>：执行遵循指定步骤集的常规计算或操作的能力</li>
<li><strong>批判性推理</strong>：批判性地检查论点的每个步骤的能力，将其分解为其子组件，解释它需要的内容，它与其余论点相关以及为什么是正确的</li>
</ol>
<p>这一节作者发现 GPT-4 的很多缺陷，如：</p>
<ul>
<li>在执行一些很常规且机械的计算时经常算错和混淆</li>
<li>GPT-4 由于是自回归模型，因此是实时线性输出的，而没有办法“打腹稿”</li>
</ul>
<p><strong>与世界的交互</strong></p>
<p>智能的另一重要方面是交互能力，即跟外界环境和智能体进行交互的能力，作者主要通过<strong>工具调用</strong>和<strong>具身交互</strong>两个维度来评估。</p>
<p>工具调用这里不多赘述了，具身交互方面测试了文字跑团游戏，以及交互式地指导人员找到天花板漏水的地方并进行修补，逐步根据人类的每一步反馈，给出行动建议和指示。</p>
<p><strong>与人类的交互</strong></p>
<p>GPT-4 在推理他人心理状态方面表现非常突出，特别是在模拟现实场景中，它的解释能力也很强，能对自己的判断和言论进行自我解释。</p>
<p><strong>判别能力</strong></p>
<p>判别能力主要指模型区别不同事物、概念和情景的能力。比如，区分两个食物哪个是可以安全食用，哪个是有毒的。</p>
<p>这一节的测试里，揭示出当前的评测指标存在的缺陷：对于语句相似度捕捉不够，依然严重依赖单词和短句的相似度，因此在很多时候参考答案很短，而 GPT-4 生成的答案很长，会被 ROUGE 这样的指标判定为答案不匹配，而人工检查后发现 GPT-4 的答案更加高质量和具有说服力。</p>
<p>另一方面作者也测试了用 GPT-4 作为评分员，对回答进行打分，实验现实尽管距离人类打分还存在一些差距，但在一些强约束的场景下已经很具有竞争力了。</p>
<p><strong>自回归结构的局限性</strong></p>
<p>自回归结构的输出是实时进行的，因此不存在“打草稿”的机会，因此无法“step-by-step”地处理问题，而引入思维链则可以显著地提升模型准确度。</p>
<p>对于一些依赖递归回溯的问题，比如一步一步输出汉诺塔问题的解法，GPT-4 表现非常差，在解决不能以连续方式处理的复杂或创造性问题时，都暴露出了严重的局限性。</p>
<p>比如要求 GPT-4 修改“9 * 4 + 6 * 6 = 72”这个等式左边的一个数字，来让等式计算结果变成 99，这就是一个无法简单“step-by-step”推理得到答案的问题，GPT-4 最后的准确率也非常低。</p>
<p>这一节的讨论中，作者指出，理解这些局限性的一个方法是类比诺奖作者卡尼曼提出的“快思考”和“慢思考”的概念。卡尼曼认为人类思维分成快、慢两个系统，快思考是一种自动的、直观的、不需要花费精力的思考方式，速度快但是容易出错和偏见。慢思考是一种受控的、理性的、耗费精力的思考方式，虽然速度慢但是准确可靠。</p>
<p>当前的 GPT-4 很大程度上可以看成是在执行快思考，但缺少慢思考能力。</p>
<p><strong>未来方向</strong></p>
<p>作者在这一节总结了未来 GPT-4 可以研究和改进的方向：</p>
<ul>
<li><strong>置信度校验</strong>：模型的输出缺乏置信度，既会编造训练集中没有的内容（open-domain 幻觉），也会生成与 Prompt 不一致的内容（close-domain 幻觉）</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li><strong>长期记忆</strong>：即长的上下文</li>
<li><strong>持续学习</strong>：当前微调和自我更新成本过高、缺乏有效且稳定的手段（保持已有能力不丢失和遗忘）</li>
<li><strong>个性化</strong>：根据应用和需求进行定制、扮演、调整风格等</li>
<li><strong>提前规划和概念性跳跃</strong>：推理过程过于线性，在需要思维跳跃性的任务上表现不佳</li>
<li><strong>透明度、可解释性和一致性</strong></li>
<li><strong>认知谬误和非理性</strong>：数据中存在的偏见、成见或错误引入了认知偏差和非理性</li>
<li><strong>对输入的敏感性</strong>：对于 Prompt 过于敏感，鲁棒性不够</li>
</ul>
<p>这些局限性均指向一个核心问题：<strong>哪些缺陷是自回归架构的先天缺陷，哪些是在已有架构上可以通过处理数据、增加外挂的组件和增大参数量解决的。</strong></p>
<h3 id="7-the-dawn-of-lmms-preliminary-explorations-with-gpt-4vision-230917421">7. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) (2309.17421)</h3>
<p>微软发布的 166 页的 GPT-4V 报告，主要围绕以下四个点进行展开研究：</p>
<ol>
<li>GPT-4V 支持哪些输入和工作模式？</li>
<li>GPT-4V 在不同领域和任务上的能力质量和通用性如何？</li>
<li>GPT-4V 有效使用和提示方法有哪些？</li>
<li>未来有哪些有前途的方向？</li>
</ol>
<p><strong>GPT-4V 的输入模式</strong></p>
<ul>
<li>纯文本</li>
<li>单个图像-文本对</li>
<li>图像文本交替输入</li>
</ul>
<p>前两种相对来说比较简单，第三种交替输入的情况，已经非常接近于人的聊天模式了。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>GPT-4V 的工作模式和提示技术</strong></p>
<p>实验证明，在 LLM 上研究出来上各种提示技术，在 GPT-4V 上也是好使的，比如思维链、few-shot 提示等。</p>
<p>这一节提到了一个“LLMs don&rsquo;t want to succeed”的理论，貌似是来自于 Andrej Karpathy 的某次演讲，里面展示了一种类似于催眠一样的提示技术，即，<strong>你想要你的 LLM 表现更出色，你就要用直接的提示词说“你是xxx方面的专家”，否则它之后表现出一般普通人水平的能力</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>完整的 PPT 可以看这里：https://karpathy.ai/stateofgpt.pdf</p>
<p>在论文中作者是举了一个数苹果的案例，让 GPT-4V 来数一下画面中有几个苹果。一开始 GPT-4V 并不能轻易得到正确答案，但经过一系列我们已知的 LLM Prompt 技巧加强后，GPT-4V 变得可靠，能够正确计数：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在日常的人和人交互中，在图片中画圈、画箭头来指向关键信息是一种很自然且常见的方式， 经实验 GPT-4V 在这方面的理解能力非常强大。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>作者实验了一些很有挑战性的 case，发现基本上难不倒它：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在 In-context few-shot learning 方面，作者也用一个很有代表性的例子说明了提供示例样本的重要性。作者给了一张仪表的图，让 GPT-4V 读出当前仪表指针指向的数值，一开始不论如何改良 prompt 都无法得到正确的结果。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>甚至在给出一个示例的情况下模型仍然表现不佳，但当示例增加到两个后，GPT-4V 就突然能成功读数了，可见<strong>提供上下文示例对于提升大模型性能至关重要</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>视觉语言能力</strong></p>
<p>在大部分已有的 CV 子任务上，GPT-4V 都表现出了不错的能力，常见的场景描述等更是表现出色，在相对小众的领域，如医学图像上，同样让人印象深刻，GPT-4V 可以根据 CT 图判断出智齿和骨折等。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>当然，在一些已经被做得非常深入的子任务上，GPT-4V 相较于 SOTA 模型还有不小的差距，但还是那句话，潜力大于绝对精度，目前 GPT-4V 已经展现出了让人鼓舞的性能，优化个别任务上的表现只是时间问题。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>GPT-4V 甚至能看懂梗图，解释其中的笑点：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>借助 GPT-4V 强大的推理能力和具备的常识，我们甚至可以“<strong>假如你是一名侦探，你可以从图中推理出哪些线索？</strong>”</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>时间序列和视频理解</strong></p>
<p>作者实验了多图像序列，GPT-4V 能够识别出这是一组动态图像序列，并且能结合起来判断画面中的人正在做俯卧撑：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>情商测试</strong></p>
<p>在这一节，GPT-4V 可以基于予以内容和图像样式解释视觉情感，如满意、愤怒、敬畏和恐惧等：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>甚至可以一张图片让 GPT-4V 用两种不同方式来描述，分别让人感到不安和感到舒适（新闻学让它玩明白了）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>新兴应用亮点</strong></p>
<ul>
<li>行业：
<ul>
<li>缺陷检测</li>
<li>安全检查</li>
<li>杂货结账</li>
</ul>
</li>
<li>医疗</li>
<li>汽车保险
<ul>
<li>损害评估</li>
<li>保险报告</li>
</ul>
</li>
<li>定制化
<ul>
<li>照片组织</li>
<li>密集标注与分割</li>
</ul>
</li>
<li>图像生成
<ul>
<li>生成图像的评估</li>
<li>图像编辑的提示生成</li>
</ul>
</li>
<li>具象化智能体
<ul>
<li>操作机器</li>
<li>导航</li>
</ul>
</li>
<li>GUI 导航（软件层面的交互和导航）</li>
</ul>
<p>整篇报告篇幅较多，并且举了大量详细的例子，在这里就不一一展开了，感兴趣的同学可以自行翻阅。</p>
<hr>
<p>至此，我总结了 GPT 系列工作里一些我关注到的点，从中可以感受到 OpenAI 的工作之间都有着很深的逻辑链条，很多推广都似乎是最符合直觉的。OpenAI 早期公开的论文里各种细节还是很丰富的，不仅细致地告诉你如何清洗和构造数据，甚至还教你如何找到一个合适的标注员给你标数据。</p>
<p>作为一个从 CV 转到 LLM 的新人，难免犯一些常见或低级的错误，欢迎任何读者及时指出和斧正，也欢迎任何留言讨论。</p>
<p>本篇笔记的写作参考了沐神的几期 B 站视频，以及知乎@苏打的文章，特此感谢</p>
]]></description></item><item><title>强化学习笔记 [19] | AlphaGo Zero强化学习原理</title><link>https://jianye0428.github.io/posts/rl_learning_note_19/</link><pubDate>Sun, 25 Feb 2024 19:53:22 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_19/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。</p>
<p>本篇主要参考了AlphaGo Zero的<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, <a href="https://www.hhyz.me/2018/08/08/2018-08-08-AlphaGO-Zero/"target="_blank" rel="external nofollow noopener noreferrer">AlphaGo Zero综述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和AlphaGo Zero Cheat Sheet。</p>
<h1 id="1-alphago-zero模型基础">1. AlphaGo Zero模型基础</h1>
<p>AlphaGo Zero不需要学习人类的棋谱，通过自我对弈完成棋力提高。主要使用了两个模型，第一个就是我们上一节介绍MCTS树结构，另一个是一个神经网络。MCTS上一篇已经有基本介绍了，对于神经网络，它的输入是当前的棋局状态，输出两部分，第一部分输出是在当前棋局状态下各个可能的落子动作对应的获胜概率p，可以简单理解为Actor-Critic策略函数部分。另一部分输出为获胜或者失败的评估[-1,1]，可以简单理解为Actor-Critic价值函数部分。</p>
<p>AlphaGo Zero的行棋主要是由MCTS指导完成的，但是在MCTS搜索的过程中，由于有一些不在树中的状态需要仿真，做局面评估，因此需要一个简单的策略来帮助MCTS评估改进策略，这个策略改进部分由前面提到的神经网络完成。</p>
<p>这两部分的关系如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">AlphaGo Zero 中的MCTS和NN</div>
</center>
<br>
<p>具体AlphaGo Zero的MCTS如何搜索，神经网络如何训练，如何指导MCTS搜索我们在后面再讲。</p>
<h1 id="2-alphago-zero的训练过程简介">2. AlphaGo Zero的训练过程简介</h1>
<p>在讨论AlphaGo Zero的MCTS如何搜索，神经网络如何训练等细节之前，我们先看看AlphaGo Zero的训练过程是什么样的。</p>
<p>AlphaGo Zero训练过程主要分为三个阶段：自我对战学习阶段，训练神经网络阶段和评估网络阶段。</p>
<p>自我对战学习阶段主要是AlphaGo Zero自我对弈，产生大量棋局样本的过程，由于AlphaGo Zero并不使用围棋大师的棋局来学习，因此需要自我对弈得到训练数据用于后续神经网络的训练。在自我对战学习阶段，每一步的落子是由MCTS搜索来完成的。在MCTS搜索的过程中，遇到不在树中的状态，则使用神经网络的结果来更新MCTS树结构上保存的内容。在每一次迭代过程中，在每个棋局当前状态 $s$ 下，每一次移动使用1600次MCTS搜索模拟。最终MCTS给出最优的落子策略 $π$ ,这个策略 $π$ 和神经网络的输出 $p$ 是不一样的。当每一局对战结束后，我们可以得到最终的胜负奖励 $z$ ,1或者-1. 这样我们可以得到非常多的样本 $(s,π,z)$,这些数据可以训练神经网络阶段。</p>
<p>在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合(s,π,z)(�,�,�),训练我们神经网络的模型参数。训练的目的是对于每个输入 $s$, 神经网络输出的 $p,v$和我们训练样本中的 $π$, $z$差距尽可能的少。这个损失函数 $L$ 其实是很简单的：</p>
<p>$$L=(z-v)^2-\pi^Tlog(p)+c||\theta||^2$$</p>
<p>损失函数由三部分组成，第一部分是均方误差损失函数，用于评估神经网络预测的胜负结果和真实结果之间的差异。第二部分是交叉熵损失函数，用于评估神经网络的输出策略和我们MCTS输出的策略的差异。第三部分是L2正则化项。</p>
<p>通过训练神经网络，我们可以优化神经网络的参数 $θ$,用于后续指导我们的MCTS搜索过程。</p>
<p>当神经网络训练完毕后，我们就进行了评估阶段，这个阶段主要用于确认神经网络的参数是否得到了优化，这个过程中，自我对战的双方各自使用自己的神经网络指导MCTS搜索，并对战若干局，检验AlphaGo Zero在新神经网络参数下棋力是否得到了提高。除了神经网络的参数不同，这个过程和第一阶段的自我对战学习阶段过程是类似的。</p>
<h1 id="3-alphago-zero的神经网络结构">3. AlphaGo Zero的神经网络结构</h1>
<p>在第二节我们已经讨论了AlphaGo Zero的主要训练过程，但是还有两块没有讲清楚，一是AlphaGo Zero的MCTS搜索过程是怎么样的，二是AlphaGo Zero的神经网络的结构具体是什么样的。这一节我们来看看AlphaGo Zero的神经网络的细节。</p>
<p>首先我们看看AlphaGo Zero的输入，当前的棋局状态。由于围棋是19x19的361个点组成的棋局，每个点的状态有二种：如果当前是黑方行棋，则当前有黑棋的点取值1，有白棋或者没有棋子的位置取值为0，反过来，如果当前是白方行棋，则当前有白棋的点取值1，有黑棋或者没有棋子的位置取值为0。同时，为了提供更多的信息，输入的棋局状态不光只有当前的棋局状态，包括了黑棋白棋各自前8步对应的棋局状态。除了这16个棋局状态，还有一个单独的棋局状态用于标识当前行棋方，如果是当前黑棋行棋，则棋局状态上标全1，白棋则棋局状态上标全0。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Game State</div>
</center>
<br>
<p>最终神经网络的输入是一个19x19x17的张量。里面包含黑棋和白棋的最近8步行棋状态和当前行棋方的信息。</p>
<p>接着我们看看神经网络的输出，神经网络的输出包括策略部分和价值部分。对于策略部分，它预测当前各个行棋点落子的概率。由于围棋有361个落子点，加上还可以Pass一手，因此一共有362个策略端概率输出。对于价值端，输出就简单了，就是当前局面胜负的评估值，在[-1,1]之间。</p>
<p>看完了神经网络的输入和输出，我们再看看神经网络的结构，主要是用CNN组成的深度残差网络。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>在19x19x17的张量做了一个基本的卷积后，使用了19层或者39层的深度残差网络，这个是ResNet的经典结构。理论上这里也可以使用DenseNet等其他流行的网络结构。神经网络的损失函数部分我们在第二节已经将了。整个神经网络就是为了当MCTS遇到没有见过的局面时，提供的当前状态下的局面评估和落子概率参考。这部分信息会被MCTS后续综合利用。</p>
<h1 id="4-alphago-zero的mcts搜索">4. AlphaGo Zero的MCTS搜索</h1>
<p>　　　　现在我们来再看看AlphaGo Zero的MCTS搜索过程，在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们已经介绍了MCTS的基本原理，和4个主要的搜索阶段：选择，扩展，仿真和回溯。和上一篇的内容相比，这里MCTS的不同主要体现在树结构上保存的信息不同，进而UCT的计算公式也稍有不同。最后MCTS搜索完毕后，AlphaGo Zero也有自己选择真正落子点的策略。</p>
<p>　　　　在上一篇里，我们的MCTS上保存的数据很简单，就是下的总盘数和赢的总盘数。在AlphaGo Zero这里，我们保存的信息会多一些。主要包括下面的4部分：</p>
<ul>
<li>$N(s,a)$:记录边的访问次数</li>
<li>$W(s,a)$: 合计行动价值</li>
<li>$Q(s,a)$:平均行动价值</li>
<li>$P(s,a)$:选择该条边的先验概率</li>
</ul>
<p>其中 $s$ 为当前棋局状态，$a$ 为某一落子选择对应的树分支。</p>
<p>有了MCTS上的数据结构，我们看看AlphaGo Zero的MCTS搜索的4个阶段流程：</p>
<p>首先是选择，在MCTS内部，出现过的局面，我们会使用UCT选择子分支。子分支的UCT原理和上一节一样。但是具体的公式稍有不同，如下：</p>
<p>$$\begin{gathered}
U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)} \\
a_t=\arg\max_a(Q(s_t,a)+U(s_t,a))
\end{gathered}$$</p>
<p>最终我们会选择 $Q+U$最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数,上一篇已讲过。</p>
<p>如果到了没有到终局的MCTS叶子节点，那么我们就需要进入MCTS的第二步，扩展阶段,以及后续的第三步仿真阶段。我们这里一起讲。对于叶子节点状态s�，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL��落子的概率p�和对应的价值v�,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为：</p>
<p>$$\{N(s_L,a)=0,W(s_L,a)=0,Q(s_L,a)=0,P(s_L,a)=P_a\}$$</p>
<p>这个过程如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>这样扩展后，之前的叶子节点 $s$，现在就是内部节点了。做完了扩展和仿真后，我们需要进行回溯，将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点 $L$ 依次向根节点回溯，并依次更新上层分支数据结构如下：</p>
<p>$$\begin{gathered}
N(s_t,a_t)=N(s_t,a_t)+1 \\
W(s_t,a_t)=W(s_t,a_t)+v \\
Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}
\end{gathered}$$</p>
<p>这个MCTS搜索过程在一次真正行棋前，一般会进行约1600次搜索，每次搜索都会进行上述4个阶段。</p>
<p>这上千次MCTS搜索完毕后，AlphaGo Zero就可以在MCTS的根节点 $s$ 基于以下公式选择行棋的MCTS分支了:</p>
<p>$$\pi(a|s)=\frac{N(s,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}$$</p>
<p>其中，$τ$ 为温度参数，控制探索的程度，$τ$ 越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。每一次完整的自我对弈的前30步，参数 $τ=1$，这是早期鼓励探索的设置。游戏剩下的步数，该参数将逐渐降低至0。如果是比赛，则直接为0.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>同时在随后的时间步中，这个MCTS搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 。</p>
<p>以上就是AlphaGo Zero MCTS搜索的过程。</p>
<h1 id="5-alphago-zero小结与强化学习系列小结">5. AlphaGo Zero小结与强化学习系列小结</h1>
<p>AlphaGo Zero巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。两者一主一辅，非常优雅的解决了这类状态完全可见，信息充分的棋类问题。</p>
<p>当然这类强化学习算法只对特定的这类完全状态可见，信息充分的问题有效，遇到信息不对称的强化学习问题，比如星际，魔兽之类的对战游戏问题，这个算法就不那么有效了。要推广AlphaGo Zero的算法到大多数普通强化学习问题还是很难的。因此后续强化学习算法应该还有很多发展的空间。</p>
<p>至此强化学习系列就写完了，之前预计的是写三个月，结果由于事情太多，居然花了大半年。但是总算还是完成了，没有烂尾。生活不易，继续努力！</p>
]]></description></item><item><title>强化学习笔记 [18] | 基于模拟的搜索与蒙特卡罗树搜索(MCTS)</title><link>https://jianye0428.github.io/posts/rl_learning_note_18/</link><pubDate>Sun, 25 Feb 2024 19:53:18 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_18/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。</p>
<p>本篇主要参考了UCL强化学习课程的第八讲，第九讲部分。</p>
<h1 id="1-基于模拟的搜索概述">1. 基于模拟的搜索概述</h1>
<p>什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这是数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。</p>
<p>那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点 $S_t$ 开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以 $S_t$ 为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到 $S_t$状态最应该采用的动作 $A_t$。前向搜索的sub-MDP如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">forward search sub-MDP</div>
</center>
<br>
<p>前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。</p>
<h1 id="2-简单蒙特卡罗搜索">2. 简单蒙特卡罗搜索</h1>
<p>首先我们看看基于模拟的搜索中比较简单的一种方法：简单蒙特卡罗搜索。</p>
<p>简单蒙特卡罗搜索基于一个强化学习模型 $M_v$ 和一个模拟策略 $π$.在此基础上，对于当前我们要选择动作的状态 $S_t$, 对每一个可能采样的动作 $a∈A$,都进行 $K$ 轮采样，这样每个动作 $a$ 都会得到 $K$ 组经历完整的状态序列(episode)。即：</p>
<p>$$\{S_t,a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>现在对于每个 $(S_t,a)$ 组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1K\sum_{k=1}^KG_t\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别,那么简单蒙特卡罗搜索也太慢了。同时，由于使用蒙特卡罗法计算其动作价值函数，模拟采样得到的一些中间状态和对应行为的价值就被忽略了，这部分数据能不能利用起来呢？</p>
<p>下面我们看看蒙特卡罗树搜索(Monte-Carlo Tree Search，以下简称MCTS)怎么优化这个问题的解决方案。</p>
<h1 id="3-mcts的原理">3. MCTS的原理</h1>
<p>MCTS摒弃了简单蒙特卡罗搜索里面对当前状态 $S_t$ 每个动作都要进行K次模拟采样的做法，而是总共对当前状态 $S_t$进行K次采样，这样采样到的动作只是动作全集 $A$ 中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。</p>
<p>在MCTS中，基于一个强化学习模型 $M_v$和一个模拟策略$π$，当前状态 $S_t$ 对应的完整的状态序列(episode)是这样的:</p>
<p>$$\{S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算 $Q(st,a)$和最大 $Q(s_t,a)$对应的动作。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1{N(S_t,a)}\sum_{k=1}^K\sum_{u=t}^T1(S_{uk}=S_t,A_{uk}=a)G_u\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>MCTS搜索的策略分为两个阶段：第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使 $ϵ−$贪婪策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。</p>
<p>这里讲到的是最经典的强化学习终MCTS的用户，每一步都有延时奖励，但是在棋类之类的零和问题中，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。</p>
<h1 id="4-上限置信区间算法uct">4. 上限置信区间算法UCT</h1>
<p>在讨论棋类游戏的MCTS搜索之前，我们先熟悉下上限置信区间算法(Upper Confidence Bound Applied to Trees, 以下简称UCT)。它是一种策略算法，我们之前最常用的是 $ϵ−$贪婪策略。但是在棋类问题中，UCT更常使用。</p>
<p>在棋类游戏中，经常有这样的问题，我们发现在某种棋的状态下，有2个可选动作，第一个动作历史棋局中是0胜1负，第二个动作历史棋局中是8胜10负，那么我们应该选择哪个动作好呢？如果按 $ϵ−$贪婪策略，则第二个动作非常容易被选择到。但是其实虽然第一个动作胜利0%，但是很可能是因为这个动作的历史棋局少，数据不够导致的，很可能该动作也是一个不错的动作。那么我们如何在最优策略和探索度达到一个选择平衡呢？ $ϵ−$贪婪策略可以用，但是UCT是一个更不错的选择。</p>
<p>UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度吗，一个常用的公式如下：</p>
<p>$$\text{score}=\left.\frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}\right.$$</p>
<p>其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$ 是所有模拟次数，c 是探索常数，理论值为$√2$，可根据经验调整，$c$ 越大就越偏向于广度搜索，$c$ 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。</p>
<p>比如对于下面的棋局，对于根节点来说，有3个选择，第一个选择7胜3负，第二个选择5胜3负，第三个选择0胜3负。</p>
<p>如果我们取c=10,则第一个节点的分数为：$$score(7,10)=7/10+C\cdot\sqrt{\frac{\log(21)}{10}}\approx6.2$$</p>
<p>第二个节点的分数为：$$score(5,8)=5/8+C\cdot\sqrt{\frac{\log(21)}8}\approx6.8$$</p>
<p>第三个节点的分数为：$$score(0,3)=0/3+C\cdot\sqrt{\frac{\log(21)}3}\approx10$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>可见，由于我们把探索率 $c$ 设置的比较大，第三个节点是被UCT选中要执行的动作节点。当然如果我们把c设置的比较小的话，第一个或者第二个可能就变成最大的分数了。</p>
<h1 id="5-棋类游戏mcts搜索">5. 棋类游戏MCTS搜索</h1>
<p>在像中国象棋，围棋这样的零和问题中，一个动作只有在棋局结束才能拿到真正的奖励，因此我们对MCTS的搜索步骤和树结构上需要根据问题的不同做一些细化。</p>
<p>对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步，如下图(图来自维基百科)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。</p>
<p>第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。</p>
<p>第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。</p>
<p>第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。</p>
<p>以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。</p>
<h1 id="6-mcts小结">6. MCTS小结</h1>
<p>MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。</p>
]]></description></item><item><title>强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架</title><link>https://jianye0428.github.io/posts/rl_learning_note_17/</link><pubDate>Sun, 25 Feb 2024 19:53:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_17/</guid><description><![CDATA[<h1 id="强化学习十七-基于模型的强化学习与dyna算法框架httpswwwcnblogscompinardp10384424html"><a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h1>
<p>在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。</p>
<p>本篇主要参考了UCL强化学习课程的第8讲和Dyna-2的<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-基于模型的强化学习简介">1. 基于模型的强化学习简介</h1>
<p>基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率 $P^a_{ss&rsquo;}$。</p>
<p>而基于模型的强化学习则会尝试从环境的模型去学习，一般是下面两个相互独立的模型：</p>
<ul>
<li>一个是状态转化预测模型，输入当前状态 $s$和动作 $a$，预测下一个状态 $s&rsquo;$。</li>
<li>另一个是奖励预测模型，输入当前状态 $s$和动作 $a$，预测环境的奖励 $r$。</li>
</ul>
<p>即模型可以描述为下面两个式子：</p>
<p>$$\begin{gathered}S_{t+1}\sim P(S_{t+1}|S_t,A_t)\\R_{t+1}\sim R(R_{t+1}|S_t,A_t)\end{gathered}$$</p>
<p>如果模型 $P$, $R$ 可以准确的描述真正的环境的转化模型，那么我们就可以基于模型来预测，当有一个新的状态 $S$ 和动作 $A$到来时，我们可以直接基于模型预测得到新的状态和动作奖励，不需要和环境交互。当然如果我们的模型不好，那么基于模型预测的新状态和动作奖励可能错的离谱。</p>
<p>从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的主要区别：即基于模型的强化学习是从模型中学习，而不基于模型的强化学习是从和环境交互的经历去学习。</p>
<p>下面这张图描述了基于模型的强化学习的思路：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Model-based RL</div>
</center>
<br>
<h1 id="2-基于模型的强化学习算法训练流程">2. 基于模型的强化学习算法训练流程</h1>
<p>这里我们看看基于模型的强化学习算法训练流程，其流程和我们监督学习算法是非常类似的。</p>
<p>假设训练数据是若干组这样的经历：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,R_2,\ldots,S_T$$</p>
<p>对于每组经历，我们可以将其转化为 $T−1$ 组训练样本，即：</p>
<p>$$\begin{gathered}
S_1,A_1\to S_2,S_1,A_1\to R_2 \\
S_2,A_2\to S_3,S_2,A_2\to R_3 \\
&hellip;&hellip; \\
S_{T-1},A_{T-1}\rightarrow S_T,~S_{T_1},A_{T-1}\rightarrow R_T
\end{gathered}$$</p>
<p>右边的训练样本一起组成了一个分类模型或密度估计模型，输入状态和动作，输出下一个状态。 右边的训练样本一起组成了一个回归模型训练集，输入状态和动作，输出动作奖励值。</p>
<p>至此我们的强化学习求解过程和传统的监督学习算法没有太多区别了，可以使用传统的监督学习算法来求解这两个模型。</p>
<p>当然还可以更简单，即通过对训练样本进行查表法进行统计，直接得到 $P(S_{t+1}|S_t,A_t)$ 的概率和 $R(R_{t+1}|S_t,A_t)$ 的平均值，这样就可以直接预测。比使用模型更简单。</p>
<p>此外，还有其他的方法可以用来得到 $P(S_{t+1}|S_t,A_t)$和 $R(R_{t+1}|S_t,A_t)$，这个我们后面再讲。</p>
<p>虽然基于模型的强化学习思路很清晰，而且还有不要和环境持续交互优化的优点，但是用于实际产品还是有很多差距的。主要是我们的模型绝大多数时候不能准确的描述真正的环境的转化模型，那么使用基于模型的强化学习算法得到的解大多数时候也不是很实用。那么是不是基于模型的强化学习就不能用了呢？也不是，我们可以将基于模型的强化学习和不基于模型的强化学习集合起来，取长补短，这样做最常见的就是Dyna算法框架。</p>
<h1 id="3-dyna算法框架">3. Dyna算法框架</h1>
<p>Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。如果用和第一节类似的图，可以表示如下图，和第一节的图相比，多了一个“Direct RL“的箭头，这正是不基于模型的强化学习的思路。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna算法示意图</div>
</center>
<br>
<p>Dyna算法框架和不同的具体的不基于模型的强化学习一起，可以得到具体的不同算法。如果我们使用基于价值函数的Q-Learning，那么我们就得到了Dyna-Q算法。我们基于Dyna-Q来看看Dyna算法框架的一般流程.</p>
<h1 id="4-dyna-q算法流程">4. Dyna-Q算法流程</h1>
<p>这里我们给出基于价值函数的Dyna-Q算法的概要流程。假设模型使用的是查表法。</p>
<ul>
<li>(1). 初始化任意一个状态 $s$,和任意一个动作 $a$ 对应的状态价值 $Q(s,a)$, 初始化奖励模型 $R(s,a)$和状态模型 $P(s,a)$</li>
<li>(2). for $i=1$ to 最大迭代次数T：
<ul>
<li>(a) $S \leftarrow \text{current state}$</li>
<li>(b) $A \leftarrow \text{ϵ−greedy(S,Q)}$</li>
<li>(c) 执行动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>(d) 使用Q-Learning更新价值函数：$Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
<li>(e) 使用 $S,A,S^{\prime}$ 更新状态模型 $P(s,a)$，使用 $S,A,R$ 更新状态模型 $R(s,a)$</li>
<li>(f) $\text{for} \space \space j=1 \space \space \text{to} \text{最大次数}n$：
<ul>
<li>(i) 随机选择一个之前出现过的状态 $S$ , 在状态 $S$ 上出现过的动作中随机选择一个动作 $A$</li>
<li>(ii) 基于模型 $P(S,A)$ 得到 $S&rsquo;$, 基于模型 $R(S,A)$ 得到 $R$</li>
<li>(iii) 使用Q-Learning更新价值函数: $Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>从上面的流程可以看出，Dyna框架在每个迭代轮中，会先和环境交互，并更新价值函数和（或）策略函数，接着进行n次模型的预测，同样更新价值函数和（或）策略函数。这样同时利用上了和环境交互的经历以及模型的预测。</p>
<h1 id="5-dyna-2算法框架">5. Dyna-2算法框架</h1>
<p>在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为<strong>永久性记忆</strong>（permanent memory）和<strong>瞬时记忆</strong>（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。</p>
<p>永久性记忆的Q函数定义为：</p>
<p>$$Q(S,A)=\phi(S,A)^T\theta $$</p>
<p>瞬时记忆的Q函数定义为：</p>
<p>$$Q^{\prime}(S,A)=\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>组合起来后记忆的Q函数定义为：</p>
<p>$$\overline{Q}(S,A)=\phi(S,A)^T\theta+\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似于 $SARSA(λ)$</p>
<p>以下是Dyna-2的算法流程：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna-2 算法流程</div>
</center>
<br>
<h1 id="6-基于模型的强化学习总结">6. 基于模型的强化学习总结</h1>
<p>基于模型的强化学习一般不单独使用，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。对于模型部分，我们可以用查表法和监督学习法等方法，预测或者采样得到模拟的经历。而对于非模型部分，使用前面的Q-Learning系列的价值函数近似，或者基于Actor-Critic的策略函数的近似都是可以的。</p>
<p>除了Dyna算法框架，我们还可以使用基于模拟的搜索(simulation-based search)来结合基于模型的强化学习和不基于模型的强化学习,并求解问题。这部分我们在后面再讨论。</p>
]]></description></item><item><title>强化学习笔记 [16] | 深度确定性策略梯度(DDPG)</title><link>https://jianye0428.github.io/posts/rl_learning_note_16/</link><pubDate>Sun, 25 Feb 2024 19:53:12 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_16/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10334127.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十五) A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DDPG)。</p>
<p>本篇主要参考了DDPG的<a href="https://arxiv.org/pdf/1509.02971.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-从随机策略到确定性策略">1. 从随机策略到确定性策略</h1>
<p>从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。PG(Policy Gradient)我们在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里已经讨论过。那什么是确定性策略梯度(Deterministic Policy Gradient，以下简称DPG)呢？</p>
<p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成：</p>
<p>$$\pi_\theta(s)=a$$</p>
<h1 id="2-从dpg到ddpg">2. 从DPG到DDPG</h1>
<p>在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>其中状态的采样空间为$\rho^\pi$, $\nabla_\theta log\pi_\theta(s,a)$是分值函数，可见随机性策略梯度需要在整个动作的空间$\pi_\mathrm{\theta}$进行采样。</p>
<p>而DPG基于Q值的确定性策略梯度的梯度计算公式是：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi}[\nabla_\theta\pi_\theta(s)\nabla_aQ_\pi(s,a)|<em>{a=\pi</em>\theta(s)}]$$</p>
<p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p>
<p>而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？</p>
<h1 id="3-ddpg的原理">3. DDPG的原理</h1>
<p>DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。可以复习<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>DDQN的当前Q网络负责对当前状态 $S$ 使用 $ϵ$−贪婪法选择动作 $A$，执行动作 $A$,获得新状态 $S&rsquo;$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态 $S&rsquo;$使用贪婪法选择动作 $A&rsquo;$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。</p>
<p>DDQN的目标Q网络则负责基于经验回放池计算目标Q值, 提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。</p>
<p>现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要 $ϵ$−贪婪法这样的选择方法，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态 $S&rsquo;$ 使用贪婪法选择动作 $A&rsquo;$，这部分工作由于用来估计目标Q值，因此可以放到Actor目标网络完成。</p>
<p>基于经验回放池和目标Actor网络提供的 $S&rsquo;$, $A&rsquo;$ 计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。</p>
<p>此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。</p>
<p>有了上面的思路，我们总结下DDPG 4个网络的功能定位：</p>
<ul>
<li>
<p>(1). <strong>Actor当前网络</strong>: 负责策略网络参数 $θ$的迭代更新，负责根据当前状态 $S$选择当前动作 $A$，用于和环境交互生成 $S&rsquo;$, $R$。</p>
</li>
<li>
<p>(2). <strong>Actor目标网络</strong>: 负责根据经验回放池中采样的下一状态 $S&rsquo;$ 选择最优下一动作$A&rsquo;$。网络参数 $θ&rsquo;$定期从 $θ$复制。</p>
</li>
<li>
<p>(3). <strong>Critic当前网络</strong>: 负责价值网络参数 $w$的迭代更新，负责计算负责计算当前Q值 $Q(S,A,w)$。目标Q值$y_i=R+γQ&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$</p>
</li>
<li>
<p>(4). <strong>Critic目标网络</strong>: 负责计算目标Q值中的 $Q&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$部分。网络参数 $w&rsquo;$ 定期从 $w$复制。</p>
</li>
</ul>
<p>DDPG除了这4个网络结构，还用到了经验回放，这部分用于计算目标Q值，和DQN没有什么区别，这里就不展开了。</p>
<p>此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w$′=$w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：</p>
<p>$$\begin{gathered}
w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</p>
<p>其中 $τ$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。</p>
<p>同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作 $A$会增加一定的噪声 $N$, 即最终和环境交互的动作 $A$ 的表达式是：</p>
<p>$$A=\pi_\theta(S)+\mathcal{N}$$</p>
<p>最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：</p>
<p>$$J(w)=\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>而对于 Actor当前网络，其损失函数就和之前讲的PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：</p>
<p>$$\nabla_J(\theta)=\frac1m\sum_{j=1}^m[\nabla_aQ_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>\theta(s)}\nabla_\theta\pi_{\theta(s)}|_{s=s_i}]$$</p>
<p>这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作 $a_1$和$a_2$，从Critic当前网络得到了两个反馈的 $Q$ 值，分别是 $Q_1$,$Q_2$，假设 $Q_1&gt;Q_2$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加 $a_1$的概率，降低$a_2$的概率，也就是说，Actor想要尽可能的得到更大的Q值。所以我们的Actor的损失可以简单的理解为得到的反馈Q值越大损失越小，得到的反馈Q值越小损失越大，因此只要对状态估计网络返回的Q值取个负号即可，即：</p>
<p>$$J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,w)$$</p>
<h1 id="4-ddpg算法流程">4. DDPG算法流程</h1>
<p>这里我们总结下DDPG的算法流程</p>
<p>输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为 $θ$,$θ&rsquo;$,$w$,$w&rsquo;$,衰减因子 $γ$, 软更新系数 $τ$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$。最大迭代次数 $T$。随机噪音函数 $\mathcal{N}$</p>
<p>输出：最优Actor当前网络参数 $θ$,Critic当前网络参数 $w$</p>
<ul>
<li>(1). 随机初始化$θ$,$w$, $w$′=$w$,$θ$′=$θ$。清空经验回放的集合$D$</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor当前网络基于状态 $S$ 得到动作 $A=π_θ(ϕ(S))+\mathcal{N}$</li>
<li>c) 执行动作$A$,得到新状态$S$′,奖励$R$,是否终止状态%is_end$</li>
<li>d) 将 ${ϕ(S), A, R, ϕ(S&rsquo;), is_end}$ 这个五元组存入经验回放集合$D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本${\phi(S_j),A_j,R_j,\phi(S_j^{\prime}),is_end_j},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\pi_{\theta^{\prime}}(\phi(S_j^{\prime})),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Critic当前网络的所有参数 $w$</li>
<li>h) 使用 $\begin{aligned}J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,\theta)\end{aligned}$，通过神经网络的梯度反向传播来更新Actor当前网络的所有参数 $θ$</li>
<li>i) 如果 i%C=1, 则更新Critic目标网络和Actor目标网络参数：
<ul>
<li>$$\begin{gathered} w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</li>
</ul>
</li>
<li>j) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤(b)</li>
</ul>
</li>
</ul>
<p>以上就是DDPG算法的主流程，要注意的是上面2.f中的 $\pi_{\theta^{\prime}}(\phi(S_j^{\prime}))$ 是通过Actor目标网络得到，而 $Q^{\prime}(\phi(S_i^{\prime}),\pi_{\theta^{\prime}}(\phi(S_i^{\prime})),w^{\prime})$ 则是通过Critic目标网络得到的。</p>
<h1 id="5-ddpg实例">5. DDPG实例</h1>
<p>这里我们给出DDPG第一个算法实例，代码主要参考自莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG_update.py"target="_blank" rel="external nofollow noopener noreferrer">Github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。增加了测试模型效果的部分，优化了少量参数。代码详见：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py</p>
<p>这里我们没有用之前的CartPole游戏，因为它不是连续动作。我们使用了Pendulum-v0这个游戏。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a href="https://github.com/openai/gym/wiki/Pendulum-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。输入状态是角度的sin，cos值，以及角速度。一共三个值。动作是一个连续的力矩值。</p>
<p>两个Actor网络和两个Critic网络的定义参见：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_bound</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;scaled_a&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_c</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_l1</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_s</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_s&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_a&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">w1_s</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">w1_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>  <span class="c1"># Q(s,a)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor当前网络和Critic当前网络损失函数的定义参见：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">q_target</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">td_error</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ce_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">a_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>    <span class="c1"># maximize the q</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">atrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">a_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ae_params</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor目标网络和Critic目标网络参数软更新，Actor当前网络和Critic当前网络反向传播更新部分的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># soft target replacement</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">soft_replace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">MEMORY_CAPACITY</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">bt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">ba</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">br</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs_</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">:</span> <span class="n">ba</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">:</span> <span class="n">br</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_</span><span class="p">:</span> <span class="n">bs_</span><span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余的可以对照算法和代码一起学习，应该比较容易理解。</p>
<h1 id="6-ddpg总结">6. DDPG总结</h1>
<p>DDPG参考了DDQN的算法思想吗，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p>
<p>到此，我们的Policy Based RL系列也讨论完了，而在更早我们讨论了Value Based RL系列，至此，我们还剩下Model Based RL没有讨论。后续我们讨论Model Based RL的相关算法。</p>
]]></description></item><item><title>强化学习笔记 [15] | A3C</title><link>https://jianye0428.github.io/posts/rl_learning_note_15/</link><pubDate>Sun, 25 Feb 2024 15:36:01 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_15/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C的算法原理和算法流程。</p>
<p>本文主要参考了A3C的<a href="http://proceedings.mlr.press/v48/mniha16.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，以及ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-a3c的引入">1. A3C的引入</h1>
<p>上一篇Actor-Critic算法的代码，其实很难收敛，无论怎么调参，最后的CartPole都很难稳定在200分，这是Actor-Critic算法的问题。但是我们还是有办法去有优化这个难以收敛的问题的。</p>
<p>回忆下之前的DQN算法，为了方便收敛使用了经验回放的技巧。那么我们的Actor-Critic是不是也可以使用经验回放的技巧呢？当然可以！不过A3C更进一步，还克服了一些经验回放的问题。经验回放有什么问题呢？ 回放池经验数据相关性太强，用于训练的时候效果很可能不佳。举个例子，我们学习下棋，总是和同一个人下，期望能提高棋艺。这当然没有问题，但是到一定程度就再难提高了，此时最好的方法是另寻高手切磋。</p>
<p>A3C的思路也是如此，它<font color=green>利用多线程的方法，同时在多个线程里面分别和环境进行交互学习，每个线程都把学习的成果汇总起来，整理保存在一个公共的地方</font>。并且，定期从公共的地方把大家的齐心学习的成果拿回来，指导自己和环境后面的学习交互。</p>
<p>通过这种方法，A3C避免了经验回放相关性过强的问题，同时做到了异步并发的学习模型。</p>
<h1 id="2-a3c的算法优化">2. A3C的算法优化</h1>
<p>现在我们来看看相比Actor-Critic，A3C到底做了哪些具体的优化。</p>
<p>相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p>我们首先来看这个异步训练框架，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">异步训练框架</div>
</center>
<br>
<p>图中上面的Global Network就是上一节说的共享的公共部分，主要是一个公共的神经网络模型，这个神经网络包括Actor网络和Critic网络两部分的功能。下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。</p>
<p>每个线程和环境交互到一定量的数据后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度却并不更新自己线程里的神经网络，而是去更新公共的神经网络。也就是n个线程会独立的使用累积的梯度分别更新公共部分的神经网络模型参数。每隔一段时间，线程会将自己的神经网络的参数更新为公共神经网络的参数，进而指导后面的环境交互。</p>
<p>可见，公共部分的网络模型就是我们要学习的模型，而线程里的网络模型主要是用于和环境交互使用的，这些线程里的模型可以帮助线程更好的和环境交互，拿到高质量的数据帮助模型更快收敛。</p>
<p>现在我们来看看第二个优化，网络结构的优化。之前在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们使用了两个不同的网络Actor和Critic。在A3C这里，我们把两个网络放到了一起，即输入状态 $S$,可以输出状态价值 $V$,和对应的策略 $π$, 当然，我们仍然可以把Actor和Critic看做独立的两块，分别处理，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">把Actor和Critic看做独立的两块，分别处理</div>
</center>
<br>
<p>第三个优化点是Critic评估点的优化，在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节中，我们讨论了不同的Critic评估点的选择，其中d部分讲到了使用优势函数 $A$ 来做Critic评估点，优势函数 $A$ 在时刻t不考虑参数的默认表达式为：</p>
<p>$$A(S,A,t)=Q(S,A)-V(S)$$</p>
<p>$Q(S,A)$的值一般可以通过单步采样近似估计，即：</p>
<p>$$Q(S,A)=R+\gamma V(S^{\prime})$$</p>
<p>这样优势函数去掉动作可以表达为：</p>
<p>$$A(S,t)=R+\gamma V(S^{\prime})-V(S)$$</p>
<p>其中 $V(S)$的值需要通过Critic网络来学习得到。</p>
<p>在A3C中，采样更进一步，使用了N步采样，以加速收敛。这样A3C中使用的优势函数表达为：</p>
<p>$$A(S,t)=R_t++\gamma R_{t+1}+\ldots\gamma^{n-1}R_{t+n-1}+\gamma^nV(S^{\prime})-V(S)$$</p>
<p>对于Actor和Critic的损失函数部分，和Actor-Critic基本相同。有一个小的优化点就是在Actor-Critic策略函数的损失函数中，加入了策略 $π$ 的熵项,系数为 $c$, 即策略参数的梯度更新和Actor-Critic相比变成了这样：</p>
<p>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,t)+c\nabla_\theta H(\pi(S_t,\theta))$$</p>
<p>以上就是A3C和Actor-Critic相比有优化的部分。下面我们来总价下A3C的算法流程。</p>
<h1 id="3-a3c算法流程">3. A3C算法流程</h1>
<p>这里我们对A3C算法流程做一个总结，由于A3C是异步多线程的，我们这里给出任意一个线程的算法流程。</p>
<ul>
<li>
<p>输入：公共部分的A3C神经网络结构，对应参数位 $θ$ , $w$，本线程的A3C神经网络结构，对应参数 $θ&rsquo;$, $w&rsquo;$, 全局共享的迭代轮数 $T$，全局最大迭代次数 $T_{max}$, 线程内单次迭代时间序列最大长度 $T_{local}$,状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，熵系数 $c$, 衰减因子 $γ$</p>
</li>
<li>
<p>输出：公共部分的A3C神经网络参数 $θ$, $w$</p>
<ul>
<li>(1). 更新时间序列 $t=1$</li>
<li>(2). 重置Actor和Critic的梯度更新量: $dθ←0$,$dw←0$</li>
<li>(3). 从公共部分的A3C神经网络同步参数到本线程的神经网络：$θ&rsquo;=θ,w&rsquo;=w$</li>
<li>(4). $t_{start}=t$，初始化状态 $s_t$</li>
<li>(5). 基于策略 $π(at|st;θ)$ 选择出动作 $a_t$</li>
<li>(6). 执行动作 $a_t$得到奖励 $r_t$ 和新状态 $s_{t+1}$</li>
<li>(7). $t←t+1$, $T←T+1$</li>
<li>(8). 如果 $s_t$是终止状态，或 $t − t_{start}==t_{local}$,则进入步骤(9)，否则回到步骤(5)</li>
<li>(9). 计算最后一个时间序列位置 $s_t$的 $Q(s,t)$:
<ul>
<li>$$\left.Q(s,t)=\left\{\begin{array}{ll}0&amp;terminal~state\\V(s_t,w^{\prime})&amp;none~terminal~state,bootstrapping\end{array}\right.\right.$$</li>
</ul>
</li>
<li>(10). for $i∈(t−1,t−2,&hellip;t_{start})$:
<ul>
<li>1). 计算每个时刻的$Q(s,i)$： $Q(s,i)=r_i+\gamma Q(s,i+1)$</li>
<li>2). 累计Actor的本地梯度更新：
<ul>
<li>$$d\theta\leftarrow d\theta+\nabla_{\theta^{\prime}}log\pi_{\theta^{\prime}}(s_i,a_i)(Q(s,i)-V(S_i,w^{\prime}))+c\nabla_{\theta^{\prime}}H(\pi(s_i,\theta^{\prime}))$$</li>
</ul>
</li>
<li>3). 累计Critic的本地梯度更新：
<ul>
<li>$$\begin{aligned}dw&amp;\leftarrow dw+\frac{\partial(Q(s,i)-V(S_i,w^{\prime}))^2}{\partial w^{\prime}}\end{aligned}$$</li>
</ul>
</li>
</ul>
</li>
<li>(11). 更新全局神经网络的模型参数：
<ul>
<li>$$\theta=\theta+\alpha d\theta,~w=w-\beta dw$$</li>
</ul>
</li>
<li>(12). 如果 $T&gt;T_{max}$,则算法结束，输出公共部分的A3C神经网络参数 $θ$, $w$,否则进入步骤(3)</li>
</ul>
</li>
</ul>
<p>以上就是A3C算法单个线程的算法流程。</p>
<h1 id="4-a3c算法实例">4. A3C算法实例</h1>
<p>下面我们基于上述算法流程给出A3C算法实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法代码大部分参考了莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/10_A3C/A3C_discrete_action.py"target="_blank" rel="external nofollow noopener noreferrer">A3C代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，增加了模型测试部分的代码并调整了部分模型参数。完整的代码参见我的Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py</p>
<p>整个算法的Actor和Critic的网络结构都定义在这里， 所有的线程中的网络结构，公共部分的网络结构都在这里定义。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;actor&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;la&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_a</span><span class="p">,</span> <span class="n">N_A</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ap&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;critic&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lc&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">)</span>  <span class="c1"># state value</span>
</span></span><span class="line"><span class="cl">  <span class="n">a_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/actor&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">c_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/critic&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">a_prob</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">a_params</span><span class="p">,</span> <span class="n">c_params</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所有线程初始化部分，以及本线程和公共的网络结构初始化部分如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;/cpu:0&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropA&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">GLOBAL_AC</span> <span class="o">=</span> <span class="n">ACNet</span><span class="p">(</span><span class="n">GLOBAL_NET_SCOPE</span><span class="p">)</span>  <span class="c1"># we only need its params</span>
</span></span><span class="line"><span class="cl">  <span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Create worker</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">i_name</span> <span class="o">=</span> <span class="s1">&#39;W_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>   <span class="c1"># worker name</span>
</span></span><span class="line"><span class="cl">    <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker</span><span class="p">(</span><span class="n">i_name</span><span class="p">,</span> <span class="n">GLOBAL_AC</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本线程神经网络将本地的梯度更新量用于更新公共网络参数的逻辑在update_global函数中，而从公共网络把参数拉回到本线程神经网络的逻辑在pull_global中。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">update_a_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_c_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>  <span class="c1"># local grads applies to global net</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">pull_global</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_a_params_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_c_params_op</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>详细的内容大家可以对照代码和算法流程一起看。在主函数里我新加了一个测试模型效果的过程，大家可以试试看看最后的模型效果如何。</p>
<h1 id="5-a3c小结">5. A3C小结</h1>
<p>A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，提供了一种通用的异步的并发的强化学习框架，也就是说，这个并发框架不光可以用于A3C，还可以用于其他的强化学习算法。这是A3C最大的贡献。目前，已经有基于GPU的A3C框架，这样A3C的框架训练速度就更快了。</p>
<p>除了A3C, DDPG算法也可以改善Actor-Critic难收敛的问题。它使用了Nature DQN，DDQN类似的思想，用两个Actor网络，两个Critic网络，一共4个神经网络来迭代更新模型参数。在下一篇我们讨论DDPG算法。</p>
]]></description></item><item><title>强化学习笔记 [14] | Actor-Critic</title><link>https://jianye0428.github.io/posts/rl_learning_note_14/</link><pubDate>Sun, 25 Feb 2024 15:35:58 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_14/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。</p>
<p>在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。</p>
<p>本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。</p>
<h1 id="1-actor-critic算法简介">1. Actor-Critic算法简介</h1>
<p>Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。</p>
<p>回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。</p>
<p>也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：</p>
<p>$$
\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)
$$</p>
<p>第二组是价值函数的近似，对于状态价值和动作价值函数分别是：</p>
<p>$$
\hat{v}(s,w)\approx v_\pi(s)
$$</p>
<p>$$
\hat{q}(s,a,w)\approx q_\pi(s,a)
$$</p>
<p>对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：</p>
<p>$$
\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t
$$</p>
<p>梯度更新部分中，$\nabla_\theta log\pi_\theta(s_t,a_t)$是我们的分值函数，不用动，要变成Actor的话改动的是$v_t$，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。</p>
<p>而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。</p>
<p>现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值$v_t$,而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$,在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{te}$</p>
<h1 id="2-actor-critic算法可选形式">2. Actor-Critic算法可选形式</h1>
<p>在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值 $v_t$实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：</p>
<ul>
<li>
<p>a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)V(s,w)$$</li>
</ul>
</li>
<li>
<p>b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)Q(s,a,w)$$</li>
</ul>
</li>
<li>
<p>c) 基于TD误差：在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了TD误差，它的表达式是 $\delta(t)=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 或者 $\delta(t)=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t)$, 这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)$$</li>
</ul>
</li>
<li>
<p>d) 基于优势函数：在<a href="https://www.cnblogs.com/pinard/p/9923859.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十二) Dueling DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到过优势函数A的定义：$A(S,A,w,\beta)=Q(S,A,w,\alpha,\beta)-V(S,w,\alpha)$, 即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,A,w,\beta)$$</li>
</ul>
</li>
<li>
<p>e) 基于 $TD(λ)$ 误差：一般都是基于后向 $TD(λ)$误差, 在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)E(t)$</li>
</ul>
</li>
</ul>
<p>对于Critic本身的模型参数 $w$ ，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如 $Q(s,a,w)=ϕ(s,a)^Tw$,则Critic本身的模型参数 $w$的更新公式可以表示为：</p>
<p>$$\begin{gathered}
\delta=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t) \\
w=w+\beta\delta\phi(s,a)
\end{gathered}$$</p>
<p>通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。</p>
<h1 id="3-actor-critic算法流程">3. Actor-Critic算法流程</h1>
<p>这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数　　</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，衰减因子 $γ$, 探索率 $ϵ$, Critic网络结构和Actor网络结构。</p>
<p>输出：Actor 网络参数 $θ$, Critic网络参数 $w$</p>
<ul>
<li>(1). 随机初始化所有的状态和动作对应的价值Q�. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor网络中使用 $ϕ(S)$ 作为输入，输出动作 $A$,基于动作 $A$得到新的状态 $S&rsquo;$,反馈 $R$。</li>
<li>c) 在Critic网络中分别使用 $ϕ(S)$，$ϕ(S&rsquo;)$ 作为输入，得到Q值输出 $V(S)$，$V(S&rsquo;)$</li>
<li>d) 计算TD误差 $\delta=R+\gamma V(S^{\prime})-V(S)$</li>
<li>e) 使用均方差损失函数 $\sum(R+\gamma V(S^{\prime})-V(S,w))^2$ 作Critic网络参数 $w$的梯度更新</li>
<li>f) 更新Actor网络参数 $θ$:
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(S_t,A)\delta $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对于Actor的分值函数 $∇_θlogπ_θ(S_t,A)$,可以选择softmax或者高斯分值函数。</p>
<p>上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。</p>
<h1 id="4-actor-critic算法实例">4. Actor-Critic算法实例</h1>
<p>下面我们用一个具体的例子来演示上面的Actor-Critic算法。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法流程可以参考上面的第三节，这里的分值函数我们使用的是softmax函数，和上一片的类似。完整的代码参见Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py</p>
<p>代码主要分为两部分，第一部分是Actor，第二部分是Critic。对于Actor部分，大家可以和上一篇策略梯度的代码对比，改动并不大，主要区别在于梯度更新部分，策略梯度使用是蒙特卡罗法计算出的价值 $v(t)$,则我们的actor使用的是TD误差。</p>
<p>在策略梯度部分，对应的位置如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而我们的Actor对应的位置的代码是：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">td_error</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处要注意的是，由于使用的是TD误差，而不是价值 $v(t)$,此处需要最大化<code>self.exp</code>,而不是最小化它，这点和策略梯度不同。对应的Actor代码为：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除此之外，Actor部分的代码和策略梯度的代码区别并不大。</p>
<p>对于Critic部分，我们使用了类似于DQN的三层神经网络。不过我们简化了这个网络的输出，只有一维输出值，而不是之前DQN使用的有多少个可选动作，就有多少维输出值。网络结构如下:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">],</span> <span class="s2">&#34;state&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layerq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layerq</span><span class="p">,</span> <span class="n">W2q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2q</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和之前的DQN相比，这里还有一个区别就是我们的critic没有使用DQN的经验回放，只是使用了反馈和当前网络在下一个状态的输出来拟合当前状态。</p>
<p>对于算法中Actor和Critic交互的逻辑，在main函数中：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># e-greedy action for train</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">critic</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>  <span class="c1"># gradient = grad[r + gamma * V(s_) - V(s)]</span>
</span></span><span class="line"><span class="cl">  <span class="n">actor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">td_error</span><span class="p">)</span>  <span class="c1"># true_gradient = grad[logPi(s,a) * td_error]</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大家对照第三节的算法流程和代码应该可以比较容易理清这个过程。但是这个程序很难收敛。因此大家跑了后发现分数总是很低的话是可以理解的。我们需要优化这个问题。</p>
<h1 id="5-actor-critic算法小结">5. Actor-Critic算法小结</h1>
<p>基本版的Actor-Critic算法虽然思路很好，但是由于难收敛的原因，还需要做改进。</p>
<p>目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法我们在从DQN到Nature DQN的过程中已经用过一次了。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p>
<p>在后面的文章中，我们会继续讨论DDPG和A3C。</p>
<p>　</p>
]]></description></item><item><title>强化学习笔记 [13] | 策略梯度(Policy Gradient)</title><link>https://jianye0428.github.io/posts/rl_learning_note_13/</link><pubDate>Sun, 25 Feb 2024 15:35:55 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_13/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradient)，它是Policy Based强化学习方法，基于策略来学习。</p>
<p>本文参考了Sutton的强化学习书第13章和策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-value-based强化学习方法的不足">1. Value Based强化学习方法的不足</h1>
<p>DQN系列强化学习算法主要的 <strong><font color=red>问题</font></strong> 主要有三点。</p>
<ul>
<li>
<p>第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。</p>
</li>
<li>
<p>第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。</p>
</li>
<li>
<p>第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。</p>
</li>
</ul>
<p>由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述类别问题的方法，比如Policy Based强化学习方法。</p>
<h1 id="2-policy-based强化学习方法引入">2. Policy Based强化学习方法引入</h1>
<p>回想我们在Value Based强化学习方法里，我们对价值函数进行了近似表示，引入了一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到近似的动作价值，即：</p>
<p>$$\hat{q}\left(s,a,w\right)\approx q_\pi(s,a)$$</p>
<p>在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略 $π$可以被被描述为一个包含参数 $θ$ 的函数,即：</p>
<p>$$\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)$$</p>
<p>将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？</p>
<h1 id="3-策略梯度的优化目标">3. 策略梯度的优化目标</h1>
<p>我们要用梯度上升来寻找最优的梯度，首先就要找到一个可以优化的函数目标。</p>
<p>最简单的优化目标就是初始状态收获的期望，即优化目标为：</p>
<p>$$J_1(\theta)=V_{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}(G_1)$$</p>
<p>但是有的问题是没有明确的初始状态的，那么我们的优化目标可以定义平均价值，即：
$$J_{avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$$</p>
<p>其中，$d_πθ(s)$ 是基于策略 $π_θ$生成的马尔科夫链关于状态的静态分布。</p>
<p>或者定义为每一时间步的平均奖励，即：</p>
<p>$$J_{avR}(\theta)==\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$$</p>
<p>无论我们是采用 $J_1$, $J_{av}V$, 还是 $J_{av}R$ 来表示优化目标，最终对 $θ$求导的梯度都可以表示为：</p>
<p>$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>具体的证明过程这里就不再列了，如果大家感兴趣，可以去看策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的附录1，里面有详细的证明。</p>
<p>当然我们还可以采用很多其他可能的优化目标来做梯度上升，此时我们的梯度式子里面的 $\nabla_\theta log\pi_\theta(s,a)$ 部分并不改变，变化的只是后面的 $Q_\pi(s,a)$ 部分。对于 $\nabla_\theta log\pi_\theta(s,a)$,我们一般称为<strong>分值函数</strong>(score function)。</p>
<p>现在梯度的式子已经有了，后面剩下的就是策略函数 $\pi_\theta(s,a)$的设计了。</p>
<h1 id="4-策略函数的设计">4. 策略函数的设计</h1>
<p>现在我们回头看一下策略函数 $\pi_\theta(s,a)$ 的设计，在前面它一直是一个数学符号。</p>
<p>最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征 $ϕ(s,a)$ 与参数 $θ$的线性组合来权衡一个行为发生的几率,即:</p>
<p>$$\pi_\theta(s,a)=\frac{e^{\phi(s,a)^T\theta}}{\sum_be^{\phi(s,b)^T\theta}}$$</p>
<p>则通过求导很容易求出对应的分值函数为：</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)=\phi(s,a)-\mathbb{E}_{\pi_\theta}[\phi(s,.)]$$</p>
<p>另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布 $\mathbb{N}(\phi(\mathrm{s})^{\mathbb{T}}\theta,\sigma^2)$中产生。高斯策略对应的分值函数求导可以得到为:</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)==\frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}$$</p>
<p>有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。</p>
<h1 id="5-蒙特卡罗策略梯度reinforce算法">5. 蒙特卡罗策略梯度reinforce算法</h1>
<p>这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数 $v(s)$ 来近似代替策略梯度公式里面的 $Q_π(s,a)$。算法的流程很简单，如下所示:</p>
<ul>
<li>输入：N个蒙特卡罗完整序列,训练步长 $α$</li>
<li>输出：策略函数的参数 $θ$
<ul>
<li>(1). for 每个蒙特卡罗序列:
<ul>
<li>a. 用蒙特卡罗法计算序列每个时间位置t的状态价值 $v_t$</li>
<li>b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数 $θ$：
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t$$</li>
</ul>
</li>
</ul>
</li>
<li>(2).返回策略函数的参数 $θ$</li>
</ul>
</li>
</ul>
<p>　　这里的策略函数可以是softmax策略，高斯策略或者其他策略。</p>
<h1 id="6-策略梯度实例">6. 策略梯度实例</h1>
<p>这里给出第5节的蒙特卡罗策略梯度reinforce算法的一个实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py</p>
<p>这里我们采用softmax策略作为我们的策略函数，同时，softmax的前置部分，也就是我们的策略模型用一个三层的softmax神经网络来表示。这样好处就是梯度的更新可以交给神经网络来做。</p>
<p>我们的softmax神经网络的结构如下，注意这个网络不是价值Q网络，而是策略网络：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_softmax_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_num&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_value&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># softmax layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#softmax output</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">all_act_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;act_prob&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意我们的损失函数是softmax交叉熵损失函数和状态价值函数的乘积，这样TensorFlow后面可以自动帮我们做梯度的迭代优化。</p>
<p>另一个要注意的点就是蒙特卡罗法里面价值函数的计算，一般是从后向前算，这样前面的价值的计算可以利用后面的价值作为中间结果，简化计算，对应代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">      <span class="n">running_add</span> <span class="o">=</span> <span class="n">running_add</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="n">discounted_ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分和之前的DQN的代码类似。</p>
<h1 id="7-策略梯度小结">7. 策略梯度小结</h1>
<p>策略梯度提供了和DQN之类的方法不同的新思路，但是我们上面的蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，我们需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，我们的参数更新的方向很可能不是策略梯度的最优方向。</p>
<p>因此，Policy Based的强化学习方法还需要改进，注意到我们之前有Value Based强化学习方法，那么两者能不能结合起来一起使用呢？下一篇我们讨论Policy Based+Value Based结合的策略梯度方法Actor-Critic。</p>
<p>　　　　</p>
]]></description></item><item><title>强化学习笔记 [12] | Dueling DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_12/</link><pubDate>Sun, 25 Feb 2024 11:16:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_12/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9797695.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十一) Prioritized Replay DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。</p>
<h1 id="1-dueling-dqn的优化点考虑">1. Dueling DQN的优化点考虑</h1>
<p>在前面讲到的DDQN中，我们通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，我们通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，我们尝试通过<font color=red>优化神经网络的结构</font>来优化算法。</p>
<p>具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态 $S$有关，与具体要采用的动作 $A$无关，这部分我们叫做<strong>价值函数部分</strong>，记做 $V(S,w,α)$,第二部分同时与状态状态 $S$ 和动作 $A$有关，这部分叫做**优势函数(Advantage Function)**部分,记为 $A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+A(S,A,w,\beta)$$</p>
<p>其中，$w$ 是公共部分的网络参数，而 $α$ 是价值函数独有部分的网络参数，而 $β$ 是优势函数独有部分的网络参数。</p>
<h1 id="2-dueling-dqn网络结构">2. Dueling DQN网络结构</h1>
<p>由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。为了简化算法描述，这里不使用原论文的CNN网络结构，而是使用前面文中用到的最简单的三层神经网络来描述。是否使用CNN对Dueling DQN算法本身无影响。</p>
<p>在前面讲到的DDQN等DQN算法中，我使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络与Dueling DQN</div>
</center>
<br>
<p>而在Dueling DQN中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。</p>
<p>我们可以直接使用上一节的价值函数的组合公式得到我们的动作价值，但是这个式子无法辨识最终输出里面 $V(S,w,α)$ 和 $A(S,A,w,β)$各自的作用，为了可以体现这种可辨识性(identifiability),实际使用的组合公式如下：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+(A(S,A,w,\beta)-\frac1{\mathcal{A}}\sum_{a^{\prime}\in\mathcal{A}}A(S,a^{\prime},w,\beta))$$</p>
<p>其实就是对优势函数部分做了中心化的处理。以上就是Dueling DQN的主要算法思路。由于它仅仅涉及神经网络的中间结构的改进，现有的DQN算法可以在使用Duel DQN网络结构的基础上继续使用现有的算法。由于算法主流程和其他算法没有差异，这里就不单独讲Duel DQN的算法流程了。</p>
<h1 id="3-dueling-dqn实例">3. Dueling DQN实例</h1>
<p>下面我们用一个具体的例子来演示Dueling DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>这个实例代基于Nature DQN，并将网络结构改为上图中右边的Dueling DQN网络结构，完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Dueling DQN和Nature DQN的代码的不同之处。也就是网络结构定义部分，主要的代码如下，一共有两个相同结构的Q网络，每个Q网络都有状态函数和优势函数的定义，以及组合后的Q网络输出，如代码红色部分：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W21</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b21</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W21</span><span class="p">)</span> <span class="o">+</span> <span class="n">b21</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W22</span><span class="p">)</span> <span class="o">+</span> <span class="n">b22</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2v</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2a</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分代码和Nature DQN基本相同。当然，我们可以也在前面DDQN，Prioritized Replay DQN代码的基础上，把网络结构改成上面的定义，这样Dueling DQN也可以起作用。</p>
<h1 id="4-dqn总结">4. DQN总结</h1>
<p>DQN系列我花了5篇来讲解，一共5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。</p>
<p>当然DQN家族的算法远远不止这些，还有一些其他的DQN算法我没有详细介绍，比如使用一些较复杂的CNN和RNN网络来提高DQN的表达能力，又比如改进探索状态空间的方法等，主要是在DQN的基础上持续优化。</p>
<p>DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。虽然NAF DQN可以解决这个问题，但是方法过于复杂了。而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题，从下一篇我们开始讨论Policy-Based深度强化学习。</p>
]]></description></item><item><title>强化学习笔记 [11] | Prioritized Replay DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_11/</link><pubDate>Sun, 25 Feb 2024 11:16:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_11/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay DQN。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Prioritized Replay DQN的论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<h1 id="1-prioritized-replay-dqn之前算法的问题">1. Prioritized Replay DQN之前算法的问题</h1>
<p>在Prioritized Replay DQN之前，我们已经讨论了很多种DQN，比如Nature DQN， DDQN等，他们都是通过经验回放来采样，进而做目标Q值的计算的。在采样的时候，我们是一视同仁，在经验回放池里面的所有的样本都有相同的被采样到的概率。</p>
<p>但是注意到在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，由于TD误差小，对反向梯度的计算影响不大。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。</p>
<p>这样如果TD误差的绝对值 $|δ(t)|$较大的样本更容易被采样，则我们的算法会比较容易收敛。下面我们看看Prioritized Replay DQN的算法思路。</p>
<h1 id="2-prioritized-replay-dqn算法的建模">2. Prioritized Replay DQN算法的建模</h1>
<p>Prioritized Replay DQN根据每个样本的TD误差绝对值 $|δ(t)|$，给定该样本的优先级正比于 $|δ(t)|$，将这个优先级的值存入经验回放池。回忆下之前的DQN算法，我们仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。</p>
<p>由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。</p>
<p>具体的SumTree树结构如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">sum_tree 结构图</div>
</center>
<br>
<p>所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。</p>
<p>这样保存有什么好处呢？主要是方便采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样，采样到哪个区间，就是哪个样本。比如我们采样到了26， 在（25-29）这个区间，那么就是第四个叶子节点被采样到。而注意到第三个叶子节点优先级最高，是12，它的区间13-25也是最长的，会比其他节点更容易被采样到。</p>
<p>如果要采样两个样本，我们可以在[0,21],[21,42]两个区间做均匀采样，方法和上面采样一个样本类似。</p>
<p>类似的采样算法思想我们在<a href="https://www.cnblogs.com/pinard/p/7249903.html"target="_blank" rel="external nofollow noopener noreferrer">word2vec原理(三) 基于Negative Sampling的模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第四节中也有讲到。</p>
<p>除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：</p>
<p>$$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>现在我们新的考虑了样本优先级的损失函数是</p>
<p>$$\frac1m\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>其中 $w_j$是第j个样本的优先级权重，由TD误差 $|δ(t)|$归一化得到。</p>
<p>第三个要注意的点就是当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。</p>
<p>除了以上三个部分，Prioritized Replay DQN和DDQN的算法流程相同。</p>
<h1 id="3-prioritized-replay-dqn算法流程">3. Prioritized Replay DQN算法流程</h1>
<p>下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，采样权重系数 $β$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$, SumTree的叶子节点数 $S$。</li>
<li>输出：Q网络参数。</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级 $p_j$为1。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S&rsquo;),is_end}$这个五元组存入SumTree</li>
<li>e) $S=S'$</li>
<li>f) 从SumTree中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，每个样本被采样的概率基于 $P(j)=\frac{p_j}{\sum_i(p_i)}$，损失函数权重 $w_j=(N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left\\{\begin{matrix}R_j&amp;is_end_j\textit{is true}\\\\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{is false}\end{matrix}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\begin{aligned}\frac{1}{m}\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2\end{aligned}$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 重新计算所有样本的TD误差 $\delta_j=y_j-Q(\phi(S_j),A_j,w)$，更新SumTree中所有节点的优先级 $p_j=|\delta_j|$</li>
<li>i) 如果i%C=1,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>j) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-prioritized-replay-ddqn算法流程">4. Prioritized Replay DDQN算法流程</h1>
<p>下面我们给出Prioritized Replay DDQN算法的实例代码。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 代码中的SumTree的结构和经验回放池的结构参考了morvanzhou的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py"target="_blank" rel="external nofollow noopener noreferrer">github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>这里重点讲下和第三节中算法描述不同的地方，主要是 $w_j$的计算。注意到：</p>
<p>$$w_j=\frac{(N<em>P(j))^{-\beta}}{\max_i(w_i)}=\frac{(N</em>P(j))^{-\beta}}{\max_i((N*P(i))^{-\beta})}=\frac{(P(j))^{-\beta}}{\max_i((P(i))^{-\beta})}=(\frac{P_j}{\min_iP(i)})^{-\beta}$$</p>
<p>因此代码里面$w_j$，即ISWeights的计算代码是这样的：</p>
<p><a href="javascript:void%280%29;"></a></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">pri_seg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span> <span class="o">/</span> <span class="n">n</span>       <span class="c1"># priority segment</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_increment_per_sampling</span><span class="p">])</span>  <span class="c1"># max = 1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">min_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">capacity</span><span class="p">:])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>     <span class="c1"># for later calculate ISweight</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">min_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_prob</span> <span class="o">=</span> <span class="mf">0.00001</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">get_leaf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prob</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>
</span></span><span class="line"><span class="cl">    <span class="n">ISWeights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prob</span><span class="o">/</span><span class="n">min_prob</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">b_memory</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上述代码的采样在第二节已经讲到。根据树的优先级的和total_p和采样数n，将要采样的区间划分为n段，每段来进行均匀采样，根据采样到的值落到的区间，决定被采样到的叶子节点。当我们拿到第i段的均匀采样值v以后，就可以去SumTree中找对应的叶子节点拿样本数据，样本叶子节点序号以及样本优先级了。代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree structure and array storage:
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree index:
</span></span></span><span class="line"><span class="cl"><span class="s2">        0         -&gt; storing priority sum
</span></span></span><span class="line"><span class="cl"><span class="s2">      / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">    1     2
</span></span></span><span class="line"><span class="cl"><span class="s2">    / \   / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">  3   4 5   6    -&gt; storing priority for transitions
</span></span></span><span class="line"><span class="cl"><span class="s2">  Array type for storing:
</span></span></span><span class="line"><span class="cl"><span class="s2">  [0,1,2,3,4,5,6]
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">parent_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>     <span class="c1"># the while loop is faster than the method in the reference code</span>
</span></span><span class="line"><span class="cl">    <span class="n">cl_idx</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">parent_idx</span> <span class="o">+</span> <span class="mi">1</span>         <span class="c1"># this leaf&#39;s left and right kids</span>
</span></span><span class="line"><span class="cl">    <span class="n">cr_idx</span> <span class="o">=</span> <span class="n">cl_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cl_idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">):</span>        <span class="c1"># reach bottom, end search</span>
</span></span><span class="line"><span class="cl">      <span class="n">leaf_idx</span> <span class="o">=</span> <span class="n">parent_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>       <span class="c1"># downward search, always search for a higher priority node</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cl_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cr_idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">data_idx</span> <span class="o">=</span> <span class="n">leaf_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">leaf_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">leaf_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了采样部分，要注意的就是当梯度更新完毕后，我们要去更新SumTree的权重，代码如下，注意叶子节点的权重更新后，要向上回溯，更新所有祖先节点的权重。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">batch_update</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">)</span>  <span class="c1"># update priority</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">batch_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">abs_errors</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>  <span class="c1"># convert to abs and avoid 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">clipped_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">abs_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs_err_upper</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">clipped_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">ps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ti</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">change</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># then propagate the change through tree</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">tree_idx</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>    <span class="c1"># this method is faster than the recursive loop in the reference code</span>
</span></span><span class="line"><span class="cl">      <span class="n">tree_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">change</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，和DDQN比，TensorFlow的网络结构流程中多了一个TD误差的计算节点，以及损失函数多了一个ISWeights系数。此外，区别不大。</p>
<h1 id="5-prioritized-replay-dqn小结">5. Prioritized Replay DQN小结</h1>
<p>Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。</p>
<p>下一篇我们讨论DQN家族的另一个优化算法Duel DQN，它将价值Q分解为两部分，第一部分是仅仅受状态但不受动作影响的部分，第二部分才是同时受状态和动作影响的部分，算法的效果也很好。</p>
]]></description></item><item><title>强化学习笔记 [10] | Double DQN (DDQN)</title><link>https://jianye0428.github.io/posts/rl_learning_note_10/</link><pubDate>Fri, 23 Feb 2024 13:17:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_10/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9756075.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（九）Deep Q-Learning进阶之Nature DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称DDQN）。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和DDQN的论文(Deep Reinforcement Learning with Double Q-learning)。</p>
<h1 id="1-dqn的目标q值计算问题">1. DQN的目标Q值计算问题</h1>
<p>在DDQN之前，基本上所有的目标Q值都是通过<strong>贪婪法</strong>直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算如下式:</p>
<p>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</p>
<h1 id="2-ddqn的算法建模">2. DDQN的算法建模</h1>
<p>DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。</p>
<p>在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：</p>
<p>$$y_j=R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})$$</p>
<p>在DDQN(Double DQN)这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即:</p>
<p>$$a^{max}(S_j^{\prime},w)=\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w)$$</p>
<p>然后利用这个选择出来的动作 $\begin{aligned}&amp;a^{max}(S_j^{\prime},w)\end{aligned}$ 在目标网络里面去计算目标Q值。即：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),a^{max}(S_j^{\prime},w),w^{\prime})$$</p>
<p>综合起来写就是：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})$$</p>
<p>除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。</p>
<h1 id="3-ddqn算法流程">3. DDQN算法流程</h1>
<p>这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本 $m$,目标Q网络参数更新频 $C$。</li>
<li>输出：Q网络参数</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;的参数 $w′=w$ 。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S′),is_end} $,这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$, 计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数w�</li>
<li>h) 如果 $i%C=1$,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-ddqn算法实例">4. DDQN算法实例　</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注DDQN和上一节的Nature DQN的代码的不同之处。代码只有一个地方不一样，就是计算目标Q值的时候，如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">current_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_action_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">current_Q_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">target_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">target_Q_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">max_action_next</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">target_Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而之前的Nature DQN这里的目标Q值计算是如下这样的：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，两个算法的代码完全相同。</p>
<h1 id="5-ddqn小结">5. DDQN小结</h1>
<p>DDQN算法出来以后，取得了比较好的效果，因此得到了比较广泛的应用。不过我们的DQN仍然有其他可以优化的点，如上一篇最后讲到的: 随机采样的方法好吗？按道理经验回放里不同样本的重要性是不一样的，TD误差大的样本重要程度应该高。针对这个问题，我们在下一节的Prioritised Replay DQN中讨论。</p>
]]></description></item><item><title>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_9/</link><pubDate>Fri, 23 Feb 2024 13:17:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_9/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9714655.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（八）价值函数的近似表示与Deep Q-Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Nature DQN的论文。</p>
<h1 id="1-dqnnips-2013的问题">1. DQN(NIPS 2013)的问题</h1>
<p>在上一篇我们已经讨论了DQN(NIPS 2013)的算法原理和代码实现，虽然它可以训练像CartPole这样的简单游戏，但是有很多问题。这里我们先讨论第一个问题。</p>
<p>注意到DQN(NIPS 2013)里面，我们使用的目标 $Q$值的计算方式：</p>
<p>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>这里目标Q值的计算使用到了当前要训练的Q网络参数来计算$Q(\phi(S_j^{\prime}),A_j^{\prime},w)$，而实际上，我们又希望通过 $y_j$来后续更新 $Q$网络参数。这样两者循环依赖，迭代起来两者的相关性就太强了。不利于算法的收敛。</p>
<p>因此，一个改进版的DQN: Nature DQN尝试<strong>用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系</strong>。下面我们来看看Nature DQN是怎么做的。</p>
<h1 id="2-nature-dqn的建模">2. Nature DQN的建模</h1>
<p>Nature DQN的两个Q网络分别命名为当前Q网络和目标Q网络。</p>
<p>Nature DQN使用了两个Q网络，一个<strong>当前Q网络</strong>$Q$用来选择动作，更新模型参数，另一个<strong>目标Q网络</strong> $Q&rsquo;$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。</p>
<p>要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。</p>
<p>Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。</p>
<h1 id="3-nature-dqn的算法流程">3. Nature DQN的算法流程</h1>
<p>下面我们来总结下Nature DQN的算法流程， 基于DQN NIPS 2015：</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率$C$。</p>
<p>输出：$Q$网络参数</p>
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S′),is_end\\}$这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，计算当前目标Q值 $y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果 $i%C=1$, 则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$ 需要随着迭代的进行而变小。</p>
<h1 id="4-nature-dqn算法实例">4. Nature DQN算法实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Nature DQN和上一节的NIPS 2013 DQN的代码的不同之处。</p>
<p>首先是Q网络，上一篇的DQN是一个三层的神经网络，而这里我们有两个一样的三层神经网络，一个是当前Q网络，一个是目标Q网络，网络的定义部分如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2t</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对于定期将目标Q网络的参数更新的代码如下面两部分：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">t_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;target_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">e_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;current_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;soft_replacement&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">t_params</span><span class="p">,</span> <span class="n">e_params</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_target_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># update target Q netowrk</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">REPLACE_TARGET_FREQ</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#print(&#39;episode &#39;+str(episode) +&#39;, target Q network params replaced!&#39;)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此外，注意下我们计算目标Q值的部分，这里使用的目标Q网络的参数，而不是当前Q网络的参数：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分基本和上一篇DQN的代码相同。这里给出我跑的某一次的结果:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 9.6
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.0
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 34.8
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 177.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 198.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 193.2
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>注意，由于DQN不保证稳定的收敛，所以每次跑的结果会不同，如果你跑的结果后面仍然收敛的不好，可以把代码多跑几次，选择一个最好的训练结果。</p>
<h1 id="5-nature-dqn总结">5. Nature DQN总结</h1>
<p>Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：</p>
<ul>
<li>1） 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？</li>
<li>2） 随机采样的方法好吗？按道理不同样本的重要性是不一样的。</li>
<li>3） Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？</li>
</ul>
<p>第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN，这三个DQN的改进版我们在下一篇来讨论。</p>
]]></description></item><item><title>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_8/</link><pubDate>Fri, 23 Feb 2024 13:17:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_8/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在强化学习系列的<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">前七篇<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。</p>
<p>Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。</p>
<h1 id="1-为何需要价值函数的近似表示">1. 为何需要价值函数的近似表示</h1>
<p>在之前讲到了强化学习求解方法，无论是动态规划DP，蒙特卡罗方法MC，还是时序差分TD，使用的状态都是离散的有限个状态集合 $S$。此时问题的规模比较小，比较容易求解。但是假如我们遇到复杂的状态集合呢？甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。　　　　</p>
<p>比如经典的冰球世界(PuckWorld)强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间步时长的一个大小固定的力，借此来改变大圆的速度。环境会在每一个时间步内告诉个体当前的水平与垂直坐标、当前的速度在水平和垂直方向上的分量以及目标的水平和垂直坐标共6项数据，奖励值为个体与目标两者中心距离的负数，也就是距离越大奖励值越低且最高奖励值为0。</p>
<p>在这个问题中，状态是一个6维的向量，并且是连续值。没法直接用之前离散集合的方法来描述状态。当然，你可以说，我们可以把连续特征离散化。比如把这个冰球场100x100的框按1x1的格子划分成10000个格子，那么对于运动员的坐标和冰球的坐标就有$10^4∗10^4=10^8$次种，如果再加上个体速度的分量就更是天文数字了，此时之前讲过的强化学习方法都会因为问题的规模太大而无法使用。怎么办呢？必须要对问题的建模做修改了，而价值函数的近似表示就是一个可行的方法。</p>
<h1 id="2-价值函数的近似表示方法">2. 价值函数的近似表示方法</h1>
<p>由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数 $\hat{v}$, 这个函数由参数 $w$ 描述，并接受状态 $s$ 作为输入，计算后得到状态 $s$ 的价值，即我们期望：</p>
<p>$$\hat{v}(s,w)\approx v_\pi(s)$$</p>
<p>类似的，引入一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到动作价值，即我们期望：</p>
<p>$$\hat{q}(s,a,w)\approx q_\pi(s,a)$$</p>
<p>价值函数近似的方法很多，比如最简单的线性表示法，用 $ϕ(s)$表示状态 $s$ 的特征向量，则此时我们的状态价值函数可以近似表示为：</p>
<p>$$\hat{v}(s,w)=\phi(s)^Tw$$</p>
<p>当然，除了线性表示法，我们还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。因此后面我们的近似表达方法如果没有特别提到，都是指的神经网络的近似表示。</p>
<p>对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络拟合价值函数</div>
</center>
<br>
<p>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值 $\hat{v}(s,w)$。对于动作价值函数，有两种方法，一种是输入状态 $s$ 的特征向量和动作 $a$，输出对应的动作价值 $\hat{q}(s,a,w)$，另一种是只输入状态 $s$ 的特征向量，动作集合有多少个动作就有多少个输出 $\hat{q}(s,ai,w)$。这里隐含了我们的动作是有限个的离散动作。</p>
<p>对于我们前一篇讲到的Q-Learning算法，我们现在就价值函数的近似表示来将其改造，采用上面右边的第三幅图的动作价值函数建模思路来做，现在我们叫它Deep Q-Learning。</p>
<h1 id="3-deep-q-learning算法思路">3. Deep Q-Learning算法思路</h1>
<p>Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值s和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p>
<p>DQN的输入是我们的状态s对应的状态向量 $ϕ(s)$， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p>
<p>DQN主要使用的技巧是经验回放(experience replay), 即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是<strong>经验回放</strong>。</p>
<p>通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数 $w$，当 $w$ 收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。</p>
<p>下面我们总结下DQN的算法流程，基于NIPS 2013 DQN。　　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, Q网络结构, 批量梯度下降的样本数 $m$。</li>
<li>输出：Q网络参数
<ul>
<li>
<ol>
<li>随机初始化$Q$网络的所有参数 $w$，基于 $w$初始化所有的状态和动作对应的价值 $Q$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S&rsquo;),is_end\\}$这个五元组存入经验回放集合D</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\mathrm{~}is\mathrm{~}true\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\mathrm{~}is\mathrm{~}false\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{i=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的 $f$步和 $g$步的 $Q$值计算也都需要通过 $Q$网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-deep-q-learning实例">4. Deep Q-Learning实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。这里使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码参考了知乎上的一个<a href="https://zhuanlan.zhihu.com/p/21477488"target="_blank" rel="external nofollow noopener noreferrer">DQN实例<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，修改了代码中的一些错误，并用最新的Python3.6+Tensorflow1.8.0运行。要跑代码需要安装OpenAI的Gym库，使用<code>pip install gym</code>即可。</p>
<p>代码使用了一个三层的神经网络，输入层，一个隐藏层和一个输出层。下面我们看看关键部分的代码。</p>
<p>算法第2步的步骤b通过$ϵ−$贪婪法选择动作的代码如下，注意每次我们$ϵ−$贪婪法后都会减小$ϵ$值。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">egreedy_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:[</span><span class="n">state</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">})[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤c在状态S�执行当前动作A�的代码如下，这个交互是由Gym完成的。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Define reward for agent</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mf">0.1</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤d保存经验回放数据的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">perceive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span><span class="n">one_hot_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">REPLAY_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤f,g计算目标Q值，并更新Q网络的代码如下：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 1: obtain random minibatch from replay memory</span>
</span></span><span class="line"><span class="cl">  <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">y_input</span><span class="p">:</span><span class="n">y_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">action_input</span><span class="p">:</span><span class="n">action_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">state_batch</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们在每100轮迭代完后会去玩10次交互测试，计算10次的平均奖励。运行了代码后，我的3000轮迭代的输出如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 12.2
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.4
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 10.4
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.5
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 11.6
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 12.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 29.6
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 48.1
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 85.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 169.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>大概到第1000次迭代后，算法已经收敛，达到最高的200分。当然由于是$ϵ−$探索，每次前面的输出可能不同，但最后应该都可以收敛到200的分数。当然由于DQN不保证绝对的收敛，所以可能到了200分后还会有抖动。</p>
<h1 id="5-deep-q-learning小结">5. Deep Q-Learning小结　　　　</h1>
<p>DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。但是DQN有个问题，就是它并不一定能保证Q网络的收敛，也就是说，我们不一定可以得到收敛后的Q网络参数。这会导致我们训练出的模型效果很差。</p>
<p>针对这个问题，衍生出了DQN的很多变种，比如Nature DQN(NIPS 2015), Double DQN，Dueling DQN等。这些我们在下一篇讨论。</p>
]]></description></item><item><title>强化学习笔记 [7] | 时序差分离线控制算法Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_7/</link><pubDate>Fri, 23 Feb 2024 13:17:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_7/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。</p>
<p>Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-q-learning算法的引入">1. Q-Learning算法的引入　　　　</h1>
<p>Q-Learning算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解<u>不需要环境的状态转化模型</u>，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新策略，通过策略来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，比如我们上一篇讲到的SARSA, 而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。这一类的经典算法就是Q-Learning。</p>
<p>对于Q-Learning，我们会使用 $ϵ−$贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的 $ϵ−$贪婪法。这一点就是SARSA和Q-Learning本质的区别。</p>
<h1 id="2-q-learning算法概述">2. Q-Learning算法概述</h1>
<p>Q-Learning算法的拓扑图如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Q Learning 拓扑图</div>
</center>
<br>
<p>首先我们基于状态 $S$，用 $ϵ−$贪婪法选择到动作 $A$, 然后执行动作$A$，得到奖励 $R$，并进入状态 $S&rsquo;$，此时，如果是SARSA，会继续基于状态 $S&rsquo;$，用 $ϵ−$贪婪法选择 $A&rsquo;$,然后来更新价值函数。但是Q-Learning则不同。</p>
<p>对于Q-Learning，它基于状态 $S&rsquo;$，没有使用 $ϵ−$贪婪法选择 $A$，而是使用贪婪法选择 $A&rsquo;$，也就是说，选择使 $Q(S&rsquo;,a)$ 最大的 $a$ 作为 $A&rsquo;$来更新价值函数。用数学公式表示就是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</p>
<p>对应到上图中就是在图下方的三个黑圆圈动作中选择一个使 $Q(S&rsquo;,a)$最大的动作作为 $A&rsquo;$。</p>
<p>此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态 $S&rsquo;$，用 $ϵ−$贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的 $A&rsquo;$ 会作为下一阶段开始时候的执行动作。</p>
<p>下面我们对Q-Learning算法做一个总结。</p>
<h1 id="3-q-learning算法流程">3. Q-Learning算法流程</h1>
<p>下面我们总结下Q-Learning算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出: 所有的状态和动作对应的价值 $Q$
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。</li>
<li>b) 用 $ϵ−$贪婪法在当前状态 $S$ 选择出动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$和奖励 $R$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S'$</li>
<li>f) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-q-learning算法实例windy-gridworld">4. Q-Learning算法实例：Windy GridWorld</h1>
<p>我们还是使用和SARSA一样的例子来研究Q-Learning。如果对windy gridworld的问题还不熟悉，可以复习<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第4节的第二段。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>绝大部分代码和SARSA是类似的。这里我们可以重点比较和SARSA不同的部分。区别都在<code>episode()</code>这个函数里面。</p>
<p>首先是初始化的时候，我们只初始化状态 $S$,把 $A$ 的产生放到了while循环里面, 而回忆下SARSA会同时初始化状态 $S$ 和动作 $A$，再去执行循环。下面这段Q-Learning的代码对应我们算法的第二步步骤a和b：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># play for an episode</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">episode</span><span class="p">(</span><span class="n">q_value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># track the total time steps in this episode</span>
</span></span><span class="line"><span class="cl">  <span class="n">time</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="n">state</span> <span class="o">!=</span> <span class="n">GOAL</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>接着我们会去执行动作 $A$,得到 $S&rsquo;$， 由于奖励不是终止就是-1，不需要单独计算。,这部分和SARSA的代码相同。对应我们Q-Learning算法的第二步步骤c：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>后面我们用贪婪法选择出最大的 $Q(S&rsquo;,a)$,并更新价值函数，最后更新当前状态 $S$。对应我们Q-Learning算法的第二步步骤d,e。注意SARSA这里是使用ϵ−�−贪婪法，而不是贪婪法。同时SARSA会同时更新状态S�和动作A�,而Q-Learning只会更新当前状态S�。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span></span></span></code></pre></td></tr></table>
</div>
</div><p>跑完完整的代码，大家可以很容易得到这个问题的最优解，进而得到在每个格子里的最优贪婪策略。</p>
<h1 id="5-sarsa-vs-q-learning">5. SARSA vs Q-Learning</h1>
<p>现在SARSA和Q-Learning算法我们都讲完了，那么作为时序差分控制算法的两种经典方法吗，他们都有说明特点，各自适用于什么样的场景呢？</p>
<p>Q-Learning直接学习的是 <font color=red>最优策略</font>，而SARSA<font color=red>在学习最优策略的同时还在做探索</font>。这导致我们在学习最优策略的时候，如果用SARSA，为了保证收敛，需要制定一个策略，使 $ϵ−$贪婪法的超参数 $ϵ$在迭代的过程中逐渐变小。Q-Learning没有这个烦恼。</p>
<p>另外一个就是Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以<font color=red>受样本数据的影响较大</font>，因此受到训练数据方差的影响很大，甚至会影响Q函数的收敛。Q-Learning的深度强化学习版Deep Q-Learning也有这个问题。</p>
<p>在学习过程中，SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进，导致出现像Q-Learning可能遇到一些特殊的最优“陷阱”。比如经典的强化学习问题&quot;Cliff Walk&quot;。</p>
<p>在实际应用中，如果我们是在模拟环境中训练强化学习模型，推荐使用Q-Learning，如果是 <strong><font color=red>在线生产环境</font></strong> 中训练模型，则推荐使用 <strong><font color=red>SARSA</font></strong>。</p>
<h1 id="6-q-learning结语">6. Q-Learning结语　　　　　　　　</h1>
<p>对于Q-Learning和SARSA这样的时序差分算法，对于小型的强化学习问题是非常灵活有效的，但是在大数据时代，异常复杂的状态和可选动作，使Q-Learning和SARSA要维护的Q表异常的大，甚至远远超出内存，这限制了时序差分算法的应用场景。在深度学习兴起后，基于深度学习的强化学习开始占主导地位，因此从下一篇开始我们开始讨论深度强化学习的建模思路。</p>
]]></description></item><item><title>RL学习笔记 [5] | 用时序差分法（TD）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_5/</link><pubDate>Thu, 22 Feb 2024 17:25:21 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_5/</guid><description><![CDATA[<h1 id="0-引言">0 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化学习问题的方法：时序差分(Temporal-Difference, TD)。</p>
<p>时序差分这一篇对应Sutton书的第六章部分和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-时序差分td简介">1. 时序差分TD简介</h1>
<p>时序差分法和蒙特卡罗法类似，都是<strong>不基于模型的强化学习问题</strong>求解方法。所以在上一篇定义的不基于模型的强化学习控制问题和预测问题的定义，在这里仍然适用。</p>
<p>预测问题：即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
<p>控制问题：也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
<p>回顾蒙特卡罗法中计算状态收获的方法是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>而对于时序差分法来说，我们没有完整的状态序列，只有部分的状态序列，那么如何可以近似求出某个状态的收获呢？回顾<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中的贝尔曼方程：</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s)$$</p>
<p>这启发我们可以用 $R_{t+1}+\gamma v(S_{t+1})$ 来近似的代替收获 $G_t$,一般我们把 $R_{t+1}+\gamma V(S_{t+1})$ 称为TD目标值。$R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 称为TD误差，将用TD目标值近似代替收获 $G(t)$ 的过程称为引导(bootstrapping)。这样我们只需要两个连续的状态与对应的奖励，就可以尝试求解强化学习问题了。</p>
<p>现在我们有了自己的近似收获 $G_t$ 的表达式，那么就可以去求解时序差分的预测问题和控制问题了。</p>
<h1 id="2-时序差分td的预测问题求解">2. 时序差分TD的预测问题求解</h1>
<p>时序差分的预测问题求解和蒙特卡罗法类似，但是主要有两个不同点。一是收获 $G_t$ 的表达式不同，时序差分 $G(t)$ 的表达式为：</p>
<p>$$G(t)=R_{t+1}+\gamma V(S_{t+1})$$</p>
<p>二是迭代的式子系数稍有不同，回顾蒙特卡罗法的迭代式子是：</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>由于在时序差分我们没有完整的序列，也就没有对应的次数 $N(S_t)$ ,一般就用一个[0,1]的系数 $α$ 代替。这样时序差分的价值函数迭代式子是：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t)) \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t)) $$</p>
<p>这里我们用一个简单的例子来看看蒙特卡罗法和时序差分法求解预测问题的不同。</p>
<p>假设我们的强化学习问题有A,B两个状态，模型未知，不涉及策略和行为。只涉及状态转化和即时奖励。一共有8个完整的状态序列如下：</p>
<p>　　① A,0,B,0 ②B,1 ③B,1 ④ B,1 ⑤ B,1 ⑥B,1 ⑦B,1 ⑧B,0</p>
<p>只有第一个状态序列是有状态转移的，其余7个只有一个状态。设置衰减因子 $γ=1$。</p>
<p>首先我们按蒙特卡罗法来求解预测问题。由于只有第一个序列中包含状态A，因此A的价值仅能通过第一个序列来计算，也就等同于计算该序列中状态A的收获：</p>
<p>$$V(A)=G(A)=R_A+\gamma R_B=0$$</p>
<p>对于B，则需要对其在8个序列中的收获值来平均，其结果是6/8。</p>
<p><strong>再来看看时序差分法求解的过程</strong>。其收获是在计算状态序列中某状态价值时是应用其后续状态的预估价值来计算的，对于B来说，它总是终止状态，没有后续状态，因此它的价值直接用其在8个序列中的收获值来平均，其结果是6/8。</p>
<p>对于A，只在第一个序列出现，它的价值为：</p>
<p>$$V(A)=R_A+\gamma V(B)=\frac68$$</p>
<p>从上面的例子我们也可以看到蒙特卡罗法和时序差分法求解预测问题的区别。</p>
<p>一是时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。</p>
<p>二是时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。</p>
<p>三是虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。</p>
<p>从上面的描述可以看出时序差分法的优势比较大，因此现在主流的强化学习求解方法都是基于时序差分的。后面的文章也会主要基于时序差分法来扩展讨论。</p>
<h1 id="3-n步时序差分">3. n步时序差分</h1>
<p>在第二节的时序差分法中，我们使用了用 $R_{t+1}+\gamma v(S_{t+1})$ 来近似的代替收获 $G_t$。即向前一步来近似我们的收获 $G_{t}$,那么能不能向前两步呢？当然可以，这时我们的收获 $G_t$ 的近似表达式为：</p>
<p>$$G_t^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2})$$</p>
<p>从两步，到三步，再到n步，我们可以归纳出n步时序差分收获 $G^{(n)}_t$表达式为：$$G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})$$</p>
<p>当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了。</p>
<p>对于n步时序差分来说，和普通的时序差分的区别就在于收获的计算方式的差异。那么既然有这个n步的说法，那么n到底是多少步好呢？如何衡量n的好坏呢？我们在下一节讨论。</p>
<h1 id="4-tdλ">4. TD(λ)</h1>
<p>n步时序差分选择多少步数作为一个较优的计算参数是需要尝试的超参数调优问题。为了能在不增加计算复杂度的情况下综合考虑所有步数的预测，我们引入了一个新[0,1]的参数 $\lambda$,定义入—收获是 $n$ 从 $1$ 到 $\infty$ 所有步的收获乘以权重的和。每一步的权重是 $(1-\lambda)\lambda^{n-1}$,这样 $\lambda-$收获的计算公式表示为:</p>
<p>$$G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}$$</p>
<p>进而我们可以得到 $TD(λ)$ 的价值函数的迭代公式：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t^\lambda-V(S_t)) \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t^\lambda-Q(S_t,A_t)) $$</p>
<p>每一步收获的权重定义为 $(1−λ)λ^{n−1}$ 的原因是什么呢？其图像如下图所示，可以看到随着n的增大，其第n步收获的权重呈几何级数的衰减。当在T时刻到达终止状态时，未分配的权重全部给予终止状态的实际收获值。这样可以使一个完整的状态序列中所有的n步收获的权重加起来为1，离当前状态越远的收获其权重越小。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">TD(λ)</div>
</center>
<br>
<p>从前向来看 $TD(λ)$， 一个状态的价值 $V(St)$由 $G_t$得到，而Gt��又间接由所有后续状态价值计算得到，因此可以认为更新一个状态的价值需要知道所有后续状态的价值。也就是说，必须要经历完整的状态序列获得包括终止状态的每一个状态的即时奖励才能更新当前状态的价值。这和蒙特卡罗法的要求一样，因此TD(λ)��(�)有着和蒙特卡罗法一样的劣势。当 $λ=0$ 时,就是第二节讲到的普通的时序差分法，当 $λ=1$ 时,就是蒙特卡罗法。</p>
<p>从反向来看 $TD(λ)$，它可以分析我们状态对后续状态的影响。比如老鼠在依次连续接受了3 次响铃和1 次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为频率启发(frequency heuristic) 式；而把电击归因于最近少数几次状态的影响，则称为就近启发(recency heuristic) 式。</p>
<p>如果给每一个状态引入一个数值：效用(eligibility, E) 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发。而所有状态的效用值总称为效用迹(eligibility traces,ES)。定义为：</p>
<p>$$ E_0(s)=0 \\\\ \left.E_t(s)=\gamma\lambda E_{t-1}(s)+1(S_t=s)=\left\\{\begin{array}{ll}0&amp;t&lt;k\\\\(\gamma\lambda)^{t-k}&amp;t\geq k\end{array}\right.\right.,\quad s.t.\quad\lambda,\gamma\in[0,1],s\textit{ is visited once at time k}$$</p>
<p>此时我们$TD(λ)$的价值函数更新式子可以表示为：</p>
<p>$$\delta_t=R_{t+1}+\gamma v(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(s)$$</p>
<p>也许有人会问，这前向的式子和反向的式子看起来不同啊，是不是不同的逻辑呢？其实两者是等价的。现在我们从前向推导一下反向的更新式子。</p>
<p>$$\begin{aligned}
G_t^\lambda-V(S_t)&amp; =-V(S_t)+(1-\lambda)\lambda^0(R_{t+1}+\gamma V(S_{t+1})) &amp;&amp; \text{(1)}  \\\\
&amp;+(1-\lambda)\lambda^1(R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2}))&amp;&amp; (2)  \\\\
&amp;+(1-\lambda)\lambda^2(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3V(S_{t+3}))&amp;&amp; (3)  \\\\
&amp;+\ldots &amp;&amp; \text{(4)}  \\\\
&amp;=-V(S_t)+(\gamma\lambda)^0(R_{t+1}+\gamma V(S_{t+1})-\gamma\lambda V(S_{t+1}))&amp;&amp; (5)  \\\\
&amp;+(\gamma\lambda)^1(R_{t+2}+\gamma V(S_{t+2})-\gamma\lambda V(S_{t+2}))&amp;&amp; \text{(6)}  \\\\
&amp;+(\gamma\lambda)^2(R_{t+3}+\gamma V(S_{t+3})-\gamma\lambda V(S_{t+3}))&amp;&amp; \text{(7)}  \\\\
&amp;\begin{array}{c}+\ldots\end{array}&amp;&amp; \text{(8)}  \\\\
&amp;=(\gamma\lambda)^0(R_{t+1}+\gamma V(S_{t+1})-V(S_t))&amp;&amp; \left(9\right)  \\\\
&amp;+(\gamma\lambda)^1(R_{t+2}+\gamma V(S_{t+2})-V(S_{t+1}))&amp;&amp; \text{(10)}  \\\\
&amp;+(\gamma\lambda)^2(R_{t+3}+\gamma V(S_{t+3})-V(S_{t+2}))&amp;&amp; (11)  \\\\
&amp;\begin{array}{c}+\ldots\end{array}&amp;&amp; (12)  \\\\
&amp;=\delta_t+\gamma\lambda\delta_{t+1}+(\gamma\lambda)^2\delta_{t+2}+\ldots &amp;&amp; (13)
\end{aligned}$$</p>
<p>可以看出前向TD误差和反向的TD误差实际上一致的。</p>
<h1 id="5-时序差分的控制问题求解">5. 时序差分的控制问题求解</h1>
<p>现在我们回到普通的时序差分，来看看它控制问题的求解方法。回想上一篇蒙特卡罗法在线控制的方法，我们使用的是$ϵ−$贪婪法来做价值迭代。对于时序差分，我们也可以用$ϵ−$贪婪法来价值迭代，和蒙特卡罗法在线控制的区别主要只是在于收获的计算方式不同。时序差分的在线控制(on-policy)算法最常见的是SARSA算法，我们在下一篇单独讲解。</p>
<p>而除了在线控制，我们还可以做离线控制(off-policy)，离线控制和在线控制的区别主要在于在线控制一般只有一个策略(最常见的是$ϵ−$贪婪法)。而离线控制一般有两个策略，其中一个策略(最常见的是$ϵ−$贪婪法)用于选择新的动作，另一个策略(最常见的是贪婪法)用于更新价值函数。时序差分的离线控制算法最常见的是Q-Learning算法，我们在下下篇单独讲解。</p>
<h1 id="6-时序差分小结">6. 时序差分小结</h1>
<p>时序差分和蒙特卡罗法比它更加灵活，学习能力更强，因此是目前主流的强化学习求解问题的方法，现在绝大部分强化学习乃至深度强化学习的求解都是以时序差分的思想为基础的。因此后面我们会重点讨论。</p>
<p>下一篇我们会讨论时序差分的在线控制算法SARSA。</p>
]]></description></item><item><title>RL学习笔记 [6] | 时序差分在线控制算法SARSA</title><link>https://jianye0428.github.io/posts/rl_learning_note_6/</link><pubDate>Thu, 22 Feb 2024 16:29:33 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_6/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。</p>
<p>SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-sarsa算法的引入">1. SARSA算法的引入</h1>
<p>SARSA算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解不需要环境的状态转化模型，是<strong>不基于模型的强化学习问题</strong>求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作。而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。</p>
<p>我们的SARSA算法，属于在线控制这一类，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是 $ϵ−$贪婪法，在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们对于 $ϵ−$贪婪法有详细讲解，即通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 m 个可选行为中选择行为。用公式可以表示为：</p>
<p>$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</p>
<p>π(a|s)={ϵ/m+1−ϵifa∗=argmaxa∈AQ(s,a)ϵ/melse�(�|�)={�/�+1−����∗=arg⁡max�∈��(�,�)�/�����</p>
<h1 id="2-sarsa算法概述">2. SARSA算法概述</h1>
<p>作为SARSA算法的名字本身来说，它实际上是由 $S,A,R,S,A$ 几个字母组成的。而 $S,A,R$ 分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">SARSA Transition</div>
</center>
<br>
<p>在迭代的时候，我们首先基于 $ϵ−$贪婪法在当前状态 $S$ 选择一个动作 $A$ ，这样系统会转到一个新的状态 $S′$, 同时给我们一个即时奖励 $R$ , 在新的状态 $S′$，我们会基于 $ϵ−$贪婪法在状态 $S′$ 选择一个动作 $A′$，但是注意这时候我们并不执行这个动作 $A′$，只是用来更新的我们的价值函数，价值函数的更新公式是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</p>
<p>其中，$γ$ 是衰减因子，$α$ 是迭代步长。这里和蒙特卡罗法求解在线控制问题的迭代公式的区别主要是，收获 $G_t$的表达式不同，对于时序差分，收获 $G_t$的表达式是 $R+\gamma Q(S&rsquo;,A&rsquo;)$ 。这个价值函数更新的贝尔曼公式我们在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节有详细讲到。</p>
<p>除了收获 $G_t$的表达式不同，SARSA算法和蒙特卡罗在线控制算法基本类似。</p>
<h1 id="3-sarsa算法流程">3. SARSA算法流程</h1>
<p>下面我们总结下SARSA算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出：所有的状态和动作对应的价值 $Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。设置 $A$ 为 $ϵ−$贪婪法在当前状态$S$ 选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$ ,得到新状态 $S′$ 和 奖励 $R$</li>
<li>c) 用 $\epsilon-$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S′$, $A=A′$</li>
<li>f) 如果 $S′$ 是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>这里有一个要注意的是，步长 $α$一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数 $Q$ 可以收敛。当 $Q$ 收敛时，我们的策略 $ϵ−$贪婪法也就收敛了。</p>
<h1 id="4-sarsa算法实例windy-gridworld">4. SARSA算法实例：Windy GridWorld</h1>
<p>下面我们用一个著名的实例Windy GridWorld来研究SARSA算法。</p>
<p>如下图一个10×7的长方形格子世界，标记有一个起始位置 S 和一个终止目标位置 G，格子下方的数字表示对应的列中一定强度的风。当个体进入该列的某个格子时，会按图中箭头所示的方向自动移动数字表示的格数，借此来模拟世界中风的作用。同样格子世界是有边界的，个体任意时刻只能处在世界内部的一个格子中。个体并不清楚这个世界的构造以及有风，也就是说它不知道格子是长方形的，也不知道边界在哪里，也不知道自己在里面移动移步后下一个格子与之前格子的相对位置关系，当然它也不清楚起始位置、终止目标的具体位置。但是个体会记住曾经经过的格子，下次在进入这个格子时，它能准确的辨认出这个格子曾经什么时候来过。格子可以执行的行为是朝上、下、左、右移动一步，每移动一步只要不是进入目标位置都给予一个 -1 的惩罚，直至进入目标位置后获得奖励 0 同时永久停留在该位置。现在要求解的问题是个体应该遵循怎样的策略才能尽快的从起始位置到达目标位置。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Windy GridWorld</div>
</center>
<br>
<p>逻辑并不复杂，完整的代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">我的github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。这里我主要看一下关键部分的代码。</p>
<p>算法中第2步步骤a,初始化 $S$,使用 $ϵ−$贪婪法在当前状态 $S$ 选择的动作的过程：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤b,在状态S�执行当前动作A�,得到新状态S′�′的过程，由于奖励不是终止就是-1，不需要单独计算：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤c,用 $ϵ−$贪婪法在状态 $S&rsquo;$选择新的动作 $A′$的过程：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤d,e, 更新价值函数 $Q(S,A)$ 以及更新当前状态动作的过程：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">  <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl"><span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span></span></span></code></pre></td></tr></table>
</div>
</div><p>代码很简单，相信大家对照算法，跑跑代码，可以很容易得到这个问题的最优解，进而搞清楚SARSA算法的整个流程。</p>
<h1 id="5-sarsaλ">5. SARSA(λ)</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讲到了多步时序差分 $TD(λ)$ 的价值函数迭代方法，那么同样的，对应的多步时序差分在线控制算法，就是我们的 $SARSA(λ)$。</p>
<p>$TD(\lambda)$有前向和后向两种价值函数迭代方式，当然它们是等价的。在控制问题的求解时，基于反向认识的 $SARSA(\lambda)$算法将可以有效地在线学习，数据学习完即可丢弃。因此 $SARSA(\lambda)$算法默认都是基于反向来进行价值函数迭代。</p>
<p>在上一篇我们讲到了$TD(\lambda)$状态价值函数的反向迭代，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(S)\end{gathered}$$</p>
<p>对应的动作价值函数的迭代公式可以找样写出，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\\\Q(S_t,A_t)=Q(S_t,A_t)+\alpha\delta_tE_t(S,A)\end{gathered}$$</p>
<p>除了状态价值函数 $Q(S,A)$ 的更新方式，多步参数 $λ$ 以及反向认识引入的效用迹 $E(S,A)$ ，其余算法思想和 $SARSA$ 类似。这里我们总结下 $SARSA(λ)$的算法流程。　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率$ϵ$, 多步参数$λ$</li>
<li>输出：所有的状态和动作对应的价值$Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 对于终止状态其 $Q$值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化所有状态动作的效用迹 $E$ 为0，初始化S为当前状态序列的第一个状态。设置$A$为 $ϵ−$贪婪法在当前状态 $S$选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>c) 用$ϵ−$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新效用迹函数 $E(S,A)$和TD误差 $δ$:
<ul>
<li>$$\begin{gathered}E(S,A)=E(S,A)+1\\\\\delta=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\end{gathered}$$</li>
</ul>
</li>
<li>e) 对当前序列所有出现的状态s和对应动作 $a$, 更新价值函数 $Q(s,a)$和效用迹函数 $E(s,a)$:
<ul>
<li>$$\begin{gathered}Q(s,a)=Q(s,a)+\alpha\delta E(s,a)\\\\E(s,a)=\gamma\lambda E(s,a)\end{gathered}$$</li>
</ul>
</li>
<li>f) $S=S&rsquo;$, $A=A'$</li>
<li>g) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>对于步长$α$，和SARSA一样，一般也需要随着迭代的进行逐渐变小才能保证动作价值函数$Q$收敛。</p>
<h1 id="6-sarsa小结">6. SARSA小结</h1>
<p>SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和蒙特卡罗法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。</p>
<p>但是SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在 SARSA 算法中，$Q(S,A)$ 的值使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。</p>
<p>下一篇我们讨论SARSA的姊妹算法，时序差分离线控制算法Q-Learning。</p>
]]></description></item><item><title>RL学习笔记 [4] | 用蒙特卡罗法（MC）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_4/</link><pubDate>Thu, 22 Feb 2024 13:00:24 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_4/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（三）用动态规划（DP）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型 $P$ 都无法知道，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？本文要讨论的蒙特卡罗(Monte-Calo, MC)就是一种可行的方法。</p>
<p>蒙特卡罗法这一篇对应Sutton书的第五章和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-不基于模型的强化学习问题定义">1. 不基于模型的强化学习问题定义</h1>
<p>在动态规划法中，强化学习的两个问题是这样定义的:</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的6个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>可见, 模型状态转化概率矩阵 $P$ 始终是已知的，即MDP已知，对于这样的强化学习问题，我们一般称为<mark>基于模型的强化学习</mark>问题。</p>
<p>不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵 $P$ ，这时如果仍然需要我们求解强化学习问题，那么这就是<mark>不基于模型的强化学习</mark>问题了。它的两个问题一般的定义是：</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$ , 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>本文要讨论的蒙特卡罗法就是上述不基于模型的强化学习问题。</p>
<h1 id="2-蒙特卡罗法求解特点">2. 蒙特卡罗法求解特点</h1>
<p>蒙特卡罗这个词之前的博文也讨论过，尤其是在之前的<a href="https://www.cnblogs.com/pinard/p/MCMC%28%e4%b8%80%29%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%96%b9%e6%b3%95"target="_blank" rel="external nofollow noopener noreferrer">MCMC系列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中。它是一种通过采样近似求解问题的方法。这里的蒙特卡罗法虽然和MCMC不同，但是采样的思路还是一致的。那么如何采样呢？</p>
<p>蒙特卡罗法通过采样若干经历完整的状态序列(episode)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。</p>
<p>从特卡罗法法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。</p>
<h1 id="3-蒙特卡罗法求解强化学习预测问题">3. 蒙特卡罗法求解强化学习预测问题</h1>
<p>这里我们先来讨论蒙特卡罗法求解强化学习预测问题的方法，即策略评估。一个给定策略 $π$ 的完整有T个状态的状态序列如下：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</p>
<p>回忆下<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中对于价值函数 $v_π(s)$的定义:</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(G_t|S_t=s)=\mathbb{E}_\pi(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t=s)$$</p>
<p>可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解，也就是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>$$v_\pi(s)\approx average(G_t),s.t.S_t=s$$</p>
<p>可以看出，预测问题的求解思路还是很简单的。不过有几个点可以优化考虑。</p>
<ul>
<li>
<p>第一个点是: 同样一个状态可能在一个完整的状态序列中重复出现，那么该状态的收获该如何计算？有两种解决方法。第一种是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：首次访问(first visit) 和每次访问(every visit) 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p>
</li>
<li>
<p>第二个点是累进更新平均值(incremental mean)。在上面预测问题的求解公式里，我们有一个average的公式，意味着要保存所有该状态的收获值之和最后取平均。这样浪费了太多的存储空间。一个较好的方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。通过下面的公式就很容易理解这个过程：</p>
</li>
</ul>
<p>$$\mu_k=\frac1k\sum_{j=1}^kx_j=\frac1k(x_k+\sum_{j=1}^{k-1}x_j)=\frac1k(x_k+(k-1)\mu_{k-1})=\mu_{k-1}+\frac1k(x_k-\mu_{k-1})$$</p>
<p>这样上面的状态价值公式就可以改写成：</p>
<p>$$N(S_t)=N(S_t)+1$$</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>这样我们无论数据量是多还是少，算法需要的内存基本是固定的 。</p>
<p>有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数 $N(S_t)$,这时我们可以用一个系数 $α$ 来代替，即：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t))$$</p>
<p>对于动作价值函数 $Q(S_t,A_t)$,也是类似的，比如对上面最后一个式子，动作价值函数版本为：</p>
<p>$$Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t))$$</p>
<p>以上就是蒙特卡罗法求解预测问题的整个过程，下面我们来看控制问题求解。</p>
<h1 id="4-蒙特卡罗法求解强化学习控制问题">4. 蒙特卡罗法求解强化学习控制问题</h1>
<p>蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。回忆下动态规划价值迭代的的思路， 每轮迭代先做策略评估，计算出价值 $v_k(s)$ ，然后基于据一定的方法（比如贪婪法）更新当前策略 $π$。最后得到最优价值函数 $v∗$ 和最优策略 $π∗$。</p>
<p>和动态规划比，蒙特卡罗法不同之处体现在三点:</p>
<ul>
<li>一是预测问题策略评估的方法不同，这个第三节已经讲了。</li>
<li>第二是蒙特卡罗法一般是优化最优动作价值函数 $q∗$，而不是状态价值函数 $v∗$。</li>
<li>三是动态规划一般基于贪婪法更新策略。而蒙特卡罗法一般采用 $ϵ−$贪婪法更新。这个 $ϵ$ 就是我们在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中讲到的第8个模型要素 $ϵ$。$ϵ−$贪婪法通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 $m$ 个可选行为中选择行为。用公式可以表示为：
$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</li>
</ul>
<p>在实际求解控制问题时，为了使算法可以收敛，一般 $ϵ$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Mento Carlo 搜索过程示意</div>
</center>
<br>
<h1 id="5-蒙特卡罗法控制问题算法流程">5. 蒙特卡罗法控制问题算法流程</h1>
<p>在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。在线和离线的区别我们在后续的文章里面会讲。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。</p>
<p>在线蒙特卡罗法求解强化学习控制问题的算法流程如下:</p>
<ul>
<li>输入：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率$ϵ$</li>
<li>输出：最优的动作价值函数 $q∗$ 和最优策略 $π∗$</li>
<li>
<ol>
<li>初始化所有的动作价值 $Q(s,a)=0$ ， 状态次数 $N(s,a)=0$，采样次数 $k=0$，随机初始化一个策略 $π$</li>
</ol>
</li>
<li>
<ol start="2">
<li>$k=k+1$, 基于策略 $π$ 进行第k次蒙特卡罗采样，得到一个完整的状态序列:
$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</li>
</ol>
</li>
<li>
<ol start="3">
<li>对于该状态序列里出现的每一状态行为对 $(S_t,A_t)$，计算其收获 $G_t$, 更新其计数 $N(s,a)$ 和行为价值函数 $Q(s,a)$：
$$\begin{gathered}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T \\\\
N(S_t,A_t)=N(S_t,A_t)+1 \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\frac1{N(S_t,A_t)}(G_t-Q(S_t,A_t))
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="4">
<li>基于新计算出的动作价值，更新当前的 $ϵ−$贪婪策略：
$$\begin{gathered}
\epsilon=\frac1k \\\\
\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;ifa^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="5">
<li>如果所有的 $Q(s,a)$ 收敛，则对应的所有 $Q(s,a)$ 即为最优的动作价值函数 $q∗$。对应的策略 $π(a|s)$ 即为最优策略 $π∗$。否则转到第二步。</li>
</ol>
</li>
</ul>
<h1 id="6-蒙特卡罗法求解强化学习问题小结">6. 蒙特卡罗法求解强化学习问题小结</h1>
<p>蒙特卡罗法是我们第二个讲到的求解强化问题的方法，也是第一个不基于模型的强化问题求解方法。它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点，这就是它每次采样都需要一个完整的状态序列。如果我们没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了， 也就是说，我们还需要寻找其他的更灵活的不基于模型的强化问题求解方法。</p>
<p>下一篇我们讨论用时序差分方法来求解强化学习预测和控制问题的方法。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9492980.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>