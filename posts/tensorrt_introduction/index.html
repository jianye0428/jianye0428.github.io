<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>TensorRT Introduction - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="TensorRT 介绍 TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎"><meta name=keywords content='TensorRT'><meta itemprop=name content="TensorRT Introduction"><meta itemprop=description content="TensorRT 介绍 TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎"><meta itemprop=datePublished content="2023-07-14T09:23:07+08:00"><meta itemprop=dateModified content="2023-12-28T21:05:53+08:00"><meta itemprop=wordCount content="3544"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="TensorRT,"><meta property="og:title" content="TensorRT Introduction"><meta property="og:description" content="TensorRT 介绍 TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/tensorrt_introduction/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-14T09:23:07+08:00"><meta property="article:modified_time" content="2023-12-28T21:05:53+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="TensorRT Introduction"><meta name=twitter:description content="TensorRT 介绍 TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/tensorrt_introduction/><link rel=prev href=https://jianye0428.github.io/posts/dpg/><link rel=next href=https://jianye0428.github.io/posts/a_star/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"TensorRT Introduction","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/tensorrt_introduction\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"TensorRT","wordcount":3544,"url":"https:\/\/jianye0428.github.io\/posts\/tensorrt_introduction\/","datePublished":"2023-07-14T09:23:07+08:00","dateModified":"2023-12-28T21:05:53+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>TensorRT Introduction</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/distributed-computing/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Distributed Computing</a></span></div><div class=post-meta-line><span title="发布于 2023-07-14 09:23:07"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-14>2023-07-14</time></span>&nbsp;<span title="更新于 2023-12-28 21:05:53"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-12-28>2023-12-28</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3544 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 8 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="TensorRT Introduction">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#tensorrt-介绍>TensorRT 介绍</a></li><li><a href=#tensorrt-优化方式>TensorRT 优化方式</a></li><li><a href=#tensorrt-安装>TensorRT 安装</a></li></ul></li><li><a href=#自定义算子开发----scatterelements>自定义算子开发 &ndash; ScatterElements</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><h3 id=tensorrt-介绍>TensorRT 介绍</h3><p>TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。</p><p>TensorRT 是一个C++库，从 TensorRT 3 开始提供C++ API和Python API，主要用来针对 NVIDIA GPU进行 高性能推理（Inference）加速。</p><p><img loading=lazy src=images/TensorRT-Inference.jpg srcset="/posts/tensorrt_introduction/images/TensorRT-Inference.jpg, images/TensorRT-Inference.jpg 1.5x, /posts/tensorrt_introduction/images/TensorRT-Inference.jpg 2x" sizes=auto data-title=TensorRT_Inference data-alt=TensorRT_Inference width=1000 height=400 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>由以上图可以很清楚的看出，训练(training)和 推理(inference)的区别：</p><ul><li>**训练(training)**包含了前向传播和后向传播两个阶段，针对的是训练集。训练时通过误差反向传播来不断修改网络权值(weights)。</li><li>**推理(inference)**只包含前向传播一个阶段，针对的是除了训练集之外的新数据。可以是测试集，但不完全是，更多的是整个数据集之外的数据。其实就是针对新数据进行预测，预测时，速度是一个很重要的因素。</li></ul><p>一般的深度学习项目，训练时为了加快速度，会使用多GPU分布式训练。但在部署推理时，为了降低成本，往往使用单个GPU机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如caffe，TensorFlow等。</p><p>由于训练的网络模型可能会很大（比如，inception，resnet等），参数很多，而且部署端的机器性能存在差异，就会导致推理速度慢，延迟高。这对于那些高实时性的应用场合是致命的，比如自动驾驶要求实时目标检测，目标追踪等。所以为了提高部署推理的速度，出现了很多轻量级神经网络，比如squeezenet，mobilenet，shufflenet等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。</p><p>而tensorRT 则是对训练好的模型进行优化。 tensorRT就只是 推理优化器。当你的网络训练完之后，可以将训练模型文件直接丢进tensorRT中，而不再需要依赖深度学习框架（Caffe，TensorFlow等），如下:</p><p><img loading=lazy src=images/TensorRT-Optimization.png srcset="/posts/tensorrt_introduction/images/TensorRT-Optimization.png, images/TensorRT-Optimization.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-Optimization.png 2x" sizes=auto data-title=TensorRT_Optimization data-alt=TensorRT_Optimization width=491 height=420 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>可以认为tensorRT是一个只有前向传播的深度学习框架，这个框架可以将 Caffe，TensorFlow的网络模型解析，然后与tensorRT中对应的层进行一一映射，把其他框架的模型统一全部 转换到tensorRT中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略，并进行部署加速。</p><p>目前TensorRT8.0 几乎可以支持所有常用的深度学习框架，对于caffe和TensorFlow来说，tensorRT可以直接解析他们的网络模型；对于caffe2，pytorch，mxnet，chainer，CNTK等框架则是首先要将模型转为 ONNX 的通用深度学习模型，然后对ONNX模型做解析。而tensorflow和MATLAB已经将TensorRT集成到框架中去了。</p><p>**ONNX(Open Neural Network Exchange)**是微软和Facebook携手开发的开放式神经网络交换工具，也就是说不管用什么框架训练，只要转换为ONNX模型，就可以放在其他框架上面去inference。这是一种统一的神经网络模型定义和保存方式，上面提到的除了tensorflow之外的其他框架官方应该都对onnx做了支持，而ONNX自己开发了对tensorflow的支持。从深度学习框架方面来说，这是各大厂商对抗谷歌tensorflow垄断地位的一种有效方式；从研究人员和开发者方面来说，这可以使开发者轻易地在不同机器学习工具之间进行转换，并为项目选择最好的组合方式，加快从研究到生产的速度。</p><p>ONNX / TensorFlow / Custom deep-learning frame模型的工作方式：
<img loading=lazy src=images/TensorRT-ONNX-workflow.png srcset="/posts/tensorrt_introduction/images/TensorRT-ONNX-workflow.png, images/TensorRT-ONNX-workflow.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-ONNX-workflow.png 2x" sizes=auto data-title=TensorRT_Optimization data-alt=TensorRT_Optimization width=588 height=455 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层。
TensorRT-plugin
<img loading=lazy src=images/TensorRT-plugin.png srcset="/posts/tensorrt_introduction/images/TensorRT-plugin.png, images/TensorRT-plugin.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-plugin.png 2x" sizes=auto data-title=TensorRT_Plugin data-alt=TensorRT_Plugin width=625 height=519 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>目前TensorRT支持的层有:https://github.com/onnx/onnx-tensorrt/blob/main/docs/operators.md
目前ONNX支持的算子:https://github.com/onnx/onnx/blob/main/docs/Operators.md</p><h3 id=tensorrt-优化方式>TensorRT 优化方式</h3><p><img loading=lazy src=images/TensorRT-optimize-method.png srcset="/posts/tensorrt_introduction/images/TensorRT-optimize-method.png, images/TensorRT-optimize-method.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-optimize-method.png 2x" sizes=auto data-title=TensorRT-optimize-method data-alt=TensorRT-optimize-method width=1090 height=526 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>TensorRT优化方法主要有以下几种方式，最主要的是前面两种。</p><ul><li><p><strong>层间融合或张量融合(Layer & Tensor Fusion)</strong></p><p>如下图左侧是GoogLeNetInception模块的计算图。这个结构中有很多层，在部署模型推理时，这每一层的运算操作都是由GPU完成的，但实际上是GPU通过启动不同的CUDA（Compute unified device architecture）核心来完成计算的，CUDA核心计算张量的速度是很快的，但是往往大量的时间是浪费在CUDA核心的启动和对每一层输入/输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费。TensorRT通过对层间的横向或纵向合并（合并后的结构称为CBR，意指 convolution, bias, and ReLU layers are fused to form a single layer），使得层的数量大大减少。横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心。合并之后的计算图（图4右侧）的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效。</p><p><img loading=lazy src=images/TensorRT-layer-fusion.png srcset="/posts/tensorrt_introduction/images/TensorRT-layer-fusion.png, images/TensorRT-layer-fusion.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-layer-fusion.png 2x" sizes=auto data-title=TensorRT-layer-fusion data-alt=TensorRT-layer-fusion width=789 height=323 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p></li><li><p><strong>数据精度校准(Weight &amp;Activation Precision Calibration)</strong></p><p>大部分深度学习框架在训练神经网络时网络中的张量（Tensor）都是32位浮点数的精度（Full 32-bit precision，FP32），一旦网络训练完成，在部署推理的过程中由于不需要反向传播，完全可以适当降低数据精度，比如降为FP16或INT8的精度。更低的数据精度将会使得内存占用和延迟更低，模型体积更小。</p><table><thead><tr><th style=text-align:center>Precision</th><th style=text-align:center>Dynamic Range</th></tr></thead><tbody><tr><td style=text-align:center>FP32</td><td style=text-align:center>−3.4×1038 ~ 3.4×1038</td></tr><tr><td style=text-align:center>FP16</td><td style=text-align:center>−65504 ~ 65504</td></tr><tr><td style=text-align:center>INT8</td><td style=text-align:center>−128 ~ 127</td></tr><tr><td style=text-align:center>UINT8</td><td style=text-align:center>0 ~ 256</td></tr></tbody></table><p>INT8只有256个不同的数值，使用INT8来表示 FP32精度的数值，肯定会丢失信息，造成性能下降。不过TensorRT会提供完全自动化的校准（Calibration ）过程，会以最好的匹配性能将FP32精度的数据降低为INT8精度，最小化性能损失。</p></li><li><p><strong>Kernel Auto-Tuning</strong>
网络模型在推理计算时，是调用GPU的CUDA核进行计算的。TensorRT可以针对不同的算法，不同的网络模型，不同的GPU平台，进行 CUDA核的调整（怎么调整的还不清楚），以保证当前模型在特定平台上以最优性能计算。</p><p>TensorRT will pick the implementation from a library of kernels that delivers the best performance for the target GPU, input data size, filter size, tensor layout, batch size and other parameters.</p></li><li><p><strong>Dynamic Tensor Memory</strong>
在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。</p></li><li><p><strong>Multi-Stream Execution</strong>
Scalable design to process multiple input streams in parallel，这个应该就是GPU底层的优化了。</p></li></ul><h3 id=tensorrt-安装>TensorRT 安装</h3><p><strong><a href=https://zhuanlan.zhihu.com/p/72298520 target=_blank rel="external nofollow noopener noreferrer">CUDA的安装<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></strong></p><ol><li><p>安装显卡驱动</p></li><li><p>安装cuda
2.1 进入<a href=https://developer.nvidia.com/cuda-toolkit-archive target=_blank rel="external nofollow noopener noreferrer">nvidia开发者网站的CUDA下载页面<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>选择runfile格式的文件下载。</p><p>2.2 下载完成后，解压，并运行上图中的命令，会有条款，接受即可，注意安装CUDA的时候不要安装驱动
<img loading=lazy src=images/TensorRT-no-driver.png srcset="/posts/tensorrt_introduction/images/TensorRT-no-driver.png, images/TensorRT-no-driver.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-no-driver.png 2x" sizes=auto data-title=TensorRT-no-driver data-alt=TensorRT-no-driver width=727 height=471 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
2.3 路径设置</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>$ <span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span>/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/nsight-compute-2019.5.0<span class=si>${</span><span class=nv>PATH</span><span class=p>:+:</span><span class=si>${</span><span class=nv>PATH</span><span class=si>}}</span>
</span></span><span class=line><span class=cl>$ <span class=nb>export</span> <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span>/usr/local/cuda-10.2/lib64/<span class=si>${</span><span class=nv>LD_LIBRARY_PATH</span><span class=p>:+:</span><span class=si>${</span><span class=nv>LD_LIBRARY_PATH</span><span class=si>}}</span></span></span></code></pre></td></tr></table></div></div><p>并使设置生效:</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>source</span> ~/.bashrc</span></span></code></pre></td></tr></table></div></div><p>2.4 验证安装是否成功
进入/usr/local/cuda-10.1/samples/1_Utilities/目录，</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>cd</span> deviceQuery
</span></span><span class=line><span class=cl>sudo make
</span></span><span class=line><span class=cl>./deviceQuery</span></span></code></pre></td></tr></table></div></div><p>出现如下输出，则CUDA安装成功。
<img loading=lazy src=images/Tensort-cuda_install_success srcset="images/Tensort-cuda_install_success, images/Tensort-cuda_install_success 1.5x, images/Tensort-cuda_install_success 2x" sizes=auto data-title=Tensort-cuda_install_success data-alt=Tensort-cuda_install_success style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p></li><li><p>安装cuDNN
3.1进入<a href=https://developer.nvidia.com/cudnn target=_blank rel="external nofollow noopener noreferrer">cudnn下载<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>页面，下载版本合适的版
3.2 解压，并进入到相应目录，运行以下命令：</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sudo cp cuda/include/cudnn*.h /usr/local/cuda-10.2/include
</span></span><span class=line><span class=cl>sudo cp cuda/lib64/libcudnn* /usr/local/cuda-10.2/lib64
</span></span><span class=line><span class=cl>sudo chmod a+r /usr/local/cuda-10.2/include/cudnn*.h
</span></span><span class=line><span class=cl>sudo chmod a+r /usr/local/cuda-10.2/lib64/libcudnn*</span></span></code></pre></td></tr></table></div></div><p>3.3 查看cudnn版本</p><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cat /usr/local/cuda-10.2/include/cudnn.h <span class=p>|</span> grep CUDNN_MAJOR -A <span class=m>2</span></span></span></code></pre></td></tr></table></div></div><p>新版本:</p><div class=highlight id=id-6><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cat /usr/local/cuda-10.2/include/cudnn_version.h <span class=p>|</span> grep CUDNN_MAJOR -A <span class=m>2</span></span></span></code></pre></td></tr></table></div></div><p>ref: <a href="https://blog.csdn.net/weixin_43592742/article/details/115689886?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&amp;spm=1001.2101.3001.4242" target=_blank rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_43592742/article/details/115689886?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></li></ol><p><strong><a href=https://github.com/nvidia/TensorRT target=_blank rel="external nofollow noopener noreferrer">TensorRT的安装<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></strong></p><p><a href=https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html target=_blank rel="external nofollow noopener noreferrer">英伟达提供的安装指导<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><blockquote><p>tensorRT 要匹配cuda和cudnn版本。在安装之前请匹配。</p></blockquote><p>OSS 和 GA 两个版本:</p><ol><li><p>TensorRT OSS:</p><div class=highlight id=id-7><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>git clone -b master https://github.com/nvidia/TensorRT TensorRT
</span></span><span class=line><span class=cl><span class=nb>cd</span> TensorRT
</span></span><span class=line><span class=cl>git submodule update --init --recursive</span></span></code></pre></td></tr></table></div></div></li><li><p>GA 版本(<a href=https://developer.nvidia.com/nvidia-tensorrt-download target=_blank rel="external nofollow noopener noreferrer">下载地址<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>)</p></li><li><p>对GA版本和OSS版本在<code>~/.bashrc</code>文件中声明路径:
(GA: General Availability Stable Version)
(OSS: OPEN SOURCE)</p><ol><li>[oss版本路径]export TRT_SOURCE=/home/yejian/TensorRT/TensorRT_7.2.1</li><li>[GA Release 版本路径]export TRT_RELEASE=/home/yejian/TensorRT/TensorRT_7.2.1/TensorRT-7.2.1.6/TensorRT-7.2.1.6</li></ol></li><li><p>测试确保安装成功</p></li></ol><div class=highlight id=id-8><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cd /home/yejian/TensorRT/TensorRT-8.6.1.6/samples/sampleOnnxMNIST
</span></span><span class=line><span class=cl>make
</span></span><span class=line><span class=cl>../../bin
</span></span><span class=line><span class=cl>./sample_onnx_mnist</span></span></code></pre></td></tr></table></div></div><ol start=5><li>Build TensorRT RSS (这一步需要在编写自定义算子的时候编译通过，才能调用自定义算子)<div class=highlight id=id-9><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-mysql data-lang=mysql><span class=line><span class=cl><span class=n>cd</span><span class=w> </span><span class=err>$</span><span class=n>TRT_OSSPATH</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>mkdir</span><span class=w> </span><span class=o>-</span><span class=n>p</span><span class=w> </span><span class=n>build</span><span class=w> </span><span class=o>&amp;&amp;</span><span class=w> </span><span class=n>cd</span><span class=w> </span><span class=n>build</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>cmake</span><span class=w> </span><span class=p>..</span><span class=w> </span><span class=o>-</span><span class=n>DTRT_LIB_DIR</span><span class=o>=</span><span class=err>$</span><span class=n>TRT_LIBPATH</span><span class=w> </span><span class=o>-</span><span class=n>DTRT_OUT_DIR</span><span class=o>=`</span><span class=n>pwd</span><span class=o>`/</span><span class=k>out</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>make</span><span class=w> </span><span class=o>-</span><span class=n>j</span><span class=err>$</span><span class=p>(</span><span class=n>nproc</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div></li></ol><p>ref: <a href=https://blog.csdn.net/Msjiangmei/article/details/132585145 target=_blank rel="external nofollow noopener noreferrer">https://blog.csdn.net/Msjiangmei/article/details/132585145<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><h2 id=自定义算子开发----scatterelements>自定义算子开发 &ndash; ScatterElements</h2><p>在自定义算子开发过程中，需要撰写一下4个文件，并且把文件放在scatterElementsPlugin文件夹中:</p><ul><li><code>CmakeLists.txt</code></li><li><code>scatterElements.cu</code></li><li><code>scatterElementsPlugin.cpp</code></li><li><code>scatterElementsPlugin.h</code></li></ul><p>如图所示:</p><p><img loading=lazy src=images/TensorRT-custom_operator_files.png srcset="/posts/tensorrt_introduction/images/TensorRT-custom_operator_files.png, images/TensorRT-custom_operator_files.png 1.5x, /posts/tensorrt_introduction/images/TensorRT-custom_operator_files.png 2x" sizes=auto data-title=TensorRT-custom_operator_files data-alt=TensorRT-custom_operator_files width=613 height=326 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p><strong>自定义算子的生成与注册</strong></p><ul><li>将以上四个文件报括文件夹复制到TensorRT(OOS)下的plugin文件夹下;</li><li>然后修改注册信息文件:(这些文件也在plugin文件夹下)<ul><li><code>${TRT_SOURCE}/plugin: CMakeLists.txt</code></li><li><code>${TRT_SOURCE}/InferPlugin.cpp</code></li><li><code>${TRT_SOURCE}/common/kernels/kernel.h</code></li><li><code>${TRT_SOURCE}/parsers/onnx/builtin_op_importers.cpp</code></li></ul></li></ul><p>执行完以上步骤以后，重新编译OOS版本，然后就可以调用自定义算子:</p><div class=highlight id=id-10><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>cd</span> <span class=nv>$TRT_OSSPATH</span>
</span></span><span class=line><span class=cl>mkdir -p build <span class=o>&amp;&amp;</span> <span class=nb>cd</span> build
</span></span><span class=line><span class=cl>cmake .. -DTRT_LIB_DIR<span class=o>=</span><span class=nv>$TRT_LIBPATH</span> -DTRT_OUT_DIR<span class=o>=</span><span class=sb>`</span><span class=nb>pwd</span><span class=sb>`</span>/out
</span></span><span class=line><span class=cl>make -j<span class=k>$(</span>nproc<span class=k>)</span></span></span></code></pre></td></tr></table></div></div></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-12-28 21:05:53">更新于 2023-12-28&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/1e636eaca6892e98301e15e23826c1101262edde rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 1e636eaca6892e98301e15e23826c1101262edde: bug: improve image quality"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>1e636ea</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/tensorrt_introduction/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/GPU/TensorRT/TensorRT_Introduction/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/tensorrt_introduction/ data-title="TensorRT Introduction" data-hashtags=TensorRT><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/tensorrt_introduction/ data-hashtag=TensorRT><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/tensorrt_introduction/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/tensorrt_introduction/ data-title="TensorRT Introduction"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/tensorrt_introduction/ data-title="TensorRT Introduction"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/tensorrt/ class=post-tag>TensorRT</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/dpg/ class=post-nav-item rel=prev title=DPG><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>DPG</a>
<a href=/posts/a_star/ class=post-nav-item rel=next title="A star (A*) 算法">A star (A*) 算法<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.123.0">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>