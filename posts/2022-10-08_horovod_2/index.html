<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 - yejian's blog</title><meta name=author content="Jian YE"><meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。 前一篇参见如下： 深度学习分布式训练框架 Horovod[1] &ndash; 基础知识 1 Horovod 简介 Horovod 是Uber于2017年发"><meta name=keywords content="Horovod"><meta itemprop=name content="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入"><meta itemprop=description content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。 前一篇参见如下： 深度学习分布式训练框架 Horovod[1] &ndash; 基础知识 1 Horovod 简介 Horovod 是Uber于2017年发"><meta itemprop=datePublished content="2023-07-10T07:53:40+08:00"><meta itemprop=dateModified content="2023-07-15T15:28:35+08:00"><meta itemprop=wordCount content="6569"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="Horovod,"><meta property="og:title" content="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入"><meta property="og:description" content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。 前一篇参见如下： 深度学习分布式训练框架 Horovod[1] &ndash; 基础知识 1 Horovod 简介 Horovod 是Uber于2017年发"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/2022-10-08_horovod_2/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-10T07:53:40+08:00"><meta property="article:modified_time" content="2023-07-15T15:28:35+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入"><meta name=twitter:description content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。 前一篇参见如下： 深度学习分布式训练框架 Horovod[1] &ndash; 基础知识 1 Horovod 简介 Horovod 是Uber于2017年发"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/2022-10-08_horovod_2/><link rel=prev href=https://jianye0428.github.io/posts/2022-10-08_horovod_1/><link rel=next href=https://jianye0428.github.io/posts/2022-10-08_horovod_3/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/2022-10-08_horovod_2\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"Horovod","wordcount":6569,"url":"https:\/\/jianye0428.github.io\/posts\/2022-10-08_horovod_2\/","datePublished":"2023-07-10T07:53:40+08:00","dateModified":"2023-07-15T15:28:35+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i></span></span>
<span class=post-category>收录于 <a href=/categories/distributed-computing/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Distributed Computing</a></span></div><div class=post-meta-line><span title="发布于 2023-07-10 07:53:40"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-10>2023-07-10</time></span>&nbsp;<span title="更新于 2023-07-15 15:28:35"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-15>2023-07-15</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 6569 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 14 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#0-摘要>0 摘要</a></li><li><a href=#1-horovod-简介>1 Horovod 简介</a></li><li><a href=#2-hovorod-机制概述>2 Hovorod 机制概述</a><ul><li><a href=#21-horovod-机制>2.1 Horovod 机制</a></li></ul></li><li><a href=#3-示例代码>3 示例代码</a><ul><li><a href=#31--摘要代码>3.1 摘要代码</a></li><li><a href=#32-horovodrun>3.2 horovodrun</a></li></ul></li><li><a href=#4-运行逻辑>4 运行逻辑</a><ul><li><a href=#41-引入python文件>4.1 引入python文件</a></li><li><a href=#42--初始化-in-python>4.2 初始化 in python</a><ul><li><a href=#421-引入so库>4.2.1 引入SO库</a><ul><li><a href=#4211-so库>4.2.1.1 SO库</a></li><li><a href=#4222-so作用>4.2.2.2 SO作用</a></li></ul></li><li><a href=#422-初始化配置>4.2.2 初始化配置</a></li><li><a href=#423-hvdinit-初始化>4.2.3 hvd.init() 初始化</a></li></ul></li><li><a href=#43-初始化-in-c>4.3 初始化 in C++</a><ul><li><a href=#431-horovod_init_comm>4.3.1 horovod_init_comm</a></li><li><a href=#432-initializehorovodonce>4.3.2 InitializeHorovodOnce</a></li><li><a href=#433-horovodglobalstate>4.3.3 HorovodGlobalState</a></li></ul></li><li><a href=#43-hvd-概念>4.3 hvd 概念</a></li><li><a href=#45--数据处理>4.5 数据处理</a></li><li><a href=#46-广播初始化变量>4.6 广播初始化变量</a><ul><li><a href=#461-广播定义>4.6.1 广播定义</a></li><li><a href=#462-broadcast_variables>4.6.2 broadcast_variables</a></li><li><a href=#463-调用-mpi>4.6.3 调用 MPI</a></li><li><a href=#464-同步参数>4.6.4 同步参数</a></li></ul></li><li><a href=#47-distributedoptimizer>4.7 DistributedOptimizer</a></li><li><a href=#48-未来可能>4.8 未来可能</a></li></ul></li><li><a href=#5-总结>5 总结</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><h2 id=0-摘要>0 摘要</h2><p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p><p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。</p><p>前一篇参见如下：</p><p><a href=http://localhost:1313/posts/notes/2022-10-08_horovod_1/ target=_blank rel="external nofollow noopener noreferrer">深度学习分布式训练框架 Horovod[1] &ndash; 基础知识<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><h2 id=1-horovod-简介>1 Horovod 简介</h2><p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，支持TensorFlow，Keras，PyTorch和MXNet。Horovod 的名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与分布式 TensorFlow 流程使用 Horovod 互相通信的场景很像。</p><p>因为各个机器学习框架对于底层集合通信库（ nccl，openmpi，gloo 等等）的利用水平可能各不相同，使得他们无法充分利用这些底层集合通信库的威力。因而，hovorod 就整合这些框架，提供一个易用高效的解决方案。</p><p>Uber的工程师就是根据FaceBook的一篇paper：“<a href=https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf target=_blank rel="external nofollow noopener noreferrer">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>”和百度的一篇“<a href=https://research.baidu.com/bringing-hpc-techniques-deep-learning/ target=_blank rel="external nofollow noopener noreferrer">Bringing HPC Techniques to Deep Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>” 改进并发布了开源框架Horovod。</p><p>Horovod 相比于百度的工作，并无学术上的贡献。但是 Horovod 扎实的工程实现，使得它受到了更多的关注。它最大的优势在于对 RingAllReduce 进行了更高层次的抽象，使其支持多种不同的框架。同时引入了 Nvidia NCCL，对 GPU 更加友好。</p><p>Horovod依赖于Nvidia的 NCCL2 做 All Reduce，依赖于MPI做进程间通信，简化了同步多 GPU 或多节点分布式训练的开发流程。由于使用了NCCL2，Horovod也可以利用以下功能：NVLINK，RDMA，GPUDirectRDMA，自动检测通信拓扑，能够回退到 PCIe 和 TCP/IP 通信。</p><p>我们需要几个问题来引导分析：</p><ul><li>Hovorod 怎么进行数据分割？</li><li>Hovorod 怎么进行训练代码分发？</li><li>Hovorod 启动时候，python 和 C++ 都做了什么？</li><li>如何确保 Hovorod 启动时候步骤一致；</li></ul><h2 id=2-hovorod-机制概述>2 Hovorod 机制概述</h2><h3 id=21-horovod-机制>2.1 Horovod 机制</h3><p>Horovod使用<strong>数据并行化</strong>策略在GPU上分配训练。</p><p>在数据并行化中，作业中的每个GPU都会接收其自己的数据批处理的独立切片，即它的“批处理切片”。 每个GPU都使用自己分配到的数据来独立计算，进行梯度更新。</p><p>假如使用两个GPU，批处理大小为32，则第一个GPU将处理前16条记录的正向传播和向后传播，以及第二个GPU处理后16条记录的正向传播和向后传播。然后，这些梯度更新将在GPU之间平均在一起，最后应用于模型。</p><p>每一个迭代的操作方法如下：</p><ol><li><p>每个 worker 将维护自己的模型权重副本和自己的数据集副本。</p></li><li><p>收到执行信号后，每个工作进程都会从数据集中提取一个不相交的批次，并计算该批次的梯度。</p></li><li><p>Workers 使用ring all-reduce算法来同步彼此的梯度，从而在本地所有节点上计算同样的平均梯度。</p><ol><li><p>将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片，后续每一次通信都将给下一个邻居发送一个自己的分片（同时从上一个邻居接受一个新分片）。</p></li><li><p>ScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和，即每个 device 将有一个块，其中包含所有device 中该块中所有值的总和；具体如下：</p></li></ol><p><img loading=lazy src=images/Horovod_2_scatter_reduce.png#center srcset="/posts/2022-10-08_horovod_2/images/Horovod_2_scatter_reduce.png, images/Horovod_2_scatter_reduce.png#center 1.5x, /posts/2022-10-08_horovod_2/images/Horovod_2_scatter_reduce.png 2x" sizes=auto data-title="Scatter Reduce" data-alt="Scatter Reduce" width=833 height=469 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><ol start=3><li>AllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和 广播到其他 device；最终所有节点都拥有所有tensor分片和。具体如下：
<img loading=lazy src=images/Horovod_2_allgather.png#center srcset="/posts/2022-10-08_horovod_2/images/Horovod_2_allgather.png, images/Horovod_2_allgather.png#center 1.5x, /posts/2022-10-08_horovod_2/images/Horovod_2_allgather.png 2x" sizes=auto data-title=Allgather data-alt=Allgather width=834 height=469 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></li><li>在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度；</li></ol></li><li><p>每个 worker 将 梯度更新 应用于其模型的本地副本。</p></li><li><p>执行下一个batch。</p></li></ol><h2 id=3-示例代码>3 示例代码</h2><h3 id=31--摘要代码>3.1 摘要代码</h3><p>我们此处给出官网示例代码部分摘要，具体分析参见下面代码中的注释。</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>horovod.tensorflow.keras</span> <span class=k>as</span> <span class=nn>hvd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: initialize Horovod.</span>
</span></span><span class=line><span class=cl><span class=n>hvd</span><span class=o>.</span><span class=n>init</span><span class=p>()</span> <span class=c1># 初始化 Horovod，启动相关线程和MPI线程</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
</span></span><span class=line><span class=cl><span class=c1># 依据 local rank 为不同的进程分配不同的GPU</span>
</span></span><span class=line><span class=cl><span class=n>gpus</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>experimental</span><span class=o>.</span><span class=n>list_physical_devices</span><span class=p>(</span><span class=s1>&#39;GPU&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>gpu</span> <span class=ow>in</span> <span class=n>gpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>experimental</span><span class=o>.</span><span class=n>set_memory_growth</span><span class=p>(</span><span class=n>gpu</span><span class=p>,</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>gpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>experimental</span><span class=o>.</span><span class=n>set_visible_devices</span><span class=p>(</span><span class=n>gpus</span><span class=p>[</span><span class=n>hvd</span><span class=o>.</span><span class=n>local_rank</span><span class=p>()],</span> <span class=s1>&#39;GPU&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>mnist_images</span><span class=p>,</span> <span class=n>mnist_labels</span><span class=p>),</span> <span class=n>_</span> <span class=o>=</span> \
</span></span><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>(</span><span class=n>path</span><span class=o>=</span><span class=s1>&#39;mnist-</span><span class=si>%d</span><span class=s1>.npz&#39;</span> <span class=o>%</span> <span class=n>hvd</span><span class=o>.</span><span class=n>rank</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 切分数据</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>mnist_images</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span> <span class=o>/</span> <span class=mf>255.0</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span>
</span></span><span class=line><span class=cl>             <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>mnist_labels</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>int64</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>repeat</span><span class=p>()</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=mi>10000</span><span class=p>)</span><span class=o>.</span><span class=n>batch</span><span class=p>(</span><span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mnist_model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=o>......</span>
</span></span><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: adjust learning rate based on number of GPUs.</span>
</span></span><span class=line><span class=cl><span class=n>scaled_lr</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>*</span> <span class=n>hvd</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=c1># 根据Worker的数量增加学习率的大小</span>
</span></span><span class=line><span class=cl><span class=n>opt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>scaled_lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class=line><span class=cl><span class=c1># 把常规TensorFlow Optimizer通过Horovod包装起来，进而使用 ring-allreduce 来得到平均梯度</span>
</span></span><span class=line><span class=cl><span class=n>opt</span> <span class=o>=</span> <span class=n>hvd</span><span class=o>.</span><span class=n>DistributedOptimizer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>opt</span><span class=p>,</span> <span class=n>backward_passes_per_step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>average_aggregated_gradients</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
</span></span><span class=line><span class=cl><span class=c1># uses hvd.DistributedOptimizer() to compute gradients.</span>
</span></span><span class=line><span class=cl><span class=n>mnist_model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                    <span class=n>optimizer</span><span class=o>=</span><span class=n>opt</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=n>experimental_run_tf_function</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>callbacks</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>hvd</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>BroadcastGlobalVariablesCallback</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=c1># 广播初始化，将模型的参数从第一个设备传向其他设备，以保证初始化模型参数的一致性</span>
</span></span><span class=line><span class=cl>    <span class=n>hvd</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>MetricAverageCallback</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>hvd</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>LearningRateWarmupCallback</span><span class=p>(</span><span class=n>initial_lr</span><span class=o>=</span><span class=n>scaled_lr</span><span class=p>,</span> <span class=n>warmup_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them. # 只有设备0需要保存模型参数作为checkpoint</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>hvd</span><span class=o>.</span><span class=n>rank</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>callbacks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>ModelCheckpoint</span><span class=p>(</span><span class=s1>&#39;./checkpoint-</span><span class=si>{epoch}</span><span class=s1>.h5&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Horovod: write logs on worker 0.</span>
</span></span><span class=line><span class=cl><span class=n>verbose</span> <span class=o>=</span> <span class=mi>1</span> <span class=k>if</span> <span class=n>hvd</span><span class=o>.</span><span class=n>rank</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train the model.</span>
</span></span><span class=line><span class=cl><span class=c1># Horovod: adjust number of steps based on number of GPUs.</span>
</span></span><span class=line><span class=cl><span class=n>mnist_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>steps_per_epoch</span><span class=o>=</span><span class=mi>500</span> <span class=o>//</span> <span class=n>hvd</span><span class=o>.</span><span class=n>size</span><span class=p>(),</span> <span class=n>callbacks</span><span class=o>=</span><span class=n>callbacks</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>24</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=n>verbose</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><h3 id=32-horovodrun>3.2 horovodrun</h3><p>Horovod训练脚本未作为Python脚本启动。 例如，您不能使用<code>python train.py</code>运行此脚本。 需要采用特殊的CLI命令 <code>horovodrun</code> 来启动（训练代码 train.py 需要手动拷贝到各个节点上，且目录相同）：</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$ horovodrun -np 4 -H localhost:4 python train.py</span></span></code></pre></td></tr></table></div></div><h2 id=4-运行逻辑>4 运行逻辑</h2><p>我们按照顺序梳理，看看在程序初始化过程背后都做了什么。</p><h3 id=41-引入python文件>4.1 引入python文件</h3><p>如下代码会引入各种相关python文件。</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>horovod.tensorflow.keras</span> <span class=k>as</span> <span class=nn>hvd</span></span></span></code></pre></td></tr></table></div></div><h3 id=42--初始化-in-python>4.2 初始化 in python</h3><p>python 世界的初始化位于 <code>horovod-master/horovod/mxnet/mpi_ops.py</code></p><h4 id=421-引入so库>4.2.1 引入SO库</h4><h5 id=4211-so库>4.2.1.1 SO库</h5><p><code>horovod/tensorflow/mpi_ops.py</code> 之中会引入SO库。
比如 <code>dist-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so</code>。</p><p>SO库 就是 horovod 中 C++ 代码编译出来的结果。</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_load_library</span><span class=p>(</span><span class=n>name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Loads a .so file containing the specified operators.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>filename</span> <span class=o>=</span> <span class=n>resource_loader</span><span class=o>.</span><span class=n>get_path_to_datafile</span><span class=p>(</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>library</span> <span class=o>=</span> <span class=n>load_library</span><span class=o>.</span><span class=n>load_op_library</span><span class=p>(</span><span class=n>filename</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>library</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Check possible symbol not found error from tensorflow version mismatch</span>
</span></span><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_LIB</span> <span class=o>=</span> <span class=n>_load_library</span><span class=p>(</span><span class=s1>&#39;mpi_lib&#39;</span> <span class=o>+</span> <span class=n>get_ext_suffix</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>check_installed_version</span><span class=p>(</span><span class=s1>&#39;tensorflow&#39;</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>__version__</span><span class=p>,</span> <span class=n>e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=n>e</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>check_installed_version</span><span class=p>(</span><span class=s1>&#39;tensorflow&#39;</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><h5 id=4222-so作用>4.2.2.2 SO作用</h5><p>引入库的作用是获取到 C++ 的函数，并且用 python 封装一下，这样就可以在 python 世界使用 C++代码了。</p><p>由下文可以看出来，python 的 _allreduce 函数就会把功能转发给 C++，由 <code>MPI_LIB.horovod_allreduce</code> 完成。</p><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_allreduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>op</span><span class=o>=</span><span class=n>Sum</span><span class=p>,</span> <span class=n>prescale_factor</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>postscale_factor</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=n>ignore_name_scope</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>name</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>_executing_eagerly</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;HorovodAllreduce_</span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>_normalize_name</span><span class=p>(</span><span class=n>tensor</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>MPI_LIB</span><span class=o>.</span><span class=n>horovod_allreduce</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>name</span><span class=p>,</span> <span class=n>reduce_op</span><span class=o>=</span><span class=n>op</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                     <span class=n>prescale_factor</span><span class=o>=</span><span class=n>prescale_factor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                     <span class=n>postscale_factor</span><span class=o>=</span><span class=n>postscale_factor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                     <span class=n>ignore_name_scope</span><span class=o>=</span><span class=n>ignore_name_scope</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><h4 id=422-初始化配置>4.2.2 初始化配置</h4><p>我们摘录了主要部分，就是初始化 _HorovodBasics，然后从 _HorovodBasics 内获取各种函数，变量和配置，比如是否编译了mpi，gloo等等.</p><div class=highlight id=id-6><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>horovod.common.basics</span> <span class=kn>import</span> <span class=n>HorovodBasics</span> <span class=k>as</span> <span class=n>_HorovodBasics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>_basics</span> <span class=o>=</span> <span class=n>_HorovodBasics</span><span class=p>(</span><span class=vm>__file__</span><span class=p>,</span> <span class=s1>&#39;mpi_lib&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># import basic methods</span>
</span></span><span class=line><span class=cl><span class=n>init</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>init</span>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>size</span>
</span></span><span class=line><span class=cl><span class=n>local_size</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>local_size</span>
</span></span><span class=line><span class=cl><span class=n>rank</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>rank</span>
</span></span><span class=line><span class=cl><span class=n>local_rank</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>local_rank</span>
</span></span><span class=line><span class=cl><span class=n>mpi_built</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>mpi_built</span>
</span></span><span class=line><span class=cl><span class=n>gloo_enabled</span> <span class=o>=</span> <span class=n>_basics</span><span class=o>.</span><span class=n>gloo_enabled</span>
</span></span><span class=line><span class=cl><span class=o>......</span></span></span></code></pre></td></tr></table></div></div><h4 id=423-hvdinit-初始化>4.2.3 hvd.init() 初始化</h4><p>首先需要用 <code>hvd.init()</code> 来初始化，horovod 管理的所有状态都会传到 hvd 对象中。</p><div class=highlight id=id-7><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Horovod: initialize Horovod.</span>
</span></span><span class=line><span class=cl><span class=n>hvd</span><span class=o>.</span><span class=n>init</span><span class=p>()</span></span></span></code></pre></td></tr></table></div></div><p>此处调用的是 HorovodBasics 中的函数，我们看看做了什么。</p><p>可以看到，这部分会一直深入到 C++世界，调用了大量的 MPI_LIB_CTYPES 函数，所以我们接下来就要进入到 C++的世界看看。</p><div class=highlight id=id-8><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>init</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>comm</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;A function that initializes Horovod.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>atexit</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>shutdown</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>comm</span><span class=p>,</span> <span class=nb>list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mpi_built</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>MPI_LIB_CTYPES</span><span class=o>.</span><span class=n>horovod_mpi_built</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=kn>from</span> <span class=nn>mpi4py</span> <span class=kn>import</span> <span class=n>MPI</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>MPI</span><span class=o>.</span><span class=n>_sizeof</span><span class=p>(</span><span class=n>MPI</span><span class=o>.</span><span class=n>Comm</span><span class=p>)</span> <span class=o>==</span> <span class=n>ctypes</span><span class=o>.</span><span class=n>sizeof</span><span class=p>(</span><span class=n>ctypes</span><span class=o>.</span><span class=n>c_int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>MPI_Comm</span> <span class=o>=</span> <span class=n>ctypes</span><span class=o>.</span><span class=n>c_int</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>MPI_Comm</span> <span class=o>=</span> <span class=n>ctypes</span><span class=o>.</span><span class=n>c_void_p</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>MPI_LIB_CTYPES</span><span class=o>.</span><span class=n>horovod_init_comm</span><span class=o>.</span><span class=n>argtypes</span> <span class=o>=</span> <span class=p>[</span><span class=n>MPI_Comm</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>comm_obj</span> <span class=o>=</span> <span class=n>MPI_Comm</span><span class=o>.</span><span class=n>from_address</span><span class=p>(</span><span class=n>MPI</span><span class=o>.</span><span class=n>_addressof</span><span class=p>(</span><span class=n>comm</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>MPI_LIB_CTYPES</span><span class=o>.</span><span class=n>horovod_init_comm</span><span class=p>(</span><span class=n>comm_obj</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>comm_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>comm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>MPI_LIB_CTYPES</span><span class=o>.</span><span class=n>horovod_init</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>ctypes</span><span class=o>.</span><span class=n>c_int</span> <span class=o>*</span> <span class=n>comm_size</span><span class=p>)(</span><span class=o>*</span><span class=n>comm</span><span class=p>),</span> <span class=n>ctypes</span><span class=o>.</span><span class=n>c_int</span><span class=p>(</span><span class=n>comm_size</span><span class=p>))</span></span></span></code></pre></td></tr></table></div></div><p>目前逻辑如下图：</p><div class=highlight id=id-9><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-JAVA data-lang=JAVA><span class=line><span class=cl>           <span class=n>Import</span> <span class=n>python</span> <span class=n>files</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>Import</span> <span class=n>C</span><span class=o>++</span> <span class=n>SO</span> <span class=n>files</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>Create</span> <span class=n>_HorovodBasics</span>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>
</span></span><span class=line><span class=cl>                <span class=n>hvd</span><span class=o>.</span><span class=na>init</span><span class=o>()</span>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>
</span></span><span class=line><span class=cl><span class=n>Python</span>              <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>+------------------------------------------+</span>
</span></span><span class=line><span class=cl><span class=n>C</span><span class=o>++</span>                 <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span></span></span></code></pre></td></tr></table></div></div><h3 id=43-初始化-in-c>4.3 初始化 in C++</h3><h4 id=431-horovod_init_comm>4.3.1 horovod_init_comm</h4><p>在初始化的时候，Horovod 会：</p><ul><li>调用 <code>MPI_Comm_dup</code> 获取一个 Communicator，这样就有了和 MPI 协调的基础。</li><li>然后调用 <code>InitializeHorovodOnce</code>。</li></ul><div class=highlight id=id-10><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>void</span> <span class=n>horovod_init_comm</span><span class=p>(</span><span class=n>MPI_Comm</span> <span class=n>comm</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>MPI_Comm_dup</span><span class=p>(</span><span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>mpi_context</span><span class=o>.</span><span class=n>mpi_comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>InitializeHorovodOnce</span><span class=p>(</span><span class=n>nullptr</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><h4 id=432-initializehorovodonce>4.3.2 InitializeHorovodOnce</h4><p>InitializeHorovodOnce 是初始化的主要工作，主要是：</p><ul><li>依据是否编译了 mpi 或者 gloo，对各自的 context 进行处理，为 globalstate 创建对应的 controller；</li><li>启动了后台线程 BackgroundThreadLoop 用来在各个worker之间协调；</li></ul><div class=highlight id=id-11><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>void</span> <span class=n>horovod_init</span><span class=p>(</span><span class=n>const</span> <span class=nb>int</span><span class=o>*</span> <span class=n>ranks</span><span class=p>,</span> <span class=nb>int</span> <span class=n>nranks</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>InitializeHorovodOnce</span><span class=p>(</span><span class=n>ranks</span><span class=p>,</span> <span class=n>nranks</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>void</span> <span class=n>InitializeHorovodOnce</span><span class=p>(</span><span class=n>const</span> <span class=nb>int</span><span class=o>*</span> <span class=n>ranks</span><span class=p>,</span> <span class=nb>int</span> <span class=n>nranks</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=o>//</span> <span class=n>Ensure</span> <span class=n>background</span> <span class=n>thread</span> <span class=ow>is</span> <span class=n>only</span> <span class=n>started</span> <span class=n>once</span><span class=o>.</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=err>!</span><span class=n>horovod_global</span><span class=o>.</span><span class=n>initialize_flag</span><span class=o>.</span><span class=n>test_and_set</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>horovod_global</span><span class=o>.</span><span class=n>control_operation</span> <span class=o>=</span> <span class=n>ParseControllerOpsFromEnv</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=n>horovod_global</span><span class=o>.</span><span class=n>cpu_operation</span> <span class=o>=</span> <span class=n>ParseCPUOpsFromEnv</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#if HAVE_MPI // 依据是否编译了MPI进行处理</span>
</span></span><span class=line><span class=cl>    <span class=o>//</span> <span class=n>Enable</span> <span class=n>mpi</span> <span class=ow>is</span> <span class=n>it</span><span class=s1>&#39;s used either in cpu data transfer or controller</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>horovod_global</span><span class=o>.</span><span class=n>cpu_operation</span> <span class=o>==</span> <span class=n>LibType</span><span class=p>::</span><span class=n>MPI</span> <span class=o>||</span>
</span></span><span class=line><span class=cl>        <span class=n>horovod_global</span><span class=o>.</span><span class=n>control_operation</span> <span class=o>==</span> <span class=n>LibType</span><span class=p>::</span><span class=n>MPI</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>mpi_context</span><span class=o>.</span><span class=n>Enable</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>horovod_global</span><span class=o>.</span><span class=n>control_operation</span> <span class=o>==</span> <span class=n>LibType</span><span class=p>::</span><span class=n>MPI</span><span class=p>){</span>
</span></span><span class=line><span class=cl>      <span class=o>//</span> <span class=n>创建一个</span> <span class=n>MPIController</span> <span class=n>对象</span>
</span></span><span class=line><span class=cl>      <span class=n>horovod_global</span><span class=o>.</span><span class=n>controller</span><span class=o>.</span><span class=n>reset</span><span class=p>(</span><span class=n>new</span> <span class=n>MPIController</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>horovod_global</span><span class=o>.</span><span class=n>response_cache</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>horovod_global</span><span class=o>.</span><span class=n>tensor_queue</span><span class=p>,</span> <span class=n>horovod_global</span><span class=o>.</span><span class=n>timeline</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>horovod_global</span><span class=o>.</span><span class=n>parameter_manager</span><span class=p>,</span> <span class=n>horovod_global</span><span class=o>.</span><span class=n>group_table</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>mpi_context</span><span class=p>));</span>
</span></span><span class=line><span class=cl>      <span class=n>horovod_global</span><span class=o>.</span><span class=n>controller</span><span class=o>-&gt;</span><span class=n>SetRanks</span><span class=p>(</span><span class=n>ranks</span><span class=p>,</span> <span class=n>nranks</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1>#endif</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#if HAVE_GLOO // 依据是否编译了 GLOO 进行处理</span>
</span></span><span class=line><span class=cl>    <span class=o>//</span> <span class=n>Enable</span> <span class=n>gloo</span> <span class=ow>is</span> <span class=n>it</span><span class=s1>&#39;s used either in cpu data transfer or controller</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>horovod_global</span><span class=o>.</span><span class=n>cpu_operation</span> <span class=o>==</span> <span class=n>LibType</span><span class=p>::</span><span class=n>GLOO</span> <span class=o>||</span>
</span></span><span class=line><span class=cl>        <span class=n>horovod_global</span><span class=o>.</span><span class=n>control_operation</span> <span class=o>==</span> <span class=n>LibType</span><span class=p>::</span><span class=n>GLOO</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>gloo_context</span><span class=o>.</span><span class=n>Enable</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>horovod_global</span><span class=o>.</span><span class=n>control_operation</span> <span class=o>==</span> <span class=n>LibType</span><span class=p>::</span><span class=n>GLOO</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>horovod_global</span><span class=o>.</span><span class=n>controller</span><span class=o>.</span><span class=n>reset</span><span class=p>(</span><span class=n>new</span> <span class=n>GlooController</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>horovod_global</span><span class=o>.</span><span class=n>response_cache</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>horovod_global</span><span class=o>.</span><span class=n>tensor_queue</span><span class=p>,</span> <span class=n>horovod_global</span><span class=o>.</span><span class=n>timeline</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>horovod_global</span><span class=o>.</span><span class=n>parameter_manager</span><span class=p>,</span> <span class=n>horovod_global</span><span class=o>.</span><span class=n>group_table</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>gloo_context</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1>#endif</span>
</span></span><span class=line><span class=cl>    <span class=o>//</span> <span class=n>Reset</span> <span class=n>initialization</span> <span class=n>flag</span>
</span></span><span class=line><span class=cl>    <span class=o>//</span> <span class=n>启动后台线程</span>
</span></span><span class=line><span class=cl>    <span class=n>horovod_global</span><span class=o>.</span><span class=n>initialization_done</span> <span class=o>=</span> <span class=n>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>horovod_global</span><span class=o>.</span><span class=n>background_thread</span> <span class=o>=</span> <span class=n>std</span><span class=p>::</span><span class=n>thread</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>BackgroundThreadLoop</span><span class=p>,</span> <span class=n>std</span><span class=p>::</span><span class=n>ref</span><span class=p>(</span><span class=n>horovod_global</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=o>//</span> <span class=n>Wait</span> <span class=n>to</span> <span class=n>ensure</span> <span class=n>that</span> <span class=n>the</span> <span class=n>background</span> <span class=n>thread</span> <span class=n>has</span> <span class=n>finished</span> <span class=n>initializing</span> <span class=n>MPI</span><span class=o>.</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=p>(</span><span class=err>!</span><span class=n>horovod_global</span><span class=o>.</span><span class=n>initialization_done</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=p>::</span><span class=n>this_thread</span><span class=p>::</span><span class=n>sleep_for</span><span class=p>(</span><span class=n>std</span><span class=p>::</span><span class=n>chrono</span><span class=p>::</span><span class=n>milliseconds</span><span class=p>(</span><span class=mi>1</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><h4 id=433-horovodglobalstate>4.3.3 HorovodGlobalState</h4><p>在 C++ 世界，HorovodGlobalState 起到了<font color=red>集中管理各种全局变量</font>的作用。</p><p>HorovodGlobalState 在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问。HorovodGlobalState 在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）。</p><p>Horovod 主要会在backgroundThreadLoop 中完成 HorovodGlobalState 不同元素初始化，比较重要的有：</p><ul><li>controller 管理总体通信控制流；</li><li>tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等)；</li></ul><div class=highlight id=id-12><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>// All the Horovod state that must be stored globally per-process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>HorovodGlobalState</span> <span class=n>horovod_global</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cp>#if HAVE_MPI
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=n>MPIContext</span> <span class=n>mpi_context</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=cp>#if HAVE_GLOO
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=n>GlooContext</span> <span class=n>gloo_context</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=p>....</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>unique_ptr</span><span class=o>&lt;</span><span class=n>OperationManager</span><span class=o>&gt;</span> <span class=n>op_manager</span><span class=p>;</span></span></span></code></pre></td></tr></table></div></div><p>HorovodGlobalState 摘要如下：</p><div class=highlight id=id-13><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>struct</span> <span class=nc>HorovodGlobalState</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Background thread running MPI communication.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>std</span><span class=o>::</span><span class=kr>thread</span> <span class=n>background_thread</span><span class=p>;</span> <span class=c1>// 后台线程，用来在各个worker之间协调
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=n>ParameterManager</span> <span class=n>parameter_manager</span><span class=p>;</span> <span class=c1>// 维护后台总体参数配置
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=c1>// Encapsulates the fusion buffers, handles resizing and auto-tuning of buffer
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// size.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>FusionBufferManager</span> <span class=n>fusion_buffer</span><span class=p>;</span> <span class=c1>// 融合tensor，以便缩减通信开销
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=n>std</span><span class=o>::</span><span class=n>shared_ptr</span><span class=o>&lt;</span><span class=n>Controller</span><span class=o>&gt;</span> <span class=n>controller</span><span class=p>;</span> <span class=c1>//管理总体通信控制流
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=n>TensorQueue</span> <span class=n>tensor_queue</span><span class=p>;</span> <span class=c1>//处理从前端过来的通信需求（allreduce，broadcast 等）
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=c1>// Pointer to shared buffer for allgather
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>void</span><span class=o>*</span> <span class=n>shared_buffer</span> <span class=o>=</span> <span class=k>nullptr</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// LRU cache of Responses
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>ResponseCache</span> <span class=n>response_cache</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Information on registered groups.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>GroupTable</span> <span class=n>group_table</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=o>~</span><span class=n>HorovodGlobalState</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Make sure that the destructor of the background thread is safe to
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// call. If a thread is still joinable (not detached or complete) its
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// destructor cannot be called.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>background_thread</span><span class=p>.</span><span class=n>joinable</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>shut_down</span> <span class=o>=</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=n>background_thread</span><span class=p>.</span><span class=n>join</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>};</span></span></span></code></pre></td></tr></table></div></div><p>目前具体逻辑如下：</p><div class=highlight id=id-14><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl>           <span class=n>Import</span> <span class=n>python</span> <span class=n>files</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>Import</span> <span class=n>C</span><span class=o>++</span> <span class=n>SO</span> <span class=n>files</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           <span class=n>Create</span> <span class=n>_HorovodBasics</span>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>
</span></span><span class=line><span class=cl>                <span class=n>hvd</span><span class=o>.</span><span class=na>init</span><span class=o>()</span>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>
</span></span><span class=line><span class=cl><span class=n>Python</span>              <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>+-------------------------------------------------------------------------------------------------------------+</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=n>c</span><span class=o>++</span>                 <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>                                                          <span class=o>+-----------------------------+</span>
</span></span><span class=line><span class=cl>                                                                               <span class=o>|</span>  <span class=n>HorovodGlobalState</span>         <span class=o>|</span>
</span></span><span class=line><span class=cl>              <span class=n>horovod_init_comm</span>                                                <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>                             <span class=o>+------------------+</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>                             <span class=o>|</span> <span class=n>horovod_global</span> <span class=o>+---------&gt;</span> <span class=o>|</span>        <span class=n>TensorQueue</span>          <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>                             <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>                             <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>        <span class=n>background_thread</span>    <span class=o>|</span>
</span></span><span class=line><span class=cl>                                                  <span class=o>|</span> <span class=n>mpi_context</span>      <span class=o>|</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>           <span class=n>InitializeHorovodOnce</span>   <span class=o>+------------&gt;</span> <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>        <span class=n>ParameterManager</span>     <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>+</span>                             <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>                             <span class=o>|</span> <span class=n>gloo_context</span>     <span class=o>|</span>         <span class=o>|</span>        <span class=n>FusionBufferManager</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>                             <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=o>|</span>                             <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>        <span class=n>Controller</span>           <span class=o>|</span>
</span></span><span class=line><span class=cl>                    <span class=n>v</span>                             <span class=o>|</span> <span class=n>op_manager</span>       <span class=o>|</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>             <span class=n>background_threa</span>                     <span class=o>|</span>                  <span class=o>|</span>         <span class=o>|</span>        <span class=n>ResponseCache</span>        <span class=o>|</span>
</span></span><span class=line><span class=cl>                                                  <span class=o>+------------------+</span>         <span class=o>|</span>                             <span class=o>|</span>
</span></span><span class=line><span class=cl>                                                                               <span class=o>|</span>        <span class=n>shared_buffer</span>        <span class=o>|</span>
</span></span><span class=line><span class=cl>                                                                               <span class=o>+-----------------------------+</span></span></span></code></pre></td></tr></table></div></div><p>如图：</p><p><img loading=lazy src=images/Horovod_2_init_logic.png#center srcset="/posts/2022-10-08_horovod_2/images/Horovod_2_init_logic.png, images/Horovod_2_init_logic.png#center 1.5x, /posts/2022-10-08_horovod_2/images/Horovod_2_init_logic.png 2x" sizes=auto data-title="HOROVOD INIT LOGIC" data-alt="HOROVOD INIT LOGIC" width=2090 height=1413 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>至此，horovod 已经初始化完成，用户代码可以使用了。</p><h3 id=43-hvd-概念>4.3 hvd 概念</h3><p>在用户代码中，接下来是rank概念。</p><div class=highlight id=id-15><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hvd</span><span class=o>.</span><span class=n>local_rank</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>hvd</span><span class=o>.</span><span class=n>rank</span><span class=p>()</span></span></span></code></pre></td></tr></table></div></div><p>我们介绍下几个相关概念：</p><ul><li>Horovod为设备上的每个GPU启动了该训练脚本的一个副本。<strong>local rank</strong>就是分配给某一台计算机上每个执行训练的唯一编号（也可以认为是进程号或者GPU设备的ID号），范围是 0 到 n-1，其中 n 是该计算机上GPU设备的数量。</li><li>rank 可以认为是代表分布式任务里的一个执行训练的唯一全局编号（<font color=red>用于进程间通讯</font>）。Rank 0 在Horovod中通常具有特殊的意义：<strong>它是负责此同步的设备</strong>。<ul><li>在百度的实现中，不同 Rank 的角色是不一样的，Rank 0 会充当 coordinator 的角色。它会协调来自其他 Rank 的 MPI 请求，是一个工程上的考量。这一设计也被后来的 Horovod 采用。</li><li>Rank 0 也用来把参数广播到其他进程 & 存储 checkpoint。</li><li>world_size：进程总数量，会等到所有world_size个进程就绪之后才会开始训练。</li></ul></li></ul><p>hvd.init 这部分的目的就是让<strong>并行进程</strong>们可以知道自己被分配的 rank / local rank 等信息，于是后续可以根据 local rank（所在节点上的第几张 GPU 卡） 来设置所需的显存分配。</p><h3 id=45--数据处理>4.5 数据处理</h3><p>接下来是数据处理。</p><div class=highlight id=id-16><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_tensor_slices</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>mnist_images</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span> <span class=o>/</span> <span class=mf>255.0</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span>
</span></span><span class=line><span class=cl>             <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>mnist_labels</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>int64</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>repeat</span><span class=p>()</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=mi>10000</span><span class=p>)</span><span class=o>.</span><span class=n>batch</span><span class=p>(</span><span class=mi>128</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>这里有几点需要说明：</p><ul><li><p>首先，训练的数据需要放置在任何节点都能访问的地方。</p></li><li><p>其次，Horovod 需要对数据进行分片处理，需要在不同机器上按Rank进行切分，以保证每个GPU进程训练的数据集是不一样的。</p></li><li><p>数据集本体需要出于数据并行性的需求而被拆分为多个分片，Horovod的不同工作节点都将分别读取自己的数据集分片。</p></li></ul><p>从 PyTorch 示例脚本看得更加清楚。</p><div class=highlight id=id-17><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Horovod: use DistributedSampler to partition the training data.</span>
</span></span><span class=line><span class=cl><span class=n>train_sampler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>DistributedSampler</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=p>,</span> <span class=n>num_replicas</span><span class=o>=</span><span class=n>hvd</span><span class=o>.</span><span class=n>size</span><span class=p>(),</span> <span class=n>rank</span><span class=o>=</span><span class=n>hvd</span><span class=o>.</span><span class=n>rank</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>sampler</span><span class=o>=</span><span class=n>train_sampler</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><ul><li><p><code>DataLoader</code>的采样器组件从要绘制的数据集中返回可迭代的索引。 PyTorch中的默认采样器是顺序的，返回序列<code>0, 1, 2, …, n</code> 。 Horovod使用其<code>DistributedSampler</code>覆盖了此行为，该DistributedSampler处理跨计算机的数据集分区。 <code>DistributedSampler</code>本身接受两个参数作为输入： <code>hvd.size()</code> (GPU的总数，例如16)和<code>hvd.rank()</code> (从总体列表中分配给该设备的ID，例如0…15)。</p></li><li><p>Pytorch使用的是<strong>数据分布式训练</strong>，每个进程实际上是独立加载数据的，所以需要加载相同数据集后用一定的规则根据rank来顺序切割获取不同的数据子集，DistributedSampler就是用来确保dataloader只会load到整个数据集的一个特定子集的做法(实际上不用Pytorch提供的DistributedSampler工具，自己做加载数据后切分word_size个子集按rank顺序拿到子集效果也是一样)。</p></li><li><p>同时为了能够按顺序划分数据子集，拿到不同部分数据，所以数据集不能够进行随机打散，所以用了参数 <code>'shuffle': False</code>。</p></li></ul><h3 id=46-广播初始化变量>4.6 广播初始化变量</h3><p>以下代码完成广播初始化的功能。</p><div class=highlight id=id-18><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hvd</span><span class=o>.</span><span class=n>callbacks</span><span class=o>.</span><span class=n>BroadcastGlobalVariablesCallback</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>这句代码保证的是 rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</p><p>下面就介绍下 Horvod 之中广播的使用。</p><h4 id=461-广播定义>4.6.1 广播定义</h4><p>广播的具体实现是：</p><div class=highlight id=id-19><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BroadcastGlobalVariablesCallbackImpl</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>backend</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>BroadcastGlobalVariablesCallbackImpl</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>backend</span> <span class=o>=</span> <span class=n>backend</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>root_rank</span> <span class=o>=</span> <span class=n>root_rank</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>broadcast_done</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>on_batch_end</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>logs</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>broadcast_done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>hvd</span><span class=o>.</span><span class=n>_executing_eagerly</span><span class=p>()</span> <span class=ow>and</span> <span class=nb>hasattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=s1>&#39;variables&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=c1># TensorFlow 2.0 or TensorFlow eager</span>
</span></span><span class=line><span class=cl>                <span class=n>hvd</span><span class=o>.</span><span class=n>broadcast_variables</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>variables</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                        <span class=n>root_rank</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>root_rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>hvd</span><span class=o>.</span><span class=n>broadcast_variables</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>variables</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                                        <span class=n>root_rank</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>root_rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>bcast_op</span> <span class=o>=</span> <span class=n>hvd</span><span class=o>.</span><span class=n>broadcast_global_variables</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>root_rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>backend</span><span class=o>.</span><span class=n>get_session</span><span class=p>()</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>bcast_op</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>broadcast_done</span> <span class=o>=</span> <span class=kc>True</span></span></span></code></pre></td></tr></table></div></div><h4 id=462-broadcast_variables>4.6.2 broadcast_variables</h4><p>broadcast_variables 调用了 _make_broadcast_group_fn 完成功能，可以看到对于 执行图 的每个变量，调用了 broadcast。</p><div class=highlight id=id-20><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>broadcast_variables</span><span class=p>(</span><span class=n>variables</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Broadcasts variables from root rank to all other processes.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>        variables: variables for broadcast
</span></span></span><span class=line><span class=cl><span class=s2>        root_rank: rank of the process from which global variables will be broadcasted
</span></span></span><span class=line><span class=cl><span class=s2>                   to all other processes.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>broadcast_group</span> <span class=o>=</span> <span class=n>_make_broadcast_group_fn</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>broadcast_group</span><span class=p>(</span><span class=n>variables</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>以及</p><div class=highlight id=id-21><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@_cache</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>_make_broadcast_group_fn</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>_executing_eagerly</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=c1># Eager mode will parallelize independent control flow</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>broadcast_group</span><span class=p>(</span><span class=n>variables</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>var</span> <span class=ow>in</span> <span class=n>variables</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>var</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>var</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>_make_subgraph</span><span class=p>(</span><span class=n>broadcast_group</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Graph mode requires an Op</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>broadcast_group</span><span class=p>(</span><span class=n>variables</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>var</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>var</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>))</span>
</span></span><span class=line><span class=cl>                              <span class=k>for</span> <span class=n>var</span> <span class=ow>in</span> <span class=n>variables</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>broadcast_group</span></span></span></code></pre></td></tr></table></div></div><h4 id=463-调用-mpi>4.6.3 调用 MPI</h4><p>broadcast 就是调用了 MPI 函数真正完成了功能。</p><div class=highlight id=id-22><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>broadcast</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>root_rank</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>ignore_name_scope</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;An op which broadcasts the input tensor on root rank to the same input tensor
</span></span></span><span class=line><span class=cl><span class=s2>    on all other Horovod processes.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    The broadcast operation is keyed by the name of the op. The tensor type and
</span></span></span><span class=line><span class=cl><span class=s2>    shape must be the same on all Horovod processes for a given name. The broadcast
</span></span></span><span class=line><span class=cl><span class=s2>    will not start until all processes are ready to send and receive the tensor.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>      A tensor of the same shape and type as `tensor`, with the value broadcasted
</span></span></span><span class=line><span class=cl><span class=s2>      from root rank.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>name</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>_executing_eagerly</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span> <span class=o>=</span> <span class=s1>&#39;HorovodBroadcast_</span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>_normalize_name</span><span class=p>(</span><span class=n>tensor</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>MPI_LIB</span><span class=o>.</span><span class=n>horovod_broadcast</span><span class=p>(</span><span class=n>tensor</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>name</span><span class=p>,</span> <span class=n>root_rank</span><span class=o>=</span><span class=n>root_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                     <span class=n>ignore_name_scope</span><span class=o>=</span><span class=n>ignore_name_scope</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><h4 id=464-同步参数>4.6.4 同步参数</h4><p>在后台进程中，会<strong>根据情况定期同步参数</strong>。</p><div class=highlight id=id-23><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>bool</span> <span class=nf>RunLoopOnce</span><span class=p>(</span><span class=n>HorovodGlobalState</span><span class=o>&amp;</span> <span class=n>state</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=c1>// 业务逻辑功能
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>state</span><span class=p>.</span><span class=n>parameter_manager</span><span class=p>.</span><span class=n>IsAutoTuning</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>bool</span> <span class=n>should_sync</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span><span class=p>.</span><span class=n>parameter_manager</span><span class=p>.</span><span class=n>Update</span><span class=p>(</span><span class=n>tensor_names</span><span class=p>,</span> <span class=n>total_tensor_size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 看看是否需要同步，如果需要，就同步。
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>should_sync</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>state</span><span class=p>.</span><span class=n>controller</span><span class=o>-&gt;</span><span class=n>SynchronizeParameters</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>......</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>同步参数代码也是调用了 Bcast 功能完成。</p><div class=highlight id=id-24><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>void</span> <span class=n>Controller</span><span class=o>::</span><span class=n>SynchronizeParameters</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>ParameterManager</span><span class=o>::</span><span class=n>Params</span> <span class=n>param</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>is_coordinator_</span><span class=p>)</span> <span class=p>{</span> <span class=c1>// rank 0 执行操作
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>param</span> <span class=o>=</span> <span class=n>parameter_manager_</span><span class=p>.</span><span class=n>GetParams</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>void</span><span class=o>*</span> <span class=n>buffer</span> <span class=o>=</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)(</span><span class=o>&amp;</span><span class=n>param</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>size_t</span> <span class=n>param_size</span> <span class=o>=</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>param</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>Bcast</span><span class=p>(</span><span class=n>buffer</span><span class=p>,</span> <span class=n>param_size</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>Communicator</span><span class=o>::</span><span class=n>GLOBAL</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=n>is_coordinator_</span><span class=p>)</span> <span class=p>{</span> <span class=c1>// worker 执行操作
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>parameter_manager_</span><span class=p>.</span><span class=n>SetParams</span><span class=p>(</span><span class=n>param</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><h3 id=47-distributedoptimizer>4.7 DistributedOptimizer</h3><p>最后需要配置DistributedOptimizer，这就是关键点之一。</p><div class=highlight id=id-25><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class=line><span class=cl><span class=n>opt</span> <span class=o>=</span> <span class=n>hvd</span><span class=o>.</span><span class=n>DistributedOptimizer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>opt</span><span class=p>,</span> <span class=n>backward_passes_per_step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>average_aggregated_gradients</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>TF Optimizer 是模型训练的关键API，可以获取到每个OP的梯度并用来更新权重。HVD 在原始 TF Optimizer的基础上包装了hvd.DistributedOptimizer。</p><p>DistributedOptimizer包装器将原始优化器作为输入，将梯度计算委托给它。 即DistributedOptimizer会调用原始优化器进行梯度计算。这样，在集群中每台机器都会用原始优化器得到自己的梯度（Local Gradient）。</p><p><code>Horovod DistributedOptimizer</code>接下来会使用all-reduce或all-gather来完成全局梯度归并，然后将这些平均梯度应用于所有设备。</p><p>我们梳理下其中的调用关系：</p><ul><li>hvd.DistributedOptimizer继承 keras Optimizer，在计算时候，依然由传入的原始优化器做计算。</li><li>在得到计算的梯度之后，调用 hvd.allreduce 或者 hvd.allgather 来计算。</li><li>最后实施这些平均之后的梯度。从而实现整个集群的梯度归并操作。</li></ul><p>具体后文会详细介绍。</p><h3 id=48-未来可能>4.8 未来可能</h3><p>Horovod 目前架构的基础是：机器学习的模型参数在一张 GPU 上可以存下。</p><p><strong>未来是否可以把模型分片结合进来，是一个很大的看点。</strong></p><p>另外，如果模型的全连接层较多，则全连接层的强耦合性结合 allreduce 类似 bsp 的同步机制，还是会让网络通信时间成为瓶颈。因此，在 ring-allreduce 环境下，同步协议的改造，比如利用 SSP 来替换 BSP，或者利用梯度压缩来加快 allreduce 进程也是值得探索的方向。</p><h2 id=5-总结>5 总结</h2><p>针对文初提出的几个问题，我们现在回答如下：</p><ul><li>Hovorod 怎么进行数据分割？
答案：有的框架可以自动做数据分割。如果框架不提供，则需要用户自己进行数据分割，以保证每个GPU进程训练的数据集是不一样的。</li><li>Hovorod 怎么进行模型分发？
用户需要手动拷贝训练代码到各个节点上。</li><li>Hovorod 启动时候，python 和 C++ 都做了什么？
答案：python 会引入 C++库，初始化各种变量和配置。C++部分会对 MPI，GLOO上下文进行初始化，启动后台进程处理内部通信。</li><li>如何确保 Hovorod 启动时候步骤一致；
答案： rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</li></ul><p>下一篇文章将深入到python世界看看。</p><p>reference:
[1].https://www.cnblogs.com/rossiXYZ/p/14856543.html</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="<nil> 支付宝" data-alt="<nil> 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="<nil> 微信" data-alt="<nil> 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-07-15 15:28:35">更新于 2023-07-15&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/84e2876cfb14f359a6d6034596104deef86f29d1 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(yejian@zhito.com) 84e2876cfb14f359a6d6034596104deef86f29d1: feat: add transformer introduction"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>84e2876</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/2022-10-08_horovod_2/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/Horovod/2022-10-08_horovod_2/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_2/ data-title="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入" data-hashtags=Horovod><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_2/ data-hashtag=Horovod><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_2/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_2/ data-title="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_2/ data-title="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/horovod/ class=post-tag>Horovod</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/2022-10-08_horovod_1/ class=post-nav-item rel=prev title="深度学习分布式训练框架 Horovod[1] -- 基础知识"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>深度学习分布式训练框架 Horovod[1] -- 基础知识</a>
<a href=/posts/2022-10-08_horovod_3/ class=post-nav-item rel=next title="深度学习分布式训练框架 horovod[3] -- Horovodrun背后做了什么">深度学习分布式训练框架 horovod[3] -- Horovodrun背后做了什么<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2023</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>