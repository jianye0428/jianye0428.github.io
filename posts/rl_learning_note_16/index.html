<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习笔记 [16] | 深度确定性策略梯度(DDPG) - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DD"><meta name=keywords content='RL'><meta itemprop=name content="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)"><meta itemprop=description content="0. 引言 在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DD"><meta itemprop=datePublished content="2024-02-25T19:53:12+08:00"><meta itemprop=dateModified content="2024-02-25T21:12:29+08:00"><meta itemprop=wordCount content="4469"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_16/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)"><meta property="og:description" content="0. 引言 在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DD"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-25T19:53:12+08:00"><meta property="article:modified_time" content="2024-02-25T21:12:29+08:00"><meta property="article:tag" content="RL"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)"><meta name=twitter:description content="0. 引言 在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DD"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_16/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_15/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_17/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习笔记 [16] | 深度确定性策略梯度(DDPG)","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_16\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":4469,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_16\/","datePublished":"2024-02-25T19:53:12+08:00","dateModified":"2024-02-25T21:12:29+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习笔记 [16] | 深度确定性策略梯度(DDPG)</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-25 19:53:12"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span title="更新于 2024-02-25 21:12:29"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 4469 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 9 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><div class="details admonition note open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden=true></i>注意<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>本文最后更新于 2024-02-25，文中内容可能已过时。</div></div></div><h1 id=0-引言>0. 引言</h1><p>在<a href=https://www.cnblogs.com/pinard/p/10334127.html target=_blank rel="external nofollow noopener noreferrer">强化学习(十五) A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DDPG)。</p><p>本篇主要参考了DDPG的<a href=https://arxiv.org/pdf/1509.02971.pdf target=_blank rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>和ICML 2016的<a href=https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf target=_blank rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。</p><h1 id=1-从随机策略到确定性策略>1. 从随机策略到确定性策略</h1><p>从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。PG(Policy Gradient)我们在<a href=https://www.cnblogs.com/pinard/p/10137696.html target=_blank rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>里已经讨论过。那什么是确定性策略梯度(Deterministic Policy Gradient，以下简称DPG)呢？</p><p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p><p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成：</p><p>$$\pi_\theta(s)=a$$</p><h1 id=2-从dpg到ddpg>2. 从DPG到DDPG</h1><p>在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：</p><p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p><p>其中状态的采样空间为$\rho^\pi$, $\nabla_\theta log\pi_\theta(s,a)$是分值函数，可见随机性策略梯度需要在整个动作的空间$\pi_\mathrm{\theta}$进行采样。</p><p>而DPG基于Q值的确定性策略梯度的梯度计算公式是：</p><p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi}[\nabla_\theta\pi_\theta(s)\nabla_aQ_\pi(s,a)|<em>{a=\pi</em>\theta(s)}]$$</p><p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p><p>而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？</p><h1 id=3-ddpg的原理>3. DDPG的原理</h1><p>DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。可以复习<a href=https://www.cnblogs.com/pinard/p/9778063.html target=_blank rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。</p><p>DDQN的当前Q网络负责对当前状态 $S$ 使用 $ϵ$−贪婪法选择动作 $A$，执行动作 $A$,获得新状态 $S&rsquo;$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态 $S&rsquo;$使用贪婪法选择动作 $A&rsquo;$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。</p><p>DDQN的目标Q网络则负责基于经验回放池计算目标Q值, 提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。</p><p>现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要 $ϵ$−贪婪法这样的选择方法，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态 $S&rsquo;$ 使用贪婪法选择动作 $A&rsquo;$，这部分工作由于用来估计目标Q值，因此可以放到Actor目标网络完成。</p><p>基于经验回放池和目标Actor网络提供的 $S&rsquo;$, $A&rsquo;$ 计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。</p><p>此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。</p><p>有了上面的思路，我们总结下DDPG 4个网络的功能定位：</p><ul><li><p>(1). <strong>Actor当前网络</strong>: 负责策略网络参数 $θ$的迭代更新，负责根据当前状态 $S$选择当前动作 $A$，用于和环境交互生成 $S&rsquo;$, $R$。</p></li><li><p>(2). <strong>Actor目标网络</strong>: 负责根据经验回放池中采样的下一状态 $S&rsquo;$ 选择最优下一动作$A&rsquo;$。网络参数 $θ&rsquo;$定期从 $θ$复制。</p></li><li><p>(3). <strong>Critic当前网络</strong>: 负责价值网络参数 $w$的迭代更新，负责计算负责计算当前Q值 $Q(S,A,w)$。目标Q值$y_i=R+γQ&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$</p></li><li><p>(4). <strong>Critic目标网络</strong>: 负责计算目标Q值中的 $Q&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$部分。网络参数 $w&rsquo;$ 定期从 $w$复制。</p></li></ul><p>DDPG除了这4个网络结构，还用到了经验回放，这部分用于计算目标Q值，和DQN没有什么区别，这里就不展开了。</p><p>此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w$′=$w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：</p><p>$$\begin{gathered}
w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</p><p>其中 $τ$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。</p><p>同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作 $A$会增加一定的噪声 $N$, 即最终和环境交互的动作 $A$ 的表达式是：</p><p>$$A=\pi_\theta(S)+\mathcal{N}$$</p><p>最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：</p><p>$$J(w)=\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p><p>而对于 Actor当前网络，其损失函数就和之前讲的PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：</p><p>$$\nabla_J(\theta)=\frac1m\sum_{j=1}^m[\nabla_aQ_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>\theta(s)}\nabla_\theta\pi_{\theta(s)}|_{s=s_i}]$$</p><p>这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作 $a_1$和$a_2$，从Critic当前网络得到了两个反馈的 $Q$ 值，分别是 $Q_1$,$Q_2$，假设 $Q_1>Q_2$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加 $a_1$的概率，降低$a_2$的概率，也就是说，Actor想要尽可能的得到更大的Q值。所以我们的Actor的损失可以简单的理解为得到的反馈Q值越大损失越小，得到的反馈Q值越小损失越大，因此只要对状态估计网络返回的Q值取个负号即可，即：</p><p>$$J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,w)$$</p><h1 id=4-ddpg算法流程>4. DDPG算法流程</h1><p>这里我们总结下DDPG的算法流程</p><p>输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为 $θ$,$θ&rsquo;$,$w$,$w&rsquo;$,衰减因子 $γ$, 软更新系数 $τ$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$。最大迭代次数 $T$。随机噪音函数 $\mathcal{N}$</p><p>输出：最优Actor当前网络参数 $θ$,Critic当前网络参数 $w$</p><ul><li>(1). 随机初始化$θ$,$w$, $w$′=$w$,$θ$′=$θ$。清空经验回放的集合$D$</li><li>(2). for i from 1 to T，进行迭代。<ul><li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li><li>b) 在Actor当前网络基于状态 $S$ 得到动作 $A=π_θ(ϕ(S))+\mathcal{N}$</li><li>c) 执行动作$A$,得到新状态$S$′,奖励$R$,是否终止状态%is_end$</li><li>d) 将 ${ϕ(S), A, R, ϕ(S&rsquo;), is_end}$ 这个五元组存入经验回放集合$D$</li><li>e) $S=S'$</li><li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本${\phi(S_j),A_j,R_j,\phi(S_j^{\prime}),is_end_j},j=1,2.,,,m$，计算当前目标Q值$y_j$：<ul><li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\pi_{\theta^{\prime}}(\phi(S_j^{\prime})),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li></ul></li><li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Critic当前网络的所有参数 $w$</li><li>h) 使用 $\begin{aligned}J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,\theta)\end{aligned}$，通过神经网络的梯度反向传播来更新Actor当前网络的所有参数 $θ$</li><li>i) 如果 i%C=1, 则更新Critic目标网络和Actor目标网络参数：<ul><li>$$\begin{gathered} w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</li></ul></li><li>j) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤(b)</li></ul></li></ul><p>以上就是DDPG算法的主流程，要注意的是上面2.f中的 $\pi_{\theta^{\prime}}(\phi(S_j^{\prime}))$ 是通过Actor目标网络得到，而 $Q^{\prime}(\phi(S_i^{\prime}),\pi_{\theta^{\prime}}(\phi(S_i^{\prime})),w^{\prime})$ 则是通过Critic目标网络得到的。</p><h1 id=5-ddpg实例>5. DDPG实例</h1><p>这里我们给出DDPG第一个算法实例，代码主要参考自莫烦的<a href=https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG_update.py target=_blank rel="external nofollow noopener noreferrer">Github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。增加了测试模型效果的部分，优化了少量参数。代码详见：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py</p><p>这里我们没有用之前的CartPole游戏，因为它不是连续动作。我们使用了Pendulum-v0这个游戏。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a href=https://github.com/openai/gym/wiki/Pendulum-v0 target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。输入状态是角度的sin，cos值，以及角速度。一共三个值。动作是一个连续的力矩值。</p><p>两个Actor网络和两个Critic网络的定义参见：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_build_a</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>s</span><span class=p>,</span> <span class=n>scope</span><span class=p>,</span> <span class=n>trainable</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=n>scope</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>dense</span><span class=p>(</span><span class=n>s</span><span class=p>,</span> <span class=mi>30</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s1>&#39;l1&#39;</span><span class=p>,</span> <span class=n>trainable</span><span class=o>=</span><span class=n>trainable</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>dense</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>a_dim</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>tanh</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=n>trainable</span><span class=o>=</span><span class=n>trainable</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>a_bound</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s1>&#39;scaled_a&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>_build_c</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>s</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>scope</span><span class=p>,</span> <span class=n>trainable</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=n>scope</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>n_l1</span> <span class=o>=</span> <span class=mi>30</span>
</span></span><span class=line><span class=cl>    <span class=n>w1_s</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s1>&#39;w1_s&#39;</span><span class=p>,</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span><span class=p>,</span> <span class=n>n_l1</span><span class=p>],</span> <span class=n>trainable</span><span class=o>=</span><span class=n>trainable</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>w1_a</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s1>&#39;w1_a&#39;</span><span class=p>,</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>a_dim</span><span class=p>,</span> <span class=n>n_l1</span><span class=p>],</span> <span class=n>trainable</span><span class=o>=</span><span class=n>trainable</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>b1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_variable</span><span class=p>(</span><span class=s1>&#39;b1&#39;</span><span class=p>,</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_l1</span><span class=p>],</span> <span class=n>trainable</span><span class=o>=</span><span class=n>trainable</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>net</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>s</span><span class=p>,</span> <span class=n>w1_s</span><span class=p>)</span> <span class=o>+</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>w1_a</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>dense</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>trainable</span><span class=o>=</span><span class=n>trainable</span><span class=p>)</span>  <span class=c1># Q(s,a)</span></span></span></code></pre></td></tr></table></div></div><p>Actor当前网络和Critic当前网络损失函数的定义参见：</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=n>td_error</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>labels</span><span class=o>=</span><span class=n>q_target</span><span class=p>,</span> <span class=n>predictions</span><span class=o>=</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>ctrain</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>(</span><span class=n>LR_C</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>td_error</span><span class=p>,</span> <span class=n>var_list</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>ce_params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>a_loss</span> <span class=o>=</span> <span class=o>-</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>    <span class=c1># maximize the q</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>atrain</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>(</span><span class=n>LR_A</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>a_loss</span><span class=p>,</span> <span class=n>var_list</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>ae_params</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>Actor目标网络和Critic目标网络参数软更新，Actor当前网络和Critic当前网络反向传播更新部分的代码如下：</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># soft target replacement</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>soft_replace</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>MEMORY_CAPACITY</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>BATCH_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>bt</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=p>[</span><span class=n>indices</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>  <span class=n>bs</span> <span class=o>=</span> <span class=n>bt</span><span class=p>[:,</span> <span class=p>:</span><span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>ba</span> <span class=o>=</span> <span class=n>bt</span><span class=p>[:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>a_dim</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>br</span> <span class=o>=</span> <span class=n>bt</span><span class=p>[:,</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>bs_</span> <span class=o>=</span> <span class=n>bt</span><span class=p>[:,</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>s_dim</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>atrain</span><span class=p>,</span> <span class=p>{</span><span class=bp>self</span><span class=o>.</span><span class=n>S</span><span class=p>:</span> <span class=n>bs</span><span class=p>})</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ctrain</span><span class=p>,</span> <span class=p>{</span><span class=bp>self</span><span class=o>.</span><span class=n>S</span><span class=p>:</span> <span class=n>bs</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>a</span><span class=p>:</span> <span class=n>ba</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>R</span><span class=p>:</span> <span class=n>br</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>S_</span><span class=p>:</span> <span class=n>bs_</span><span class=p>})</span></span></span></code></pre></td></tr></table></div></div><p>其余的可以对照算法和代码一起学习，应该比较容易理解。</p><h1 id=6-ddpg总结>6. DDPG总结</h1><p>DDPG参考了DDQN的算法思想吗，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p><p>到此，我们的Policy Based RL系列也讨论完了，而在更早我们讨论了Value Based RL系列，至此，我们还剩下Model Based RL没有讨论。后续我们讨论Model Based RL的相关算法。</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-25 21:12:29">更新于 2024-02-25&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/23cd7c478708235ece71456fee411313e37ef46b rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 23cd7c478708235ece71456fee411313e37ef46b: feat: add rl learning note to 19"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>23cd7c4</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_16/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_16/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_16/ data-title="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_16/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_16/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_16/ data-title="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_16/ data-title="强化学习笔记 [16] | 深度确定性策略梯度(DDPG)"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_15/ class=post-nav-item rel=prev title="强化学习笔记 [15] | A3C"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>强化学习笔记 [15] | A3C</a>
<a href=/posts/rl_learning_note_17/ class=post-nav-item rel=next title="强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架">强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.126.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>