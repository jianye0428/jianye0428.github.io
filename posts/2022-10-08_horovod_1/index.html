<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>深度学习分布式训练框架 Horovod[1] -- 基础知识 - yejian's blog</title><meta name=author content="Jian YE"><meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。 1 分布式并行训练 我们首先要介绍下分布式并行训练。 1.1 分布式并行训练的必要 传统的模型训练中，迭代计算"><meta name=keywords content="Horovod"><meta itemprop=name content="深度学习分布式训练框架 Horovod[1] -- 基础知识"><meta itemprop=description content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。 1 分布式并行训练 我们首先要介绍下分布式并行训练。 1.1 分布式并行训练的必要 传统的模型训练中，迭代计算"><meta itemprop=datePublished content="2023-07-10T07:45:42+08:00"><meta itemprop=dateModified content="2023-07-15T15:28:35+08:00"><meta itemprop=wordCount content="9369"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="Horovod,"><meta property="og:title" content="深度学习分布式训练框架 Horovod[1] -- 基础知识"><meta property="og:description" content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。 1 分布式并行训练 我们首先要介绍下分布式并行训练。 1.1 分布式并行训练的必要 传统的模型训练中，迭代计算"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/2022-10-08_horovod_1/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-10T07:45:42+08:00"><meta property="article:modified_time" content="2023-07-15T15:28:35+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="深度学习分布式训练框架 Horovod[1] -- 基础知识"><meta name=twitter:description content="0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。 1 分布式并行训练 我们首先要介绍下分布式并行训练。 1.1 分布式并行训练的必要 传统的模型训练中，迭代计算"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/2022-10-08_horovod_1/><link rel=prev href=https://jianye0428.github.io/posts/hugo_command/><link rel=next href=https://jianye0428.github.io/posts/2022-10-08_horovod_2/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"深度学习分布式训练框架 Horovod[1] -- 基础知识","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/2022-10-08_horovod_1\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"Horovod","wordcount":9369,"url":"https:\/\/jianye0428.github.io\/posts\/2022-10-08_horovod_1\/","datePublished":"2023-07-10T07:45:42+08:00","dateModified":"2023-07-15T15:28:35+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>深度学习分布式训练框架 Horovod[1] -- 基础知识</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i></span></span>
<span class=post-category>收录于 <a href=/categories/distributed-computing/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Distributed Computing</a></span></div><div class=post-meta-line><span title="发布于 2023-07-10 07:45:42"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-10>2023-07-10</time></span>&nbsp;<span title="更新于 2023-07-15 15:28:35"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-15>2023-07-15</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 9369 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 19 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="深度学习分布式训练框架 Horovod[1] -- 基础知识">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#0-摘要>0 摘要</a></li><li><a href=#1-分布式并行训练>1 分布式并行训练</a><ul><li><a href=#11-分布式并行训练的必要>1.1 分布式并行训练的必要</a></li><li><a href=#12-分布式训练>1.2 分布式训练</a></li><li><a href=#13--训练并行机制>1.3 训练并行机制</a><ul><li><a href=#131-三种机制>1.3.1 三种机制</a></li><li><a href=#132-如何使用>1.3.2 如何使用</a></li></ul></li><li><a href=#14-数据并行训练>1.4 数据并行训练</a></li></ul></li><li><a href=#2-通信和架构>2 通信和架构</a><ul><li><a href=#21-方法和架构>2.1 方法和架构</a></li><li><a href=#22-异步-vs-同步>2.2 异步 vs 同步</a></li></ul></li><li><a href=#3-具体架构>3 具体架构</a><ul><li><a href=#31-mapreduce>3.1 MapReduce</a></li><li><a href=#32-parameter-server-参数服务器>3.2 Parameter Server 参数服务器</a></li><li><a href=#33--decentralized-network>3.3 Decentralized Network</a></li></ul></li><li><a href=#4-allreduce>4 AllReduce</a><ul><li><a href=#41-参数服务器劣势>4.1 参数服务器劣势</a></li><li><a href=#42-并行任务通信分类>4.2 并行任务通信分类</a></li><li><a href=#43-mpi_allreduce>4.3 MPI_AllReduce</a></li></ul></li><li><a href=#5--ring-allreduce>5 ring-allreduce</a><ul><li><a href=#51-特点>5.1 特点</a></li><li><a href=#52-策略>5.2 策略</a><ul><li><a href=#521-结构>5.2.1 结构</a></li><li><a href=#522-scatter-reduce>5.2.2 scatter reduce</a><ul><li><a href=#5221-分块>5.2.2.1 分块</a></li><li><a href=#5222-第一次迭代>5.2.2.2 第一次迭代</a></li><li><a href=#5222-全部迭代>5.2.2.2 全部迭代</a></li></ul></li><li><a href=#523-allgather>5.2.3 Allgather</a><ul><li><a href=#5231-第一次迭代>5.2.3.1 第一次迭代</a></li><li><a href=#5232-全部迭代>5.2.3.2 全部迭代</a></li></ul></li><li><a href=#524-horovod-架构图>5.2.4 Horovod 架构图</a></li><li><a href=#525-百度思路>5.2.5 百度思路</a></li></ul></li><li><a href=#53-区别>5.3 区别</a></li></ul></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><h2 id=0-摘要>0 摘要</h2><p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p><p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。</p><h2 id=1-分布式并行训练>1 分布式并行训练</h2><p>我们首先要介绍下<strong>分布式并行训练</strong>。</p><h3 id=11-分布式并行训练的必要>1.1 分布式并行训练的必要</h3><p>传统的模型训练中，<font color=red><strong>迭代计算</strong></font>只能利用当前进程所在主机上的所有硬件资源，可是<u>单机扩展性始终有限</u>。而目前的机器学习有如下特点：</p><ul><li><strong>样本数量大</strong> 目前训练数据越来越多，在大型互联网场景下，每天的样本量可以达到百亿级别。</li><li><strong>特征维度多</strong> 因为巨大样本量导致机器学习模型参数越来越多，特征维度可以达到千亿或者万亿级别。</li><li><strong>训练性能要求高</strong> 虽然样本量和模型参数巨大，但是业务需要我们在短期内训练出一个优秀的模型来验证。</li><li><strong>模型实时上线</strong> 对于推荐资讯类应用，往往要求根据用户最新行为及时调整模型进行预测。</li></ul><p>因此，单机面对海量数据和巨大模型时是无能为力的，有必要把数据或者模型分割成为多份，在多个机器上借助不同主机上的硬件资源进行训练加速。</p><h3 id=12-分布式训练>1.2 分布式训练</h3><p>本文所说的训练，指的是<font color=red>利用训练数据通过计算梯度下降的方式迭代地去优化神经网络参数，并最终输出网络模型的过程</font>。在单次模型训练迭代中，会有如下操作：</p><ul><li>首先利用数据对模型进行前向的计算。所谓的前向计算，就是将模型上一层的输出作为下一层的输入，并计算下一层的输出，从输入层一直算到输出层为止。</li><li>其次会根据目标函数，我们将反向计算模型中每个参数的导数，并且结合学习率来更新模型的参数。</li></ul><p>而并行梯度下降的基本思想便是：<font color=red>多个处理器分别利用自己的数据来计算梯度，最后通过聚合或其他方式来实现并行计算梯度下降以加速模型训练过程</font>。 比如两个处理器分别处理一半数据计算梯度 g_1, g_2，然后把两个梯度结果进行聚合更新，这样就实现了并行梯度下降。</p><h3 id=13--训练并行机制>1.3 训练并行机制</h3><h4 id=131-三种机制>1.3.1 三种机制</h4><p>由于使用小批量算法，可以把宽度$(∝W)$和深度$(∝D)$的前向传播和反向传播分发到并行的处理器上，这样深度训练的并行机制主要有三种：</p><ul><li>第一个是<font color=red><strong>模型并行机制</strong></font>（按照网络结构分区）。<ul><li>通常是针对一个节点无法存下整个模型的情况下，去对图进行拆分。</li><li>将模型参数进行分布式存储。<strong><u>计算机上每个计算可以建模为一个有向无环图（DAG），顶点是计算指令，边是数据依赖（数据流）。</u></strong>&ldquo;基于图去拆分&rdquo; 会根据每一层中的神经元（即四维张量中的C、H或W维）来把一张大的图拆分成很多部分，每个部分都会在很多设备上去计算。</li><li>或者可以这么理解：深度学习的计算主要是矩阵运算，有时候矩阵非常大无法放到显存中，就只能把超大矩阵拆分了放到不同卡上计算。</li><li>模型较后部分的计算必须等前面计算完成，因此不同节点间的计算实际是串行的。但每个部分计算互不妨碍，更像是流水线结构。</li></ul></li><li>第二个是<font color=red><strong>数据并行机制</strong></font>（按照输入样本分区）。<ul><li>更多场景下我们模型规模不大，在一张 GPU 可以容纳，但是训练数据量会比较大，这时候就采用数据并行机制。</li><li>具体就是在多节点上并行分割数据和训练。</li></ul></li><li>第三种不常用的并行机制是 <font color=red><strong>流水线机制</strong></font>（按层分区）。<ul><li>在深度学习中，流水线可以是指重叠的计算，即在一层和下一层之间（当数据准备就绪时）连续计算；或者根据深度划分DNN，将层分配给特定处理器。</li><li>流水线可以看作是数据并行的一种形式，因为元素（样本）是通过网络并行处理的，但也可以看作是模型并行，因为流水线的长度是由DNN结构决定的。</li></ul></li></ul><p>具体可见下图:
<img loading=lazy src=images/Horovod_1_parallel_mechanism.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_parallel_mechanism.png, images/Horovod_1_parallel_mechanism.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_parallel_mechanism.png 2x" sizes=auto data-title="Parallel Mechanism" data-alt="Parallel Mechanism" width=1003 height=283 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h4 id=132-如何使用>1.3.2 如何使用</h4><p><u>数据的并行往往意味着<strong>计算性能</strong>的可扩展，而模型的并行往往意味着<strong>内存使用</strong>的可扩展。</u></p><p>需要注意的是：<font color=green>数据并行和模型并行也并不冲突，两者可以同时存在，而流水线机制也可以和模型并行一起混用。</font>比如，DistBelief分布式深度学习系统结合了三种并行策略。训练在同时复制的多个模型上训练，每个模型副本在不同的样本上训练（数据并行），每个副本上，依据同一层的神经元（模型并行性）和不同层（流水线）上划分任务，进行分布训练。</p><p>另外也需要根据具体问题具体分析，比如现代卷积神经网络主要由两种层构成，他们具有不一样的属性和性能。</p><ul><li><strong>卷积层</strong>，占据了90% ~ 95% 的计算量，5% 的参数，但是对结果具有很大的表达能力。</li><li><strong>全连接层</strong>，占据了 5% ~ 10% 的计算量， 95% 的参数，但是对于结果具有相对较小的表达的能力。</li></ul><p>综上：卷积层计算量大，所需参数系数 W 少，全连接层计算量小，所需参数系数 W 多。因此对于卷积层适合使用数据并行，对于全连接层适合使用模型并行。</p><p><img loading=lazy src=images/Horovod_1_model_parallel_and_data_parallel.jpg#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_model_parallel_and_data_parallel.jpg, images/Horovod_1_model_parallel_and_data_parallel.jpg#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_model_parallel_and_data_parallel.jpg 2x" sizes=auto data-title="model parallel and data parallel" data-alt="model parallel and data parallel" width=989 height=482 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h3 id=14-数据并行训练>1.4 数据并行训练</h3><p>我们本系列主要讨论数据并行训练（其中的一种架构）.</p><p>数据并行训练只是一种逻辑架构。我们从沐神的书里面摘录：</p><blockquote><p>假设机器上有k个GPU。给定要训练的模型，每个GPU将独立地维护一组完整的模型参数，尽管GPU上的参数值是相同且同步的。例如，下图演示了在k=2时使用数据并行的训练。</p></blockquote><blockquote><p><img loading=lazy src=images/Horovod_1_data_parallel.svg#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_data_parallel.svg, images/Horovod_1_data_parallel.svg#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_data_parallel.svg 2x" sizes=auto data-title="data parallel" data-alt="data parallel" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p></blockquote><blockquote><p>一般来说，训练过程如下：</p><ul><li>在训练的任何迭代中，给定一个随机的小批量，我们将该小批量中的样本分成k个部分，并将它们均匀地分在多个GPU上。</li><li>每个GPU根据分配给它的小批量子集计算模型参数的损失和梯度。</li><li>将k个GPU中每个GPU的局部梯度聚合以获得当前的小批量随机梯度。</li><li>聚合梯度被重新分配到每个GPU。</li><li>每个GPU使用这个小批量随机梯度来更新它维护的完整的模型参数集。</li></ul></blockquote><h2 id=2-通信和架构>2 通信和架构</h2><p>前面提到并行梯度下降的例子：两个处理器分别处理一般数据计算梯度 $g_1$, $g_2$，然后把两个梯度结果进行聚合，最后再把最新参数发给各个分布计算单元，这种训练算法叫<strong>模型一致性方法</strong>（consistent model methods）。<font color=red>这就涉及到了通信问题，即如何做聚合</font>。</p><h3 id=21-方法和架构>2.1 方法和架构</h3><p>一般有两种通信方法：<strong>Share memory</strong> 和 <strong>Message passing</strong>。</p><ul><li><strong>Share memory</strong> 就是<u>所有处理器共享同一块内存</u>，这样通信很容易，<u>但是同一个节点内的处理器之间才可以共享内存，不同节点处理器之间无法共享内存</u>。</li></ul><p><img loading=lazy src=images/Horovod_1_share_memory.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_share_memory.png, images/Horovod_1_share_memory.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_share_memory.png 2x" sizes=auto data-title="share memory" data-alt="share memory" width=633 height=374 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><ul><li><strong>Message passing</strong> 就是<u>不同节点之间用消息</u>（比如基于 TCP/IP 或者 RDMA）进行传递/通信，这样容易扩展，可以进行大规模训练。</li></ul><p><img loading=lazy src=images/Horovod_1_message_passing.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_message_passing.png, images/Horovod_1_message_passing.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_message_passing.png 2x" sizes=auto data-title="message passing" data-alt="message passing" width=872 height=686 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>因此我们知道，Message passing 才是解决方案，于是带来了问题：<font color=red>如何协调这些节点之间的通讯</font>。</p><p>有两种架构：</p><ul><li><font color=red>Client-Server</font>架构: 一个 server 节点协调其他节点工作，其他节点是用来执行计算任务的 worker。</li><li><font color=red>Peer-to-Peer</font>架构：每个节点都有邻居，邻居之间可以互相通信。</li></ul><h3 id=22-异步-vs-同步>2.2 异步 vs 同步</h3><p>异步 vs 同步 是通信的另外一个侧面。</p><p>在数据并行训练之中，各个计算设备分别根据各自获得的batch，前向计算获得损失，进而反向传播计算梯度。计算好梯度后，就涉及到一个<font color=red><strong>梯度同步的问题</strong></font>：每个计算设备 都有根据自己的数据计算的梯度，如何在不同GPU之间维护模型的不同副本之间的一致性？ 如果不同的模型以某种方式最终获得不同的权重，则权重更新将变得不一致，并且模型训练将有所不同。</p><blockquote><blockquote><p><font color=red><strong>怎么做这个同步就是设计分布式机器学习系统的一个核心问题</strong></font>。</p></blockquote></blockquote><p>分布式训练的梯度同步策略可分为<strong>异步（asynchronous）梯度更新</strong> 和 <strong>同步（synchronous）梯度更新</strong>机制。</p><ul><li><p><font color=red><strong>同步</strong></font>指的是所有的设备都是采用相同的模型参数来训练，<u>等待所有设备的mini-batch训练完成后，收集它们的梯度然后取均值，然后执行模型的一次参数更新</u>。</p><ul><li>同步训练相当于通过聚合很多设备上的mini-batch形成一个很大的batch来训练模型，Facebook就是这样做的，但是他们发现当batch大小增加时，同时线性增加学习速率会取得不错的效果。</li><li>同步训练看起来很不错，但是实际上需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡。</li><li>因为每一轮结束时算得快的节点都需等待算得慢的节点算完，再进行下一轮迭代。类似于木桶效应，一个拖油瓶会严重拖慢训练进度，所以同步训练方式相对来说训练速度会慢一些。这个拖油瓶一般就叫做 straggler。(缺点)</li></ul></li><li><p><font color=red><strong>异步</strong></font>训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数，这样总体会训练速度会快很多</p><ul><li>异步训练的一个很严重的问题是<strong>梯度失效问题</strong>（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数其实已经被其它设备更新过了，此时这个梯度就过期了，因为现在的模型参数和训练前采用的参数是不一样的。由于梯度失效问题，异步训练虽然速度快，但是可能陷入次优解（sub-optimal training performance）。</li></ul></li></ul><p>具体如图所示:</p><p><img loading=lazy src=images/Horovod_1_synchronous_data_parallel.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_synchronous_data_parallel.png, images/Horovod_1_synchronous_data_parallel.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_synchronous_data_parallel.png 2x" sizes=auto data-title="synchronous data parallel" data-alt="synchronous data parallel" width=660 height=331 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
<img loading=lazy src=images/Horovod_1_asynchronous_data_parallel.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_asynchronous_data_parallel.png, images/Horovod_1_asynchronous_data_parallel.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_asynchronous_data_parallel.png 2x" sizes=auto data-title="asynchronous data parallel" data-alt="asynchronous data parallel" width=703 height=327 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>这两种更新方式各有优缺点：</p><ul><li>异步更新可能会更快速地完成整个梯度计算。</li><li>同步更新 可以更快地进行一个收敛。</li></ul><p>选择哪种方式取决于实际的应用场景。</p><h2 id=3-具体架构>3 具体架构</h2><p>接下来，我们看看几种具体架构实现，先给出一个总体说明：</p><table><thead><tr><th>名称</th><th>通信</th><th>架构</th><th>并行性</th></tr></thead><tbody><tr><td>MapReduce</td><td>消息传递</td><td>client-server</td><td>批同步</td></tr><tr><td>Parameter Server</td><td>消息传递</td><td>client-server</td><td>异步</td></tr><tr><td>Decentralized Network</td><td>消息传递</td><td>P2P(Peer to Peer)</td><td>同步或异步</td></tr></tbody></table><h3 id=31-mapreduce>3.1 MapReduce</h3><p>MapReduce是Client-Server架构。以 Spark 为例看看是如何进行并行化：</p><ul><li>Spark Driver 就是 Server，Spark Executor 就是 Worker 节点，每一个梯度下降过程包含一个<font color=red>广播</font>、<font color=red>map</font>和一个 <font color=red>reduce</font> 操作。</li><li>Server 定义了 map操作（就是具体的训练），也可以把信息广播到worker节点。</li><li>Worker 会执行 map 操作进行训练，在此过程中，数据被分给 worker 进行计算。</li><li>计算结束后，worker把计算结果传回 driver 处理，这个叫做reduce。</li><li>在 reduce 过程中，Server 节点对 worker 传来的计算结果进行聚合之后，把聚合结果广播到各个worker节点，进行下一次迭代。</li></ul><h3 id=32-parameter-server-参数服务器>3.2 Parameter Server 参数服务器</h3><p>Parameter server 也是一种client-server架构。<u>和MapReduce不同在于 Parameter server 可以是异步的</u>，MapReduce只有等所有map都完成了才能做reduce操作。</p><p>参数服务器架构中，计算设备被划分为参数服务器（PS）和worker。</p><ul><li><strong>参数服务器（server</strong>）。是中心化的组件，主要是负责模型参数的存储，平均梯度和交换更新。参数服务器可以按照不同比例的参数服务器和工作线程进行配置，每个参数服务器都有着不同的配置数据。</li><li><strong>工作节点（worker）</strong>。每个工作节点会负责它领域内的数据分片所对应模型参数的更新计算（比如前向和反向传播这类计算密集的运算），同时它们又会向参数服务器去传递它所计算的梯度，由参数服务器来汇总所有的梯度，再进一步反馈到所有节点。</li></ul><p>具体步骤如下：</p><ul><li>所有的参数都存储在参数服务器中，而 工作节点（worker） 是万年打工仔。</li><li>工作节点 们只负责计算梯度，待所有计算设备完成梯度计算之后，把计算好的梯度发送给参数服务器，这样参数服务器收到梯度之后，执行一定的计算（梯度平均等）之后，就更新其维护的参数，做到了在节点之间对梯度进行平均，利用平均梯度对模型进行更新。</li><li>然后参数服务器再把更新好的新参数返回给所有的工作节点，以对每个节点中的模型副本应用一致化更新。</li><li>打工仔们会再进行下一轮的前后向计算。</li></ul><p>逻辑如下：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>     <span class=o>+----------------------------------------------+</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>  <span class=n>Parameter</span> <span class=n>Server</span>                            <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>                                              <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>                                              <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>   <span class=n>Compute</span> <span class=p>:</span> <span class=n>New</span> <span class=n>P</span> <span class=o>=</span> <span class=n>P</span> <span class=o>+</span> <span class=n>Sum</span><span class=p>(</span><span class=n>Delta</span> <span class=n>P</span> <span class=o>...</span><span class=p>)</span>     <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>                                              <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>                                              <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>   <span class=n>Parameter</span> <span class=mi>1</span><span class=p>,</span> <span class=n>Parameter</span> <span class=mi>2</span><span class=p>,</span> <span class=n>Parameter</span> <span class=mi>3</span> <span class=o>...</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>                                              <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>|</span>                                              <span class=o>|</span>
</span></span><span class=line><span class=cl>     <span class=o>+--+----+----------+--+----------------+--+----+</span>
</span></span><span class=line><span class=cl>        <span class=o>^</span>    <span class=o>|</span>          <span class=o>^</span>  <span class=o>|</span>                <span class=o>^</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl>        <span class=o>|</span>    <span class=o>|</span>          <span class=o>|</span>  <span class=o>|</span>                <span class=o>|</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=n>Delta</span> <span class=n>P</span> <span class=o>|</span>    <span class=o>|</span>   <span class=n>Delta</span> <span class=n>P</span><span class=o>|</span>  <span class=o>|</span>         <span class=n>Delta</span> <span class=n>P</span><span class=o>|</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=o>+-----+</span>    <span class=o>|</span>          <span class=o>|</span>  <span class=o>|</span>                <span class=o>|</span>  <span class=o>+------+</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span>    <span class=o>+-----+</span>          <span class=o>|</span>  <span class=o>|</span>                <span class=o>|</span>         <span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span>    <span class=o>|</span> <span class=n>New</span> <span class=n>P</span>          <span class=o>|</span>  <span class=o>|</span> <span class=n>New</span> <span class=n>P</span>          <span class=o>+------+</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span>    <span class=o>|</span>                <span class=o>|</span>  <span class=o>|</span>                       <span class=o>|</span>  <span class=o>|</span>  <span class=n>New</span> <span class=n>P</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span>    <span class=n>v</span>                <span class=o>|</span>  <span class=o>|</span>                       <span class=o>|</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span>                     <span class=o>|</span>  <span class=n>v</span>                       <span class=o>|</span>  <span class=n>v</span>
</span></span><span class=line><span class=cl><span class=o>+-+-----------+</span>   <span class=o>+-----+--+---+</span>             <span class=o>+-----+--+---+</span>
</span></span><span class=line><span class=cl><span class=o>|</span> <span class=n>Worker</span>      <span class=o>|</span>   <span class=o>|</span> <span class=n>Worker</span>     <span class=o>|</span>             <span class=o>|</span> <span class=n>Worker</span>     <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span>             <span class=o>|</span>   <span class=o>|</span>            <span class=o>|</span>             <span class=o>|</span>            <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span>             <span class=o>|</span>   <span class=o>|</span>            <span class=o>|</span>   <span class=o>......</span>    <span class=o>|</span>            <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span>       <span class=n>Model</span> <span class=o>|</span>   <span class=o>|</span>     <span class=n>Model</span>  <span class=o>|</span>             <span class=o>|</span>     <span class=n>Model</span>  <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>+------+------+</span>   <span class=o>+------+-----+</span>             <span class=o>+----+-------+</span>
</span></span><span class=line><span class=cl>       <span class=o>^</span>                 <span class=o>^</span>                        <span class=o>^</span>
</span></span><span class=line><span class=cl>       <span class=o>|</span>                 <span class=o>|</span>                        <span class=o>|</span>
</span></span><span class=line><span class=cl>       <span class=o>|</span>                 <span class=o>|</span>                        <span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=o>+----+----+</span>       <span class=o>+----+-----+</span>               <span class=o>+--+-----+</span>
</span></span><span class=line><span class=cl>  <span class=o>|</span> <span class=n>Data</span> <span class=mi>1</span>  <span class=o>|</span>       <span class=o>|</span>  <span class=n>Data</span> <span class=mi>2</span>  <span class=o>|</span>               <span class=o>|</span> <span class=n>Data</span> <span class=mi>3</span> <span class=o>|</span>
</span></span><span class=line><span class=cl>  <span class=o>+---------+</span>       <span class=o>+----------+</span>               <span class=o>+--------+</span></span></span></code></pre></td></tr></table></div></div><p>如图:
<img loading=lazy src=images/Horovod_1_parameter_server.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_parameter_server.png, images/Horovod_1_parameter_server.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_parameter_server.png 2x" sizes=auto data-title="asynchronous data parallel" data-alt="asynchronous data parallel" width=603 height=650 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>参数服务器既可以用在数据并行上，也可以被用到模型并行训练上。比如可以将模型切分为多个部分，存储在不同的PS Server节点上，并提供方便的访问服务，这是参数服务器的本质。</p><h3 id=33--decentralized-network>3.3 Decentralized Network</h3><p>Decentralized Network 就是去中心化网络，其特点如下：</p><ul><li>去中心化网络没有一个中心节点，属于 Peer-to-Peer 架构。</li><li>采用 message passing 进行通信，且节点只和邻居通信。</li><li>并行方式可以采用异步或者同步。</li><li>去中心化网络的收敛情况取决于网络连接情况：<ul><li>连接越紧密，收敛性越快，当强连接时候，模型可以很快收敛；</li><li>如果不是强连接，它可能不收敛；</li></ul></li></ul><h2 id=4-allreduce>4 AllReduce</h2><p>因为本系列是 Horovod，所以我们要先说说参数服务器的劣势，下一个系列我们再说参数服务器优势。</p><h3 id=41-参数服务器劣势>4.1 参数服务器劣势</h3><p>尽管参数服务器可以提升表现，但仍然面临几个问题：</p><ul><li><font color=red>确定工作者与参数服务器的正确比例</font>：如果使用一个参数服务器，它可能会成为网络或计算瓶颈。 如果使用多个参数服务器，则通信模式变为“All-to-All”，这可能使网络饱和。</li><li><font color=red>处理程序复杂性</font>：参数服务器的概念较多，这通常导致陡峭的学习曲线和大量的代码重构，压缩了实际建模的时间。</li><li><font color=red>硬件成本</font> : 参数服务器的引入也增加了系统的硬件成本。</li></ul><p>人们发现，MPI_AllReduce 语义也可以很好地满足数据并行训练这一需要。</p><p>需要注意的是：AllReduce <strong>既可以是去中心化，也可以是主从式的。</strong></p><h3 id=42-并行任务通信分类>4.2 并行任务通信分类</h3><p>并行任务的通信一般可以分为 <font color=red><strong>Point-to-point communication</strong></font>和 <font color=red><strong>Collective communication</strong></font>。</p><ul><li>P2P 这种模式只有一个sender和一个receiver，实现起来比较简单，比如NV GPU Direct P2P技术服务于单机多卡的单机卡间数据通信 。</li><li>Collective communication包含多个sender和多个receiver，一般的通信原理包括 broadcast，gather,all-gather，scatter，reduce，all-reduce，reduce-scatter，all-to-all等。</li></ul><h3 id=43-mpi_allreduce>4.3 MPI_AllReduce</h3><p>AllReduce<font color=red>(对m个独立参数进行规约，并将规约结果返回给所有进程)</font>, 其实是最显然和直接的<strong>分布式机器学习抽象</strong>，因为大部分算法的结构都是分布数据。<u>在每个子集上面算出一些局部统计量，然后整合出全局统计量，并且再分配给各个节点去进行下一轮的迭代，这样一个过程就是AllReduce</u>。</p><ul><li><p>可以把每个 Worker 看作是 MPI 概念中的一个进程，比如可以用 4 个 Worker 组成了一个组，该组由 4 个进程组成。我们在这四个进程中对梯度进行一次 MPI_AllReduce。</p></li><li><p>根据 MPI_AllReduce 的语义，所有参与计算的进程都有结果，所以梯度就完成了分发。只要在初始化的时候，我们可以保证每个 Worker 的参数是一致的，那在后续的迭代计算中，参数会一直保持一致，因为梯度信息是一致的。</p></li><li><p>AllReduce 跟 MapReduce 有类似，但后者采用的是<u>面向通用任务处理的多阶段执行任务的方式</u>，而AllReduce则让一个程序在必要的时候占领一台机器，并且在所有迭代的时候一直跑到底，来防止重新分配资源的开销，这更加适合于机器学习的任务处理。</p></li></ul><p>所以，MPI_AllReduce 的语义可以很好地解决深度学习中梯度同步的问题。但是到底能不能使用它，还是要看下层的实现对这一场景是否足够友好。</p><h2 id=5--ring-allreduce>5 ring-allreduce</h2><p>百度提出使用新算法来平均梯度，取消 Reducer，并让这些梯度在所有节点之间交流，这被称为 ring-allreduce，他们使用 TensorFlow 也实现了这种算法（https://github.com/baidu-research/tensorflow-allreduce）。</p><h3 id=51-特点>5.1 特点</h3><p><strong>Ring-Allreduce</strong>特点如下：</p><ul><li>Ring Allreduce 算法使用定义良好的成对消息传递步骤序列在一组进程之间同步状态（在这种情况下为张量）。</li><li>Ring-Allreduce 的命名中 Ring 意味着设备之间的拓扑结构为一个逻辑环形，每个设备都应该有一个左邻和一个右邻居，且本设备只会<strong>向它右邻居发送数据，并且从它的左邻居接受数据</strong>。</li><li>Ring-Allreduce 的命名中的 Allreduce 则代表着没有中心节点，架构中的每个节点都是梯度的汇总计算节点。</li><li>此种算法各个节点之间只与相邻的两个节点通信，并不需要参数服务器。因此，所有节点都参与计算也参与存储，也避免产生中心化的通信瓶颈。</li><li>相比PS架构，Ring-Allreduce 架构是<strong>带宽优化</strong>的，因为集群中每个节点的带宽都被充分利用。<ul><li>在 ring-allreduce 算法中，每个 N 节点与其他两个节点进行 2 * (N-1) 次通信。在这个通信过程中，一个节点发送并接收数据缓冲区传来的块。<strong>在第一个N-1迭代中，接收的值被添加到节点缓冲区中的值</strong>。<strong>在第二个N-1迭代中，接收的值代替节点缓冲区中保存的值</strong>。百度的文章证明了这种算法是带宽上最优的，这意味着如果缓冲区足够大，它将最大化地利用可用的网络。</li></ul></li><li>在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于后面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。</li><li>Ring架构下的同步算法将参数在通信环中依次传递，往往需要多步才能完成一次参数同步。在大规模训练时会引入很大的通信开销，并且对小尺寸张量（tensor）不够友好。对于小尺寸张量，可以采用批量操作（batch）的方法来减小通信开销。</li></ul><p>综上所述，Ring-based AllReduce 架构的网络通讯量如果处理适当，不会随着机器增加而增加，而仅仅和模型 & 网络带宽有关，这针对参数服务器是个巨大的提升。</p><h3 id=52-策略>5.2 策略</h3><p>Ring-based AllReduce 策略包括 <font color=red>Scatter-Reduce</font> 和 <font color=red>AllGather</font> 两个阶段。</p><ul><li><p>首先是scatter-reduce，scatter-reduce 会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分，是最终结果的一个块。</p><ul><li>假设环中有 N 个 worker，每个 worker 有长度相同的数组，需要将 worker 的数组进行求和。在 Scatter-Reduce 阶段，每个 worker 会将数组分成 N 份数据块，然后 worker 之间进行 N 次数据交换。在第 k 次数据交换时，第 i 个 worker 会将自己的 (i - k) % N 份数据块发送给下一个 worker。接收到上一个 worker 的数据块后，worker 会将其与自己对应的数据块求和。</li></ul></li><li><p>然后是allgather。<u>GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的最终融合梯度</u>。</p><ul><li>在执行完 Scatter-Reduce 后，每个 worker 的数组里都有某个数据块是最终求和的结果，现在需要将各数据块的最后求和结果发送到每个 worker 上。和 Scatter-Reduce 一样，也需要 N 次循环。在第 k 次循环时，第 i 个 worker 会将其第 (i+1-k)%N 个数据块发送给下一个 worker 。接收到前一个 worker 的数据块后，worker 会用接收的数据快覆盖自己对应的数据块。进行 N 次循环后，每个 worker 就拥有了数组各数据块的最终求和结果了。</li></ul></li></ul><p>以下部分来自 <a href=https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/ target=_blank rel="external nofollow noopener noreferrer">https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>，这是我能找到最优秀的解读。</p><h4 id=521-结构>5.2.1 结构</h4><p>环形结构如下，每个 GPU 应该有一个左邻居和一个右邻居；它只会向其右侧邻居发送数据，并从其左侧邻居接收数据。</p><p><img loading=lazy src=images/Horovod_1_ring_allreduce.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_ring_allreduce.png, images/Horovod_1_ring_allreduce.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_ring_allreduce.png 2x" sizes=auto data-title="ring allreduce" data-alt="ring allreduce" width=745 height=587 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h4 id=522-scatter-reduce>5.2.2 scatter reduce</h4><p>scatter-reduce：会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分。</p><p>为简单起见，我们假设目标是按元素对单个大型浮点数数组的所有元素求和；系统中有 N 个 GPU，每个 GPU 都有一个相同大小的数组，在 allreduce 的最后环节，每个 GPU 都应该有一个相同大小的数组，其中包含原始数组中数字的总和。</p><h5 id=5221-分块>5.2.2.1 分块</h5><p>首先，GPU 将阵列划分为 N 个较小的块（其中 N 是环中的 GPU 数量）。</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce.png, images/Horovod_1_scatter_reduce.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=832 height=499 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>接下来，GPU 将进行 N-1 次 scatter-reduce 迭代。</p><p>在每次迭代中，GPU 会将其一个块发送到其右邻居，并将从其左邻居接收一个块并累积到该块中。每个 GPU 发送和接收的数据块每次迭代都不同。第 n 个 GPU 通过发送块 n 和接收块 n – 1 开始，然后逐步向后进行，每次迭代发送它在前一次迭代中接收到的块。</p><h5 id=5222-第一次迭代>5.2.2.2 第一次迭代</h5><p>在第一次迭代中，上图中的五个 GPU 将发送和接收以下块：</p><table><thead><tr><th>GPU</th><th>发送</th><th>接收</th></tr></thead><tbody><tr><td>0</td><td>块0</td><td>块4</td></tr><tr><td>1</td><td>块1</td><td>块0</td></tr><tr><td>2</td><td>块2</td><td>块1</td></tr><tr><td>3</td><td>块3</td><td>块2</td></tr><tr><td>4</td><td>块4</td><td>块3</td></tr></tbody></table><p>scatter-reduce 的第一次迭代中的数据传输如下：</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_1.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_1.png, images/Horovod_1_scatter_reduce_iter_1.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_1.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=856 height=597 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>第一次发送和接收完成后，每个 GPU 都会有一个块，该块由两个不同 GPU 上相同块的总和组成。例如，第二个 GPU 上的第一个块将是该块中来自第二个 GPU 和第一个 GPU 的值的总和。</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_2.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_2.png, images/Horovod_1_scatter_reduce_iter_2.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_2.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=850 height=612 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h5 id=5222-全部迭代>5.2.2.2 全部迭代</h5><p>在后续迭代中，该过程继续直到最后。最终每个 GPU 将有一个块，这个块包含所有 GPU 中该块中所有值的总和。</p><p>下面系列图展示了所有数据传输和中间结果，从第一次迭代开始，一直持续到scatter-reduce完成。</p><p>iter 1：</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_1.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_1.png, images/Horovod_1_scatter_reduce_iter_1.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_1.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=856 height=597 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>iter2：</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_2.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_2.png, images/Horovod_1_scatter_reduce_iter_2.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_2.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=850 height=612 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>iter3：</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_3.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_3.png, images/Horovod_1_scatter_reduce_iter_3.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_3.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=850 height=597 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>iter4：</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_4.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_4.png, images/Horovod_1_scatter_reduce_iter_4.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_4.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=855 height=597 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>所有 scatter-reduce 传输后的最终状态</p><p><img loading=lazy src=images/Horovod_1_scatter_reduce_iter_5.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_5.png, images/Horovod_1_scatter_reduce_iter_5.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_scatter_reduce_iter_5.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=833 height=469 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h4 id=523-allgather>5.2.3 Allgather</h4><p>在 scatter-reduce 步骤完成后，在每个 GPU 的数组中都有某一些值（每个 GPU 有一个块）是最终值，其中包括来自所有 GPU 的贡献。为了完成 allreduce，GPU 必须接下来交换这些块，以便所有 GPU 都具有最终所需的值。</p><p>ring allgather 与 scatter-reduce 进行相同的处理（发送和接收的 N-1 次迭代），但是他们这次不是累积 GPU 接收的值，而只是简单地覆盖块。第 n 个 GPU 开始发送第 n+1 个块并接收第 n 个块，然后在以后的迭代中始终发送它刚刚接收到的块。</p><h5 id=5231-第一次迭代>5.2.3.1 第一次迭代</h5><p>例如，在我们的 5-GPU 设置的第一次迭代中，GPU 将发送和接收以下块：</p><table><thead><tr><th>GPU</th><th>发送</th><th>接收</th></tr></thead><tbody><tr><td>0</td><td>块1</td><td>块0</td></tr><tr><td>1</td><td>块2</td><td>块1</td></tr><tr><td>2</td><td>块3</td><td>块2</td></tr><tr><td>3</td><td>块4</td><td>块3</td></tr><tr><td>4</td><td>块0</td><td>块4</td></tr></tbody></table><p>allgather 的第一次迭代中的数据传输如下。</p><p><img loading=lazy src=images/Horovod_1_allreduce_iter_1.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_1.png, images/Horovod_1_allreduce_iter_1.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_1.png 2x" sizes=auto data-title="scatter reduce" data-alt="scatter reduce" width=855 height=601 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>第一次迭代完成后，每个 GPU 都会有最终数组的两个块。在接下来的迭代中，该过程继续一直到最后，最终每个 GPU 将拥有整个数组的完全累加值。</p><h5 id=5232-全部迭代>5.2.3.2 全部迭代</h5><p>下面系列图展示了所有数据传输和中间结果，从第一次迭代开始，一直持续到全部收集完成。</p><p>Allgather 数据传输（迭代 1）
<img loading=lazy src=images/Horovod_1_allreduce_iter_1.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_1.png, images/Horovod_1_allreduce_iter_1.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_1.png 2x" sizes=auto data-title=allreduce data-alt=allreduce width=855 height=601 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>Allgather 数据传输（迭代 2）如下：
<img loading=lazy src=images/Horovod_1_allreduce_iter_2.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_2.png, images/Horovod_1_allreduce_iter_2.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_2.png 2x" sizes=auto data-title=allreduce data-alt=allreduce width=858 height=602 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>Allgather 数据传输（迭代 3）：</p><p><img loading=lazy src=images/Horovod_1_allreduce_iter_3.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_3.png, images/Horovod_1_allreduce_iter_3.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_3.png 2x" sizes=auto data-title=allreduce data-alt=allreduce width=854 height=601 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>Allgather 数据传输（迭代 4）：</p><p><img loading=lazy src=images/Horovod_1_allreduce_iter_4.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_4.png, images/Horovod_1_allreduce_iter_4.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_4.png 2x" sizes=auto data-title=allreduce data-alt=allreduce width=853 height=601 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p>所有全部转移后的最终状态。</p><p><img loading=lazy src=images/Horovod_1_allreduce_iter_5.png#center srcset="/posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_5.png, images/Horovod_1_allreduce_iter_5.png#center 1.5x, /posts/2022-10-08_horovod_1/images/Horovod_1_allreduce_iter_5.png 2x" sizes=auto data-title=allreduce data-alt=allreduce width=834 height=469 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h4 id=524-horovod-架构图>5.2.4 Horovod 架构图</h4><p>工作原理也可以借助<a href=https://www.uber.com/blog/manifold-open-source/ target=_blank rel="external nofollow noopener noreferrer">Horovod<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>的发布帖子 来看看。</p><p><img loading=lazy src=images/Horovod_1_horovod_structure.png#center srcset="images/Horovod_1_horovod_structure.png#center, images/Horovod_1_horovod_structure.png#center 1.5x, images/Horovod_1_horovod_structure.png#center 2x" sizes=auto data-title="horovod structure" data-alt="horovod structure" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><h4 id=525-百度思路>5.2.5 百度思路</h4><p>或者我们从百度的源码中也可以直接看到思路，现在摘录给大家。</p><p>具体代码参见 <a href=https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517 target=_blank rel="external nofollow noopener noreferrer">https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=cm>/* Perform a ring allreduce on the data. Allocate the necessary output tensor and
</span></span></span><span class=line><span class=cl><span class=cm> * store it in the output parameter.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> * Assumes that all MPI processes are doing an allreduce of the same tensor,
</span></span></span><span class=line><span class=cl><span class=cm> * with the same dimensions.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> * A ring allreduce is a bandwidth-optimal way to do an allreduce. To do the allreduce,
</span></span></span><span class=line><span class=cl><span class=cm> * the nodes involved are arranged in a ring:
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *                   .--0--.
</span></span></span><span class=line><span class=cl><span class=cm> *                  /       \
</span></span></span><span class=line><span class=cl><span class=cm> *                 3         1
</span></span></span><span class=line><span class=cl><span class=cm> *                  \       /
</span></span></span><span class=line><span class=cl><span class=cm> *                   *--2--*
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  Each node always sends to the next clockwise node in the ring, and receives
</span></span></span><span class=line><span class=cl><span class=cm> *  from the previous one.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  The allreduce is done in two parts: a scatter-reduce and an allgather. In
</span></span></span><span class=line><span class=cl><span class=cm> *  the scatter reduce, a reduction is done, so that each node ends up with a
</span></span></span><span class=line><span class=cl><span class=cm> *  chunk of the final output tensor which has contributions from all other
</span></span></span><span class=line><span class=cl><span class=cm> *  nodes.  In the allgather, those chunks are distributed among all the nodes,
</span></span></span><span class=line><span class=cl><span class=cm> *  so that all nodes have the entire output tensor.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  Both of these operations are done by dividing the input tensor into N
</span></span></span><span class=line><span class=cl><span class=cm> *  evenly sized chunks (where N is the number of nodes in the ring).
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  The scatter-reduce is done in N-1 steps. In the ith step, node j will send
</span></span></span><span class=line><span class=cl><span class=cm> *  the (j - i)th chunk and receive the (j - i - 1)th chunk, adding it in to
</span></span></span><span class=line><span class=cl><span class=cm> *  its existing data for that chunk. For example, in the first iteration with
</span></span></span><span class=line><span class=cl><span class=cm> *  the ring depicted above, you will have the following transfers:
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 0:  Node 0 --&gt; Node 1
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 1:  Node 1 --&gt; Node 2
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 2:  Node 2 --&gt; Node 3
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 3:  Node 3 --&gt; Node 0
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  In the second iteration, you&#39;ll have the following transfers:
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 0:  Node 1 --&gt; Node 2
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 1:  Node 2 --&gt; Node 3
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 2:  Node 3 --&gt; Node 0
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 3:  Node 0 --&gt; Node 1
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  After this iteration, Node 2 has 3 of the four contributions to Segment 0.
</span></span></span><span class=line><span class=cl><span class=cm> *  The last iteration has the following transfers:
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 0:  Node 2 --&gt; Node 3
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 1:  Node 3 --&gt; Node 0
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 2:  Node 0 --&gt; Node 1
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 3:  Node 1 --&gt; Node 2
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  After this iteration, Node 3 has the fully accumulated Segment 0; Node 0
</span></span></span><span class=line><span class=cl><span class=cm> *  has the fully accumulated Segment 1; and so on. The scatter-reduce is complete.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *  Next, the allgather distributes these fully accumululated chunks across all nodes.
</span></span></span><span class=line><span class=cl><span class=cm> *  Communication proceeds in the same ring, once again in N-1 steps. At the ith step,
</span></span></span><span class=line><span class=cl><span class=cm> *  node j will send chunk (j - i + 1) and receive chunk (j - i). For example, at the
</span></span></span><span class=line><span class=cl><span class=cm> *  first iteration, the following transfers will occur:
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 0:  Node 3 --&gt; Node 0
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 1:  Node 0 --&gt; Node 1
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 2:  Node 1 --&gt; Node 2
</span></span></span><span class=line><span class=cl><span class=cm> *      Segment 3:  Node 2 --&gt; Node 3
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> * After the first iteration, Node 0 will have a fully accumulated Segment 0
</span></span></span><span class=line><span class=cl><span class=cm> * (from Node 3) and Segment 1. In the next iteration, Node 0 will send its
</span></span></span><span class=line><span class=cl><span class=cm> * just-received Segment 0 onward to Node 1, and receive Segment 3 from Node 3.
</span></span></span><span class=line><span class=cl><span class=cm> * After this has continued for N - 1 iterations, all nodes will have a the fully
</span></span></span><span class=line><span class=cl><span class=cm> * accumulated tensor.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> * Each node will do (N-1) sends for the scatter-reduce and (N-1) sends for the allgather.
</span></span></span><span class=line><span class=cl><span class=cm> * Each send will contain K / N bytes, if there are K bytes in the original tensor on every node.
</span></span></span><span class=line><span class=cl><span class=cm> * Thus, each node sends and receives 2K(N - 1)/N bytes of data, and the performance of the allreduce
</span></span></span><span class=line><span class=cl><span class=cm> * (assuming no latency in connections) is constrained by the slowest interconnect between the nodes.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> */</span></span></span></code></pre></td></tr></table></div></div><h3 id=53-区别>5.3 区别</h3><p><strong>在中等规模模型情况下，all-reduce 更适合。当规模巨大时候则应该使用参数服务器</strong>。</p><p>参数服务器 适合的是高维稀疏模型训练，它利用的是维度稀疏的特点，每次 pull or push 只更新有效的值。但是深度学习模型是典型的dense场景，embedding做的就是把稀疏变成稠密。所以这种 pull or push 的不太适合。而 网络通信上更优化的 all-reduce 适合中等规模的深度学习。</p><p>又比如由于推荐搜索领域模型的 Embedding 层规模庞大以及训练数据样本长度不固定等原因，导致容易出现显存不足和卡间同步时间耗费等问题，所以 all-reduce 架构很少被用于搜索推荐领域。</p><p>至此，背景知识已经介绍完毕，下一篇我们开始介绍 Horovod 的使用。</p><p>reference:
[1] <a href=https://www.cnblogs.com/rossiXYZ/p/14856464.html target=_blank rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14856464.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.jpg srcset="/images/alipay.jpg, /images/alipay.jpg 1.5x, /images/alipay.jpg 2x" sizes=auto data-title="<nil> 支付宝" data-alt="<nil> 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.jpg srcset="/images/wechatpay.jpg, /images/wechatpay.jpg 1.5x, /images/wechatpay.jpg 2x" sizes=auto data-title="<nil> 微信" data-alt="<nil> 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-07-15 15:28:35">更新于 2023-07-15&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/84e2876cfb14f359a6d6034596104deef86f29d1 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(yejian@zhito.com) 84e2876cfb14f359a6d6034596104deef86f29d1: feat: add transformer introduction"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>84e2876</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/2022-10-08_horovod_1/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/Horovod/2022-10-08_horovod_1/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_1/ data-title="深度学习分布式训练框架 Horovod[1] -- 基础知识" data-hashtags=Horovod><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_1/ data-hashtag=Horovod><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_1/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_1/ data-title="深度学习分布式训练框架 Horovod[1] -- 基础知识"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/2022-10-08_horovod_1/ data-title="深度学习分布式训练框架 Horovod[1] -- 基础知识"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/horovod/ class=post-tag>Horovod</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/hugo_command/ class=post-nav-item rel=prev title=Hugo_command><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Hugo_command</a>
<a href=/posts/2022-10-08_horovod_2/ class=post-nav-item rel=next title="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入">深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.115.3">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2023</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> 李瑞豪',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>