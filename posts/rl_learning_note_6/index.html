<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>RL学习笔记 [6] | 时序差分在线控制算法SARSA - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1. SARSA算法的引入 SAR"><meta name=keywords content='RL'><meta itemprop=name content="RL学习笔记 [6] | 时序差分在线控制算法SARSA"><meta itemprop=description content="0. 引言 在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1. SARSA算法的引入 SAR"><meta itemprop=datePublished content="2024-02-22T16:29:33+08:00"><meta itemprop=dateModified content="2024-02-28T09:10:39+08:00"><meta itemprop=wordCount content="3512"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_6/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="RL学习笔记 [6] | 时序差分在线控制算法SARSA"><meta property="og:description" content="0. 引言 在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1. SARSA算法的引入 SAR"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-22T16:29:33+08:00"><meta property="article:modified_time" content="2024-02-28T09:10:39+08:00"><meta property="article:tag" content="RL"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="RL学习笔记 [6] | 时序差分在线控制算法SARSA"><meta name=twitter:description content="0. 引言 在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1. SARSA算法的引入 SAR"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_6/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_4/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_5/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"RL学习笔记 [6] | 时序差分在线控制算法SARSA","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_6\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":3512,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_6\/","datePublished":"2024-02-22T16:29:33+08:00","dateModified":"2024-02-28T09:10:39+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>RL学习笔记 [6] | 时序差分在线控制算法SARSA</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-22 16:29:33"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-22>2024-02-22</time></span>&nbsp;<span title="更新于 2024-02-28 09:10:39"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-28>2024-02-28</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3512 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 8 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="RL学习笔记 [6] | 时序差分在线控制算法SARSA">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><h1 id=0-引言>0. 引言</h1><p>在<a href=https://www.cnblogs.com/pinard/p/9529828.html target=_blank rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。</p><p>SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p><h1 id=1-sarsa算法的引入>1. SARSA算法的引入</h1><p>SARSA算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$。</p><p>这一类强化学习的问题求解不需要环境的状态转化模型，是<strong>不基于模型的强化学习问题</strong>求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p><p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作。而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。</p><p>我们的SARSA算法，属于在线控制这一类，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是 $ϵ−$贪婪法，在<a href=https://www.cnblogs.com/pinard/p/9492980.html target=_blank rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们对于 $ϵ−$贪婪法有详细讲解，即通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 m 个可选行为中选择行为。用公式可以表示为：</p><p>$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</p><p>π(a|s)={ϵ/m+1−ϵifa∗=argmaxa∈AQ(s,a)ϵ/melse�(�|�)={�/�+1−����∗=arg⁡max�∈��(�,�)�/�����</p><h1 id=2-sarsa算法概述>2. SARSA算法概述</h1><p>作为SARSA算法的名字本身来说，它实际上是由 $S,A,R,S,A$ 几个字母组成的。而 $S,A,R$ 分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：</p><br><center><img src=images/2_01.jpg width=400 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">SARSA Transition</div></center><br><p>在迭代的时候，我们首先基于 $ϵ−$贪婪法在当前状态 $S$ 选择一个动作 $A$ ，这样系统会转到一个新的状态 $S′$, 同时给我们一个即时奖励 $R$ , 在新的状态 $S′$，我们会基于 $ϵ−$贪婪法在状态 $S′$ 选择一个动作 $A′$，但是注意这时候我们并不执行这个动作 $A′$，只是用来更新的我们的价值函数，价值函数的更新公式是：</p><p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</p><p>其中，$γ$ 是衰减因子，$α$ 是迭代步长。这里和蒙特卡罗法求解在线控制问题的迭代公式的区别主要是，收获 $G_t$的表达式不同，对于时序差分，收获 $G_t$的表达式是 $R+\gamma Q(S&rsquo;,A&rsquo;)$ 。这个价值函数更新的贝尔曼公式我们在<a href=https://www.cnblogs.com/pinard/p/9529828.html target=_blank rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>第2节有详细讲到。</p><p>除了收获 $G_t$的表达式不同，SARSA算法和蒙特卡罗在线控制算法基本类似。</p><h1 id=3-sarsa算法流程>3. SARSA算法流程</h1><p>下面我们总结下SARSA算法的流程。</p><ul><li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li><li>输出：所有的状态和动作对应的价值 $Q$</li><li><ol><li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li></ol></li><li><ol start=2><li>for i from 1 to T，进行迭代。</li></ol><ul><li>a) 初始化 $S$ 为当前状态序列的第一个状态。设置 $A$ 为 $ϵ−$贪婪法在当前状态$S$ 选择的动作。</li><li>b) 在状态 $S$ 执行当前动作 $A$ ,得到新状态 $S′$ 和 奖励 $R$</li><li>c) 用 $\epsilon-$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li><li>d) 更新价值函数 $Q(S,A)$:<ul><li>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</li></ul></li><li>e) $S=S′$, $A=A′$</li><li>f) 如果 $S′$ 是终止状态，当前轮迭代完毕，否则转到步骤b)</li></ul></li></ul><p>这里有一个要注意的是，步长 $α$一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数 $Q$ 可以收敛。当 $Q$ 收敛时，我们的策略 $ϵ−$贪婪法也就收敛了。</p><h1 id=4-sarsa算法实例windy-gridworld>4. SARSA算法实例：Windy GridWorld</h1><p>下面我们用一个著名的实例Windy GridWorld来研究SARSA算法。</p><p>如下图一个10×7的长方形格子世界，标记有一个起始位置 S 和一个终止目标位置 G，格子下方的数字表示对应的列中一定强度的风。当个体进入该列的某个格子时，会按图中箭头所示的方向自动移动数字表示的格数，借此来模拟世界中风的作用。同样格子世界是有边界的，个体任意时刻只能处在世界内部的一个格子中。个体并不清楚这个世界的构造以及有风，也就是说它不知道格子是长方形的，也不知道边界在哪里，也不知道自己在里面移动移步后下一个格子与之前格子的相对位置关系，当然它也不清楚起始位置、终止目标的具体位置。但是个体会记住曾经经过的格子，下次在进入这个格子时，它能准确的辨认出这个格子曾经什么时候来过。格子可以执行的行为是朝上、下、左、右移动一步，每移动一步只要不是进入目标位置都给予一个 -1 的惩罚，直至进入目标位置后获得奖励 0 同时永久停留在该位置。现在要求解的问题是个体应该遵循怎样的策略才能尽快的从起始位置到达目标位置。</p><br><center><img src=images/4_01.jpg width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Windy GridWorld</div></center><br><p>逻辑并不复杂，完整的代码在<a href=https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py target=_blank rel="external nofollow noopener noreferrer">我的github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。这里我主要看一下关键部分的代码。</p><p>算法中第2步步骤a,初始化 $S$,使用 $ϵ−$贪婪法在当前状态 $S$ 选择的动作的过程：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># initialize state</span>
</span></span><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>START</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>EPSILON</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>ACTIONS</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>values_</span> <span class=o>=</span> <span class=n>q_value</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>  <span class=n>action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=n>action_</span> <span class=k>for</span> <span class=n>action_</span><span class=p>,</span> <span class=n>value_</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>values_</span><span class=p>)</span> <span class=k>if</span> <span class=n>value_</span> <span class=o>==</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>values_</span><span class=p>)])</span></span></span></code></pre></td></tr></table></div></div><p>算法中第2步步骤b,在状态S�执行当前动作A�,得到新状态S′�′的过程，由于奖励不是终止就是-1，不需要单独计算：</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>i</span><span class=p>,</span> <span class=n>j</span> <span class=o>=</span> <span class=n>state</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>action</span> <span class=o>==</span> <span class=n>ACTION_UP</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=nb>max</span><span class=p>(</span><span class=n>i</span> <span class=o>-</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>WIND</span><span class=p>[</span><span class=n>j</span><span class=p>],</span> <span class=mi>0</span><span class=p>),</span> <span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>elif</span> <span class=n>action</span> <span class=o>==</span> <span class=n>ACTION_DOWN</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=nb>max</span><span class=p>(</span><span class=nb>min</span><span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>WIND</span><span class=p>[</span><span class=n>j</span><span class=p>],</span> <span class=n>WORLD_HEIGHT</span> <span class=o>-</span> <span class=mi>1</span><span class=p>),</span> <span class=mi>0</span><span class=p>),</span> <span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>elif</span> <span class=n>action</span> <span class=o>==</span> <span class=n>ACTION_LEFT</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=nb>max</span><span class=p>(</span><span class=n>i</span> <span class=o>-</span> <span class=n>WIND</span><span class=p>[</span><span class=n>j</span><span class=p>],</span> <span class=mi>0</span><span class=p>),</span> <span class=nb>max</span><span class=p>(</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>  <span class=k>elif</span> <span class=n>action</span> <span class=o>==</span> <span class=n>ACTION_RIGHT</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=nb>max</span><span class=p>(</span><span class=n>i</span> <span class=o>-</span> <span class=n>WIND</span><span class=p>[</span><span class=n>j</span><span class=p>],</span> <span class=mi>0</span><span class=p>),</span> <span class=nb>min</span><span class=p>(</span><span class=n>j</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>WORLD_WIDTH</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=kc>False</span></span></span></code></pre></td></tr></table></div></div><p>算法中第2步步骤c,用 $ϵ−$贪婪法在状态 $S&rsquo;$选择新的动作 $A′$的过程：</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>next_state</span> <span class=o>=</span> <span class=n>step</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>EPSILON</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>next_action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>ACTIONS</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>values_</span> <span class=o>=</span> <span class=n>q_value</span><span class=p>[</span><span class=n>next_state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>  <span class=n>next_action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=n>action_</span> <span class=k>for</span> <span class=n>action_</span><span class=p>,</span> <span class=n>value_</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>values_</span><span class=p>)</span> <span class=k>if</span> <span class=n>value_</span> <span class=o>==</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>values_</span><span class=p>)])</span></span></span></code></pre></td></tr></table></div></div><p>算法中第2步步骤d,e, 更新价值函数 $Q(S,A)$ 以及更新当前状态动作的过程：</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Sarsa update</span>
</span></span><span class=line><span class=cl><span class=n>q_value</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>action</span><span class=p>]</span> <span class=o>+=</span> \
</span></span><span class=line><span class=cl>  <span class=n>ALPHA</span> <span class=o>*</span> <span class=p>(</span><span class=n>REWARD</span> <span class=o>+</span> <span class=n>q_value</span><span class=p>[</span><span class=n>next_state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>next_action</span><span class=p>]</span> <span class=o>-</span> <span class=n>q_value</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>action</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl><span class=n>action</span> <span class=o>=</span> <span class=n>next_action</span></span></span></code></pre></td></tr></table></div></div><p>代码很简单，相信大家对照算法，跑跑代码，可以很容易得到这个问题的最优解，进而搞清楚SARSA算法的整个流程。</p><h1 id=5-sarsaλ>5. SARSA(λ)</h1><p>在<a href=https://www.cnblogs.com/pinard/p/9529828.html target=_blank rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中我们讲到了多步时序差分 $TD(λ)$ 的价值函数迭代方法，那么同样的，对应的多步时序差分在线控制算法，就是我们的 $SARSA(λ)$。</p><p>$TD(\lambda)$有前向和后向两种价值函数迭代方式，当然它们是等价的。在控制问题的求解时，基于反向认识的 $SARSA(\lambda)$算法将可以有效地在线学习，数据学习完即可丢弃。因此 $SARSA(\lambda)$算法默认都是基于反向来进行价值函数迭代。</p><p>在上一篇我们讲到了$TD(\lambda)$状态价值函数的反向迭代，即：</p><p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(S)\end{gathered}$$</p><p>对应的动作价值函数的迭代公式可以找样写出，即：</p><p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\\\Q(S_t,A_t)=Q(S_t,A_t)+\alpha\delta_tE_t(S,A)\end{gathered}$$</p><p>除了状态价值函数 $Q(S,A)$ 的更新方式，多步参数 $λ$ 以及反向认识引入的效用迹 $E(S,A)$ ，其余算法思想和 $SARSA$ 类似。这里我们总结下 $SARSA(λ)$的算法流程。　　　</p><ul><li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率$ϵ$, 多步参数$λ$</li><li>输出：所有的状态和动作对应的价值$Q$</li><li><ol><li>随机初始化所有的状态和动作对应的价值 $Q$. 对于终止状态其 $Q$值初始化为0.</li></ol></li><li><ol start=2><li>for i from 1 to T，进行迭代。</li></ol><ul><li>a) 初始化所有状态动作的效用迹 $E$ 为0，初始化S为当前状态序列的第一个状态。设置$A$为 $ϵ−$贪婪法在当前状态 $S$选择的动作。</li><li>b) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li><li>c) 用$ϵ−$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li><li>d) 更新效用迹函数 $E(S,A)$和TD误差 $δ$:<ul><li>$$\begin{gathered}E(S,A)=E(S,A)+1\\\\\delta=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\end{gathered}$$</li></ul></li><li>e) 对当前序列所有出现的状态s和对应动作 $a$, 更新价值函数 $Q(s,a)$和效用迹函数 $E(s,a)$:<ul><li>$$\begin{gathered}Q(s,a)=Q(s,a)+\alpha\delta E(s,a)\\\\E(s,a)=\gamma\lambda E(s,a)\end{gathered}$$</li></ul></li><li>f) $S=S&rsquo;$, $A=A'$</li><li>g) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li></ul></li></ul><p>对于步长$α$，和SARSA一样，一般也需要随着迭代的进行逐渐变小才能保证动作价值函数$Q$收敛。</p><h1 id=6-sarsa小结>6. SARSA小结</h1><p>SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和蒙特卡罗法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。</p><p>但是SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在 SARSA 算法中，$Q(S,A)$ 的值使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。</p><p>下一篇我们讨论SARSA的姊妹算法，时序差分离线控制算法Q-Learning。</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-28 09:10:39">更新于 2024-02-28&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/d426456642d56dc3ffe2ec26e60d7ea7c402054d rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) d426456642d56dc3ffe2ec26e60d7ea7c402054d: feat: update post default setting"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>d426456</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_6/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_6/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_6/ data-title="RL学习笔记 [6] | 时序差分在线控制算法SARSA" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_6/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_6/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_6/ data-title="RL学习笔记 [6] | 时序差分在线控制算法SARSA"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_6/ data-title="RL学习笔记 [6] | 时序差分在线控制算法SARSA"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_4/ class=post-nav-item rel=prev title="RL学习笔记 [4] | 用蒙特卡罗法（MC）求解"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>RL学习笔记 [4] | 用蒙特卡罗法（MC）求解</a>
<a href=/posts/rl_learning_note_5/ class=post-nav-item rel=next title="RL学习笔记 [5] | 用时序差分法（TD）求解">RL学习笔记 [5] | 用时序差分法（TD）求解<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.125.5">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>