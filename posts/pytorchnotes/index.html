<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>PyTorch Notes - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)->Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， &ldquo;ik,kj->ij&rdquo;，k 在输入中重复出现，所"><meta name=keywords content='draft'><meta itemprop=name content="PyTorch Notes"><meta itemprop=description content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)->Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， &ldquo;ik,kj->ij&rdquo;，k 在输入中重复出现，所"><meta itemprop=datePublished content="2023-07-15T18:15:53+08:00"><meta itemprop=dateModified content="2023-07-15T18:58:05+08:00"><meta itemprop=wordCount content="2394"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="Draft"><meta property="og:url" content="https://jianye0428.github.io/posts/pytorchnotes/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="PyTorch Notes"><meta property="og:description" content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)-&amp;gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， &amp;ldquo;ik,kj-&amp;gt;ij&amp;rdquo;，k 在输入中重复出现，所"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-15T18:15:53+08:00"><meta property="article:modified_time" content="2023-07-15T18:58:05+08:00"><meta property="article:tag" content="Draft"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="PyTorch Notes"><meta name=twitter:description content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)->Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， &ldquo;ik,kj->ij&rdquo;，k 在输入中重复出现，所"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/pytorchnotes/><link rel=prev href=https://jianye0428.github.io/posts/metrics/><link rel=next href=https://jianye0428.github.io/posts/datasetanddataloader/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"PyTorch Notes","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/pytorchnotes\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"draft","wordcount":2394,"url":"https:\/\/jianye0428.github.io\/posts\/pytorchnotes\/","datePublished":"2023-07-15T18:15:53+08:00","dateModified":"2023-07-15T18:58:05+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian Ye"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>PyTorch Notes</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian Ye" data-alt="Jian Ye" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian Ye</span></span>
<span class=post-category>收录于 <a href=/categories/pytorch/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> PyTorch</a></span></div><div class=post-meta-line><span title="发布于 2023-07-15 18:15:53"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-15>2023-07-15</time></span>&nbsp;<span title="更新于 2023-07-15 18:58:05"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-15>2023-07-15</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2394 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 5 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="PyTorch Notes">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#torch-基本函数>Torch 基本函数</a><ul><li><a href=#1-torcheinsum>1. <strong><code>torch.einsum()</code></strong></a></li><li><a href=#2-torchpermutetorchtranspose>2. <strong><code>torch.permute()/torch.transpose()</code></strong></a></li><li><a href=#3-torchrand>3. <strong><code>torch.rand()</code></strong></a></li><li><a href=#4-torchsizetorchshape>4. <strong><code>torch.size()/torch.shape</code></strong></a></li><li><a href=#5-torchtensordot>5. <strong><code>torch.tensordot()</code></strong></a></li><li><a href=#6-torchtranspose>6. <strong><code>torch.transpose()</code></strong></a></li></ul></li><li><a href=#torch-nn-module>Torch NN Module</a><ul><li><a href=#1-nnconv1d>1. <strong><code>nn.Conv1d()</code></strong></a></li><li><a href=#2-nnconv2d>2. <strong><code>nn.Conv2d()</code></strong></a></li><li><a href=#3-nnfunctionalinterpolate>3. <strong><code>nn.functional.interpolate()</code></strong></a></li><li><a href=#4-nnfunctionalrelu>4. <strong><code>nn.functional.ReLU()</code></strong></a></li><li><a href=#5-nnmaxpool2d>5. <strong><code>nn.MaxPool2d()</code></strong></a></li><li><a href=#6-nnavgpool2d>6. <strong><code>nn.AvgPool2d()</code></strong></a></li></ul></li><li><a href=#torchcuda>torch.cuda</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><div class="details admonition warning open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-exclamation-triangle fa-fw" aria-hidden=true></i>警告<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>本文最后更新于 2023-07-15，文中内容可能已过时。</div></div></div><h2 id=torch-基本函数>Torch 基本函数</h2><h3 id=1-torcheinsum>1. <strong><code>torch.einsum()</code></strong></h3><p><code>torch.einsum(equation, *operands)->Tensor</code>:爱因斯坦求和
ref1: 算子部署: <a href=https://blog.csdn.net/HW140701/article/details/120654252 target=_blank rel="external nofollow noopener noreferrer">https://blog.csdn.net/HW140701/article/details/120654252<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
ref2: 例子: <a href=https://zhuanlan.zhihu.com/p/361209187 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/361209187<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><p><strong>三条基本规则:</strong></p><ul><li><strong>规则一:</strong> equation 箭头左边，在不同输入之间<font color=red>重复出现的索引</font>表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， &ldquo;ik,kj->ij&rdquo;，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作；</li><li><strong>规则二:</strong> 只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引；</li><li><strong>规则三:</strong> equation 箭头右边的索引顺序可以是任意的，比如上面的 &ldquo;ik,kj->ij&rdquo; 如果写成 &ldquo;ik,kj->ji&rdquo;，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成</li></ul><p><strong>特殊规则:</strong></p><ul><li>equation 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 &ldquo;ik,kj->ij&rdquo; 也可以简化为 &ldquo;ik,kj&rdquo;，根据默认规则，输出就是 &ldquo;ij&rdquo; 与原来一样；</li><li>equation 中支持 &ldquo;&mldr;&rdquo; 省略号，用于表示用户并不关心的索引。比如只对一个高维张量的最后两维做转置可以这么写：<div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nv>a</span> <span class=o>=</span> torch.randn<span class=o>(</span>2,3,5,7,9<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1># i = 7, j = 9</span>
</span></span><span class=line><span class=cl><span class=nv>b</span> <span class=o>=</span> torch.einsum<span class=o>(</span><span class=s1>&#39;...ij-&gt;...ji&#39;</span>, <span class=o>[</span>a<span class=o>])</span></span></span></code></pre></td></tr></table></div></div></li></ul><h3 id=2-torchpermutetorchtranspose>2. <strong><code>torch.permute()/torch.transpose()</code></strong></h3><p><code>torch.permute(dim0, dim1, dim2)</code>:用于调换不同维度的顺序
<code>torch.transpose(input, dim0, dim1)</code>:交换矩阵的两个维度</p><h3 id=3-torchrand>3. <strong><code>torch.rand()</code></strong></h3><p><code>torch.rand(dim0, dim1)</code>:生成dim0 x dim1的tensor</p><h3 id=4-torchsizetorchshape>4. <strong><code>torch.size()/torch.shape</code></strong></h3><p><code>torch.size()</code>:返回tensor的size
<code>torch.shape</code>:返回tensor的size</p><h3 id=5-torchtensordot>5. <strong><code>torch.tensordot()</code></strong></h3><p>ref: tensordot()和einsum()的区别: <a href=https://blog.csdn.net/Eric_1993/article/details/105670381 target=_blank rel="external nofollow noopener noreferrer">https://blog.csdn.net/Eric_1993/article/details/105670381<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
<code>torch.tensordot(tensor1， tensor2， axes=([dim1,dim2],[dim0, dim1]))</code>: 将axes指定的子数组进行点乘, axes 指定具体的维度.</p><h3 id=6-torchtranspose>6. <strong><code>torch.transpose()</code></strong></h3><p><code>torch.transpose(tensor, dim0, dim2) —> Tensor</code>:在dim0和dim1方向上转置</p><p>###7. <strong><code>torch.index_add_()</code></strong></p><p><code>Tensor.index_add_(dim, index, source, *, alpha=1) → Tensor</code></p><p>demo:</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>],</span> <span class=p>[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>]],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>index</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span><span class=o>.</span><span class=n>index_add_</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>index</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([[</span>  <span class=mf>2.</span><span class=p>,</span>   <span class=mf>3.</span><span class=p>,</span>   <span class=mf>4.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>8.</span><span class=p>,</span>   <span class=mf>9.</span><span class=p>,</span>  <span class=mf>10.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>5.</span><span class=p>,</span>   <span class=mf>6.</span><span class=p>,</span>   <span class=mf>7.</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span><span class=o>.</span><span class=n>index_add_</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>index</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span>  <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>,</span>   <span class=mf>1.</span><span class=p>]])</span></span></span></code></pre></td></tr></table></div></div><h2 id=torch-nn-module>Torch NN Module</h2><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>functional</span> <span class=k>as</span> <span class=n>F</span></span></span></code></pre></td></tr></table></div></div><h3 id=1-nnconv1d>1. <strong><code>nn.Conv1d()</code></strong></h3><p><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code></p><p><strong>Shape:</strong>
- Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$
- Output: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$, where
$$L_{out} = \frac{L_{in} + 2 \cdot \text{padding} - \text{dilation} \cdot (\text{kernel_size} - 1) - 1}{stride}$$</p><p><strong>Demo:</strong></p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv1d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>33</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span> <span class=c1># B x C x H or N x C x L</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>m</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># torch.Size([20, 33, 24])</span></span></span></code></pre></td></tr></table></div></div><h3 id=2-nnconv2d>2. <strong><code>nn.Conv2d()</code></strong></h3><p><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code></p><p><strong>Shape:</strong></p><ul><li>Input: $(N, C_{\text in}, H_{\text in}, W_{\text in})$ or $(C_{\text in}, H_{\text in}, W_{\text in})$
- Output: $(N, C_{\text out}, H_{\text out}, W_{\text out})$ or $(C_{\text out}, H_{\text out}, W_{\text out})$, where
$$
H_{out} = \frac{H_{in} + 2 \cdot \text{padding[0]} - \text{dilation[0]} \cdot (\text{kernel_size[0]} - 1) - 1}{stride[0]} + 1
$$
$$
W_{out} = \frac{W_{in} + 2 \cdot \text{padding[1]} - \text{dilation[1]} \cdot (\text{kernel_size[1]} - 1) - 1}{stride[1]} + 1
$$</li></ul><p><strong>Demo:</strong></p><div class=highlight id=id-5><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># With square kernels and equal stride</span>
</span></span><span class=line><span class=cl>  <span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>33</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=c1># non-square kernels and unequal stride and with padding</span>
</span></span><span class=line><span class=cl>  <span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>33</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>),</span> <span class=n>stride</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=c1># output.shape: 20 x 33 x 28 x 100</span>
</span></span><span class=line><span class=cl>  <span class=c1># non-square kernels and unequal stride and with padding and dilation</span>
</span></span><span class=line><span class=cl>  <span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>33</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>),</span> <span class=n>stride</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>dilation</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span> <span class=c1># output.shape: 20 x 33 x 26 x 100</span>
</span></span><span class=line><span class=cl>  <span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>output</span> <span class=o>=</span> <span class=n>m</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span> <span class=c1>#</span></span></span></code></pre></td></tr></table></div></div><h3 id=3-nnfunctionalinterpolate>3. <strong><code>nn.functional.interpolate()</code></strong></h3><p><code>torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)</code></p><h3 id=4-nnfunctionalrelu>4. <strong><code>nn.functional.ReLU()</code></strong></h3><p>$$ \text{ReLU} = (x)^+ = \max {(0,x)}$$</p><p><code>torch.nn.ReLU(inplace=False)</code></p><p><strong>作用:</strong></p><ul><li><p>Sigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接近于0，所以这会造成梯度弥散，而ReLU函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象。</p></li><li><p>ReLU函数在负半区的导数为0 ，所以一旦神经元激活值进入负半区，那么梯度就会为0，而正值不变，这种操作被成为单侧抑制。（也就是说：<strong>在输入是负值的情况下，它会输出0，那么神经元就不会被激活。这意味着同一时间只有部分神经元会被激活，从而使得网络很稀疏，进而对计算来说是非常有效率的。</strong>）<u>正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</u>尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。</p></li><li><p>relu函数的导数<strong>计算更快</strong>，程序实现就是一个if-else语句，而sigmoid函数要进行浮点四则运算。</p></li></ul><p><strong>Shape:</strong></p><ul><li>Input: $(∗)$, where $*$ means any number of dimensions.</li><li>Output: $(∗)$, same shape as the input.</li></ul><p><img loading=lazy src=images/ReLU.png srcset="/posts/pytorchnotes/images/ReLU.png, images/ReLU.png 1.5x, /posts/pytorchnotes/images/ReLU.png 2x" sizes=auto data-title="执行echo $PATH的结果" data-alt="执行echo $PATH的结果" width=640 height=480 style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></p><p><strong>Demo:</strong></p><div class=highlight id=id-6><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>m</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># An implementation of CReLU - https://arxiv.org/abs/1603.05201</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>m</span><span class=p>(</span><span class=nb>input</span><span class=p>),</span><span class=n>m</span><span class=p>(</span><span class=o>-</span><span class=nb>input</span><span class=p>)))</span></span></span></code></pre></td></tr></table></div></div><h3 id=5-nnmaxpool2d>5. <strong><code>nn.MaxPool2d()</code></strong></h3><p><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></p><p><strong>Shape:</strong></p><ul><li>Input: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$</li><li>Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$</li></ul><p>where,</p><p>$$ H_{out} = \frac{H_{in} + 2 * \text{padding}[0] - \text{dilation}[0] * (\text{kernel_size}[0]-1) - 1}{\text{stride}[0]} + 1$$</p><p>$$ W_{out} = \frac{W_{in} + 2 * \text{padding}[1] - \text{dilation}[1] * (\text{kernel_size}[1]-1) - 1}{\text{stride}[1]} + 1$$</p><p><strong>demo:</strong></p><div class=highlight id=id-7><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># pool of square window of size=3, stride=2</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># pool of non-square window</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>stride</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>m</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span> <span class=c1># 20 16 24 31</span></span></span></code></pre></td></tr></table></div></div><h3 id=6-nnavgpool2d>6. <strong><code>nn.AvgPool2d()</code></strong></h3><div class=highlight id=id-8><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)</span></span></code></pre></td></tr></table></div></div><p><strong>Shape:</strong></p><ul><li>Input: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$</li><li>Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$</li></ul><p>where,</p><p>$$ H_{out} = \frac{H_{in} + 2 * \text{padding}[0] - (\text{kernel_size}[0])}{\text{stride}[0]} + 1$$</p><p>$$ W_{out} = \frac{W_{in} + 2 * \text{padding}[1] - (\text{kernel_size}[1])}{\text{stride}[1]} + 1$$</p><p><strong>demo:</strong></p><div class=highlight id=id-9><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># pool of square window of size=3, stride=2</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AvgPool2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># pool of non-square window</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AvgPool2d</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>stride</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>m</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span> <span class=c1># 20 16, 24 31</span></span></span></code></pre></td></tr></table></div></div><h2 id=torchcuda>torch.cuda</h2><p>ref link: <a href=https://zhuanlan.zhihu.com/p/76908135 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/76908135<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><ol><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_device target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.current_device()</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>: 返回当前选择的设备的索引</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_stream target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.current_stream()</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>: 返回参数设备的当前的<a href=https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream target=_blank rel="external nofollow noopener noreferrer">Stream<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_stream target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.default_stream()</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>: 返回当前参数设备的<a href=https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream target=_blank rel="external nofollow noopener noreferrer">Stream<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></li><li><p><em>CLASS</em> <a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#device target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.device</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>: 可以改变选择的设备的上下文管理器
Parameters：device (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None.</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#device_count target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.device_count()</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>: 返回可使用GPU的数量</p></li><li><p><em>CLASS</em> <a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#device_of target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.device_of(obj)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
Context-manager 将参数对象的设备改成当前的设备。你可以使用张量或者存储作为参数。如果传入的对象没有分配在GPU上，这个操作是无效的。</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#empty_cache target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.empty_cache()</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
释放caching allocator当前持有的所有未占用的cached memory，使其可以用在其他GPU应用且可以在 nvidia-smi可视化。</p><blockquote><blockquote><p>注意：<a href=https://pytorch.org/docs/stable/cuda.html#torch.cuda.empty_cache target=_blank rel="external nofollow noopener noreferrer">empty_cache()<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> 并不会增加PyTorch可以使用的GPU显存的大小。 查看 <a href=https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management target=_blank rel="external nofollow noopener noreferrer">Memory management<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> 来获取更多的GPU显存管理的信息。</p></blockquote></blockquote></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#get_device_capability target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.get_device_capability(device=None)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
Gets the cuda capability of a device.</p><p>Parameters：device (torch.device or int, optional) – device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given bycurrent_device(), if device is None (default).</p><p>Returns：the major and minor cuda capability of the device</p><p>Return type ： tuple(int, int)</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#get_device_name target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.get_device_name(device=None)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#init target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.init()</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
初始化PyTorch的CUDA状态。如果你通过C API与PyTorch进行交互，你可能需要显式调用这个方法。只有CUDA的初始化完成，CUDA的功能才会绑定到Python。用户一般不应该需要这个，因为所有PyTorch的CUDA方法都会自动在需要的时候初始化CUDA。如果CUDA的状态已经初始化了，将不起任何作用。</p></li><li><p>[<code>torch.cuda.is_available()</code>]</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#max_memory_allocated target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.max_memory_allocated(device=None)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#max_memory_cached target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.max_memory_cached(device=None)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#memory_allocated target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.memory_allocated(device=None)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
Parameters：device (torch.device or int, optional) – selected device. Returns statistic for the current device, given by current_device(), if device is None (default).</p></li><li><p><a href=https://pytorch.org/docs/stable/_modules/torch/cuda.html#memory_cached target=_blank rel="external nofollow noopener noreferrer"><code>torch.cuda.memory_cached(devide=None)</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></li><li><p>[``]</p></li></ol></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian Ye 支付宝" data-alt="Jian Ye 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian Ye 微信" data-alt="Jian Ye 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-07-15 18:58:05">更新于 2023-07-15&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/0c96c3ed998b4982a84dc515a44d8afab4a795b6 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(yejian@zhito.com) 0c96c3ed998b4982a84dc515a44d8afab4a795b6: feat: add PyTorch Notes"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>0c96c3e</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/pytorchnotes/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/PyTorch/PyTorchNotes/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/pytorchnotes/ data-title="PyTorch Notes" data-hashtags=draft><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/pytorchnotes/ data-hashtag=draft><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/pytorchnotes/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/pytorchnotes/ data-title="PyTorch Notes"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/pytorchnotes/ data-title="PyTorch Notes"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/draft/ class=post-tag>Draft</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/metrics/ class=post-nav-item rel=prev title="Classification and Regression Metrics"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Classification and Regression Metrics</a>
<a href=/posts/datasetanddataloader/ class=post-nav-item rel=next title="PyTorch Dataset And DataLoader">PyTorch Dataset And DataLoader<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.125.3">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>