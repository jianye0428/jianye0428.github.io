<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。 本章内容主要参"><meta name=keywords content='RL'><meta itemprop=name content="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN"><meta itemprop=description content="0. 引言 在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。 本章内容主要参"><meta itemprop=datePublished content="2024-02-23T13:17:48+08:00"><meta itemprop=dateModified content="2024-02-28T09:10:39+08:00"><meta itemprop=wordCount content="2652"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL,"><meta property="og:title" content="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN"><meta property="og:description" content="0. 引言 在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。 本章内容主要参"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_9/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-23T13:17:48+08:00"><meta property="article:modified_time" content="2024-02-28T09:10:39+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN"><meta name=twitter:description content="0. 引言 在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。 本章内容主要参"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_9/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_8/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_10/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_9\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":2652,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_9\/","datePublished":"2024-02-23T13:17:48+08:00","dateModified":"2024-02-28T09:10:39+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-23 13:17:48"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-23>2024-02-23</time></span>&nbsp;<span title="更新于 2024-02-28 09:10:39"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-28>2024-02-28</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2652 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 6 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><h1 id=0-引言>0. 引言</h1><p>在<a href=https://www.cnblogs.com/pinard/p/9714655.html target=_blank rel="external nofollow noopener noreferrer">强化学习（八）价值函数的近似表示与Deep Q-Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。</p><p>本章内容主要参考了ICML 2016的<a href=https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf target=_blank rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>和Nature DQN的论文。</p><h1 id=1-dqnnips-2013的问题>1. DQN(NIPS 2013)的问题</h1><p>在上一篇我们已经讨论了DQN(NIPS 2013)的算法原理和代码实现，虽然它可以训练像CartPole这样的简单游戏，但是有很多问题。这里我们先讨论第一个问题。</p><p>注意到DQN(NIPS 2013)里面，我们使用的目标 $Q$值的计算方式：</p><p>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p><p>这里目标Q值的计算使用到了当前要训练的Q网络参数来计算$Q(\phi(S_j^{\prime}),A_j^{\prime},w)$，而实际上，我们又希望通过 $y_j$来后续更新 $Q$网络参数。这样两者循环依赖，迭代起来两者的相关性就太强了。不利于算法的收敛。</p><p>因此，一个改进版的DQN: Nature DQN尝试<strong>用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系</strong>。下面我们来看看Nature DQN是怎么做的。</p><h1 id=2-nature-dqn的建模>2. Nature DQN的建模</h1><p>Nature DQN的两个Q网络分别命名为当前Q网络和目标Q网络。</p><p>Nature DQN使用了两个Q网络，一个<strong>当前Q网络</strong>$Q$用来选择动作，更新模型参数，另一个<strong>目标Q网络</strong> $Q&rsquo;$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。</p><p>要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。</p><p>Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。</p><h1 id=3-nature-dqn的算法流程>3. Nature DQN的算法流程</h1><p>下面我们来总结下Nature DQN的算法流程， 基于DQN NIPS 2015：</p><p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率$C$。</p><p>输出：$Q$网络参数</p><ul><li><ol><li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。清空经验回放的集合 $D$。</li></ol></li><li><ol start=2><li>for i from 1 to T，进行迭代。</li></ol><ul><li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li><li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li><li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态<code>is_end</code></li><li>d) 将 $\\{ϕ(S),A,R,ϕ(S′),is_end\\}$这个五元组存入经验回放集合 $D$</li><li>e) $S=S'$</li><li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，计算当前目标Q值 $y_j$：<ul><li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li></ul></li><li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li><li>h) 如果 $i%C=1$, 则更新目标Q网络参数 $w&rsquo;=w$</li><li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li></ul></li></ul><p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$ 需要随着迭代的进行而变小。</p><h1 id=4-nature-dqn算法实例>4. Nature DQN算法实例</h1><p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href=https://github.com/openai/gym/wiki/CartPole-v0 target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p><p>完整的代码参见github: <a href=https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py target=_blank rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><p>这里我们重点关注Nature DQN和上一节的NIPS 2013 DQN的代码的不同之处。</p><p>首先是Q网络，上一篇的DQN是一个三层的神经网络，而这里我们有两个一样的三层神经网络，一个是当前Q网络，一个是目标Q网络，网络的定义部分如下：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>create_Q_network</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># input layer</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>state_input</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=s2>&#34;float&#34;</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># network weights</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;current_net&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>W1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>W2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># hidden layers</span>
</span></span><span class=line><span class=cl>      <span class=n>h_layer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>,</span><span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1># Q Value layer</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>Q_value</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer</span><span class=p>,</span><span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;target_net&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>W1t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b1t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>W2t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b2t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># hidden layers</span>
</span></span><span class=line><span class=cl>      <span class=n>h_layer_t</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>,</span><span class=n>W1t</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1># Q Value layer</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>target_Q_value</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer</span><span class=p>,</span><span class=n>W2t</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2t</span></span></span></code></pre></td></tr></table></div></div><p>对于定期将目标Q网络的参数更新的代码如下面两部分：</p><div class=highlight id=id-2><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=n>t_params</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_collection</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>GraphKeys</span><span class=o>.</span><span class=n>GLOBAL_VARIABLES</span><span class=p>,</span> <span class=n>scope</span><span class=o>=</span><span class=s1>&#39;target_net&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>e_params</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>get_collection</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>GraphKeys</span><span class=o>.</span><span class=n>GLOBAL_VARIABLES</span><span class=p>,</span> <span class=n>scope</span><span class=o>=</span><span class=s1>&#39;current_net&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;soft_replacement&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>target_replace_op</span> <span class=o>=</span> <span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=n>e</span><span class=p>)</span> <span class=k>for</span> <span class=n>t</span><span class=p>,</span> <span class=n>e</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>t_params</span><span class=p>,</span> <span class=n>e_params</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_target_q_network</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>episode</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># update target Q netowrk</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>episode</span> <span class=o>%</span> <span class=n>REPLACE_TARGET_FREQ</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>session</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>target_replace_op</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1>#print(&#39;episode &#39;+str(episode) +&#39;, target Q network params replaced!&#39;)</span></span></span></code></pre></td></tr></table></div></div><p>此外，注意下我们计算目标Q值的部分，这里使用的目标Q网络的参数，而不是当前Q网络的参数：</p><div class=highlight id=id-3><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=c1># Step 2: calculate y</span>
</span></span><span class=line><span class=cl>  <span class=n>y_batch</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=n>Q_value_batch</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>target_Q_value</span><span class=o>.</span><span class=n>eval</span><span class=p>(</span><span class=n>feed_dict</span><span class=o>=</span><span class=p>{</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>:</span><span class=n>next_state_batch</span><span class=p>})</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=n>BATCH_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>done</span> <span class=o>=</span> <span class=n>minibatch</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>4</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>y_batch</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>reward_batch</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>y_batch</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>reward_batch</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>GAMMA</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>Q_value_batch</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span></span></span></code></pre></td></tr></table></div></div><p>其余部分基本和上一篇DQN的代码相同。这里给出我跑的某一次的结果:</p><div class=highlight id=id-4><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>episode: <span class=m>0</span> Evaluation Average Reward: 9.8
</span></span><span class=line><span class=cl>episode: <span class=m>100</span> Evaluation Average Reward: 9.8
</span></span><span class=line><span class=cl>episode: <span class=m>200</span> Evaluation Average Reward: 9.6
</span></span><span class=line><span class=cl>episode: <span class=m>300</span> Evaluation Average Reward: 10.0
</span></span><span class=line><span class=cl>episode: <span class=m>400</span> Evaluation Average Reward: 34.8
</span></span><span class=line><span class=cl>episode: <span class=m>500</span> Evaluation Average Reward: 177.4
</span></span><span class=line><span class=cl>episode: <span class=m>600</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>700</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>800</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>900</span> Evaluation Average Reward: 198.4
</span></span><span class=line><span class=cl>episode: <span class=m>1000</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1100</span> Evaluation Average Reward: 193.2
</span></span><span class=line><span class=cl>episode: <span class=m>1200</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1300</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1400</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1500</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1600</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1700</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1800</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>1900</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2000</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2100</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2200</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2300</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2400</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2500</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2600</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2700</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2800</span> Evaluation Average Reward: 200.0
</span></span><span class=line><span class=cl>episode: <span class=m>2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table></div></div><p>注意，由于DQN不保证稳定的收敛，所以每次跑的结果会不同，如果你跑的结果后面仍然收敛的不好，可以把代码多跑几次，选择一个最好的训练结果。</p><h1 id=5-nature-dqn总结>5. Nature DQN总结</h1><p>Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：</p><ul><li>1） 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？</li><li>2） 随机采样的方法好吗？按道理不同样本的重要性是不一样的。</li><li>3） Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？</li></ul><p>第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN，这三个DQN的改进版我们在下一篇来讨论。</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-28 09:10:39">更新于 2024-02-28&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/d426456642d56dc3ffe2ec26e60d7ea7c402054d rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) d426456642d56dc3ffe2ec26e60d7ea7c402054d: feat: update post default setting"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>d426456</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_9/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_9/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_9/ data-title="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_9/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_9/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_9/ data-title="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_9/ data-title="强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_8/ class=post-nav-item rel=prev title="强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</a>
<a href=/posts/rl_learning_note_10/ class=post-nav-item rel=next title="强化学习笔记 [10] | Double DQN (DDQN)">强化学习笔记 [10] | Double DQN (DDQN)<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.123.6">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>