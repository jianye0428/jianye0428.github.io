<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>RL | 强化学习 -- 简介 - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="1. 强化学习 Reinforcement Learning (RL): 强化学习 强化学习是人工智能（AI）和机器学习（ML）领域的一个重要子领域，不同于监督学习和无监督学习，强化学习通过智能体与环境的不断交互(即采取动作)，进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。强化学习特别适合序贯决策问题(涉及一系列有序的决策问题)"><meta name=keywords content="RL"><meta itemprop=name content="RL | 强化学习 -- 简介"><meta itemprop=description content="1. 强化学习 Reinforcement Learning (RL): 强化学习 强化学习是人工智能（AI）和机器学习（ML）领域的一个重要子领域，不同于监督学习和无监督学习，强化学习通过智能体与环境的不断交互(即采取动作)，进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。强化学习特别适合序贯决策问题(涉及一系列有序的决策问题)"><meta itemprop=datePublished content="2023-07-14T08:21:32+08:00"><meta itemprop=dateModified content="2024-01-23T08:04:01+08:00"><meta itemprop=wordCount content="5060"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL,"><meta property="og:title" content="RL | 强化学习 -- 简介"><meta property="og:description" content="1. 强化学习 Reinforcement Learning (RL): 强化学习 强化学习是人工智能（AI）和机器学习（ML）领域的一个重要子领域，不同于监督学习和无监督学习，强化学习通过智能体与环境的不断交互(即采取动作)，进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。强化学习特别适合序贯决策问题(涉及一系列有序的决策问题)"><meta property="og:type" content="article"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_introduction/"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-14T08:21:32+08:00"><meta property="article:modified_time" content="2024-01-23T08:04:01+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="RL | 强化学习 -- 简介"><meta name=twitter:description content="1. 强化学习 Reinforcement Learning (RL): 强化学习 强化学习是人工智能（AI）和机器学习（ML）领域的一个重要子领域，不同于监督学习和无监督学习，强化学习通过智能体与环境的不断交互(即采取动作)，进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。强化学习特别适合序贯决策问题(涉及一系列有序的决策问题)"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_introduction/><link rel=prev href=https://jianye0428.github.io/posts/distributedtraining_5/><link rel=next href=https://jianye0428.github.io/posts/dqn/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"RL | 强化学习 -- 简介","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_introduction\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":5060,"url":"https:\/\/jianye0428.github.io\/posts\/rl_introduction\/","datePublished":"2023-07-14T08:21:32+08:00","dateModified":"2024-01-23T08:04:01+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>RL | 强化学习 -- 简介</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2023-07-14 08:21:32"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2023-07-14>2023-07-14</time></span>&nbsp;<span title="更新于 2024-01-23 08:04:01"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-01-23>2024-01-23</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 5060 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 11 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="RL | 强化学习 -- 简介">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-强化学习>1. 强化学习</a><ul><li><ul><li><a href=#11-强化学习的定义>1.1 强化学习的定义</a></li></ul></li><li><a href=#12-强化学习的相关概念>1.2 强化学习的相关概念</a></li><li><a href=#13-强化学习的数学建模>1.3 强化学习的数学建模</a></li></ul></li><li><a href=#2-深度强化学习>2. 深度强化学习</a></li><li><a href=#3-常见深度强化学习算法>3. 常见深度强化学习算法</a><ul><li><a href=#31-deep-q-networks-dqn>3.1 Deep Q-Networks （DQN）</a></li><li><a href=#32-deep-deterministic-policy-gradientddpg>3.2 Deep Deterministic Policy Gradient（DDPG）</a></li><li><a href=#33-proximal-policy-optimizationppo>3.3 Proximal Policy Optimization（PPO）</a></li></ul></li><li><a href=#4-深度强化学习算法分类>4. 深度强化学习算法分类</a><ul><li><a href=#41-根据agent训练与测试所采用的策略是否一致>4.1 根据Agent训练与测试所采用的策略是否一致</a><ul><li><a href=#411-off-policy-离轨策略离线策略>4.1.1 off-policy (离轨策略、离线策略)</a></li><li><a href=#412-on-policy-同轨策略在线策略>4.1.2 on-policy (同轨策略、在线策略)</a></li></ul></li><li><a href=#42-策略优化的方式不同>4.2 策略优化的方式不同</a><ul><li><a href=#421-value-based-algorithms基于价值的算法>4.2.1 Value-based algorithms(基于价值的算法)</a></li><li><a href=#422-policy-based-algorithms基于策略的算法>4.2.2 Policy-based algorithms(基于策略的算法)</a></li><li><a href=#423-actor-critic-algorithms-演员-评论家方法>4.2.3 Actor-Critic algorithms (演员-评论家方法)</a></li></ul></li><li><a href=#43-参数更新的方式不同>4.3 参数更新的方式不同</a><ul><li><a href=#431-monte-carlo-method蒙特卡罗方法>4.3.1 Monte Carlo method(蒙特卡罗方法)</a></li><li><a href=#432-temporal-difference-method时间差分方法>4.3.2 Temporal Difference method(时间差分方法)</a></li></ul></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div><div class=content id=content data-end-flag=（完）><h2 id=1-强化学习>1. 强化学习</h2><p>Reinforcement Learning (RL): 强化学习</br>强化学习是人工智能（AI）和机器学习（ML）领域的一个重要子领域，不同于<code>监督学习</code>和<code>无监督学习</code>，强化学习通过智能体与环境的不断交互(即采取动作)，进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。强化学习特别适合序贯决策问题(涉及一系列有序的决策问题)。</p><br><center><img src=images/1_01.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ML Categories</div></center><br><p>在实际应用中，针对某些任务，我们往往无法给每个数据或者状态贴上准确的标签，但是能够知道或评估当前情况或数据是好还是坏，可以采用强化学习来处理。例如，下围棋(Go)，星际争霸II(Starcraft II)等游戏。</p><h4 id=11-强化学习的定义>1.1 强化学习的定义</h4><p>Agent interacts with its surroundings known as the environment. Agent will get a reward from the environemnt once it takes an action in the current enrivonment. Meanwhile, the environment evolves to the next state. The goal of the agent is to maximize its total reward (the Return) in the long run.</p><p>智能体与环境的不断交互(即在给定状态采取动作)，进而获得奖励，此时环境从一个状态转移到下一个状态。智能体通过不断优化自身动作策略，以期待最大化其长期回报或收益(奖励之和)。</p><br><center><img src=images/1_01.ppm width=640 height=280 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">强化学习流程图</div></center><br><h3 id=12-强化学习的相关概念>1.2 强化学习的相关概念</h3><p>(1) <font color=red>状态 State ($S$)</font>: agent’s observation of its environment;</br></p><p>(2) <font color=red>动作 Action ($A$)</font>: the approaches that agent interacts with the environment;</br></p><p>(3) <font color=red>奖励 Reward ($R_t$)</font>: the bonus that agent get once it takes an action in the environment at the given time step t.回报(Return)为Agent所获得的奖励之和。</br></p><p>(4) <font color=red>转移概率 Transistion Probability ($P$)</font>: the transition possibility that environment evolves from one state to another. 环境从一个状态转移到另一个状态，可以是确定性转移过程，例如，$S_{t+1} = f(S_t, A_t)$, 也可以是随机性转移过程，例如 $S_{t+1} \sim p\left( S_{t+1}|S_t, A_t \right)$</br></p><p>(5) <font color=red>折扣因子 Discount factor ( $\gamma$ )</font>: to measure the importance of future reward to agent at the current state.</br></p><p>(6) <font color=red>轨迹(Trajectory)</font>:是一系列的状态、动作、和奖励，可以表述为：</p><p>$$\tau = (S_0, A_0, R_0, S_1, A_1, R_1, &mldr; )$$</p><p>用轨迹$\tau$来记录Agent如何和环境交互。轨迹的初始状态是从起始状态分布中随机采样得到的。一条轨迹有时候也称为片段(Episode)或者回合，是一个从初始状态(Initial State，例如游戏的开局)到最终状态(Terminal State，如游戏中死亡或者胜利)的序列。</br></p><p>(7) <font color=red>探索-利用的折中(Exploration-Exploitation Tradeoff)</font>:
这里，探索是指Agent通过与环境的交互来获取更多的信息，而利用是指使用当前已知信息来使得Agent的表现达到最佳，例如，贪心(greedy)策略。同一时间，只能二者选一。因此，如何平衡探索和利用二者，以实现长期回报(Long-term Return)最大，是强化学习中非常重要的问题。</br></p><p>因此，可以用$ (S，A，P，R，\gamma) $来描述强化学习过程。</p><h3 id=13-强化学习的数学建模>1.3 强化学习的数学建模</h3><p>(1) 马尔可夫过程 (Markov Process，MP) 是一个具备马尔可夫性质的离散随机过程。</p><p>马尔可夫性质是指下一状态 $ S_{t+1} $ 只取决于当前状态 $S_t$.</p><p>$$p(S_{t+1}|S_{t}) = p(S_{t+1} | S_0, S_1, S_2, &mldr;, S_t)$$</p><p>可以用有限状态集合 $\mathcal{S}$ 和状态转移矩阵 $\mathbf{P}$ 表示MP过程为 $&lt;\mathcal{S}, \mathbf{P}>$。</p><p>为了能够刻画环境对Agent的反馈奖励，马尔可夫奖励过程将上述MP从 $&lt;\mathcal{S}, \mathbf{P}>$ 扩展到了$ &lt;\mathcal{S}, \mathbf{P}, R, \gamma>$。这里，$R$表示奖励函数，而 $\gamma$ 表示奖励折扣因子。</p><p>$$R_t = R(S_t)$$</p><p>回报(Return)是Agent在一个轨迹上的累计奖励。折扣化回报定义如下：</p><p>$$G_{t=0:T} = R(\tau) = \sum_{t=0}^{T}\gamma^{t}R_t$$</p><p>价值函数(Value Function) $V(s)$是Agent在状态$s$的期望回报(Expected Return)。</p><p>$$V^{\pi} (s) = \mathbb{E}[R(\tau) | S_0 = s]$$</p><p>(3) 马尔可夫决策过程 (Markov Decision Process，MDP)</br></p><p>MDP被广泛应用于经济、控制论、排队论、机器人、网络分析等诸多领域。
马尔可夫决策过程的立即奖励(Reward，$R$)与状态和动作有关。MDP可以用$&lt;\mathcal{S},\mathcal{A}, \mathbf{P}, R, \gamma>$来刻画。
$\mathcal{A}$表示有限的动作集合，此时，立即奖励变为</p><p>$$R_t = R(S_t, A_t)$$</p><p>策略(Policy)用来刻画Agent根据环境观测采取动作的方式。Policy是从一个状态 $s \in \mathcal{S}$ 到动作 $a \in \mathcal{A}$的概率分布$\pi(a|s)$ 的映射，$\pi(a|s)$ 表示在状态$s$下，采取动作 $a$ 的概率。</p><p>$$\pi (a|s) = p (A_t = a | S_t = s), \exist{t} $$</p><p>期望回报(Expected Return)是指在一个给定策略下所有可能轨迹的回报的期望值，可以表示为：</p><p>$$J(\pi) = \int_{\tau} p(\tau | \pi) R(\tau) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]$$</p><p>这里, $p(\tau|\pi)$表示给定初始状态分布 $\rho_0$ 和策略 $\pi$，马尔可夫决策过程中一个 $T$ 步长的轨迹 $\tau$ 的发生概率，如下：</p><p>$$p(\tau | \pi) = \rho_0(s_0)\prod \limits_{t=0}^{T-1} p(S_{t+1} | S_t, A_t) \pi (A_t | S_t)$$</p><p>强化学习优化问题通过优化方法来提升策略，以最大化期望回报。最优策略$\pi^*$ 可以表示为:</p><p>$$\pi ^ * = \argmax_{\pi} J(\pi)$$</p><p>给定一个策略 $\pi$，价值函数$V(s)$，即给定状态下的期望回报，可以表示为:</p><p>$$V^{\pi}(s) = \mathbb{E}_{\tau \sim \pi} [R(\tau) | S_0 = s] = \mathbb{E}_{A_t \sim \pi(\cdot | S_t)} [\sum_{t=0}^{\infin}\gamma^t R(S_t, A_t) | S_0 = s]$$</p><p>在MDP中，给定一个动作，就有动作价值函数(Action-Value Function)，是基于状态和动作的期望回报。其定义如下：</p><p>$$Q^{\pi}(s, a) = \mathbb{E}_{\tau \sim \pi}[R(\tau) | S_0 = s, A_0 = a] = \mathbb{E}_{A_t \sim \pi(\cdot | S_t)}[\sum_{t=0}^{\infin}\gamma^t R(S_t, A_t)|S_0 = s, A_0 = a]$$</p><p>根据上述定义，可以得到：</p><p>$$V^{\pi}(s) = \mathbb{E}_{a \sim \pi}[Q^{\pi}(s,a)]$$</p><h2 id=2-深度强化学习>2. 深度强化学习</h2><p>Deep Learning + Reinforcement Learning = Deep Reinforcement Learning (DRL)
深度学习DL有很强的抽象和表示能力，特别适合建模RL中的值函数，例如: 动作价值函数 $Q^\pi \left(s, a \right)$。
二者结合，极大地拓展了RL的应用范围。</p><h2 id=3-常见深度强化学习算法>3. 常见深度强化学习算法</h2><p>深度强化学习的算法比较多，常见的有：DQN，DDPG，PPO，TRPO，A3C，SAC 等等。</p><br><center><img src=images/3_01.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">常见深度强化学习算法</div></center><br><h3 id=31-deep-q-networks-dqn>3.1 Deep Q-Networks （DQN）</h3><p>DQN网路将Q-Learning和深度学习结合起来，并引入了两种新颖的技术来解决以往采用神经网络等非线性函数逼近器表示动作价值函数 <code>Q(s,a)</code> 所产生的不稳定性问题：</p><ul><li>技术1: 经验回放缓存（Replay Buffer）：将Agent获得的经验存入缓存中，然后从该缓存中均匀采用（也可考虑基于优先级采样）小批量样本用于Q-Learning的更新；</li><li>技术2: 目标网络（Target Network）：引入独立的网络，用来代替所需的Q网络来生成Q-Learning的目标，进一步提高神经网络稳定性。</li></ul><p>其中, 技术1 能够提高样本使用效率，降低样本间相关性，平滑学习过程；技术2 能够是目标值不受最新参数的影响，大大较少发散和震荡。</p><p>DQN算法具体描述如下：<br></p><center><img src=images/dqn.png width=640 height=320 align=left style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DQN 伪代码</div></center><br></br>注意：这里随机动作选择概率$\epsilon$一般是随着迭代Episode和Time Step的增加，而逐渐降低，目的是降低随机策略的影响，逐步提高Q网络对Agent动作选择的影响。<p>该算法中，Line 14 具体更新方式如下：</p><p>$$\theta^Q\leftarrow\theta^Q+\beta\sum_{i\in\mathcal{N}}\frac{\partial Q(s,a|\theta^Q)}{\partial\theta^Q}\left[y_i-Q(s,a|\theta^Q)\right]$$</p><p>其中，集合$N$中为<code>minibatch</code>的$N$个$(S_t,A_t,R_t,S_{t+1})$经验样本集合，$\beta$表示一次梯度迭代中的迭代步长。</p><div class="details admonition quote"><div class="details-summary admonition-title"><i class="icon fa-solid fa-quote-right fa-fw" aria-hidden=true></i>参考文献<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>[1] V. Mnih et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.</div></div></div><h3 id=32-deep-deterministic-policy-gradientddpg>3.2 Deep Deterministic Policy Gradient（DDPG）</h3><p>DDPG算法可以看作Deterministic Policy Gradient（DPG）算法和深度神经网络的结合，是对上述深度Q网络（DQN）在连续动作空间的扩展。</p><p>DDPG同时建立Q值函数（Critic）和策略函数（Actor）。这里，Critic与DQN相同，采用TD方法进行更新；而Actor利用Critic的估计，通过策略梯度方法进行更新。</p><p>DDPG算法具体描述如下：</p><br><center><img src=images/ddpg.png width=640 height=320 align=left style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DDPG 伪代码</div></center><br><p>原论文中采用Ornstein-Uhlenbeck过程（O-U过程）作为添加噪声项N \mathcal{N}N，也可以采用时间不相关的零均值高斯噪声（相关实践表明，其效果也很好）。</p><div class="details admonition quote"><div class="details-summary admonition-title"><i class="icon fa-solid fa-quote-right fa-fw" aria-hidden=true></i>参考文献<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>[1] Lillicrap, Timothy P., et al. “Continuous control with deep reinforcement learning”，arXiv preprint, 2015, online: <a href=https://arxiv.org/pdf/1509.02971.pdf target=_blank rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/1509.02971.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></div></div></div><h3 id=33-proximal-policy-optimizationppo>3.3 Proximal Policy Optimization（PPO）</h3><p>PPO算法是对信赖域策略优化算法(Trust Region Policy Optimization, TRPO) 的一个改进，用一个更简单有效的方法来强制策略$\pi_\theta$与$\pi_{\theta}^{\prime}$相似。</p><p>具体来说，TRPO中的优化问题如下：</p><p>$$\begin{gathered}\max_{\pi_{\theta}^{\prime}}\mathcal{L}_{\pi_{\theta}}(\pi_{\theta}^{\prime})\\s.t.\mathbb{E}_{s\sim\rho_{\pi_\theta}}[D_{KL}\left(\pi_\theta\left|\left|\pi_\theta^{\prime}\right.\right)\right]\leq\delta \end{gathered}$$</p><p>而PPO算法直接优化上述问题的正则版本，即：</p><p>$$\max_{\pi_{\theta}^{\prime}}\mathcal{L}_{\pi_{\theta}}\left(\pi_{\theta}^{\prime}\right)-\lambda\mathbb{E}_{s\sim\rho_{\pi_{\theta}}}\quad[D_{KL}\left(\pi_{\theta}||\pi_{\theta}^{\prime}\right)]$$</p><p>这里，入为正则化系数，对应TRPO优化问题中的每一个$\delta$,都存在一个相应的$\lambda$,使得上述两个优化问题有相同的解。然而，入的值依赖于$\pi_\theta$,因此，在PPO中，需要使用一个可动态调整的$\lambda$。具体来说有两种方法：
(1) 通过检验KL散度值来决定$\lambda$是增大还是减小，该版本的PPO算法称为PPO-Penalty;
(2) 直接截断用于策略梯度的目标函数，从而得到更保守的更新，该方法称为PPO-Clip。</p><p>PPO-Clip算法具体描述如下：</p><br><center><img src=images/ppo.png width=640 height=320 align=left style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">PPO 伪代码</div></center><br><p>$$f(\theta&rsquo;)=\min\left(\ell_t\left(\theta&rsquo;\right)A^{\pi_{\theta_{dd}}}(S_t,A_t),clip(\ell_t\left(\theta&rsquo;\right),1-\epsilon,1+\epsilon)A^{\pi_{\theta_{dd}}}(S_t,A_t)\right)$$</p><p>这里，$clip(x,1-\epsilon,1+\epsilon)$表示将$x$截断在$[1-\epsilon,1+\epsilon]$中。</p><div class="details admonition quote"><div class="details-summary admonition-title"><i class="icon fa-solid fa-quote-right fa-fw" aria-hidden=true></i>参考文献<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>[1] Schulman, J. , et al. “Proximal Policy Optimization Algorithms”，arXiv preprint, 2017, online: <a href=https://arxiv.org/pdf/1707.06347.pdf target=_blank rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/1707.06347.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>
[2] Schulman J, Levine S, Abbeel P, et al. “Trust region policy optimization”, International conference on machine learning. PMLR, 2015: 1889-1897, online: <a href=http://proceedings.mlr.press/v37/schulman15.pdf target=_blank rel="external nofollow noopener noreferrer">http://proceedings.mlr.press/v37/schulman15.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></div></div></div><h2 id=4-深度强化学习算法分类>4. 深度强化学习算法分类</h2><h3 id=41-根据agent训练与测试所采用的策略是否一致>4.1 根据Agent训练与测试所采用的策略是否一致</h3><h4 id=411-off-policy-离轨策略离线策略>4.1.1 off-policy (离轨策略、离线策略)</h4><p>Agent在训练(产生数据)时所使用的策略 $\pi_1$与 agent测试(方法评估与实际使用&ndash;目标策略)时所用的策略 $\pi_2$ 不一致。</p><p>例如，在DQN算法中，训练时，通常采用 $\epsilon-greedy$ 策略；而在测试性能或者实际使用时，采用 $ a^* = arg \max\limits_{a} Q^{\pi}\left( s, a \right) $ 策略。</p><p>常见算法有：DDPG，TD3，Q-learning，DQN等。</p><h4 id=412-on-policy-同轨策略在线策略>4.1.2 on-policy (同轨策略、在线策略)</h4><p>Agent在训练时(产生数据)所使用的策略与其测试(方法评估与提升)时使用的策略为同一个策略 $\pi$。</p><p>常见算法有：Sarsa，Policy Gradient，TRPO，PPO，A3C等。</p><h3 id=42-策略优化的方式不同>4.2 策略优化的方式不同</h3><h4 id=421-value-based-algorithms基于价值的算法>4.2.1 Value-based algorithms(基于价值的算法)</h4><p>基于价值的方法通常意味着对动作价值函数 $Q^{\pi}(s,a)$的优化，最优策略通过选取该函数 $Q^{\pi}(s,a)$ 最大值所对应的动作，即 $\pi^* \approx \arg \max\limits_{\pi}Q^{\pi}(s,a)$，这里，$\approx$ 由函数近似误差导致。</p><p>基于价值的算法具有采样效率相对较高，值函数估计方差小，不易陷入局部最优等优点，缺点是通常不能处理连续动作空间问题，最终策略通常为确定性策略。</p><p>常见算法有 Q-learning，DQN，Double DQN，等，适用于 Discrete action space。其中，DQN算法是基于state-action function $Q(s,a)$ 来进行选择最优action的。</p><h4 id=422-policy-based-algorithms基于策略的算法>4.2.2 Policy-based algorithms(基于策略的算法)</h4><p>基于策略的方法直接对策略进行优化，通过对策略迭代更新，实现累计奖励(回报)最大化。其具有策略参数化简单、收敛速度快的优点，而且适用于连续或者高维动作空间。</p><p>**策略梯度方法(Policy Gradient Method，PGM)**是一类直接针对期望回报通过梯度下降(Gradient Descent，针对最小化问题)进行策略优化的强化学习方法。其不需要在动作空间中求解价值最大化的优化问题，从而比较适用于 continuous and high-Dimension action space，也可以自然地对随机策略进行建模。</p><p>PGM方法通过梯度上升的方法直接在神经网络的参数上优化Agent的策略。</p><p>根据相关理论，期望回报 $J(\pi_{\theta})$ 关于参数 $\theta$ 的梯度可以表示为：</p><p>$$\nabla_\theta J(\pi_\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^TR_t\nabla_\theta\sum_{t^{\prime}=0}^T\log\pi_\theta(A_{t^{\prime}}|S_{t^{\prime}})\right]=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t^{\prime}=0}^T\nabla_\theta\log\pi_\theta\left(A_{t^{\prime}}|S_{t^{\prime}}\right)\sum_{t=0}^TR_t\right]$$</p><p>当$T \rightarrow \infin$ 时，上式可以表示为：</p><p>$$\nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t&rsquo;=0}^{\infin}\nabla_{\theta} \log \pi_{\theta}(A_{t&rsquo;} | S_{t&rsquo;}) \gamma^{t&rsquo;}\sum_{t=t&rsquo;}^{\infin} \gamma^{t-t&rsquo;}R_t]$$</p><p>在实际中，经常去掉 $ \gamma^{t^{\prime}} $，从而避免过分强调轨迹早期状态的问题。</p><p>上述方法往往对梯度的估计有较大的方法(奖励 $R_t$ 的随机性可能对轨迹长度L呈指数级增长)。为此，常用的方法是引进一个基准函数 $b(S_i)$，仅是状态 $S_i$ 的函数。可将上述梯度修改为：</p><p>$$\nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t&rsquo;=0}^{\infin}\nabla_{\theta} \log \pi_{\theta}(A_{t&rsquo;} | S_{t&rsquo;}) (\sum_{t=t&rsquo;}^{\infin} \gamma^{t-t&rsquo;}R_t - b(S_{t&rsquo;}))]$$</p><p>常见的PGM算法有REINFORCE，PG，PPO，TRPO 等。</p><h4 id=423-actor-critic-algorithms-演员-评论家方法>4.2.3 Actor-Critic algorithms (演员-评论家方法)</h4><p>Actor-Critic方法结合了上述 <font color=red>基于价值</font> 的方法和 <font color=red>基于策略</font> 的方法，利用基于价值的方法学习Q值函数或状态价值函数V来提高采样效率(Critic)，并利用基于策略的方法学习策略函数(Actor)，从而适用于连续或高维动作空间。其缺点也继承了二者的缺点，例如，Critic存在过估计问题，而Actor存在探索不足的问题等。</p><p>常见算法有 DDPG, A3C，TD3，SAC等，适用于 continuous and high-Dimension action space</p><h3 id=43-参数更新的方式不同>4.3 参数更新的方式不同</h3><p>Parameters updating methods</p><h4 id=431-monte-carlo-method蒙特卡罗方法>4.3.1 Monte Carlo method(蒙特卡罗方法)</h4><p>蒙特卡罗方法：必须等待一条轨迹 $\tau_k$ 生成(真实值)后才能更新。</p><p>常见算法有：Policy Gradient，TRPO，PPO等。</p><h4 id=432-temporal-difference-method时间差分方法>4.3.2 Temporal Difference method(时间差分方法)</h4><p>时间差分方法：在每一步动作执行都可以通过自举法(Bootstrapping)(估计值)及时更新。</p><p>常见算法有：DDPG，Q-learning，DQN等。</p><h2 id=参考>参考</h2><p>[1]. <a href=https://blog.csdn.net/b_b1949/article/details/128997146 target=_blank rel="external nofollow noopener noreferrer">https://blog.csdn.net/b_b1949/article/details/128997146<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></br>[2]. <a href=https://blog.csdn.net/magicyangjay111/article/details/132645347 target=_blank rel="external nofollow noopener noreferrer">https://blog.csdn.net/magicyangjay111/article/details/132645347<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-01-23 08:04:01">更新于 2024-01-23&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/a5b55cbc83051c74c5002d6a88ae9f2fbfb30ae0 rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) a5b55cbc83051c74c5002d6a88ae9f2fbfb30ae0: feat: add planning algos"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>a5b55cb</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_introduction/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Introduction/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_introduction/ data-title="RL | 强化学习 -- 简介" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_introduction/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_introduction/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_introduction/ data-title="RL | 强化学习 -- 简介"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_introduction/ data-title="RL | 强化学习 -- 简介"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/distributedtraining_5/ class=post-nav-item rel=prev title="分布式训练 - 第5篇 - 分布式训练服务框架基本原理与架构解析"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>分布式训练 - 第5篇 - 分布式训练服务框架基本原理与架构解析</a>
<a href=/posts/dqn/ class=post-nav-item rel=next title=DQN>DQN<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.121.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>