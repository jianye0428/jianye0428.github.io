<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>强化学习笔记 [12] | Dueling DQN - yejian's blog</title><meta name=author content="Jian YE">
<meta name=author-link content="https://github.com/jianye0428"><meta name=description content="0. 引言 在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。 1. Dueling DQN的优化点考虑"><meta name=keywords content='RL'><meta itemprop=name content="强化学习笔记 [12] | Dueling DQN"><meta itemprop=description content="0. 引言 在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。 1. Dueling DQN的优化点考虑"><meta itemprop=datePublished content="2024-02-25T11:16:52+08:00"><meta itemprop=dateModified content="2024-02-25T21:12:29+08:00"><meta itemprop=wordCount content="2136"><meta itemprop=image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta itemprop=keywords content="RL"><meta property="og:url" content="https://jianye0428.github.io/posts/rl_learning_note_12/"><meta property="og:site_name" content="yejian's blog"><meta property="og:title" content="强化学习笔记 [12] | Dueling DQN"><meta property="og:description" content="0. 引言 在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。 1. Dueling DQN的优化点考虑"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-25T11:16:52+08:00"><meta property="article:modified_time" content="2024-02-25T21:12:29+08:00"><meta property="article:tag" content="RL"><meta property="og:image" content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianye0428.github.io/images/favicon/jian_icon.png"><meta name=twitter:title content="强化学习笔记 [12] | Dueling DQN"><meta name=twitter:description content="0. 引言 在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。 1. Dueling DQN的优化点考虑"><meta name=application-name content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=apple-mobile-web-app-title content="菠菜阿九时代峰峻啊；数量可根据；"><meta name=theme-color data-light=#ffffff data-dark=#252627 content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/png href=/jian_icon.png><link rel=icon type=image/png sizes=32x32 href=/jian_icon.png><link rel=icon type=image/png sizes=16x16 href=/jian_icon.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jianye0428.github.io/posts/rl_learning_note_12/><link rel=prev href=https://jianye0428.github.io/posts/rl_learning_note_11/><link rel=next href=https://jianye0428.github.io/posts/rl_learning_note_13/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习笔记 [12] | Dueling DQN","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_12\/"},"image":["https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"],"genre":"posts","keywords":"RL","wordcount":2136,"url":"https:\/\/jianye0428.github.io\/posts\/rl_learning_note_12\/","datePublished":"2024-02-25T11:16:52+08:00","dateModified":"2024-02-25T21:12:29+08:00","publisher":{"@type":"Organization","name":"Jian YE","logo":"https:\/\/jianye0428.github.io\/images\/favicon\/jian_icon.png"},"author":{"@type":"Person","name":"Jian YE"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title="yejian's blog" data-alt="yejian's blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class="menu-item has-children"><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yejian's blog"><img loading=lazy src=/images/favicon/jian_icon.png srcset="/images/favicon/jian_icon.png, /images/favicon/jian_icon.png 1.5x, /images/favicon/jian_icon.png 2x" sizes=auto data-title=/images/favicon/jian_icon.png data-alt=/images/favicon/jian_icon.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>Jian's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/guestbook/><i class="fa-solid fa-comments fa-fw fa-sm" aria-hidden=true></i> 留言</a></li><li class=menu-item><span class=nested-item><a class=menu-link href=/about/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 关于</a>
<i class="dropdown-icon fa-solid fa-chevron-right" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item><a class=menu-link href=/projects/_index.zh-tw/ title=項目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的項目</a></li><li class=menu-item><a class=menu-link href=/projects/ title=项目><i class="fa-solid fa-laptop-code fa-fw fa-sm" aria-hidden=true></i> 我的项目</a></li></ul></li><li class=menu-item><a class=menu-link href=/pilot/><i class="fa-solid fa-user-tie fa-fw fa-sm" aria-hidden=true></i> 导航</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/jianye0428/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class="container container-reverse"><aside class=toc id=toc-auto></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>强化学习笔记 [12] | Dueling DQN</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/jianye0428 title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img loading=lazy src="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp" srcset="https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 1.5x, https://gravatar.loli.net/avatar/75a41975a5281767bf6bdba838de4238?s=32&amp;d=mp 2x" sizes=auto data-title="Jian YE" data-alt="Jian YE" class=avatar style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>&nbsp;Jian YE</a></span>
<span class=post-category>收录于 <a href=/categories/rl/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> RL</a></span></div><div class=post-meta-line><span title="发布于 2024-02-25 11:16:52"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span title="更新于 2024-02-25 21:12:29"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-25>2024-02-25</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2136 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 5 分钟</span>&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title="强化学习笔记 [12] | Dueling DQN">
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=content id=content data-end-flag=（完）><h1 id=0-引言>0. 引言</h1><p>在<a href=https://www.cnblogs.com/pinard/p/9797695.html target=_blank rel="external nofollow noopener noreferrer">强化学习(十一) Prioritized Replay DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的<a href=https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf target=_blank rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。</p><h1 id=1-dueling-dqn的优化点考虑>1. Dueling DQN的优化点考虑</h1><p>在前面讲到的DDQN中，我们通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，我们通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，我们尝试通过<font color=red>优化神经网络的结构</font>来优化算法。</p><p>具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态 $S$有关，与具体要采用的动作 $A$无关，这部分我们叫做<strong>价值函数部分</strong>，记做 $V(S,w,α)$,第二部分同时与状态状态 $S$ 和动作 $A$有关，这部分叫做**优势函数(Advantage Function)**部分,记为 $A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：</p><p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+A(S,A,w,\beta)$$</p><p>其中，$w$ 是公共部分的网络参数，而 $α$ 是价值函数独有部分的网络参数，而 $β$ 是优势函数独有部分的网络参数。</p><h1 id=2-dueling-dqn网络结构>2. Dueling DQN网络结构</h1><p>由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。为了简化算法描述，这里不使用原论文的CNN网络结构，而是使用前面文中用到的最简单的三层神经网络来描述。是否使用CNN对Dueling DQN算法本身无影响。</p><p>在前面讲到的DDQN等DQN算法中，我使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：</p><br><center><img src=images/2_01.png width=640 height=320 align=center style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">神经网络与Dueling DQN</div></center><br><p>而在Dueling DQN中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。</p><p>我们可以直接使用上一节的价值函数的组合公式得到我们的动作价值，但是这个式子无法辨识最终输出里面 $V(S,w,α)$ 和 $A(S,A,w,β)$各自的作用，为了可以体现这种可辨识性(identifiability),实际使用的组合公式如下：</p><p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+(A(S,A,w,\beta)-\frac1{\mathcal{A}}\sum_{a^{\prime}\in\mathcal{A}}A(S,a^{\prime},w,\beta))$$</p><p>其实就是对优势函数部分做了中心化的处理。以上就是Dueling DQN的主要算法思路。由于它仅仅涉及神经网络的中间结构的改进，现有的DQN算法可以在使用Duel DQN网络结构的基础上继续使用现有的算法。由于算法主流程和其他算法没有差异，这里就不单独讲Duel DQN的算法流程了。</p><h1 id=3-dueling-dqn实例>3. Dueling DQN实例</h1><p>下面我们用一个具体的例子来演示Dueling DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href=https://github.com/openai/gym/wiki/CartPole-v0 target=_blank rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p><p>这个实例代基于Nature DQN，并将网络结构改为上图中右边的Dueling DQN网络结构，完整的代码参见github: <a href=https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py target=_blank rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><p>这里我们重点关注Dueling DQN和Nature DQN的代码的不同之处。也就是网络结构定义部分，主要的代码如下，一共有两个相同结构的Q网络，每个Q网络都有状态函数和优势函数的定义，以及组合后的Q网络输出，如代码红色部分：</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_Q_network</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># input layer</span>
</span></span><span class=line><span class=cl>  <span class=bp>self</span><span class=o>.</span><span class=n>state_input</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=s2>&#34;float&#34;</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=c1># network weights</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;current_net&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>W1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>b1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden layer 1</span>
</span></span><span class=line><span class=cl>    <span class=n>h_layer_1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>,</span><span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden layer  for state value</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;Value&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>W21</span><span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b21</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>V</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer_1</span><span class=p>,</span> <span class=n>W21</span><span class=p>)</span> <span class=o>+</span> <span class=n>b21</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden layer  for action value</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;Advantage&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>W22</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b22</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>A</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer_1</span><span class=p>,</span> <span class=n>W22</span><span class=p>)</span> <span class=o>+</span> <span class=n>b22</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Q Value layer</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>Q_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>V</span> <span class=o>+</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>A</span> <span class=o>-</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>A</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keep_dims</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;target_net&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>W1t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dim</span><span class=p>,</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>b1t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden layer 1</span>
</span></span><span class=line><span class=cl>    <span class=n>h_layer_1t</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_input</span><span class=p>,</span><span class=n>W1t</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden layer  for state value</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;Value&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>W2v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b2v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>VT</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer_1t</span><span class=p>,</span> <span class=n>W2v</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># hidden layer  for action value</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;Advantage&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>W2a</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_variable</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>b2a</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias_variable</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>action_dim</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>AT</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>h_layer_1t</span><span class=p>,</span> <span class=n>W2a</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2a</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      <span class=c1># Q Value layer</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>target_Q_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>VT</span> <span class=o>+</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>AT</span> <span class=o>-</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>AT</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keep_dims</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span></span></span></code></pre></td></tr></table></div></div><p>其余部分代码和Nature DQN基本相同。当然，我们可以也在前面DDQN，Prioritized Replay DQN代码的基础上，把网络结构改成上面的定义，这样Dueling DQN也可以起作用。</p><h1 id=4-dqn总结>4. DQN总结</h1><p>DQN系列我花了5篇来讲解，一共5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。</p><p>当然DQN家族的算法远远不止这些，还有一些其他的DQN算法我没有详细介绍，比如使用一些较复杂的CNN和RNN网络来提高DQN的表达能力，又比如改进探索状态空间的方法等，主要是在DQN的基础上持续优化。</p><p>DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。虽然NAF DQN可以解决这个问题，但是方法过于复杂了。而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题，从下一篇我们开始讨论Policy-Based深度强化学习。</p></div><div class=post-reward><div class=comment>Buy me a coffee~</div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward>赞赏</label><div class=reward-ways data-mode=fixed><div><img loading=lazy src=/images/alipay.png srcset="/images/alipay.png, /images/alipay.png 1.5x, /images/alipay.png 2x" sizes=auto data-title="Jian YE 支付宝" data-alt="Jian YE 支付宝" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>支付宝</span></div><div><img loading=lazy src=/images/wechatpay.png srcset="/images/wechatpay.png, /images/wechatpay.png 1.5x, /images/wechatpay.png 2x" sizes=auto data-title="Jian YE 微信" data-alt="Jian YE 微信" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span data-animation>微信</span></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-25 21:12:29">更新于 2024-02-25&nbsp;<a class=git-hash href=https://github.com/jianye0428/JianBlog/commit/23cd7c478708235ece71456fee411313e37ef46b rel="external nofollow noopener noreferrer" target=_blank title="commit by yejian(18817571704@163.com) 23cd7c478708235ece71456fee411313e37ef46b: feat: add rl learning note to 19"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>23cd7c4</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/rl_learning_note_12/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/jianye0428/JianBlog/edit/docs/content/posts/RL/RL_Learning_Notes/rl_learning_note_12/index.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://jianye0428.github.io/posts/rl_learning_note_12/ data-title="强化学习笔记 [12] | Dueling DQN" data-hashtags=RL><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://jianye0428.github.io/posts/rl_learning_note_12/ data-hashtag=RL><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Linkedin" data-sharer=linkedin data-url=https://jianye0428.github.io/posts/rl_learning_note_12/><i class="fa-brands fa-linkedin fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://jianye0428.github.io/posts/rl_learning_note_12/ data-title="强化学习笔记 [12] | Dueling DQN"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 百度" data-sharer=baidu data-url=https://jianye0428.github.io/posts/rl_learning_note_12/ data-title="强化学习笔记 [12] | Dueling DQN"><i data-svg-src=/lib/simple-icons/icons/baidu.min.svg aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/rl/ class=post-tag>RL</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/rl_learning_note_11/ class=post-nav-item rel=prev title="强化学习笔记 [11] | Prioritized Replay DQN"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>强化学习笔记 [11] | Prioritized Replay DQN</a>
<a href=/posts/rl_learning_note_13/ class=post-nav-item rel=next title="强化学习笔记 [13] | 策略梯度(Policy Gradient)">强化学习笔记 [13] | 策略梯度(Policy Gradient)<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.126.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.18"><img class=fixit-icon src=/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2024</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/jianye0428 target=_blank rel="external nofollow noopener noreferrer">Jian YE</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics order-first"><span class=site-time title=网站运行中……><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i><span class="ms-1 d-none">博客已运行</span><span class="run-times ms-1">网站运行中……</span></span></div><div class="footer-line visitor"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://github.com/jianye0428/JianBlog title="在 GitHub 上查看程式碼，訂閱請點 Watch" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/blue/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/lib/instant-page/instantpage.min.js async defer type=module></script><script src=/lib/twemoji/twemoji.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:50},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{algoliaAppID:"MTJNHU0JVB",algoliaIndex:"index",algoliaSearchKey:"5486225134d99f43826da401ee9bad57",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2018-05-28T20:01:01+08:00",twemoji:!0,watermark:{appendto:".wrapper>main",colspacing:30,content:'<img style="height: 0.85rem;" src="/images/favicon/jian_icon.png" alt="logo" /> jianye',enable:!0,fontfamily:"MMT_LRH,沐目体",fontsize:1.1,height:20,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script><script src=/js/custom.min.js defer></script></body></html>