<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>yejian's blog</title><link>https://jianye0428.github.io/</link><description>Lruihao's Note 李瑞豪的博客：探索、分享、记录自己在工作生活学习到一些东西。人知道得越多，就就会发现无知的越多。有更广袤世界可以探索，真是莫大的快乐啊！</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sun, 12 May 2024 17:56:54 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>点、线、面之间的关系</title><link>https://jianye0428.github.io/posts/pointlineplane/</link><pubDate>Sun, 12 May 2024 17:56:54 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/pointlineplane/</guid><description><![CDATA[<h2 id="1-点到直线的距离">1. 点到直线的距离</h2>
<p><mark><strong><font color=red>第一种:</font></strong></mark>
本文默认情况下，直线的方程为 $l:Ax+By+C=0$，$A$, $B$ 均不为0，斜率为 $k_l$，点的坐标为P(x0, y0)，点 $P$ 到 $l$ 的距离为 $d$ 。</p>
<p>则距离为:</p>
<p>$$d=\frac{|Ax_0+By_0+C|}{\sqrt{A^2+B^2}}$$</p>
<p>推导过程如下: <a href="https://zhuanlan.zhihu.com/p/26307123"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/26307123<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><mark><strong><font color=red>第二种:</font></strong></mark>
直线的方程为 $l: y = ax + b$，$a$, $b$ 均不为0，斜率为 $a$，点的坐标为P(x0, y0)，点 $P$ 到直线 $l$ 的距离为 $d$ 。</p>
<p>$$d=\frac{|ax_{0} - y_{0} + b|}{\sqrt{a^2+b^2}}$$</p>
<h2 id="2-如何判断两点在直线的两侧">2. 如何判断两点在直线的两侧</h2>
<p><mark><strong><font color=red>判断点在直线的一侧:</font></strong></mark></p>
<p><strong>方法一:</strong></p>
<p>已知 $P(0,0)$, $Q(3,2)$ 两点，试判断 $P$ , $Q$是否在直线 $2x+3y=4$ 的同一侧。</p>
<p>解：直线2x+3y=4, 即直线2x+3y-4=0, 把P、Q代入2x+3y-4得到:</p>
<p>$$2 \times 0 + 3 \times 0-4=-4&lt;0$$</p>
<p>$$2 {\times} 3+3 {\times} 2 - 4 = 8 &gt; 0$$</p>
<p>所以，在<strong>两侧</strong>。</p>
<p><strong>方法2:</strong></p>
<blockquote>
<p>怎么判断坐标为(xp,yp)的点P是在直线的哪一侧呢?</p>
</blockquote>
<p>设直线是由其上两点 $(x_1,y_1)$，$(x_2,y_2)$ 确定的，直线方向是由 $(x_1,y_1)$ 到 $(x_2,y_2)$ 的方向。</p>
<p>假设直线方程为：$Ax+By+C=0$，则有:</p>
<p>$A=y2-y1$; $B=x1-x2$; $C=x2<em>y1-x1</em>y2$;</p>
<p>$$\left\{\begin{aligned}
A&amp;=y_2-y_1\\
B&amp;=x_1-x_2\\
C&amp;=x_2<em>y_1-x_1</em>y_2
\end{aligned}\right.$$</p>
<p>这时可以通过计算D,来判断点P是在直线的哪一侧:</p>
<p>$$D=A<em>x_p+B</em>y_p+C$$</p>
<p>若D&lt;0, 则点P在直线的左侧;
若D&gt;0, 则点P在直线的右侧;
若D=0, 则点P在直线上。</p>
<blockquote>
<p>注：这里的直线是有方向性的！</p>
</blockquote>
<p><strong>方法3：</strong></p>
<blockquote>
<p>利用矢量计算快速判定一点在直线的哪一侧!</p>
</blockquote>
<p>例如矢量A×矢量B=矢量C</p>
<p>设想矢量A沿小于180度的角度转向矢量B</p>
<p>将右手的四指指向矢量A的方向，右手的四指弯曲代表上述旋转方向，则伸直的拇指指向它们的叉乘得到的矢量C</p>
<p>如果矢量C的方向相同，则在同侧；否则在两侧。</p>
<blockquote>
<p>注：叉乘计算公式！</p>
</blockquote>
<p>若将向量用坐标表示（三维向量），向量 $a=(x_1,y_1,z_1)$，向量 $b=(x_2,y_2,z_2)$，则：</p>
<p><strong>点乘</strong>，也叫向量的内积、数量积。</p>
<p>$$向量a·向量b = |a||b|cos\theta$$
$$向量a·向量b = x_1 * x_2 + y_1 * y_2 + z_1 * z_2$$</p>
<p><strong>叉乘</strong>，也叫向量的外积、向量积。</p>
<p>$$|向量c| = |向量a×向量b| = |a||b|sin \theta$$</p>
<p>向量c的方向与a,b所在的平面垂直，且方向要用“右手法则”判断（用右手的四指先表示向量a的方向，然后手指朝着手心的方向&lt;180摆动到向量b的方向，大拇指所指的方向就是向量c的方向）；</p>
<p>$$a\times b=\begin{vmatrix}\mathrm{i}&amp;\mathrm{j}&amp;\mathrm{k}\\x_{1}&amp;y_{1}&amp;z_{1}\\x_{2}&amp;y_{2}&amp;z_{2}\end{vmatrix}=(y_{1}z_{2}-y_{2}z_{1})i-(x_{1}z_{2}-x_{2}z_{1})j+(x_{1}y_{2}-x_{2}y_{1})k$$</p>
<p>（i、j、k分别为空间中相互垂直的三条坐标轴的单位向量）
ref: <br>
[1]. <a href="https://aipiano.github.io/2019/01/25/%E5%8F%89%E4%B9%98%E9%80%9F%E6%9F%A5%E6%89%8B%E5%86%8C/"target="_blank" rel="external nofollow noopener noreferrer">Cross Product叉乘速查手册<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><br>
[2].<a href="https://zhuanlan.zhihu.com/p/359975221"target="_blank" rel="external nofollow noopener noreferrer">叉乘几何意义<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><br>
[3].https://blog.csdn.net/wzyaiwl/article/details/106310705</p>
<h2 id="3-判断点是否在矩形多边形中">3. 判断点是否在矩形、多边形中</h2>
<p><strong>方法一:</strong></p>
<blockquote>
<p>只要判断该点的横坐标和纵坐标是否夹在矩形的左右边和上下边之间。</p>
</blockquote>
<p>例如: 判断一个点是否在两条线段之间夹着就转化成，判断一个点是否在某条线段的一边上，就可以利用叉乘的方向性，来判断夹角是否超过了180度</p>
<p>如下图:
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>只要判断 $(AB \times AE ) * (CD \times CE)  &gt;= 0$ 就说明E在AD和BC中间夹着，同理 $ (DA \times DE ) * (BC \times BE) &gt;= 0 $ 计算另两边AB,CD就可以了。(<a href="https://www.cnblogs.com/fangsmile/p/14690062.html"target="_blank" rel="external nofollow noopener noreferrer">备注可进一步学习：向量点乘，叉乘的意义和几何意义<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>最后就是只需要判断</p>
<p>$$(AB \times AE ) * (CD \times CE)  &gt;= 0 \text{且} (DA \times DE ) * (BC \times BE) &gt;= 0$$ 。</p>
<p>参考代码:</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 计算 |p1 p2| X |p1 p|
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">function</span> <span class="nf">GetCross</span><span class="p">(</span><span class="nl">p1</span><span class="p">:</span> <span class="n">Point</span><span class="p">,</span> <span class="nl">p2</span><span class="p">:</span> <span class="n">Point</span><span class="p">,</span> <span class="nl">p</span><span class="p">:</span> <span class="n">Point</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span><span class="n">p2</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">p1</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">p1</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p2</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">//判断点p是否在p1p2p3p4的正方形内
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">function</span> <span class="nf">IsPointInMatrix</span><span class="p">(</span><span class="nl">p1</span><span class="p">:</span> <span class="n">Point</span><span class="p">,</span> <span class="nl">p2</span><span class="p">:</span> <span class="n">Point</span><span class="p">,</span> <span class="nl">p3</span><span class="p">:</span> <span class="n">Point</span><span class="p">,</span> <span class="nl">p4</span><span class="p">:</span> <span class="n">Point</span><span class="p">,</span> <span class="nl">p</span><span class="p">:</span> <span class="n">Point</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">isPointIn</span> <span class="o">=</span> <span class="n">GetCross</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">GetCross</span><span class="p">(</span><span class="n">p3</span><span class="p">,</span> <span class="n">p4</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">GetCross</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">GetCross</span><span class="p">(</span><span class="n">p4</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">isPointIn</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>举例: <a href="https://www.cnblogs.com/fangsmile/p/9306510.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/fangsmile/p/9306510.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><strong>方法2：</strong></p>
<blockquote>
<p>采用点是否包含在多边形中判断</p>
</blockquote>
<p>以该点为顶点，做一条射线，使得矩形四个顶点中任意一点都不在射线上。</p>
<p>若该射线与矩形有且仅有一个交点，则在矩形内；若有零个或两个焦点，则在矩形外。</p>
<p>至于射线，可以通过选择肯定在矩形外的一点和已知点练成线段来构成。</p>
<p><strong>References:</strong>
[1] <a href="https://oi-wiki.org/geometry/2d/"target="_blank" rel="external nofollow noopener noreferrer">二维计算几何基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="4-判断一个点是否在三角形的内部">4. 判断一个点是否在三角形的内部</h2>
<p><strong>方法一：面积比较</strong></p>
<p>判断△ABO+△BOC+△COA的面积与△ABC是否相等。若相等则O在内部，反之则在外部。
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>如何计算三角形的面积呢？通过坐标，很容易计算三角形的边长。</p>
<p>再由海伦公式计算面积。
$$S=\sqrt{p(p-a)(p-b)(p-c)}$$</p>
<p>其中，a,b,c为三边长度, $p=\frac{a+b+c}{2}$</p>
<p>代码实现:</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;math.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">Point</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">double</span> <span class="nf">getDist</span><span class="p">(</span><span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">Point</span> <span class="n">p2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//两点之间计算距离公式
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">pow</span><span class="p">(</span><span class="n">p1</span><span class="p">.</span><span class="n">x</span><span class="o">-</span><span class="n">p2</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">pow</span><span class="p">(</span><span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="o">-</span><span class="n">p2</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="mi">2</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">double</span> <span class="nf">getArea</span><span class="p">(</span><span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">Point</span> <span class="n">p2</span><span class="p">,</span><span class="n">Point</span> <span class="n">p3</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">a</span> <span class="o">=</span> <span class="n">getDist</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">b</span> <span class="o">=</span> <span class="n">getDist</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">p3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">c</span> <span class="o">=</span> <span class="n">getDist</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">c</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">bool</span> <span class="nf">isInTriangle</span><span class="p">(</span><span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">Point</span> <span class="n">p2</span><span class="p">,</span><span class="n">Point</span> <span class="n">p3</span><span class="p">,</span><span class="n">Point</span> <span class="n">o</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">s1</span> <span class="o">=</span> <span class="n">getArea</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">getArea</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">s3</span> <span class="o">=</span> <span class="n">getArea</span><span class="p">(</span><span class="n">p3</span><span class="p">,</span><span class="n">p1</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">s</span> <span class="o">=</span> <span class="n">getArea</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">s1</span><span class="o">+</span><span class="n">s2</span><span class="o">+</span><span class="n">s3</span> <span class="o">==</span> <span class="n">s</span><span class="p">;</span> <span class="c1">//此处没有用fabs(a-b)&lt;eps比较，是方便大家理解思路
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">,</span><span class="n">o</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">p1</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">p2</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">p2</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">p3</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">p3</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">o</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">o</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="n">isInTriangle</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span> <span class="n">puts</span><span class="p">(</span><span class="s">&#34;Yes&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="n">puts</span><span class="p">(</span><span class="s">&#34;No&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>方法二：向量叉乘</strong></p>
<p>若点O在三角形内部，则沿着三角形的边逆时针走，点O一定保持在边的左侧。如图示，点在逆时针行走时，在AB，BC，CA的左侧。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>如何判断点在一个边的左侧呢？</p>
<p>可以借助向量叉乘来判断O是否在向量AB的哪一侧。通过计算向量AO与向量AB的叉乘的值为正，则表示O在AB的左侧，反之为右侧。</p>
<p>(理解最好，理解不了也不要纠结，把叉乘公式记一下就ok)</p>
<p>向量 $\overrightarrow{a}$ 是 $(m,n)$ , $\vec{b}$ 是 $(p,q)$</p>
<p>$$\vec{a}  \times  \vec{b} = m<em>q-n</em>p$$</p>
<p>本题的核心思路就是这样。如果要让手撕代码，题目可能没有说输入的3个点是逆时针顺序的。比如，上图中如果依次输入的是A,C,B的坐标，那就不行了。</p>
<p>怎么解决呢？假设依次输入的点分别是p1,p2,p3。</p>
<p>我们判断若p3在 $\vec{p1} \vec{p2}$的右侧！则表示输入的点的顺序是顺时针的，即A,C,B式的输入，将p2,p3调换位置即可保证顺序是逆时针。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>参考代码:</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;math.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">Point</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="kt">double</span> <span class="nf">crossproduct</span><span class="p">(</span><span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">Point</span> <span class="n">p2</span><span class="p">,</span><span class="n">Point</span> <span class="n">p3</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//首先根据坐标计算p1p2和p1p3的向量，然后再计算叉乘
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//p1p2 向量表示为 (p2.x-p1.x,p2.y-p1.y)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//p1p3 向量表示为 (p3.x-p1.x,p3.y-p1.y)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="p">(</span><span class="n">p2</span><span class="p">.</span><span class="n">x</span><span class="o">-</span><span class="n">p1</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p3</span><span class="p">.</span><span class="n">y</span><span class="o">-</span><span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">p2</span><span class="p">.</span><span class="n">y</span><span class="o">-</span><span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p3</span><span class="p">.</span><span class="n">x</span><span class="o">-</span><span class="n">p1</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">bool</span> <span class="nf">isInTriangle</span><span class="p">(</span><span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">Point</span> <span class="n">p2</span><span class="p">,</span><span class="n">Point</span> <span class="n">p3</span><span class="p">,</span><span class="n">Point</span> <span class="n">o</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//保证p1，p2，p3是逆时针顺序
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">crossproduct</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p3</span><span class="p">)</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="n">isInTriangle</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p3</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">crossproduct</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">crossproduct</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">crossproduct</span><span class="p">(</span><span class="n">p3</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Point</span> <span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">,</span><span class="n">o</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">p1</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">p1</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">p2</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">p2</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">p3</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">p3</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cin</span> <span class="o">&gt;&gt;</span> <span class="n">o</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">o</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="n">isInTriangle</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">,</span><span class="n">p3</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span> <span class="n">puts</span><span class="p">(</span><span class="s">&#34;Yes&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="n">puts</span><span class="p">(</span><span class="s">&#34;No&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://leetcode.cn/circle/discuss/7OldE4/"target="_blank" rel="external nofollow noopener noreferrer">https://leetcode.cn/circle/discuss/7OldE4/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RRT (Rapidly-Exploring Random Tree) 算法详解</title><link>https://jianye0428.github.io/posts/rrt/</link><pubDate>Thu, 09 May 2024 16:18:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rrt/</guid><description><![CDATA[<h1 id="0-基于采样的运动规划算法-rrtrapidly-exploring-random-trees">0. 基于采样的运动规划算法-RRT(Rapidly-exploring Random Trees)</h1>
<p>RRT是Steven M. LaValle和James J. Kuffner Jr.提出的一种通过随机构建Space Filling Tree实现对非凸高维空间快速搜索的算法。该算法可以很容易的处理包含障碍物和差分运动约束的场景，因而广泛的被应用在各种机器人的运动规划场景中。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="1-basic-rrt算法">1、 Basic RRT算法</h2>
<p>原始的RRT算法中将搜索的起点位置作为根节点，然后通过随机采样增加叶子节点的方式，生成一个随机扩展树，当随机树的叶子节点进入目标区域，就得到了从起点位置到目标位置的路径。</p>
<p>伪代码如下：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上述伪代码中，M是地图环境，$x_{init}$是起始位置，$x_{goal}$是目标位置。路径空间搜索的过程从起点开始，先随机撒点$x_rand$;然后查找距离 $x_rand$ 最近的节点 $x_{near}$;然后沿着 $x_{near}$ 到 $x_{rand}$方向前进stepsize的距离得到$x_{new}$; CollisionFree(M, $E_i$) 方法检测Edge $(x_{new},x_{near})$ 是否与地图环境中的障碍物有碰撞，如果没有碰撞，则将成功完成一次空间搜索拓展。重复上述过程，直至达到目标位置。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源:https://www.researchgate.net/profile/Burak_Boyacioglu/publication/306404973/figure/fig1/AS:398553223581697@1472033901892/Basic-RRT-algorithm.png</p>
<h2 id="2基于概率的rrt算法">2、基于概率的RRT算法</h2>
<p>为了加快随机树收敛到目标位置的速度，基于概率的RRT算法在随机树的扩展的步骤中引入一个概率 $p$，根据概率 $p$ 的值来选择树的生长方向是随机生长($x_{rand}$) 还是朝向目标位置 $x_{goal}$ 生长。引入向目标生长的机制可以加速路径搜索的收敛速度。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">基于概率的RRT算法</div>
</center>
<br>
<h2 id="3rrt-connect算法">3、RRT Connect算法</h2>
<p>RRT Connect算法从初始状态点和目标状态点同时扩展随机树从而实现对状态空间的快速搜索。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源:https://www.cs.cmu.edu/~motionplanning/lecture/lec20.pdf</p>
<h2 id="4rrt算法">4、RRT*算法</h2>
<p>RRT*算法的目标在于解决RRT算法难以求解最优的可行路径的问题，它在路径查找的过程中持续的优化路径，随着迭代次数和采样点的增加，得到的路径越来越优化。迭代的时间越久，就越可以得到相对满意的规划路径。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源：https://blog.csdn.net/gophae/article/details/103231053</p>
<p>RRT*算法与RRT算法的区别主要在于两点：</p>
<ol>
<li>rewrite的过程。即为 $x_{new}$ 重新选择父节点的过程；</li>
<li>随机树重布线的过程；</li>
</ol>
<h2 id="41-rewrite"><strong>4.1 Rewrite</strong></h2>
<p>下面我们看看Rewrite是如何实现的。RRT*在找到距离 $x_{rand}$ 最近的节点 $x_{nearest}$ 并通过CollisionFree检测之后，并不立即将 Edge(x_{nearest},x_{rand}) 加入扩展树中。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
图片来源：https://blog.csdn.net/weixin_43795921/article/details/88557317
<p>而是以 $x_{rand}$ 为中心，$r$ 为半径，找到所有潜在的父节点集合，并与 $x_{nearest}$ 父节点的Cost对比，看是否存在更优Cost的父节点。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源：https://blog.csdn.net/weixin_43795921/article/details/88557317</p>
<p>如下图所示，我们会计算路径 $x_{init} \rightarrow x_{parent} \rightarrow x_{child}$ 的Cost=$cost1$，再计算 $x_{init} \rightarrow x_{potential_parent} \rightarrow x_{child}$ 的Cost=$cost2$，比较 $cost1$和 $cost2$ 的大小。此处由于 $x_{potential_parent}$ 与 $ {x_child} $ 之间存在障碍物导致二者的直接连线不可达，所以 $cost &gt; cost1$ ，不需改变 $ x_{child} $ 的父节点。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源：https://blog.csdn.net/weixin_43795921/article/details/88557317</p>
<p>如下图所示，当路径 ${x_{init} \rightarrow x_{parent}-&gt;x_{child}} $的Cost大于 ${x_{init} \rightarrow x_{potential_parent} \rightarrow x_{child}}$的Cost时，RRT^*算法会将Edge${ x_{parent} \rightarrow x_{child}}$剔 除 , 并 新 增 Edge${ x_{potential_parent} \rightarrow x_{child}} $。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源：https://blog.csdn.net/weixin_43795921/article/details/88557317</p>
<p>至此我们就完成了一次Rewrite的过程，新生成的随机树如下。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源：https://blog.csdn.net/weixin_43795921/article/details/88557317</p>
<h2 id="42-随机树重布线的过程">4.2 随机树重布线的过程</h2>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>图片来源：https://blog.csdn.net/weixin_43795921/article/details/88557317</p>
<p>在为 $x_{new}$ 重新选择父节点之后，重布线使得生成新节点后的随机树减少冗余通路，减小路径代价。</p>
<p>如上图所示，$x_{new}$ 为新生成的节点，4、6、8是 $x_{new}$ 的近邻节点，0、 4、 5分别为近邻节点的父节点。</p>
<p>路径{0-&gt;4}的Cost为: 10</p>
<p>路径{0-&gt;4-&gt;6}的Cost为： 10 + 5 = 15</p>
<p>路径{0-&gt;1-&gt;5-&gt;8}的Cost为: 3 + 5 + 1 = 9</p>
<p>先尝试将节点4的父节点改为 $x_{new}$，到达节点4的路径变为{0-&gt;1-&gt;5-&gt;9-&gt;4}，新路径的Cost=3+5+3+4=15，新路径的Cost大于原路径Cost，所以不改变节点4的父节点。</p>
<p>再尝试改变节点8的父节点为 $x_{new}$，到达节点8的路径变为{0-&gt;1-&gt;5-&gt;9-&gt;8},新路径的Cost=3+5+3+3=14，新路径的Cost大于原路径Cost，随意不改变节点8的父节点。</p>
<p>再尝试改变节点6的父节点为 $x_{new}$，到达路径6的路径变为{0-&gt;1-&gt;5-&gt;9-&gt;6},新的Cost=3+5+3+1=12,新路径的Cost小于原路径Cost，因此将节点6的父节点更新为节点9。</p>
<p>重布线后的随机树如上右图所示。</p>
<h2 id="43-rrt算法效果">4.3 RRT*算法效果</h2>
<p>从RRT与RRT<em>的效果可以看出，RRT</em>的路径规划的结果优于RRT算法。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>RRT^* VS RRT。图片来源：https://www.cc.gatech.edu/~dellaert/11S-AI/Topics/Entries/2011/2/21_PRM,_RRT,_and_RRT__files/06-RRT.pdf</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>RRT^* VS RRT With Obstacles。图片来源：https://www.cc.gatech.edu/~dellaert/11S-AI/Topics/Entries/2011/2/21_PRM,_RRT,_and_RRT__files/06-RRT.pdf</p>
<p>RRT^*算法+赛车动力学实现车辆180度转弯。图片来源：https://www.youtube.com/watch?v=KSB_9KE6fWI</p>
<h2 id="参考链接"><strong>参考链接</strong></h2>
<p>1、基于RRT的运动规划算法综述(<a href="https://link.zhihu.com/?target=https%3A//wenku.baidu.com/view/8de40fafbdeb19e8b8f67c1cfad6195f312be80a.html"target="_blank" rel="external nofollow noopener noreferrer">https://wenku.baidu.com/view/8de40fafbdeb19e8b8f67c1cfad6195f312be80a.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>2、RRT维基百科(<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Rapidly-exploring_random_tree"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/Rapidly-exploring_random_tree<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>3、PRM, RRT, and RRT*，Frank Dellaer (<a href="https://link.zhihu.com/?target=https%3A//www.cc.gatech.edu/~dellaert/11S-AI/Topics/Entries/2011/2/21_PRM%2C_RRT%2C_and_RRT__files/06-RRT.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://www.cc.gatech.edu/~dellaert/11S-AI/Topics/Entries/2011/2/21_PRM,_RRT,_and_RRT__files/06-RRT.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>4、路径规划——改进RRT算法(<a href="https://zhuanlan.zhihu.com/p/51087819"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/51087819<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>5、运动规划RRT*算法图解(<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_43795921/article/details/88557317"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_43795921/article/details/88557317<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>6、全局路径规划：图搜索算法介绍4(RRT/RRT*)(<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/gophae/article/details/103231053"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/gophae/article/details/103231053<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</p>
<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/133224593"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/133224593<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://blog.csdn.net/gophae/article/details/103231053"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/gophae/article/details/103231053<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3]. <a href="https://xwlu.github.io/wiki/path-planning/rrt/"target="_blank" rel="external nofollow noopener noreferrer">https://xwlu.github.io/wiki/path-planning/rrt/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[4]. ※ <a href="https://dlonng.com/posts/rrt"target="_blank" rel="external nofollow noopener noreferrer">https://dlonng.com/posts/rrt<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>强化学习 | 深度解读Soft Actor-Critic 算法</title><link>https://jianye0428.github.io/posts/sac/</link><pubDate>Sat, 04 May 2024 17:42:03 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/sac/</guid><description><![CDATA[<h1 id="深度解读soft-actor-critic-算法">深度解读Soft Actor-Critic 算法</h1>
<h2 id="1-前言">1 前言</h2>
<p>机器人学习Robot Learning正在快速的发展，其中深度强化学习deep reinforcement learning（DRL），特别是面向连续控制continous control的DRL算法起着重要的作用。在这一领域中，目前可以说有三类行之有效的model free DRL算法：</p>
<ul>
<li>TRPO,PPO</li>
<li>DDPG及其拓展（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.08617"target="_blank" rel="external nofollow noopener noreferrer">D4PG<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>,TD3等）</li>
<li>Soft Q-Learning, Soft Actor-Critic</li>
</ul>
<p><strong><font color=red>PPO</font></strong> 算法是目前最主流的DRL算法，同时面向离散控制和连续控制，在<a href="https://en.wikipedia.org/wiki/OpenAI_Five"target="_blank" rel="external nofollow noopener noreferrer">OpenAI Five<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>上取得了巨大成功。但是PPO是一种on-policy的算法，也就是PPO面临着严重的sample inefficiency，需要巨量的采样才能学习，这对于真实的机器人训练来说，是无法接受的。</p>
<p><strong><font color=red>DDPG</font></strong> 及其拓展则是DeepMind开发的面向连续控制的off policy算法，相对PPO 更sample efficient。<strong>DDPG训练的是一种确定性策略deterministic policy，即每一个state下都只考虑最优的一个动作</strong>。DDPG的拓展版D4PG从paper中的结果看取得了非常好的效果，但是并没有开源，目前github上也没有人能够完全复现Deepmind的效果。</p>
<p><strong><font color=red>Soft Actor-Critic (SAC)</font></strong> 是面向Maximum Entropy Reinforcement learning 开发的一种off policy算法，和DDPG相比，Soft Actor-Critic使用的是随机策略stochastic policy，相比确定性策略具有一定的优势（具体后面分析）。Soft Actor-Critic在公开的benchmark中取得了非常好的效果，并且能直接应用到真实机器人上。最关键的是，Soft Actor-Critic是完全开源的，因此，深入理解Soft Actor-Critic 算法具有非常重要的意义，也是本篇blog的目的。</p>
<p>Soft Actor-Critic算法相关链接：</p>
<p>Paper：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.01290"target="_blank" rel="external nofollow noopener noreferrer">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.05905"target="_blank" rel="external nofollow noopener noreferrer">Soft Actor-Critic Algorithms and Applications<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08165"target="_blank" rel="external nofollow noopener noreferrer">Reinforcement Learning with Deep Energy-Based Policies<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (Soft Q-Learning)</li>
</ul>
<p>Codes:</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/rail-berkeley/softlearning"target="_blank" rel="external nofollow noopener noreferrer">rail-berkeley/softlearning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (原作者实现）</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/vitchyr/rlkit"target="_blank" rel="external nofollow noopener noreferrer">vitchyr/rlkit<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/openai/spinningup"target="_blank" rel="external nofollow noopener noreferrer">openai/spinningup<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/hill-a/stable-baselines"target="_blank" rel="external nofollow noopener noreferrer">hill-a/stable-baselines<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p>下面我们来详细解读一下SAC的算法及其具体实现。本文的阅读需要有基本的DRL算法基础知识。</p>
<h2 id="2-为什么研究-maximum-entropy-reinforcement-learning">2 为什么研究 Maximum Entropy Reinforcement Learning？</h2>
<p>对于一般的DRL，学习目标很直接，就是学习一个policy使得累加的reward期望值最大：</p>
<p>$$\pi^*=\arg\max_\pi\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[\sum_tR(s_t,a_t)]\tag{1}$$</p>
<p>而最大熵RL，除了上面的基本目标，还要求policy的每一次输出的action 熵entropy最大：</p>
<p>$$\pi^*=\arg\max_\pi\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[\sum_t\underbrace{R(s_t,a_t)}_{reward}+\alpha\underbrace{H(\pi(\cdot|s_t))}_{entropy}]\tag{2}$$</p>
<p>这样做的基本目的是什么呢？让策略随机化，即输出的每一个action的概率尽可能分散，而不是集中在一个action上。不了解entropy的同学可以看这里：<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%86%B5_%28%E4%BF%A1%E6%81%AF%E8%AE%BA%29"target="_blank" rel="external nofollow noopener noreferrer">wiki-信息熵<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>我们知道DDPG训练得到的是一个deterministic policy确定性策略，也就是说这个策略对于一种状态state只考虑一个最优的动作。所以，stochastic policy相对deterministic policy有什么优势呢？</p>
<p>Stochastic policy随机策略在实际机器人控制上往往是更好的做法。比如我们让机器人抓取一个水杯，机器人是有无数条路径去实现这个过程的，而并不是只有唯一的一种做法。因此，我们就需要drl算法能够给出一个随机策略，在每一个state上都能输出每一种action的概率，比如有3个action都是最优的，概率一样都最大，那么我们就可以从这些action中随机选择一个做出action输出。<strong>最大熵maximum entropy的核心思想就是不遗落到任意一个有用的action，有用的trajectory</strong>。对比DDPG的deterministic policy的做法，看到一个好的就捡起来，差一点的就不要了，而最大熵是都要捡起来，都要考虑。</p>
<p><strong>基于最大熵的RL算法有什么优势？</strong></p>
<p>以前用deterministic policy的算法，我们找到了一条最优路径，学习过程也就结束了。现在，我们还要求熵最大，就意味着神经网络需要去explore探索所有可能的最优路径，这可以产生以下多种优势：</p>
<ul>
<li>
<p>1）学到policy可以作为更复杂具体任务的初始化。因为通过最大熵，policy不仅仅学到一种解决任务的方法，而是所有all。因此这样的policy就更有利于去学习新的任务。比如我们一开始是学走，然后之后要学朝某一个特定方向走。</p>
</li>
<li>
<p>2）更强的exploration能力，这是显而易见的，能够更容易的在多模态reward （multimodal reward）下找到更好的模式。比如既要求机器人走的好，又要求机器人节约能源</p>
</li>
<li>
<p>3）更robust鲁棒，更强的generalization。因为要从不同的方式来探索各种最优的可能性，也因此面对干扰的时候能够更容易做出调整。（干扰会是神经网络学习过程中看到的一种state，既然已经探索到了，学到了就可以更好的做出反应，继续获取高reward）</p>
</li>
</ul>
<p>既然最大熵RL算法这么好，我们当然应该研究它了。而实际上，在之前的DRL算法<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.01783"target="_blank" rel="external nofollow noopener noreferrer">A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们其实已经用了一下最大熵：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在训练policy的时候，A3C加了entropy项，作为一个regularizer，让policy更随机。不过A3C这么做主要是为了更好做exploration，整体的训练目标依然只考虑reward。这和Soft Actor-Critic的设定还是不一样的，Soft Actor-Critic是真正最大熵DRL算法。</p>
<h2 id="3-maximum-entropy-reinforcement-learning的bellman方程">3 Maximum Entropy Reinforcement Learning的Bellman方程</h2>
<p>我们先回顾一下dynamic programming中Bellman backup equation，参考<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf"target="_blank" rel="external nofollow noopener noreferrer">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>$$q_\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})q_\pi(s^{\prime},a^{\prime})\tag{3}$$</p>
<p>那么对于最大熵（MaxEnt)的目标，其实可以把熵也作为reward的一部分，我们在计算q值时（记住q是累加reward的期望，传统rl的目标等价于让q最大），就需要计算每一个state的熵entropy (entropy的公式如下图所示）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>因此我们就可以得到Soft Bellman Backup equation (Entropy项)额外乘上 $\alpha$ 系数：</p>
<p>$$q_\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})(q_\pi(s^{\prime},a^{\prime})-\alpha\log(\pi(a^{\prime}|s^{\prime}))\quad(4)$$</p>
<p>Recall一下<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf"target="_blank" rel="external nofollow noopener noreferrer">Dynamic Programming Backup<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
对应Q值的公式是
<p>$$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q(s_{t+1},a_{t+1})]\tag{5}$$</p>
<p>根据公式（4），我们可以得到Soft Bellman Backup的 更新公式：</p>
<p>$$Q_{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]\tag{6}$$</p>
<p>上面公式（6）是直接使用dynamic programming，将entropy嵌入计算得到的结果。我们可以反过来先直接把entropy作为reward的一部分：</p>
<p>$$r_{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))\tag{7}$$</p>
<p>我们将（7）带入到公式（5）：</p>
<p>$$\begin{aligned}
{Q_{soft}(s_{t},a_{t})} &amp;=r(s_t,a_t)+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})]\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho,a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\gamma\alpha\mathbb{E}_{s_{t+1}\sim\rho}H(\pi(\cdot|s_{t+1}))\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho,a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})]+\gamma\mathbb{E}_{s_{t+1}\sim\rho}\mathbb{E}_{a_{t+1}\sim\pi}[-\alpha\log\pi(a_{t+1}|s_{t+1})]\\
&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho}[\mathbb{E}_{a_{t+1}\sim\pi}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]]\\&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1},a_{t+1}}[Q_{soft}(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))]\end{aligned}$$</p>
<p>可以得到一样的结果。</p>
<p>与此同时，我们知道:</p>
<p>$$Q(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\rho}[V(s_{t+1})]\tag{9}$$</p>
<p>因此，我们有：</p>
<p>$$V_{soft}(s_t)=\mathbb{E}_{a_t\sim\pi}[Q_{soft}(s_t,a_t)-\alpha\log\pi(a_t|s_t)]\tag{10}$$</p>
<p>至此我们理清楚了SAC paper原文中的公式(2)和(3)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>并且（7）的做法直接证明了Lemma 1 Soft Policy Evaluation (<strong>这个lemma为下一部分的soft policy iteration提供支撑</strong>）:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>但是，我们注意到上面的整个推导过程都是围绕maximum entropy，和soft 好像没有什么直接关系。所以，</p>
<p><strong>为什么称为soft？哪里soft了？以及为什么soft Q function能够实现maximum entropy？</strong></p>
<p>理解清楚这个问题是理解明白soft q-learning及sac的关键！</p>
<p>SAC这篇paper直接跳过了soft Q-function的定义问题，因此，要搞清楚上面的问题，我们从Soft Q-Learning的paper来寻找答案。</p>
<p>参考<a href="https://link.zhihu.com/?target=https%3A//bair.berkeley.edu/blog/2017/10/06/soft-q-learning/"target="_blank" rel="external nofollow noopener noreferrer">Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上面的曲线很明显的说明了stochastic policy的重要性，面对多模的（multimodal）的Q function，传统的RL只能收敛到一个选择（左图），而更优的办法是右图，让policy也直接符合Q的分布。这里，最直接的一种办法就是定义这样的energy-based policy：</p>
<p>\pi(a_t|s_t)\propto exp(-\mathcal{E}(s_t,a_t)) （11）</p>
<p>其中 \mathcal{E} 是能量函数，上面的形式就是Boltzmann Distribution <a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E5%88%86%E5%B8%83"target="_blank" rel="external nofollow noopener noreferrer">玻尔兹曼分布<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 。下图的 -f(x)=\mathcal{E}</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><a href="https://deepgenerativemodels.github.io/assets/slides/cs236_lecture13.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://deepgenerativemodels.github.io/assets/slides/cs236_lecture13.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>为了连接soft Q function，我们可以设定</p>
<p>$$\mathcal{E}(s_t,a_t)=-\frac{1}{\alpha}Q_{soft}(s_t,a_t)\tag{12}$$</p>
<p>因此，我们有</p>
<p>$$\pi(a_t|s_t)\propto exp(Q_{soft}(s_t,a_t))\tag{13}$$</p>
<p>这样的policy能够为每一个action赋值一个特定的概率符合Q值的分布，也就满足了stochastic policy的需求。</p>
<p>下面我们要<strong>发现(13)的形式正好就是最大熵RL的optimal policy最优策略的形式，而这实现了soft q function和maximum entropy的连接。</strong></p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>实际上我们理解Soft Q-Learning及Soft Actor Critic，要清楚上图三者的关系。在Soft Q-Learning那篇paper中，他是从Soft Value Function的定义出发去连接Energy-Based Policy 和Maximum Entropy Objective的关系。而在本blog中，我们从Maximum Entropy Objective出发，来连接其他两部分。</p>
<p>前面我们已经推导得到了公式（10），那么根据公式（10），我们可以直接推导得到policy的形式：</p>
<p>$$\begin{aligned}\pi(s_{t},a_{t})&amp;=\exp(\frac1\alpha(Q_{soft}(s_t,a_t)-V_{soft}(s_t)))\\&amp;&amp;\text{(14)}\\&amp;=\frac{\exp(\frac1\alpha Q_{soft}(s_t,a_t))}{\exp(\frac1\alpha V_{soft}(s_t))}\end{aligned}$$</p>
<p>（14）符合了（13）， $\frac{1}{\alpha}V_{soft}(s_t)$ 可以看做是对应的log partition function. 由此，就连接了Maximum Entropy Objective和Energy Based Policy的关系。</p>
<p>下面我们要连接Soft Value Function。从（14）的 $\frac{1}{\alpha}V_{soft}(s_t)$ 已经很明显了：</p>
<p>$$\exp(\frac1\alpha V_{soft}(s_t))=\int\exp(\frac1\alpha Q_{soft}(s_t,a))d\mathbf{a} (15)$$</p>
<p>因此，我们可以定义 $V_{soft}(s_t)$ :</p>
<p>$$V_{soft}(s_t)\triangleq\alpha\log\int\exp(\frac1\alpha Q_{soft}(s_t,a))d\mathbf{a}\tag{16}$$</p>
<p>这和soft 有什么关系呢？(16）其实是LogSumExp的积分形式，就是smooth maximum/soft maximum (软的最大）。参考<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/LogSumExp"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/LogSumExp<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>所以就可以定义</p>
<p>$$\mathrm{soft}\max_af(a):=\log\int\exp f(a)da\tag{17}$$</p>
<p>因此我们也就可以根据公式（9）定义soft的Q-function：</p>
<p>$$Q_{soft}(s_t,a_t)=\mathbb{E}\left[r_t+\gamma\text{ soft}\max_aQ(s_{t+1},a)\right]\text{(18)}$$</p>
<p>所以，为什么称为soft是从这里来的。</p>
<p>这里有一个常见的疑问就是这里的soft max和我们常见的softmax好像不一样啊。是的，我们在神经网络中常用的activation function softmax 实际上是soft argmax，根据一堆logits找到对应的软的最大值对应的index。具体参看：<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Softmax_function"target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/Softmax_function<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>上面的推导还只是面向policy的value和Q，我们下面要说明optimal policy也必然是energy-based policy的形式。</p>
<p>这一部分的证明依靠 Policy improvement theorem：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>具体证明过程见soft q-learning原文的A.1。</p>
<p>有了Theorem 4，</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>我们就可以看到optimal policy必然是energy based policy，也因此，我们有了soft q learning paper中最开始的定义：</p>
<p>$$\pi_{MaxEnt}^<em>(a_t|s_t)=\exp(\frac{1}{\alpha}(Q_{soft}^</em>(s_t,a_t)-V_{soft}^*(s_t)))\text{(19)}$$</p>
<h2 id="4-policy-iteration">4 Policy Iteration</h2>
<p>理清楚了上面的基本定义和联系，我们就可以研究怎么更新policy了，也就是policy iteration。</p>
<p>回顾一下一般的<a href="https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf"target="_blank" rel="external nofollow noopener noreferrer">Policy Iteration<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在两步中进行循环迭代（我们直接使用Q值来说明）：</p>
<ol>
<li>Policy evaluation：固定policy，使用Bellman方程更新Q值直到收敛：</li>
</ol>
<p>$$Q_\pi(s,a)=r(s,a)+\lambda\mathbb{E}_{s^{\prime},a^{\prime}}Q_\pi(s^{\prime},a^{\prime})\tag{20}$$</p>
<ol start="2">
<li>Policy improvement: 更新policy：</li>
</ol>
<p>$$\pi^{\prime}(s)=\arg\max_aQ_\pi(s,a)\tag{21}$$</p>
<p>基于同样的方法，我们有Soft Policy Iteration：</p>
<ol>
<li>Soft policy evaluation:固定policy，使用Bellman方程更新Q值直到收敛:</li>
</ol>
<p>$$\begin{aligned}&amp;Q_{soft}^\pi(s_t,a_t)=r(s_t,a_t)+\lambda\mathbb{E}_{s_{t+1},a_{t+1}}\left[Q_{soft}^\pi(s_{t+1},a_{t+1})-\alpha\log(\pi(a_{t+1}|s_{t+1}))\right]\tag{22}\end{aligned}$$</p>
<ol start="2">
<li>Soft policy improvement: 更新policy：</li>
</ol>
<p>$$\pi^{\prime}=\arg\min_{\pi_k\in\Pi}D_{KL}(\pi_k(\cdot|s_t)||\frac{\exp(\frac{1}{\alpha}Q_{soft}^{\pi}(s_t,\cdot))}{Z_{soft}^{\pi}(s_t)}) \tag{23}$$</p>
<p>(22)基于上一部分说的Lemma 1 Soft Policy Evaluation, 可收敛。</p>
<p>(23)则基于上一部分的Theorem 4 Policy Improvement Theorem。只是这里的做法不是直接赋值，而是通过KL divergence来趋近 $\exp(Q^{\pi}_{soft}(s_t,\cdot))$ 。在SAC的paper原文中，我们可以看到这么做的原因是为了限制policy在一定范围的policies $\Pi$ 中从而tractable，policy的分布可以是高斯分布。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>同样的，作者也专门证明了采用KL divergence的方法一样能够保证policy improvement，也就是Lemma 2：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>最后，就是证明上面的Soft Policy Iteration过程能保证policy收敛到最优，即Theorem 1：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>由此，基本的理论建设也就结束了，下面进入Soft Actor-Critic的算法设计。</p>
<h2 id="5-soft-actor-critic">5 Soft Actor-Critic</h2>
<p>SAC算法的构建首先是神经网络化，我们用神经网络来表示Q和Policy： $Q_{\theta}(s_t,a_t)$ 和 $\pi_{\phi}(a_t|s_t)$ 。Q网络比较简单，几层的MLP最后输出一个单值表示Q就可以了，Policy网络需要输出一个分布，一般是输出一个Gaussian 包含mean和covariance。下面就是构建神经网络的更新公式。</p>
<p>对于Q网络的更新，我们根据（10）可以得到：</p>
<p>$$\begin{aligned}
J_{Q}(\theta)&amp; =\mathbb{E}_{(s_t,a_t,s_{t+1})\sim\mathcal{D}}[\frac{1}{2}(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma V_{\bar{\theta}}(s_{t+1})))^2] \\
&amp;=\mathbb{E}_{(s_t,a_t,s_{t+1})\sim\mathcal{D},a_{t+1}\sim\pi_\phi}[\frac12(Q_\theta(s_t,a_t)-(r(s_t,a_t)+\gamma(Q_{\bar{\theta}}(s_{t+1},a_{t+1})-\alpha\log(\pi_\phi(a_{t+1}|s_{t+1})))))^2] \tag{24}
\end{aligned}$$</p>
<p>这里和DDPG一样，构造了一个target soft Q 网络带参数 $\overline{\theta}$ ，这个参数通过exponentially moving average Q网络的参数 $\theta$ 得到。(ps:在第一个版本的SAC中，他们单独定义了V网络进行更新，说是更稳定，到新版的SAC中，由于自动更新temperature $\alpha$ 就直接使用Q网络更新）</p>
<p>对于Policy 网络参数的更新，就是最小化KL divergence：</p>
<p>$$\begin{aligned}
J_{\pi}(\phi)&amp; =D_{\mathrm{KL}}\left(\pi_\phi(.\left|s_t\right)|\exp(\frac1\alpha Q_\theta(s_t,.)-\log Z(s_t))\right) \\
&amp;=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}\Big[\log\big(\frac{\pi_\phi(a_t|s_t)}{\exp(\frac{1}{\alpha}Q_\theta(s_t,a_t)-\log Z(s_t))}\big)\Big] \\
&amp;=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}[\log\pi_\phi(a_t|s_t)-\frac1\alpha Q_\theta(s_t,a_t)+\log Z(s_t)] \tag{25}
\end{aligned}$$</p>
<p>这里的action我们采用reparameterization trick来得到，即</p>
<p>$$a_t=f_\phi(\varepsilon_t;s_t)=f_\phi^\mu(s_t)+\varepsilon_t\odot f_\phi^\sigma(s_t)\tag{26}$$</p>
<p>f函数输出平均值和方差，然后 $\varepsilon$ 是noise，从标准正态分布采样。使用这个trick，整个过程就是完全可微的(loss 乘以 $\alpha$ 并去掉不影响梯度的常量log partition function Z(s_t)) ：</p>
<p>$$J_\pi(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\varepsilon\sim\mathcal{N}}[\alpha\log\pi_\phi(f_\phi(\varepsilon_t;s_t)|s_t)-Q_\theta(s_t,f_\phi(\varepsilon_t;s_t))] \tag{27}$$</p>
<p>这样基本的Soft Actor-Critic的更新方法也就得到了。</p>
<h2 id="6-temperature-hyperparameter-auto-adjustment">6 Temperature Hyperparameter Auto-Adjustment</h2>
<p>前面的SAC中，我们只是人为给定一个固定的temperature $\alpha$ 作为entropy的权重，但实际上由于reward的不断变化，采用固定的temperature并不合理，会让整个训练不稳定，因此，有必要能够自动调节这个temperature。当policy探索到新的区域时，最优的action还不清楚，应该调整temperature $\alpha$ 去探索更多的空间。当某一个区域已经探索得差不多，最优的action基本确定了，那么这个temperature就可以减小。</p>
<p>这里，SAC的作者构造了一个带约束的优化问题，让平均的entropy权重是有限制的，但是在不同的state下entropy的权重是可变的，即</p>
<p>$$\max_{\pi_0,\ldots,\pi_T}\mathbb{E}\bigg[\sum_{t=0}^Tr(s_t,a_t)\bigg]\mathrm{s.t.~}\forall t, \mathcal{H}(\pi_t)\geq\mathcal{H}_0\tag{28}$$</p>
<p>对于这部分内容，<a href="https://link.zhihu.com/?target=https%3A//lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html%23sac"target="_blank" rel="external nofollow noopener noreferrer">Policy Gradient Algorithms<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 这个openai小姐姐的blog介绍得极其清楚，大家可以参考，最后得到temperature的loss：</p>
<p>$$J(\alpha)=\mathbb{E}_{a_t\sim\pi_t}[-\alpha\log\pi_t(a_t\mid\pi_t)-\alpha\mathcal{H}_0]\tag{29}$$</p>
<p>由此，我们可以得到完整的Soft Actor-Critic算法：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>为了更快速稳定的训练，作者引入了两个Q网络，然后每次选择Q值小的一个作为target Q值。更新Q，Policy及 \alpha 使用上文的（24）（27）（29）三个公式。</p>
<h2 id="7-神经网络结构">7 神经网络结构</h2>
<p>虽然上面把算法流程确定了，但是如何构造policy的神经网络还是比较复杂的。下图是带V网络的神经网络结构图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><a href="https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html"target="_blank" rel="external nofollow noopener noreferrer">https://nervanasystems.github.io/coach/components/agents/policy_optimization/sac.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>我们主要来探究一下Policy网络的设计。</p>
<p>见上图右上角的Policy网络，前面的input embedder和Middleware不用说，就是几层的MLP。然后，接下来神经网络分成两个分支，分别输出平均值mean $\mu$ 和log 标准差 log std 。然后使用exp得到std。</p>
<p>$$\pi_\phi(s_t) = \mu_t,\log \sigma_t \tag{30}$$</p>
<p>$$\sigma_t = \exp(\log \sigma_t)$$</p>
<p>正常输出这样的高斯分布作为action 的分布distribution是OK的，但是在实际中，这个action需要限定在一定范围内。因此，这里作者使用了squashing function tanh，将action限制在（-1,1）之间，即</p>
<p>$$\mathbf{u}_t =\mu_t + \varepsilon_t \odot \sigma_t $$</p>
<p>$$a_t = \tanh (\mathbf{u}) \tag{31}$$</p>
<p>这里和上文的公式（26）对应，多了一个tanh。</p>
<p>那么这会导致分布的变化，从而影响log likelihood的计算，而这是我们计算SAC的loss必须的。作者在paper中给出了计算方法如下：</p>
<p>$$\log \pi(a|s)=\log \mu(\mathbf{u}|s)-\sum_{i=1}^{D}{\log(1-\tanh^2(u_i))} \tag{32}$$</p>
<p>其中 u_i 是 $\mathbf{u}$ 的第i个元素。这里的 $\mu(\mathbf{u}|s)$ 是没有加限制时的likelihood function也就是高斯分布的likelihood function似然函数。高斯分布的log likelihood直接使用pytorch的<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/_modules/torch/distributions/normal.html"target="_blank" rel="external nofollow noopener noreferrer">Normal<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> class就可以获得。</p>
<h2 id="8-其他细节">8 其他细节</h2>
<p>1）SAC里的target entropy 设计为</p>
<p>$$\mathcal{H}_0 = -\dim (\mathcal{A}) \tag{33}$$</p>
<p>即-动作数量。</p>
<p>2）SAC paper里完全没有说明的训练时的episode设置。SAC设置为每一个episode采样1000次然后训练1000次。</p>
<p>3）在代码中SAC使用 log alpha作为更新的参数，而不是直接使用alpha如公式（25），这和输出log std是一样的，使用log有很大的正负范围，更方便网络输出。否则alpha或者std都是正值。</p>
<p>4）SAC有一个很大的问题，它的policy的目的是趋近于玻尔兹曼分布，但是实际实现的时候，为了能够tractable，选择了输出一个高斯，也就是让高斯趋近于玻尔兹曼分布。这意味着SAC本质上还是unimodal的算法，而不是soft q-learning的multi-modal。这使得SAC的创新性打了很大的折扣。但是算法效果确实还是不错的。</p>
<h2 id="9-小结">9 小结</h2>
<p>本文从理论到具体实现层面剖析了Soft Actor-Critic这一目前极强的DRL算法，基本上理解了本文的分析，对于代码的实现也就可以了然一胸了。</p>
<p>由于本人水平有限，前面的理论分析恐有错误，望批评指正！</p>
<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/70360272"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/70360272<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>一文详解 ChatGPT RLHF 背后的 PPO 强化学习训练</title><link>https://jianye0428.github.io/posts/chatgpt_rlhf/</link><pubDate>Sat, 04 May 2024 17:00:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/chatgpt_rlhf/</guid><description><![CDATA[<h2 id="0-引言">0. 引言</h2>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>最近火出圈的🚀 ChatGPT 中 RLHF 主要采用了就是 PPO 进行强化学习训练</p>
<blockquote>
<p>主要运用在微调阶段（微调整个 10B～100B+ 参数的成本其实也非常高 ）使用<strong>策略梯度</strong>强化学习 (Policy Gradient RL) 算法、近端策略优化 (PPO) 微调初始 LM 的部分或全部参数。</p>
</blockquote>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<blockquote>
<p>以下主要参考台大李宏毅的推导过程</p>
</blockquote>
<h2 id="01-vanilla-policy-gradient">01. Vanilla policy gradient</h2>
<ul>
<li>动作/环境/奖励之间的关系：</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>轨迹可表示为集合</p>
<p>$$\begin{aligned}p_{\theta}(\tau)&amp;=p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_1|s_1)p(s_3|s_2,a_2)\ldots\\&amp;=p(s_1)\prod_{t=1}^Tp_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\end{aligned}$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>一个轨迹的奖励总和为：</p>
<p>$$R(\tau)=\sum_{t=1}^Tr_t$$</p>
<p>则奖励的期望为：</p>
<p>$$\bar{R}_\theta=\sum_\tau R(\tau)p_\theta(\tau)=E_{\tau\sim p_\theta(\tau)}[R(\tau)]$$</p>
<p>将 $R(\tau)$ 看成常量，对其求微分：</p>
<p>$$\begin{aligned}
\nabla\bar{R}_{\theta}&amp; =\sum_{\tau}R(\tau)\nabla p_{\theta}(\tau) \\
&amp;=\sum_{\tau}R(\tau)p_{\theta}(\tau)\frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\
&amp;=\sum_{\tau}R(\tau)p_{\theta}(\tau)\nabla\log p_{\theta}(\tau)\quad\nabla f(x)=f(x)\nabla\log f(x) \\
&amp;=E_{\tau\sim p_{\theta}(\tau)}[R(\tau)\nabla\log p_{\theta}(\tau)]&amp; \left(2\right) \\
&amp;\approx\frac1N\sum_{n=1}^{N}R(\tau^{n})\nabla\log p_{\theta}(\tau^{n}) \\
&amp;=\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}R(\tau^n)\nabla\log p_\theta(a_t^n|s_t^n)
\end{aligned}$$</p>
<p>策略网络梯度更新：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>可以看成一个分类问题（游戏中通过键盘输入来互动，分类类别为所有可操作的键位）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li>理想情况下， 并不一直为正数，增加一个 baseline:</li>
</ul>
<p>$$\nabla\bar{R}_{\theta}=\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{{T_{n}}}(R(\tau^{n})-b)\nabla\log p_{\theta}(a_{t}^{n}|s_{t}^{n})b\approx E[R(\tau)]$$</p>
<blockquote>
<p>在电子游戏中，奖励值常常为正（通常为游戏分数）。这时需要增加一个偏置来保证同时有正样本和负样本</p>
</blockquote>
<ul>
<li>分配合适的学分</li>
</ul>
<p>一个高分的游戏轨迹中也可能存在错误的动作，同样的，一个低分的游戏轨迹也可能存在正确的动作，而上文中的计算将最后的奖励值（最后的游戏分数）都一视同仁视为该游戏轨迹每个动作的学分。</p>
<p>为了更准确地描述每个动作所得到的学分，将一个动作执行后对应的学分为后续的所有奖励值的总和</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>$$\begin{aligned}
\nabla\bar{R}_\theta&amp; =\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}(R(\tau^n)-b)\nabla\log p_\theta(a_t^n|s_t^n) \Downarrow\nabla\bar{R}_\theta \\
&amp;= \frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}(\sum_{t^{\prime}=t}^{T_n}r_{t^{\prime}}^n-b)\nabla\log p_\theta(a_t^n|s_t^n)
\end{aligned}$$</p>
<p>当某个动作执行以后，其对后续的奖励分数的影响在慢慢减少，再增加一个衰减因子：</p>
<p>$$\begin{aligned}
\nabla\bar{R}_\theta&amp; =\frac1N\sum_{n=1}^N\sum_{t=1}^{T_n}(\sum_{t^{\prime}=t}^{T_n}r_{t^{\prime}}^n)\nabla\log p_\theta(a_t^n|s_t^n)\Downarrow\nabla\bar{R}_\theta \\
&amp; = \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}(\sum_{t^{\prime}=t}^{T_{n}}\gamma^{t^{\prime}-t}r_{t^{\prime}}^{n}-b)\nabla\log p_{\theta}(a_{t}^{n}|s_{t}^{n}),\gamma&lt;1
\end{aligned}$$</p>
<h2 id="02-从on-policy到off-policy">02. 从on-policy到off-policy</h2>
<p>两者区别:</p>
<ul>
<li>On-policy: 学习到的 agent 和与环境交互的 agent 是相同的，每一次梯度更新都需要重新采样</li>
<li>Off-policy: 学习到的 agent 和与环境交互的 agent 是不同的，每次梯度更新不需要重新采样</li>
</ul>
<p>重新看看 的表达式：
$$\nabla\bar{R}_\theta=E_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)]$$</p>
<ul>
<li>使用策略网络 收集数据。当 更新后，则需要重新收集训练样本</li>
<li>目标：使用相同的样本（通过 采样）训练 。其中 为固定的，因此我们可以重复使用其样本数据</li>
</ul>
<h3 id="21-重要性采样importance-sampling">2.1 重要性采样（Importance Sampling）</h3>
<p>考虑一个场景，假如正在尝试计算函数 $f(x)$ 的期望值，其中 $x \sim f(x)$ 服从某种分布。则对 $E(f(x))$ 有以下估计：</p>
<p>$$E_{x\sim p}[f(x)]=\int f(x)p(x)dx\approx\frac{1}{n}\sum_{i}f(x_{i})$$</p>
<p>蒙特卡洛抽样方法是简单地从分布 $p(x)$ 中抽出 ，然后取所有样本的平均值来得到期望值的估计。那么问题来了，如果  $p(x)$  非常难取样怎么办？是否能够根据一些已知的、容易抽样的分布来估计期望值？</p>
<p>答案是肯定的。公式的一个简单转换就可以做到</p>
<p>$$E_{x\sim p}[f(x)]=\int f(x)p(x)dx=\int f(x)\frac{p(x)}{q(x)}q(x)dx=E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]$$</p>
<p>其中$x$从分布$q(x)$中采样，$q(x)$不应为 0。通过这种方式，估计期望能够从另一个分布$q(x)$中采样，$p(x)/q(x)$是称为采样率或采样权重，它作为校正权重以抵消来自不同分布的概率采样。</p>
<ul>
<li>重要性采样的缺陷</li>
</ul>
<p>虽然重要性采样保证了期望的一致，但是这里来计算一下方差是否一致</p>
<p>方差的计算：</p>
<p>$$Var[X]=E[X^2]-(E[X])^2$$</p>
<p>分别计算方差：</p>
<p>$$\begin{aligned}Var_{x\sim p}[f(x)]&amp;=E_{x\sim p}[f(x)^2]-(E_{x\sim p}[f(x)])^2\\Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}]&amp;=E_{x\sim q}[(f(x)\frac{p(x)}{q(x)})^2]-(E_{x\sim q}[f(x)\frac{p(x)}{q(x)}])^2\\&amp;=E_{x\sim p}[f(x)^2\frac{p(x)}{q(x)}]-(E_{x\sim p}[f(x)])^2\end{aligned}$$</p>
<p>可以发现两者虽然期望相等但方差并不一致</p>
<h3 id="22-从-on-policy-到-off-policy">2.2 从 on-policy 到 off-policy</h3>
<p>我们使用重要性采样将 on-policy 调整为 off-policy</p>
<p>$$\nabla\bar{R}_\theta=E_{\tau\sim p_{\theta^{\prime}}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta^{\prime}}(\tau)}R(\tau)\nabla\log p_\theta(\tau)]$$</p>
<ul>
<li>从 $\theta&rsquo;$ 采样得到数据集</li>
<li>使用该 数据集多次训练 $\theta$</li>
</ul>
<p>梯度更新过程：</p>
<p>$$\begin{aligned}
&amp;=E_{(s_t,a_t)\sim\pi_\theta}[A^\theta(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \\
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(s_t,a_t)}{p_{\theta^{\prime}}(s_t,a_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \\
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{\prime}}(a_t|s_t)}\frac{p_\theta(s_t)}{p_{\theta^{\prime}}(s_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)]&amp; \text{(4)} \\
&amp;=E_{(s_t,a_t)\sim\pi_{\theta^{\prime}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{\prime}}(a_t|s_t)}A^{\theta^{\prime}}(s_t,a_t)\nabla\log p_\theta(a_t^n|s_t^n)]
\end{aligned}$$</p>
<ul>
<li>其中 $A^\theta(s_t,a_t)$ 指的是 advantage 函数,其计算方式为加上衰减机制后的奖励值并减去基线。</li>
<li>由于 $\frac{p_\theta(s_t)}{p_{\theta&rsquo;}(s_t)}$ 的值难以计算，将其设置为 1，简化计算</li>
</ul>
<p>目标函数可以表示为：</p>
<p>由于 $\nabla f(x)=f(x)\nabla\log f(x)$ 再结合不定积分，目标函数可以表示为:</p>
<p>$$J^{\theta&rsquo;}(\theta)=E_{(s_t,a_t)\sim\pi_{\theta&rsquo;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&rsquo;}(a_t|s_t)}A^{\theta&rsquo;}(s_t,a_t)]$$</p>
<h2 id="03-ppotrpo">03. PPO/TRPO</h2>
<p>为了消除重要性采样的缺陷的影响，以下为两种方式</p>
<ul>
<li>PPO（Proximal Policy Optimization）
<ul>
<li>初始化策 略网络参数</li>
<li>在每次迭代过程中:</li>
<li>目标函数:</li>
<li>使用 与环境互动以收集 ，并计算出 advantage 值</li>
<li>更新 优化</li>
<li>算法:</li>
</ul>
</li>
</ul>
<p>$$\begin{aligned}
PPO algorithm: \\
J_{PPO}^{\theta^k}(\theta) &amp; = J^{\theta^k}(\theta)-\beta KL(\theta,\theta^k)J^{\theta^k}(\theta) \\
&amp; = E_{(s_{t},a_{t})\sim\pi_{\theta^{k}}}[\frac{p_{\theta}(a_{t}|s_{t})}{p_{\theta^{k}}(a_{t}|s_{t})}A^{\theta^{k}}(s_{t},a_{t})] \\
&amp; \approx \sum_{(s_{t},a_{t})}\frac{p_{\theta}(a_{t}|s_{t})}{p_{\theta^{k}}(a_{t}|s_{t})}A^{\theta^{k}}(s_{t},a_{t})
\end{aligned}$$</p>
<p>自适应 KL 惩罚：如果 $KL(\theta,\theta^k)&gt;KL_{\max}$ ,增大 $\beta$; 如果 $KL(\theta,\theta^k) &lt;KL_{\min}$,减小 $\beta$。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li>TRPO（Trust Region Policy Optimizatio）</li>
</ul>
<p>$$J_{TRPO}^{\theta&rsquo;}(\theta)=E_{(s_t,a_t)\sim\pi_{\theta&rsquo;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&rsquo;}(a_t|s_t)}A^{\theta&rsquo;}(s_t,a_t)]KL(\theta,\theta&rsquo;)&lt;\delta $$</p>
<p>TRPO 和 PPO 在各个测试上性能差不多。但相比 PPO ，TRPO 计算要更复杂</p>
<p><strong>参考文献</strong>:</p>
<p>[1] <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html"target="_blank" rel="external nofollow noopener noreferrer">https://spinningup.openai.com/en/latest/algorithms/ppo.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[2] <a href="https://openai.com/research/openai-baselines-ppo"target="_blank" rel="external nofollow noopener noreferrer">https://openai.com/research/openai-baselines-ppo<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[3] <a href="https://huggingface.co/blog/deep-rl-ppo"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/blog/deep-rl-ppo<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[4] <a href="https://huggingface.co/blog/rlhf"target="_blank" rel="external nofollow noopener noreferrer">https://huggingface.co/blog/rlhf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[5] <a href="https://mp.weixin.qq.com/s/zhkNDNDEJV3BEdcgeuHkOA"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/zhkNDNDEJV3BEdcgeuHkOA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>LLM预训练之RLHF（一）：RLHF及其变种</title><link>https://jianye0428.github.io/posts/pretrain_rlhf_one/</link><pubDate>Sat, 04 May 2024 14:23:08 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/pretrain_rlhf_one/</guid><description><![CDATA[<h2 id="0-引言">0. 引言</h2>
<p>在ChatGPT引领的大型语言模型时代，国内外的大模型呈现爆发式发展，尤其是以年初的LLaMA模型为首的开源大模型和最近百川智能的baichuan模型，但无一例外，都使用了「基于人类反馈的强化学习」（RLHF）来提升语言模型的性能，并在模型重注入了人类的偏好，以提高模型的有用性和安全性。不过RLHF也早已更新换代，我们以如下目录进行详细讲述RLHF及其变种：</p>
<ul>
<li>LLM的经典预训练Pipeline</li>
<li>Llama 2中的RLHF</li>
<li>RLHF替代方案</li>
</ul>
<h2 id="一llm的经典预训练pipeline">一、LLM的经典预训练Pipeline</h2>
<p>​  目前基于Transformer decoder的LLM，比如ChatGPT、LLaMA、baichuan等，通常都会有基于预训练的base模型和在base模型至少使用RLHF微调的Chat模型，Chat模型的训练一般都包括如下三个步骤：预训练，有监督微调和对齐。</p>
<p>​  在<strong>预训练</strong>阶段，模型会从大量无标注文本数据集中学习通用知识，然后使用「<strong>有监督微调」（SFT）<strong>优化模型以更好地遵守特定指令，最后使用</strong>对齐</strong>技术使LLM可以更有用且更安全地响应用户提示。</p>
<h3 id="11-预训练pre-training">1.1 预训练（Pre-training）</h3>
<p>预训练阶段通常需要包含数十亿到数万亿个token的庞大文本语料库，但训练目标是<strong>模型需要根据提供的文本来预测「下一个单词」</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>1.2 有监督微调（Supervised Finetuning）</strong></p>
<p>​SFT的训练过程类似Pre-training阶段，也是预测「下一个单词」，但是<strong>需要人工标注的指令数据集</strong>，其中模型的输入是一个指令（根据任务的不同，也可能包含一段输入文本），输出为模型的预期回复内容。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>数据形式类似于：</p>
<blockquote>
<p>Instruction: &ldquo;Write a limerick about a pelican.&rdquo;</p>
<p>指令：“写一首关于鹈鹕的打油诗。“</p>
<p>Output: &ldquo;There once was a pelican so fine&hellip;&rdquo;</p>
<p>输出：“从前有一只鹈鹕很好&hellip;“</p>
</blockquote>
<p>模型会把“Write a limerick about a pelican”作为输入，逐个token进行预测，输出“There once was a pelican so fine&hellip;”</p>
<p>虽然两个阶段都采用类似的训练目标，但有监督微调数据集通常比预训练数据小得多，指令数据集需要人类（或其他高质量的LLM）提供标注结果，所以无法大规模应用。</p>
<p><strong>1.3 对齐（Alignment）</strong></p>
<p>第三阶段依然是微调，不过其主要目标在于将语言模型与人类的偏好、价值观进行对齐，这也是RLHF机制发挥的地方。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="二reinforcement-learning-with-human-feedback-rlhf">二、Reinforcement Learning with Human Feedback (RLHF)</h2>
<p>上节，我们讨论了现代LLM的三个训练过程；本小节，我们重点讨论「上述两个微调阶段」（Supervised Tinetuning和Alignment）中使用的RLHF技术。</p>
<p>RLHF主要包括三步：</p>
<ol>
<li>在预训练好的模型上进行「有监督微调」（SFT）；</li>
<li>在有监督微调模型基础上创建一个reward model（RM）模型；</li>
<li>基于RM模型使用PPO算法微调SFT模型；</li>
</ol>
<h3 id="21-在预训练好的模型上进行有监督微调">2.1 在预训练好的模型上进行有监督微调**</h3>
<p>先收集一个Prompts集合，并要求标注人员写出高质量的回复，然后使用该数据集以监督的方式微调预训练的基础模型。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>​该步骤与上小节的Supervised Finetuning类似，但这是RLHF不可或缺的一个步骤。</p>
<h3 id="22-在有监督微调模型基础上创建一个rm模型">2.2 在有监督微调模型基础上创建一个RM模型</h3>
<p>对于每个Prompt，要求有监督微调后的LLM生成四到九个回复，再由标注人员根据个人偏好对所有回复进行排序。虽然排序过程很耗时，但工作量还是比第一步的有监督数据集构建要少一些。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在处理排序数据时，使用了一个奖励模型RM，RM来自RLHF第一步的「有监督微调语言模型」（SFT），SFT的输出通过一个回归层（单个输出节点）转换为奖励分数，即可称为<strong>RM模型</strong>。</p>
<h3 id="23-基于rm模型使用ppo算法微调sft模型">2.3 基于RM模型使用PPO算法微调SFT模型</h3>
<p>基于RM模型使用proximal policy optimization (PPO)算法微调SFT模型</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>PPO的具体技术细节可以参考InstructGPT或下面的论文列表。</p>
<ol>
<li>Asynchronous Methods for Deep Reinforcement Learning (2016) ，https://arxiv.org/abs/1602.01783</li>
<li>Proximal Policy Optimization Algorithms (2017)，https://arxiv.org/abs/1707.06347</li>
<li>Fine-Tuning Language Models from Human Preferences (2020)，https://arxiv.org/abs/1909.08593</li>
<li>Learning to Summarize from Human Feedback (2022) ，https://arxiv.org/abs/2009.01325</li>
</ol>
<h2 id="三llama-2的rlhf">三、LLaMA 2的RLHF**</h2>
<p>Meta AI在创建Llama-2-chat模型时也使用了RLHF技术，不过与ChatGPT相比还是有些细微区别。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>简单来说，Llama-2-chat在第一步RLHF微调上使用相同的指令数据，但在第二步使用了两个奖励模型；通过多个阶段的不断进化，奖励模型也会根据Llama-2-chat模型出现的错误进行更新；并且增加了拒绝采样（rejection sampling）步骤。</p>
<h3 id="31-margin-loss">3.1 Margin Loss</h3>
<p>​在标准InstructGPT中使用的RLHF PPO方法，研究人员需要收集同一个提示下的4-9个模型输出并进行排序，比如四个回复的排序结果为A&lt;C&lt; D&lt;B，那么就可以得到六个对比结果：A &lt; C，A &lt; D ，A &lt; B，C &lt; D，C &lt; B，D &lt; B。</p>
<p>​Llama 2的数据集也采用类似的方式，不过标注人员每次只能看到两个（而非4-9个）回复并进行对比，但新增了一个边际（margin）标签，对比结果可以为「显著更好」（significantly better）和「好的不明显」（negligibly better）。</p>
<p>在排序训练时中，Llama 2相比InstructGPT增加了边际损失：</p>
<p>$$\mathcal{L}<em>{\mathrm{ranking}}=-\log\left(\sigma\left(r</em>\theta\left(x,y_c\right)-r_\theta\left(x,y_r\right)-m(r)\right)\right)$$</p>
<p>其中，$r_θ(x，y)$是提示x和生成的回复y的标量分数输出; θ为模型权重; σ是将层输出转换为范围从0到1的分数的逻辑S形函数; $y_c$是由标注人员选择的更优回复; $y_r$是较差的回复。$m(r)$可以调节两个回复之间的差值，如果对比结果为「显著更好」，则会增加梯度值，加快更新速度。</p>
<h3 id="32-两个rm模型">3.2 两个RM模型</h3>
<p>​Llama 2中的两个奖励模型分别侧重「有用性」（helpfulness）和「安全性」（safety），用于模型优化的最终奖励函数会将两个分数进行线性组合。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="33-拒绝采样">3.3 拒绝采样</h3>
<p>​Llama 2的作者使用了一个训练流水线，<strong>同时使用PPO和拒绝采样算法</strong>，迭代地产生多个RLHF模型（从RLHF-V1到RLHF-V5），模型在拒绝采样时会得到K个输出，并使用最高奖励的输出更新梯度，而PPO每次只基于单样本进行更新。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在监督微调的初始阶段之后，模型只使用拒绝采样进行训练，然后再结合拒绝采样和PPO。</p>
<p>从实验结果来看，RLHF微调模型在无害性和有用性上都得到了改善，并且在最后阶段RLHF-v5使用PPO算法的性能最好。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h2 id="四rlhf的替代方案">四、RLHF的替代方案</h2>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>RLHF在InstructGPT和Llama 2论文中被证明是有效的，但是RLHF的过程是比较复杂的，下面将介绍一下最近RLHF的替代方案：</p>
<h3 id="41-constitutional-ai-harmlessness-from-ai-feedback-dec-2022-httpsarxivorgabs221208073">4.1 Constitutional AI: Harmlessness from AI Feedback (Dec 2022, <a href="https://arxiv.org/abs/2212.08073"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2212.08073<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>研究人员提出了一种 <strong><font color=red>基于人类提供的规则列表的自我训练机制</font></strong>。与前面提到的InstructGPT论文类似，也使用了强化学习方法。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>上图中的「红队」（Red Team）指的是测试目标系统的防御能力，即外部或内部专家模拟潜在对手的过程，通过模仿现实世界攻击者的战术、技术和程序来挑战、测试并最终改进系统。</p>
<h3 id="42-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-feb-2023-httpsarxivorgabs230205206">4.2 The Wisdom of Hindsight Makes Language Models Better Instruction Followers (Feb 2023, <a href="https://arxiv.org/abs/2302.05206"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2302.05206<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>研究人员提出了一种**<font color=red>基于重新标记的监督微调方法HIR</font>**，该方法在12个BigBench任务上优于RLHF。</p>
<p>​HIR是如何工作的？简而言之，HIR方法包括两个步骤，即<strong>采样</strong>和<strong>训练</strong>。在采样步骤中，Prompt和指令输入给LLM来获取答案，根据对齐得分，在训练阶段适当的地方重新标注指令；然后，重新标记的指令和原始的Prompt用于微调LLM。使用这种重新标记的方法，研究人员有效地将失败案例（LLM创建的输出与原始指令不匹配的案例）转化为有用的训练数据，用于监督学习。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="43-direct-preference-optimization-your-language-model-is-secretly-a-reward-model-httpsarxivorgabs230518290-may-2023">4.3 Direct Preference Optimization: Your Language Model is Secretly a Reward Model (<a href="https://arxiv.org/abs/2305.18290"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2305.18290<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, May 2023)</h3>
<p><strong><font color=red>直接偏好优化（DPO）是具有PPO的RLHF的替代方案</font></strong>，其中研究人员表明，在RLHF中拟合奖励模型的交叉熵损失可以直接用于微调LLM。根据他们的基准，使用DPO更有效，而且在响应质量方面通常也优于RLHF/PPO。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="44-reinforced-self-training-rest-for-language-modeling-aug-2023-httpsarxivorgabs230808998">4.4 Reinforced Self-Training (ReST) for Language Modeling (Aug 2023, <a href="https://arxiv.org/abs/2308.08998"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2308.08998<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>ReST是人类反馈强化学习（RLHF）的一种替代方案，它<strong>使LLM与人类偏好保持一致</strong>。 <strong><font color=red>ReST使用采样方法创建改进的数据集</font></strong>，在质量越来越高的子集上迭代训练，以完善其奖励函数。根据作者的说法，与标准的在线RLHF方法（如具有近端策略优化的RLHF，PPO）相比，ReST通过离线生成训练数据集实现了更高的效率，但缺少与InstructGPT或Llama 2中使用的标准RLHF PPO方法的全面比较。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<h3 id="45-rlaif-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback-sep-2023-httpsarxivorgabs230900267">4.5 RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (Sep 2023, <a href="https://arxiv.org/abs/2309.00267"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/2309.00267<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</h3>
<p>最近的人工智能反馈强化学习（RLAIF）研究表明，RLHF中奖励模型训练的评级不一定必须由人类提供，而是可以由LLM生成（此处：PaLM 2）。标注人员在一半的案例中更喜欢RLAIF模型，也就意味着两个模型的差距并不大，RLHF和RLAIF都大大优于纯通过监督指令微调训练的模型。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这项研究的结果非常有用和有趣，因为它基本上意味着我们可能能够使基于RLHF的训练更加高效和容易。然而，这些RLAIF模型在专注于信息内容的安全性和真实性的定性研究中的表现还有待观察，而人类偏好研究仅部分捕捉到了这一点。</p>
<p><strong>参考文献：</strong></p>
<p>[1] <a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives"target="_blank" rel="external nofollow noopener noreferrer">https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><br>
[2] <a href="https://mp.weixin.qq.com/s/3Ff6C5zT7fXggQ1FwxvWAQ"target="_blank" rel="external nofollow noopener noreferrer">https://mp.weixin.qq.com/s/3Ff6C5zT7fXggQ1FwxvWAQ<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>大模型学习笔记 | GPT 系列</title><link>https://jianye0428.github.io/posts/survey/</link><pubDate>Fri, 03 May 2024 16:33:37 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/survey/</guid><description><![CDATA[<h1 id="万字长文cver-转-llm-学习笔记之大模型gpt-系列">万字长文，CVer 转 LLM 学习笔记之大模型GPT 系列</h1>
<h2 id="导读">导读</h2>
<p>本文是作者对 GPT 系列文章的学习笔记，从个人角度梳理了 GPT 系列的迭代逻辑，从技术的逻辑连续性和关联性都有很好的讲解，篇幅较长，建议大家点赞收藏。</p>
<p>这个系列的笔记主要面向像我一样已经具备一定的深度学习基础，但是新接触 NLP 和大模型领域的读者，目的是能提纲挈领地快速把握这个领域的一系列关键工作节点。</p>
<p>这篇笔记涵盖的内容有：</p>
<ul>
<li>GPT-1 论文</li>
<li>GPT-2 论文</li>
<li>GPT-3 论文</li>
<li>InstructGPT 论文（GPT-3.5 背后的技术）</li>
<li>GPT-4 技术报告</li>
<li>GPT-4 微软评测报告</li>
<li>GPT-4V 微软评测报告</li>
</ul>
<p>作为一个从 CV 转到 LLM 的新人，难免犯一些常见或低级的错误，欢迎任何读者及时指出和斧正，也欢迎任何留言讨论。</p>
<h2 id="要点-tldl">要点 (TL;DL)</h2>
<ul>
<li><strong>注重能力，而非过程</strong>：预训练任务其实形式不重要，可以是 classification，可以是预测 next token，真正重要的是 model 和 data 的 scaling up，以此快速高效地得到一个有优异泛化能力的特征提取器:
<ul>
<li>data: 找到/设计一个能把海量的数据用起来的任务，能用的数据越多越好，训练越快越好</li>
<li>model: <strong>模型性能</strong>可以随着参数量巨量地提升而不会快速饱和，一个优秀的模型架构是 scaling up 的基础保障</li>
</ul>
</li>
<li><strong>三种范式</strong>：
<ul>
<li>我们可以将专属任务上微调得到的模型，看成一种用户输入 0 个特殊 token，解决 1 种任务的范式，所以<strong>第一范式</strong>下的每个模型只能用于解决一个专属任务。</li>
<li><strong>第二范式</strong>的模型是为每一种任务准备 1 个特殊 token，因此通过改变输入 token，就能用一个模型解决不同的任务。</li>
<li><strong>第三范式</strong>的模型把特殊 token 替换成了特殊 token 序列，而自然语言正好就是一种最符合人类习惯和直觉的特殊 token 序列。</li>
</ul>
</li>
<li><strong>RLHF</strong>：
<ul>
<li>SFT 模型(16 epoch)</li>
<li>RM</li>
<li>PPO 继续训练出来的最终模型</li>
<li>SFT 数据(13k 条): 人工设计的问题，人工标注答案</li>
<li>Feedback 数据(33k 条): 针对上面人工设计的问题，模型输出的几份答案的排序（打分）</li>
<li>PPO 使用的数据(31k 条): 人工设计的问题(上面的模型没见过的新问题)，用 RM 的评分来继续训练，这份数据不需要人工标注答案</li>
</ul>
</li>
<li><strong>对 GPT-4 的全面探索</strong>：
<ul>
<li><strong>心理学角度</strong>：人类思维是快思考与慢思考两个系统的混合体，而 GPT-4 目前更类似于单纯的快思考</li>
<li>GPT-4 还有哪些<strong>局限性</strong>，以及哪些可以<strong>改进的地方</strong>（见 GPT-4 微软报告）</li>
</ul>
</li>
<li><strong>GPT-4V 的全面探索</strong>：
<ul>
<li>支持哪些输入和工作模式</li>
<li>在不同领域和任务上的能力质量和通用性如何</li>
<li>有效使用和提示方法</li>
<li>未来方向</li>
</ul>
</li>
</ul>
<h2 id="1-improving-language-understanding-by-generative-pre-training-201806">1. Improving Language Understanding by Generative Pre-Training (2018.06)</h2>
<p>GPT 系列的第一篇论文，定下了纯 <strong><font color=red>Transformer-Decoder</font></strong> 路线。</p>
<p>深度学习的早期突破很多来自于 CV 领域，其中很重要的一个原因是 CV 有 ImageNet 这个百万量级的有标注数据集，在 ImageNet 上训练分类任务得到的模型 Backbone 天然就是一个优秀的图片特征提取器，基于这个特征提取器在任意的子任务上做 fine-tuning 效果都不会太差（至少能展现出一定的泛化能力）。而 NLP 领域缺少这样大的数据集，因此，一直以来 NLP 模型发力卡在了特征提取器的构筑上，<font color=red>GPT 提出用训练语言模型的方式来得到这个特征提取器，然后用它来做子任务上的微调。</font></p>
<p>语言模型由于做的是“预测下一个词”这样的一个任务，因此不依赖于人工标注，可以实现海量数据的预训练和泛化。</p>
<p>GPT 工作是在 BERT 之前的，很多的 setting 都被 BERT 直接沿用了，比如 12 层 Transformer，768 的维度，800M 的 BookCorpus 数据集 等。</p>
<p>文章剩余部分介绍了如何在 NLP 四大主流任务类型上运用 GPT，即如何把不同形式的任务都表示成一个序列+对应的标签的形式。</p>
<p>对笔者的启示:</p>
<ul>
<li><strong>注重能力，而非过程</strong>：预训练任务其实形式不重要，可以是 classification，可以是预测 next token，真正重要的是 model 和 data 的 scaling up，以此快速高效地得到一个有优异泛化能力的特征提取器:
<ul>
<li>data: 找到/设计一个能把海量的数据用起来的任务，能用的数据越多越好，训练越快越好</li>
<li>model: 模型性能可以随着参数量巨量地提升而不会快速饱和，一个优秀的模型架构是 scaling up 的基础保障</li>
</ul>
</li>
</ul>
<h2 id="2-language-models-are-unsupervised-multitask-learners-201902">2. Language Models are Unsupervised Multitask Learners (2019.02)</h2>
<p>GPT 系列的第二篇工作。</p>
<p>参数量提升到了 1.5B，也用了更大量的数据。GPT-2 最大的贡献是把研究的重点从单个任务上的针对性调参刷榜，转向了 <strong>zero-shot</strong>，即， <font color=red>训练好的模型不再需要微调就能去做不同任务了，尽管性能上距离每个任务的 SOTA 都还有距离，但方案的可行性已经验证了</font>。要实现这一点，原来为特殊任务准备特殊 token 的做法就不合适了，因为预训练阶段模型是没见过这些 token 的，毕竟语言模型预训练阶段只见过自然语言，所以非常自然地就引出了 prompt 的概念，用自然语言来替代原本的任务 token，实现不同任务的 zero-shot。</p>
<p>可以看到，在这个时候 GPT-2 就已经初具 ChatGPT 的雏形了，只不过用户的输入还不完全是任意自然语言，而是类似于这样的模板输入。：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">翻译任务：
</span></span><span class="line"><span class="cl">(translate to
</span></span><span class="line"><span class="cl">french, english text, french text)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">QA 任务：
</span></span><span class="line"><span class="cl">(answer the question, document,
</span></span><span class="line"><span class="cl">question, answer)</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-language-models-are-few-shot-learners-200514165">3. Language Models are Few-Shot Learners (2005.14165)</h2>
<p>GPT 系列的第三篇工作。</p>
<p>GPT-3 的参数量来到了 175B，训练数据从一开始 GPT-1 的几千本书的数据集，开始进入到了网站爬虫数据和清洗的模式。</p>
<p>其实在 GPT-2 中就已经提到了一个叫 Common Crawl 的公开网络数据，但是因为他们觉得这份数据实在太脏了所以放弃了，而现在为了训练更大的模型也不得不用起来，因此清洗数据是免不了的。</p>
<p>数据清洗经历了两个过程：</p>
<ol>
<li><strong>过滤</strong>：他们将原来 GPT-2 训练用的数据作为正样本，Common Crawl 作为负样本训练了一个二分类器，然后用这个分类器来做数据筛选，过滤掉一些特别显著的脏数据。</li>
<li><strong>去重</strong>：用经典的 LSH 算法进行去重</li>
</ol>
<p>另一方面，GPT-3 也正式提出了“in-context learning”的概念，在模型参数不进行更新的情况下，通过输入的上下文来帮助模型提升表现。</p>
<p>对于笔者而言，这张图相当形象:</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这揭示了模型学习的另一个维度，提升模型表现并不只有 SGD 梯度更新这一个优化方向。结合 GPT-2 中 prompt 的由来，prompt 的前身是语言模型做多任务 zero-shot 时，针对不同任务给的特殊 token，因此一个更加富有信息量的“特殊 token 序列”能提升模型表现似乎是一件非常符合直觉的事情。</p>
<p>相比于过去使用一个特殊 token 来代表某一种特定的任务，GPT-3 的 few-shot prompt，或者说 in-context learning 形式，在笔者看来是一种推广，用户输入的自然语言和 few-shot 样例可以看成是一组特殊 token 的序列，因为自然语言的 token 具有语义和逻辑关联性，一个强大的预训练模型做到了“特殊 token”之间的泛化。</p>
<p>通过实验我们也可以观察到，随着给出的示例样本数变多，模型的表现也在提升：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>从笔者个人的角度来总结：</p>
<ul>
<li>我们可以将专属任务上微调得到的模型，看成一种用户输入 0 个特殊 token，解决 1 种任务的范式，所以<strong>第一范式</strong>下的每个模型只能用于解决一个专属任务。</li>
<li><strong>第二范式</strong>的模型是为每一种任务准备 1 个特殊 token，因此通过改变输入 token，就能用一个模型解决不同的任务。</li>
<li><strong>第三范式</strong>的模型把特殊 token 替换成了特殊 token 序列，而自然语言正好就是一种最符合人类习惯和直觉的特殊 token 序列。</li>
</ul>
<h2 id="4-training-language-models-to-follow-instructions-with-human-feedback-220302155">4. Training language models to follow instructions with human feedback (2203.02155)</h2>
<p>InstructGPT 被认为是 ChatGPT（GPT3.5） 背后的技术，核心点是把 RLHF，即<strong>基于人类反馈的强化学习</strong>，用到了语言模型上来<strong>进行人类喜好对齐</strong>。经过 RLHF 的 1.3B GPT 模型能在人类主观评分上超过 175B 的 GPT-3.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>这篇工作里将模型输出与人类意愿不一致的这个现象称为“misaligned”，并分析原因在于语言模型的训练目标只是预测下一个 token，这跟我们希望模型“follow the user&rsquo;s instructions helpfully and safely”的目标之间显然是存在差距的。</p>
<p>用 Anthropic 的工作里的话来说，大模型应该遵循 3H 原则，即：</p>
<ul>
<li>helpful：帮助用户解决问题</li>
<li>honest：不能伪造信息或误导用户</li>
<li>harmless：不能对人或环境造成身体、心理或社会伤害</li>
</ul>
<p>语言模型之所以存在这个问题，原因也很简单，因为使用的是无监督学习，本身的学习目标里就没有人为控制，所以很直观地可以想到用**监督微调（SFT）**的方式来把缺失的人类监督信号加进来。</p>
<p>但是前面 GPT 三篇工作好不容易才把模型做到 175B 这么大，现在又重新开始标数据做监督学习显然是有点不聪明的，而且模型大了以后也更容易过拟合，对于人类偏好这一类的问题标注起来难度又很大，简单地全靠 SFT 肯定是行不通的。所以很自然地，OpenAI 想到了用他们家的拿手好戏强化学习，要知道 OpenAI 本身就是做强化学习起家的，本文使用的强化学习方法 PPO 也全是之前他们已经提出的算法，没有任何新的算法被提出，甚至论文里都没有对已有的算法进行太多的解释和铺垫，需要你感兴趣自己去翻他们的论文。</p>
<br>
<center>
  
  <br>
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>他们的方法整体可以概括如下：</p>
<ol>
<li>人工标一批 SFT 数据（包含问题和回答），对 GPT-3 模型（在强化学习里对应 Policy）进行微调</li>
<li>用 SFT 得到的模型，针对每个问题生成几份回答，然后人工给这些回答质量打分（排序）</li>
<li>用这份打分数据训练一个奖励模型（Reward Model, RM），让奖励模型代替人工打分</li>
<li>采用 PPO 算法，基于奖励模型的评分来继续训练 GPT-3 模型</li>
</ol>
<p>换言之，他们一共<strong>造了三份数据</strong>：</p>
<ul>
<li>SFT 数据（13k 条）：人工设计的问题，人工标注答案</li>
<li>Feedback 数据（33k 条）：针对上面人工设计的问题，模型输出的几份答案的排序（打分）</li>
<li>PPO 使用的数据（31k 条）：人工设计的问题（上面的模型没见过的新问题），用 RM 的评分来继续训练，这份数据不需要人工标注答案</li>
</ul>
<p><strong>训练三个模型</strong>：</p>
<ul>
<li>SFT 模型（16 epoch）</li>
<li>RM (Reward Model 奖励模型)</li>
<li>PPO 继续训练出来的最终模型</li>
</ul>
<p>他们甚至对问题类型进行了一些分类：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>模型训练部分，个人觉得值得注意的点有：</p>
<ol>
<li>SFT 阶段，在训了 1 epoch 后模型就已经过拟合了，但他们发现继续训练过拟合的模型依然可以提升 RM 性能，所以他们训练了 16 epoch</li>
<li>RM 的权重是直接用 SFT 模型初始化的，因为评分模型也需要语言能力，直接拷贝一份权重是比较省事的。RM 的输入是问题+几份答案，输出是排序。</li>
<li>RM 模型只有 6B，因为 175B 的 RM 很难训</li>
<li>强化学习阶段加了一个逐 token 的 KL 散度，用来让最终模型跟第一版 SFT 模型的要输出分布尽量保持一致，因为 RM 是在训练 SFT 模型的数据上训的，如果分布差异太大，RM 的评分就不准了</li>
</ol>
<p>强化学习的目标函数如下：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>简单翻译一下：</p>
<p>$$损失 = RM 评分 + 新旧模型输出token分布的KL散度 + 旧问题上SFT的损失$$</p>
<p>其中：</p>
<ul>
<li>RM 评分是 RM 对当前正在训练的模型在新问题（第三份数据）上的输出的评分</li>
<li>KL 散度是当前模型跟旧的 SFT 模型输出之间计算的</li>
<li>旧问题 SFT 损失是用第一份数据集继续按 SFT 方法训当前模型得到的</li>
</ul>
<p>另外，关于训练数据和评测方面的取舍也有所不同，训练中他们更看重 helpful，而评测阶段则更看重 honest 和 harmless。</p>
<h2 id="5-gpt-4-technical-report-230308774">5. GPT-4 Technical Report (2303.08774)</h2>
<p><strong>多模</strong></p>
<p>基于上面笔者三种模型范式的思路，多模态的模型可以看成是让特殊 token 的类型从文本 token 拓宽到了视觉 token，将模型解决的任务从 NLP 任务拓宽到了 CV 任务，而两种模态 token 对齐的技术也早被 OpenAI 研究过了，也就是大名鼎鼎的 CLIP。因此，GPT-4 具备多模能力本身并不是一件意外的事情。</p>
<p><strong>RLHF</strong></p>
<p>在笔者看来，RLHF 等技术更多地是在不限制输入 token 序列的情况下，去约束模型输出的技术，当然从某种意义上，也可以看成是在监督 prompts -&gt; task 的映射关系的技术（其实发展到现在，task 这个词已经不太准确了，可以意会一下）。</p>
<p>GPT-4 的报告中明确指出，RLHF 并不能提升模型解决任务的质量（不会增加知识），甚至很多时候调的不好还会损害各个任务上的指标。RLHF 更多地是在构建一些明确的 prompts -&gt; task 映射关系，因为自然语言是具有歧义性的，尤其是在输入信息较少的情况下，模型根据 prompts “理解”到的那个 task，并不一定是人类真正心里的那个意图，RLHF 实现了一种定制化的映射搭建，或者说，人类喜好对齐。</p>
<p><strong>Predictable scaling</strong></p>
<p>由于大模型实验的成本日渐高昂，我们不再能像小模型那样随便起实验调参了。因此 OpenAI 的大部分实验应该是在一个比 GPT-4 小很多倍的模型上进行的，然后通过这个小模型的训练 loss，来预测大模型最终训练出来的 loss</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>同样的， predictable scaling laws 也在很多 HumanEval 集上得到了观察。当然，在一部分的任务上也还无法被拟合的性能曲线，因此 OpenAI 说后续还会进一步优化他们模型性能预测的方法。</p>
<h2 id="6-sparks-of-artifificial-general-intelligence-early-experiments-with-gpt-4-230312712">6. Sparks of Artifificial General Intelligence: Early experiments with GPT-4 (2303.12712)</h2>
<p>智能（Intelligence）是一个多方面且难以捉摸的概念，长期以来缺乏一个共识性的定义。1994 年 52 名 心理学家组成的公式小组出版的关于智力科学的社论中，将只能定义为一种非常普遍的心理能力，包括<strong>推理、计划、解决问题、抽象思考、理解复杂想法、快速学习和从经验中学习的能力</strong>。</p>
<p>这篇是微软关于 GPT-4 的研究报告，长达 55 页，文中的实验都是在 <strong>早期的文本单一模态版的 GPT-4</strong> 上进行的（而不是后面更新的多模态版本）, 其目标是生成一些新颖而困难的任务和问题，来证明 GPT-4 的能力并不是单纯的记忆，并且它对<strong>概念、技能和领域有深刻而灵活的理解</strong>。另外还旨在探索 GPT-4 的响应和行为，以验证其<strong>一致性、连贯性和正确性</strong>，揭示其<strong>局限性和偏差</strong>。</p>
<p><strong>如何衡量 GPT-4 的智能</strong></p>
<p>传统机器学习的标准做法是准备一组标准评测数据集，确保它们独立于训练数据之外，覆盖一系列的任务和领域。</p>
<p>但这种方法并不太适用于 GPT-4，因为 GPT-4 是闭源模型，相关的训练数据集信息不公开，并且可以预见地非常庞大，因此我们不能保证目前公开的基准测试集不在它的训练数据里。也正因为此，本文采用的研究方法更接近于传统心理学，而不是机器学习方法。</p>
<p><strong>多模态与跨学科整合</strong></p>
<p>智能的一个重要衡量指标是综合不同领域信息的能力，以及跨学科地应用知识和技能的能力。这一节中作者举了四个例子来说明 GPT-4 具有很强的多模态与跨学科整合能力：</p>
<ol>
<li>写 JavaScript 代码来生成画家 Wassily Kandinsky 风格的作品。图一是该画家的原作，后面分别是 GPT-4 和 ChatGPT 写的代码画出的。</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="2">
<li>用莎士比亚的文风来证明素数无穷定理</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="3">
<li>以圣雄甘地的口吻写一封信给他的妻子，内容是支持“电子”成为美国总统候选人</li>
</ol>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ol start="4">
<li>写 Python 代码，以年龄、性别、体重、身高和血液测试结果向量作为输入，来预测用户是否有患糖尿病的风险</li>
</ol>
<p>这些对比可以体现 GPT-4 能创新性地整合不同领域的概念，并且显著强于 ChatGPT。除此之外作者也测试了 GPT-4 在音乐、绘图、空间理解方面的能力。</p>
<p><strong>代码</strong></p>
<p>除了常见的 leetcode 刷题，作者测试了 GPT-4 生成逆向工程代码、解释已有代码、用自然语言模拟代码执行过程、运行伪代码等能力。</p>
<p><strong>数学</strong></p>
<p>在一系列的分析实验后，作者从以下三方面总结了GPT-4的数学能力：</p>
<ol>
<li><strong>创造性推理</strong>：识别每个阶段可能相关的参数、中间步骤、计算或代数操作的能力。该组件通常基于启发式猜测或直觉，通常被认为是数学解决问题的最实质性和最深刻的方面</li>
<li><strong>技术熟练程度</strong>：执行遵循指定步骤集的常规计算或操作的能力</li>
<li><strong>批判性推理</strong>：批判性地检查论点的每个步骤的能力，将其分解为其子组件，解释它需要的内容，它与其余论点相关以及为什么是正确的</li>
</ol>
<p>这一节作者发现 GPT-4 的很多缺陷，如：</p>
<ul>
<li>在执行一些很常规且机械的计算时经常算错和混淆</li>
<li>GPT-4 由于是自回归模型，因此是实时线性输出的，而没有办法“打腹稿”</li>
</ul>
<p><strong>与世界的交互</strong></p>
<p>智能的另一重要方面是交互能力，即跟外界环境和智能体进行交互的能力，作者主要通过<strong>工具调用</strong>和<strong>具身交互</strong>两个维度来评估。</p>
<p>工具调用这里不多赘述了，具身交互方面测试了文字跑团游戏，以及交互式地指导人员找到天花板漏水的地方并进行修补，逐步根据人类的每一步反馈，给出行动建议和指示。</p>
<p><strong>与人类的交互</strong></p>
<p>GPT-4 在推理他人心理状态方面表现非常突出，特别是在模拟现实场景中，它的解释能力也很强，能对自己的判断和言论进行自我解释。</p>
<p><strong>判别能力</strong></p>
<p>判别能力主要指模型区别不同事物、概念和情景的能力。比如，区分两个食物哪个是可以安全食用，哪个是有毒的。</p>
<p>这一节的测试里，揭示出当前的评测指标存在的缺陷：对于语句相似度捕捉不够，依然严重依赖单词和短句的相似度，因此在很多时候参考答案很短，而 GPT-4 生成的答案很长，会被 ROUGE 这样的指标判定为答案不匹配，而人工检查后发现 GPT-4 的答案更加高质量和具有说服力。</p>
<p>另一方面作者也测试了用 GPT-4 作为评分员，对回答进行打分，实验现实尽管距离人类打分还存在一些差距，但在一些强约束的场景下已经很具有竞争力了。</p>
<p><strong>自回归结构的局限性</strong></p>
<p>自回归结构的输出是实时进行的，因此不存在“打草稿”的机会，因此无法“step-by-step”地处理问题，而引入思维链则可以显著地提升模型准确度。</p>
<p>对于一些依赖递归回溯的问题，比如一步一步输出汉诺塔问题的解法，GPT-4 表现非常差，在解决不能以连续方式处理的复杂或创造性问题时，都暴露出了严重的局限性。</p>
<p>比如要求 GPT-4 修改“9 * 4 + 6 * 6 = 72”这个等式左边的一个数字，来让等式计算结果变成 99，这就是一个无法简单“step-by-step”推理得到答案的问题，GPT-4 最后的准确率也非常低。</p>
<p>这一节的讨论中，作者指出，理解这些局限性的一个方法是类比诺奖作者卡尼曼提出的“快思考”和“慢思考”的概念。卡尼曼认为人类思维分成快、慢两个系统，快思考是一种自动的、直观的、不需要花费精力的思考方式，速度快但是容易出错和偏见。慢思考是一种受控的、理性的、耗费精力的思考方式，虽然速度慢但是准确可靠。</p>
<p>当前的 GPT-4 很大程度上可以看成是在执行快思考，但缺少慢思考能力。</p>
<p><strong>未来方向</strong></p>
<p>作者在这一节总结了未来 GPT-4 可以研究和改进的方向：</p>
<ul>
<li><strong>置信度校验</strong>：模型的输出缺乏置信度，既会编造训练集中没有的内容（open-domain 幻觉），也会生成与 Prompt 不一致的内容（close-domain 幻觉）</li>
</ul>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<ul>
<li><strong>长期记忆</strong>：即长的上下文</li>
<li><strong>持续学习</strong>：当前微调和自我更新成本过高、缺乏有效且稳定的手段（保持已有能力不丢失和遗忘）</li>
<li><strong>个性化</strong>：根据应用和需求进行定制、扮演、调整风格等</li>
<li><strong>提前规划和概念性跳跃</strong>：推理过程过于线性，在需要思维跳跃性的任务上表现不佳</li>
<li><strong>透明度、可解释性和一致性</strong></li>
<li><strong>认知谬误和非理性</strong>：数据中存在的偏见、成见或错误引入了认知偏差和非理性</li>
<li><strong>对输入的敏感性</strong>：对于 Prompt 过于敏感，鲁棒性不够</li>
</ul>
<p>这些局限性均指向一个核心问题：<strong>哪些缺陷是自回归架构的先天缺陷，哪些是在已有架构上可以通过处理数据、增加外挂的组件和增大参数量解决的。</strong></p>
<h3 id="7-the-dawn-of-lmms-preliminary-explorations-with-gpt-4vision-230917421">7. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) (2309.17421)</h3>
<p>微软发布的 166 页的 GPT-4V 报告，主要围绕以下四个点进行展开研究：</p>
<ol>
<li>GPT-4V 支持哪些输入和工作模式？</li>
<li>GPT-4V 在不同领域和任务上的能力质量和通用性如何？</li>
<li>GPT-4V 有效使用和提示方法有哪些？</li>
<li>未来有哪些有前途的方向？</li>
</ol>
<p><strong>GPT-4V 的输入模式</strong></p>
<ul>
<li>纯文本</li>
<li>单个图像-文本对</li>
<li>图像文本交替输入</li>
</ul>
<p>前两种相对来说比较简单，第三种交替输入的情况，已经非常接近于人的聊天模式了。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>GPT-4V 的工作模式和提示技术</strong></p>
<p>实验证明，在 LLM 上研究出来上各种提示技术，在 GPT-4V 上也是好使的，比如思维链、few-shot 提示等。</p>
<p>这一节提到了一个“LLMs don&rsquo;t want to succeed”的理论，貌似是来自于 Andrej Karpathy 的某次演讲，里面展示了一种类似于催眠一样的提示技术，即，<strong>你想要你的 LLM 表现更出色，你就要用直接的提示词说“你是xxx方面的专家”，否则它之后表现出一般普通人水平的能力</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>完整的 PPT 可以看这里：https://karpathy.ai/stateofgpt.pdf</p>
<p>在论文中作者是举了一个数苹果的案例，让 GPT-4V 来数一下画面中有几个苹果。一开始 GPT-4V 并不能轻易得到正确答案，但经过一系列我们已知的 LLM Prompt 技巧加强后，GPT-4V 变得可靠，能够正确计数：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在日常的人和人交互中，在图片中画圈、画箭头来指向关键信息是一种很自然且常见的方式， 经实验 GPT-4V 在这方面的理解能力非常强大。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>作者实验了一些很有挑战性的 case，发现基本上难不倒它：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>在 In-context few-shot learning 方面，作者也用一个很有代表性的例子说明了提供示例样本的重要性。作者给了一张仪表的图，让 GPT-4V 读出当前仪表指针指向的数值，一开始不论如何改良 prompt 都无法得到正确的结果。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>甚至在给出一个示例的情况下模型仍然表现不佳，但当示例增加到两个后，GPT-4V 就突然能成功读数了，可见<strong>提供上下文示例对于提升大模型性能至关重要</strong>。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>视觉语言能力</strong></p>
<p>在大部分已有的 CV 子任务上，GPT-4V 都表现出了不错的能力，常见的场景描述等更是表现出色，在相对小众的领域，如医学图像上，同样让人印象深刻，GPT-4V 可以根据 CT 图判断出智齿和骨折等。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>当然，在一些已经被做得非常深入的子任务上，GPT-4V 相较于 SOTA 模型还有不小的差距，但还是那句话，潜力大于绝对精度，目前 GPT-4V 已经展现出了让人鼓舞的性能，优化个别任务上的表现只是时间问题。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>GPT-4V 甚至能看懂梗图，解释其中的笑点：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>借助 GPT-4V 强大的推理能力和具备的常识，我们甚至可以“<strong>假如你是一名侦探，你可以从图中推理出哪些线索？</strong>”</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>时间序列和视频理解</strong></p>
<p>作者实验了多图像序列，GPT-4V 能够识别出这是一组动态图像序列，并且能结合起来判断画面中的人正在做俯卧撑：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>情商测试</strong></p>
<p>在这一节，GPT-4V 可以基于予以内容和图像样式解释视觉情感，如满意、愤怒、敬畏和恐惧等：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>甚至可以一张图片让 GPT-4V 用两种不同方式来描述，分别让人感到不安和感到舒适（新闻学让它玩明白了）：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p><strong>新兴应用亮点</strong></p>
<ul>
<li>行业：
<ul>
<li>缺陷检测</li>
<li>安全检查</li>
<li>杂货结账</li>
</ul>
</li>
<li>医疗</li>
<li>汽车保险
<ul>
<li>损害评估</li>
<li>保险报告</li>
</ul>
</li>
<li>定制化
<ul>
<li>照片组织</li>
<li>密集标注与分割</li>
</ul>
</li>
<li>图像生成
<ul>
<li>生成图像的评估</li>
<li>图像编辑的提示生成</li>
</ul>
</li>
<li>具象化智能体
<ul>
<li>操作机器</li>
<li>导航</li>
</ul>
</li>
<li>GUI 导航（软件层面的交互和导航）</li>
</ul>
<p>整篇报告篇幅较多，并且举了大量详细的例子，在这里就不一一展开了，感兴趣的同学可以自行翻阅。</p>
<hr>
<p>至此，我总结了 GPT 系列工作里一些我关注到的点，从中可以感受到 OpenAI 的工作之间都有着很深的逻辑链条，很多推广都似乎是最符合直觉的。OpenAI 早期公开的论文里各种细节还是很丰富的，不仅细致地告诉你如何清洗和构造数据，甚至还教你如何找到一个合适的标注员给你标数据。</p>
<p>作为一个从 CV 转到 LLM 的新人，难免犯一些常见或低级的错误，欢迎任何读者及时指出和斧正，也欢迎任何留言讨论。</p>
<p>本篇笔记的写作参考了沐神的几期 B 站视频，以及知乎@苏打的文章，特此感谢</p>
]]></description></item><item><title>Index</title><link>https://jianye0428.github.io/posts/dl_basics_one/</link><pubDate>Fri, 26 Apr 2024 17:15:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/dl_basics_one/</guid><description><![CDATA[<h3 id="1softmax如何防止指数上溢">1、⭐softmax如何防止指数上溢</h3>
<ul>
<li>原softmax公式：</li>
</ul>
<center>

</center>
<ul>
<li>工程化实现，防止指数上溢：</li>
</ul>
<center>

</center>
，使a等于x中最大值。
<h3 id="2transformer中的positional-encoding">2、⭐Transformer中的positional encoding</h3>
<ul>
<li>为什么需要PE: 因为transfomer是同时处理所有输入的，失去了位置信息。</li>
<li>编码应该满足的条件：a、对于每个位置词语，编码是唯一的 b、词语之间的间隔对于不同长度句子是一致的 c、能适应任意长度句子</li>
<li>公式：每个词语位置编码为不同频率的余弦函数，从1到1/10000。如下将每个词语位置编码为d维向量&ndash;&gt;</li>
</ul>
<center>

</center>
<center>

</center>
<p>可理解为一种二进制编码，二进制的不同位变化频率不一样，PE的不同位置变化频率也不一样</p>
<center>

</center>
<ul>
<li>如何获取相对位置关系：两个位置编码进行点积。</li>
</ul>
<center>

</center>
<h3 id="3求似然函数步骤">3、⭐求似然函数步骤</h3>
<ul>
<li>定义：概率是给定参数，求某个事件发生概率；似然则是给定已发生的事件，估计参数。</li>
</ul>
<ol>
<li>写出似然函数</li>
<li>对似然函数取对数并整理</li>
<li>求导数，导数为0处为最佳参数</li>
<li>解似然方程</li>
</ol>
<h3 id="4hmm和crf区别">4、⭐HMM和CRF区别</h3>
<ul>
<li>CRF是判别模型，对问题的条件概率分布建模；HMM是生成模型，对联合概率分布建模</li>
<li>HMM是概率有向图，CRF是概率无向图</li>
<li>HMM求解过程可能是局部最优，CRF是全局最优</li>
</ul>
<h3 id="5空洞卷积实现">5、⭐空洞卷积实现</h3>
<p>相比于常规卷积多了dilation rate超参数，例如dilation rate=2代表相邻两个卷积点距离为2，如图(b)。</p>
<center>

</center>
<ul>
<li>存在问题：gridding effect, 由于卷积的像素本质上是采样得到的，所以图像的局部相关性丢失了，同时远距离卷积得到的信息也没有相关性。</li>
</ul>
<h3 id="6汉明距离">6、⭐汉明距离</h3>
<p>两个字符串对应位置的不同字符的个数。</p>
<h3 id="7训练过程中发现loss快速增大应该从哪些方面考虑">7、⭐训练过程中发现loss快速增大应该从哪些方面考虑?</h3>
<ul>
<li>学习率过大</li>
<li>训练样本中有坏数据</li>
</ul>
<h3 id="8pytorch和tensorflow区别">8、⭐Pytorch和TensorFlow区别</h3>
<ul>
<li>图生成：pytorch动态图，tensorflow静态图</li>
<li>设备管理：pytorch cuda，tensorflow 自动化</li>
</ul>
<h3 id="9modeleval-vs和torchno_grad区别">9、⭐model.eval vs和torch.no_grad区别</h3>
<ul>
<li>model.eval: 依然计算梯度，但是不反传；dropout层保留概率为1；batchnorm层使用全局的mean和var</li>
<li>with torch.no_grad: 不计算梯度</li>
</ul>
<h3 id="10每个卷积层的flops计算">10、⭐每个卷积层的FLOPS计算</h3>
<center>

</center>
即计算feature map每个点需要的乘法和加法运算量，定义一个乘法和加法为一次flop，则FLOPS计算如下：
<center>

</center>
<h3 id="11pca主成分分析">11、⭐PCA(主成分分析)</h3>
<ul>
<li>⭐PCA是一种降维方法，用数据里面最主要的方面来代替原始数据，例如将$m$个$n$维数据降维到$n&rsquo;$维，希望这$m$个$n&rsquo;$维数据尽可能地代表原数据。</li>
<li>⭐两个原则：最近重构性&ndash;&gt;样本点到这个超平面的距离足够近；最大可分性&ndash;&gt;样本点在这个超平面的投影尽可能的分开。</li>
<li>⭐流程：基于特征值分解协方差矩阵和基于奇异值分解SVD分解协方差矩阵。</li>
</ul>
<p>（1）对所有样本进行中心化</p>
<p>（2）计算样本的协方差矩阵$XX^T$</p>
<p>（3）对协方差矩阵进行特征值分解</p>
<p>（4）取出最大的$n&rsquo;$个特征值对应的特征向量，将所有特征向量标准化，组成特征向量矩阵W</p>
<p>（5）对样本集中的每一个样本$x^(i)$转化为新的样本$z^(i)=W^T x^(i)$，即将每个原数据样本投影到特征向量组成的空间上</p>
<p>（6）得到输出的样本集$z^(1)、z^(2)&hellip;$</p>
<ul>
<li>⭐意义：a、使得结果容易理解 b、数据降维，降低算法计算开销 c、去除噪声</li>
</ul>
<h3 id="12k-means如何改进">12、⭐k-means如何改进？</h3>
<ul>
<li>⭐缺点：1、k个初始化的质心对最后的聚类效果有很大影响 2、对离群点和孤立点敏感 3、K值人为设定</li>
<li>⭐改进：
<ul>
<li>K-means++：从数据集随机选择一个点作为第一个聚类中心，对于数据集中每一个点计算和该中心的距离，选择下一个聚类中心，优先选择和上一个聚类中心距离较大的点。重复上述过程，得到k个聚类中心。</li>
<li>K-medoids：计算质心时，质心一定是某个样本值的点。距离度量：每个样本和所有其他样本的曼哈顿距离$((x,y), |x|+|y|)$。</li>
<li>ISODATA，又称为迭代自组织数据分析法，是为了解决K值需要人为设定的问题。核心思想：当属于某个类别的样本数过少时或者类别之间靠得非常近就将该类别去除；当属于某个类别的样本数过多时，把这个类别分为两个子类别。</li>
</ul>
</li>
<li>⭐和分类问题的区别：分类的类别已知，且需要监督；k-means是聚类问题，类别未知，不需要监督。</li>
<li>⭐终止条件：a、相邻两轮迭代过程中，非质心点所属簇发生改变的比例小于某个阈值 b、所有簇的质心均未改变 c、达到最大迭代次数</li>
<li>⭐时间复杂度：$O(迭代次数 \ast 数据个数 \ast k \ast 数据维度)$，k为k个类别中心</li>
<li>⭐空间复杂度：$O(数据个数 \ast 数据维度+k \ast 数据维度)$</li>
</ul>
<h3 id="13dropout实现">13、⭐Dropout实现</h3>
<center>

</center>
以p的概率使神经元失效，即使其激活函数输出值为0：
<center>

</center>
<p>为了使训练和测试阶段输出值期望相同，需要在训练时将输出值乘以1/(1-p)或者在测试时将权重值乘以(1-p)。</p>
<ul>
<li><strong>Dropout和Batch norm能否一起使用？</strong></li>
</ul>
<p>可以，但是只能将Dropout放在Batch norm之后使用。因为Dropout训练时会改变输入X的方差，从而影响Batch norm训练过程中统计的滑动方差值；而测试时没有Dropout，输入X的方差和训练时不一致，这就导致Batch norm测试时期望的方差和训练时统计的有偏差。</p>
<h3 id="14梯度消失和梯度爆炸">14、⭐梯度消失和梯度爆炸</h3>
<p><strong>梯度消失的原因和解决办法</strong></p>
<p>（1）隐藏层的层数过多</p>
<p>反向传播求梯度时的链式求导法则，某部分梯度小于1，则多层连乘后出现梯度消失</p>
<p>（2）采用了不合适的激活函数</p>
<p>如sigmoid函数的最大梯度为1/4，这意味着隐藏层每一层的梯度均小于1（权值小于1时），出现梯度消失。</p>
<p>解决方法：1、relu激活函数，使导数衡为1 2、batch norm 3、残差结构</p>
<p><strong>梯度爆炸的原因和解决办法</strong></p>
<p>（1）隐藏层的层数过多，某部分梯度大于1，则多层连乘后，梯度呈指数增长，产生梯度爆炸。</p>
<p>（2）权重初始值太大，求导时会乘上权重</p>
<p>解决方法：1、梯度裁剪 2、权重L1/L2正则化 3、残差结构 4、batch norm</p>
<h3 id="15yolov1-yolov4改进">15、⭐YOLOV1-YOLOV4改进</h3>
<p>⭐<strong>YOLOV1</strong>:</p>
<ul>
<li>one-stage开山之作，将图像分成S*S的单元格，根据物体中心是否落入某个单元格来决定哪个单元格来负责预测该物体，每个单元格预测两个框的坐标、存在物体的概率（和gt的IoU）、各类别条件概率。</li>
<li>损失函数：均采用均方误差。</li>
<li>优点：速度快。</li>
<li>缺点：1、每个单元格预测两个框，并且只取和gt IoU最大的框，相当于每个单元格只能预测一个物体，<strong>无法处理密集物体场景</strong>。2、输出层为<strong>全连接层</strong>，只能输入固定分辨率图片 3、<strong>计算IoU损失时，将大小物体同等对待</strong>，但同样的小误差，对大物体来说是微不足道的，而对小物体来说是很严重的，这会导致定位不准的问题。4、没有密集锚框、没有RPN，导致召回率低</li>
</ul>
<p><strong>⭐YOLOV2:</strong></p>
<ul>
<li>改进点：</li>
</ul>
<p>(1)、<strong>Batch normalization</strong>替代dropout，防止过拟合</p>
<p>(2)、<strong>去掉全连接层，使用类似RPN的全卷积层</strong></p>
<p>(3)、<strong>引入Anchor</strong>，并使用k-means聚类确定anchor大小、比例，提高了recall</p>
<p>(4)、高分辨率预训练backbone</p>
<p>(5)、<strong>限定预测框的中心点只能在cell内，具体通过预测相对于cell左上角点的偏移实现</strong>，这样网络收敛更稳定</p>
<p>(6)、添加passthrough层，相当于多尺度特征融合，$1 \ast 1$卷积将$26 \ast 26 \ast 512$ feature map降维成$26 \ast 26 \ast 64$, 然后将特征重组，拆分为4份$13 \ast 13 \ast 64$，concate得到$13 \ast 13 \ast 256$ feature map，和低分辨率的$13  \ast 13 \ast 1024$ feature map进行concate</p>
<p>(7)、提出Darknet进行特征提取，参数更少，速度更快</p>
<p>(8)、提出<strong>YOLO9000</strong>，建立层级分类的World Tree结构，可以进行细粒度分类</p>
<p><strong>⭐YOLOV3:</strong></p>
<p>(1)、<strong>使用sigmoid分类器替代softmax分类器</strong>，可以处理多标签分类问题</p>
<p>(2)、<strong>引入残差结构，进一步加深网络深度</strong></p>
<p>(3)、<strong>多尺度预测</strong>，每个尺度预测3个bbox</p>
<p><strong>⭐YOLOV4:</strong></p>
<p>(1)、<strong>Mosaic data augmentation</strong>：四张图片拼接成一张图片</p>
<p>(2)、<strong>DropBlock</strong>：drop out只丢弃单个像素，而因为二维图像中相邻特征强相关，所以丢弃后网络依然可以推断出丢失区域信息，导致过拟合；所以dropblock选择丢弃一块连续区域。</p>
<p>(3)、label smoothing</p>
<center>

</center>
<p>(4)、CIoU loss
CIoU = IoU + bbox中心距离/对角线距离+长宽比例之差</p>
<center>

</center>
<center>

</center>
<p>-1&lt;=CIoU&lt;=1</p>
<p>(5)、YOLO with SPP：就是用不同大小的卷积核对特征图进行卷积，得到不同感受野的特征，然后concate到一起。</p>
<h3 id="16ap计算">16、⭐AP计算</h3>
<p>⭐AP是对每一类先计算AP，再将所有类平均得到最终AP。
以COCO中AP计算为例。先选定用来判断TP和FP的IoU阈值，如0.5，则代表计算的是AP0.5，然后对每类做计算，例如对于class1:</p>
<div class="center">
<table>
<thead>
<tr>
<th style="text-align:left"><div style="width:300px"></th>
<th style="text-align:left">class1 <div style="width:300px"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">box1</td>
<td style="text-align:left">score1</td>
</tr>
<tr>
<td style="text-align:left">box2</td>
<td style="text-align:left">score2</td>
</tr>
<tr>
<td style="text-align:left">box3</td>
<td style="text-align:left">score3</td>
</tr>
</tbody>
</table>
</div>
<p>若box1与某gt的IoU大于指定阈值（如0.5)，记为Positive；若有多个bbox与gt的IoU大于指定阈值，选择其中score最大的记为Positive，其它记为Negative。可理解为确定box1对应的label。box2、box3同理。</p>
<p>而后要得到PR曲线，需要先对box1,2,3按score从高到低排序，假设排序结果如下：</p>
<div class="center">
<table>
<thead>
<tr>
<th style="text-align:left"><div style="width:200px"></th>
<th style="text-align:left">True class <div style="width:200px"></th>
<th style="text-align:left">class1 <div style="width:200px"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">box2</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score2</td>
</tr>
<tr>
<td style="text-align:left">box1</td>
<td style="text-align:left">Negative</td>
<td style="text-align:left">score1</td>
</tr>
<tr>
<td style="text-align:left">box3</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score3</td>
</tr>
</tbody>
</table>
</div>
<p>然后逐行计算score阈值为score2、score1、score3的Precision和Recall，score大于阈值的box才算做模型预测的Positive（TP+FP)。假设共有三个gt box，则计算结果如下：</p>
<div class="center">
<table>
<thead>
<tr>
<th style="text-align:left"><div style="width:60px"></th>
<th style="text-align:left">True class <div style="width:110px"></th>
<th style="text-align:left">class1 <div style="width:110px"></th>
<th style="text-align:left">Precision=TP/(TP+FP)</th>
<th style="text-align:left">Recall <div style="width:110px"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">box2</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score2</td>
<td style="text-align:left">1/1</td>
<td style="text-align:left">1/3</td>
</tr>
<tr>
<td style="text-align:left">box1</td>
<td style="text-align:left">Negative</td>
<td style="text-align:left">score1</td>
<td style="text-align:left">1/2</td>
<td style="text-align:left">1/3</td>
</tr>
<tr>
<td style="text-align:left">box3</td>
<td style="text-align:left">Positive</td>
<td style="text-align:left">score3</td>
<td style="text-align:left">2/3</td>
<td style="text-align:left">2/3</td>
</tr>
</tbody>
</table>
</div>
<p>这样得到一个个PR曲线上的点，然后利用插值法计算PR曲线的面积，得到class1的AP。
具体插值方法：COCO中是均匀取101个recall值即0,0.01,0.02,&hellip;,1，对于每个recall值r，precision值取所有recall&gt;=r中的最大值$p_{interp(r)}$。</p>
<center>

</center>
<center>

</center>
<center>

</center>
<p>然后每个recall值区间（0-0.01，0.01-0.02，&hellip;）都对应一个矩形，将所有矩形面积加起来即为PR曲线面积，得到一个类别的AP，如下图所示。对所有类别（如COCO中80类）、所有IoU阈值（例如0.5:0.95）的AP取均值即得到最终AP。</p>
<center>

</center>
<p><strong>AR计算</strong>
计算过程同AP，也是在所有IoU阈值和类别上平均。
每给定一个IoU阈值和类别，得到一个P_R曲线，当P不为0时最大的Recall作为当前Recall。</p>
<h3 id="17iou变种合集">17、⭐IoU变种合集</h3>
<p><strong>IoU</strong></p>
<center>

</center>
<p><strong>GIoU</strong></p>
<center>

</center>
<p>$Ac$为bbox A和bbox B的最小凸集面积，U为A U B的面积，即第二项为不属于A和B的区域占最小闭包的比例。
-1&lt;=GIoU&lt;=1，当A和B不重合，仍可以计算损失，因此可作为损失函数。</p>
<center>

</center>
<p>✒️优点：在不重叠的情况下，能让预测框向gt框接近。</p>
<p>✒️缺点：遇到A框被B框包含的情况下，GIoU相同。</p>
<center>

</center>
<p><strong>DIoU</strong></p>
<center>

</center>
<p>其中， $b$和$b^{gt}$分别代表了预测框和真实框的中心点，且$ρ$代表的是计算两个中心点间的欧式距离。$c$代表的是能够同时包含预测框和真实框的最小闭包区域的对角线距离。</p>
<center>

</center>
<p>$-1 &lt; DIoU \leq 1$, 将目标与anchor之间的距离，重叠率以及尺度都考虑进去，使得目标框回归变得更加稳定</p>
<p>✒️优点：直接优化距离，解决GIoU包含情况。</p>
<center>

</center>
<p><strong>CIoU</strong></p>
<p>在DIoU的基础上考虑bbox的长宽比：</p>
<center>

</center>
<center>

</center>
<p>$-1 \leq CIoU \leq 1$</p>
<p>✒️优点：考虑bbox长宽比</p>
<h3 id="18depth-wise-separable-conv-深度可分离卷积">18、⭐depth-wise separable conv (深度可分离卷积)</h3>
<p>例如$5 \ast 5 \ast 3$图片要编码为$5 \ast 5 \ast 4$ feature map，则depth-wise conv分为两步：
先是depth wise卷积，利用3个$3 \ast 3$ conv对每个通道单独进行卷积，这一步只能改变feature map长宽，不能改变通道数：</p>
<center>

</center>
<p>参数量：</p>
<center>

</center>
<p>计算量：</p>
<center>

</center>
<p>然后是point wise卷积，利用4个$1 \ast 1 \ast 3$ conv对depth wise生成的feature map进行卷积，这一步只能改变通道数，不能改变feature map长宽：</p>
<center>

</center>
<p>参数量：</p>
<center>

</center>
<p>计算量：</p>
<center>

</center>
<p>所以与一般卷积对比，总参数量和计算量：</p>
<ul>
<li>总参数量：常规卷积&ndash;&gt;</li>
</ul>
<center>

</center>
<p>， 深度可分离卷积&ndash;&gt;</p>
<center>

</center>
<ul>
<li>总计算量：常规卷积&ndash;&gt;</li>
</ul>
<center>

</center>
<p>, 深度可分离卷积&ndash;&gt;</p>
<center>

</center>
<h3 id="19检测模型里为啥用smoothl1去回归bbox">19、⭐检测模型里为啥用smoothL1去回归bbox</h3>
<p>首先，对于L2 loss，其导数包含了(f(x)-Y)，所以当预测值与gt差异过大时，容易梯度爆炸；
而对于L1 loss，即使训练后期预测值和gt差异较小，梯度依然为常数，损失函数将在稳定值附近波动，难以收敛到更高精度。</p>
<center>

</center>
<p>所以SmoothL1 loss结合了两者的优势，当预测值和gt差异较大时使用L1 loss；差异较小时使用L2 loss：</p>
<center>

</center>
<h3 id="20deformable-conv如何实现梯度可微">20、⭐Deformable conv如何实现梯度可微？</h3>
<p>指的是对offset可微，因为offset后卷积核所在位置可能是小数，即不在feature map整数点上，所以无法求导；Deformable conv通过双线性插值实现了offset梯度可微。
用如下表达式表达常规CNN:</p>
<center>

</center>
<center>

</center>
<p>则Deformable conv可表达为：</p>
<center>

</center>
<p>$x(p_0+p_n+\Delta p_n)$可能不是整数，需要进行插值，任意点p（周围四个整数点为q）的双线性插值可表达为下式：</p>
<center>

</center>
<center>

</center>
<p>其中$g(a, b) = max(0, 1 − |a − b|)$。</p>
<p>则offest delta_pn求导公式为：</p>
<center>

</center>
<p>从而实现对offset的梯度可微。</p>
<h3 id="21swin-transformer">21、⭐Swin Transformer</h3>
<p>(1)⭐<strong>motivation</strong>
高分辨率图像作为序列输入计算量过大问题；和nlp不同，cv里每个物体的尺度不同，而nlp里每个物体的单词长度都差不多。</p>
<p>(2)⭐<strong>idea</strong>
问题一：一张图分成多个窗口，每个窗口分成多个patch，每个窗口内的多个patch相互计算自注意力；问题二：模仿CNN pooling操作，将浅层尺度patch进行path merging得到一个小的patch，实现降采样，从而得到多尺度特征。</p>
<p>(3)⭐<strong>method</strong>
整体结构很像CNN，卷积被窗口自注意力代替，pooling被patch merging代替。</p>
<center>

</center>
<ul>
<li>✒️method 1: shifted window</li>
</ul>
<center>

</center>
<p>目的是让不重叠窗口区域也能有交互，操作本质是将所有窗口往右下角移动2个patch。</p>
<p>窗口数从4个变成了9个，计算量增大，为减小计算量，使用cyclic shift，将窗口拼接成4个窗口，另外因为拼接后A、B、C相较于原图发生了相对位置变化，所以A、B、C和其他区域是不可以进行交互的，因此引入了Mask操作。</p>
<center>

</center>
<p>Mask操作：</p>
<p>以3、6所在窗口的自注意力计算为例，将7*7=49个像素展平得到长度为49的一维向量，做矩阵乘法即Q*K。</p>
<center>

</center>
<p>又因为3和6是不可以交互的，所以矩阵左下角和右上角应该被mask掉，Swin采用的方法是加上左下角和右上角为-100，其余位置为0的模板，这样得到的attention矩阵在softmax后就变成0了。</p>
<center>

</center>
<ul>
<li>✒️method2: patch merging
就是间隔采样，然后在通道维度上拼接</li>
</ul>
<center>

</center>
<p>(4)⭐<strong>SwinTransformer位置编码实现</strong></p>
<p><strong>👉核心思想就是建了一个相对位置编码embedding字典，使得相同的相对位置编码成相同的embedding。👈</strong></p>
<p>例如2*2的patch之间的相对位置关系矩阵为2*2*2，相对位置范围为[-1,1]：</p>
<center>

</center>
<p>则x，y相对位置关系可用3*3 (-1,0,1三种相对位置)的table存储所有可能的相对位置关系，并用3*3*embed_n表示所有相对位置对应的embedding。<strong>为了使得索引为整数</strong>，需要将所有相对位置变为正值：</p>
<center>

</center>
<p>可以通过简单的x,y相对位置相加将相对位置映射为一维，但是会出现(0,2)和(2,0)无法区分的问题，所以需要使得x,y相对位置编码不同：</p>
<center>

</center>
<p>然后将x和y相对位置相加：</p>
<center>

</center>
<p>从而每个相对位置对应一个一维的值，作为相对位置embedding table的索引，获取对应位置的embedding。</p>
<h3 id="22yolox核心改进">22、⭐YOLOX核心改进：</h3>
<center>

</center>
<p>(1)✒️Decoupled head：就是anchor free方法里最常用的cls head和reg head</p>
<p>(2)✒️Anchor-free: 即类似FCOS，不同的是预测的是中心点相对于grid左上角坐标的offset值，以及bbox的长宽，将物体中心的某个区域内的点定义为正样本，并且每个尺度预测不同大小物体。</p>
<p>(3)✒️Label assignment(SimOTA): 将prediction和gt的匹配过程建模为运输问题，使得cost最小。</p>
<ul>
<li>cost表示：$pred_i$和$gt_j$的cls和reg loss。</li>
<li>对每个gt，选择落在指定中心区域的top-k least cost predictions当作正样本，每个gt的k值不同。</li>
<li>最佳正锚点数量估计：某个gt的适当正锚点数量应该与该gt回归良好的锚点数量正相关，所以对于每个gt，我们根据IoU值选择前q个预测。这些IoU值相加以表示此gt的正锚点估计数量。</li>
</ul>
<h3 id="23l1l2正则化的区别">23、⭐L1、L2正则化的区别</h3>
<center>

</center>
<p>✒️L1正则化容易得到稀疏解，即稀疏权值矩阵，L2正则化容易得到平滑解（权值很小）。</p>
<p>✒️原因：a、解空间角度：二维情况，L1正则化：||w1||+||w2||，则函数图像为图中方框，显然方框的角点容易是最优解，而这些最优解都在坐标轴上，权值为0.</p>
<center>

</center>
<p>b、梯度下降角度
添加正则项 $\lambda \theta^2_j$，则L对$\theta_j$的导数为$2\lambda \theta_j$，梯度更新时$\theta_j=\theta_j-2 \lambda \theta_j=(1-2 \lambda) \theta_j$，相当于每次都会乘以一个小于1的数，使得$\theta_j$越来越小。</p>
<h3 id="24深度学习花式归一化之batchlayerinstancegroup-normalization">24、⭐深度学习花式归一化之Batch/Layer/Instance/Group Normalization</h3>
<p><strong>✒️Batch Normalization</strong></p>
<ul>
<li>⭐<strong>核心过程</strong>：顾名思义，对一个batch内的数据计算均值和方差，将数据归一化为均值为0、方差为1的正态分布数据，最后用对数据进行缩放和偏移来还原数据本身的分布，如下图所示。</li>
</ul>
<center>

</center>
<ul>
<li><strong>Batch Norm 1d</strong>
输入是b*c, 输出是b*c，即在每个维度上进行normalization。</li>
<li><strong>Batch Norm 2d</strong>
例如输入是b*c*h*w，则计算normlization时是对每个通道，求b<em>h</em>w内的像素求均值和方差，输出是1*c*1*1。</li>
</ul>
<center>

</center>
<center>

</center>
<ul>
<li><strong>BN测试时和训练时不同，测试时使用的是全局训练数据的滑动平均的均值和方差。</strong></li>
<li>作用：a、防止过拟合 b、加速网络的收敛，internal covariate shift导致上层网络需要不断适应底层网络带来的分布变化 c、缓解梯度爆炸和梯度消失</li>
<li>局限：依赖于batch size，适用于batch size较大的情况</li>
</ul>
<p><strong>✒️改进：</strong></p>
<ul>
<li>Layer normalization: 对每个样本的所有特征进行归一化，如N*C*H*W，对每个C*H*W进行归一化，得到N个均值和方差。</li>
<li>Instance normalization: 对每个样本的每个通道特征进行归一化，如N*C*H*W，对每个H*W进行归一化，得到N*C个均值和方差。</li>
<li>Group normalization：每个样本按通道分组进行归一化，如N*C*H*W，对每个C*H*W，在C维度上分成g组，则共有N*g个均值和方差。</li>
</ul>
<center>

</center>
<h3 id="25深度学习常用优化器介绍">25、⭐深度学习常用优化器介绍</h3>
<p>参考https://zhuanlan.zhihu.com/p/261695487，修正了其中的一些错误。</p>
<p>(1)<strong>⭐SGD</strong></p>
<p>a. ✒️<strong>公式</strong></p>
<center>

</center>
<p>其中$\alpha$是学习率，$g_t$是当前参数的梯度。</p>
<p>b. ✒️<strong>优点</strong>：每次只用一个样本更新模型参数，训练速度快。</p>
<p>c. ✒️<strong>缺点</strong>：容易陷入局部最优；沿陡峭方向振荡，而沿平缓维度进展缓慢，难以迅速收敛</p>
<p>(2) ⭐<strong>SGD with momentum</strong></p>
<p>a. ✒️<strong>公式</strong></p>
<center>

</center>
<p>其中$m_t$为动量。</p>
<p>b. ✒️<strong>优点</strong>：可借助动量跳出局部最优点。</p>
<p>c. ✒️<strong>缺点</strong>：容易在局部最优点里来回振荡。</p>
<p>(3) ⭐<strong>AdaGrad</strong>：经常更新的参数已经收敛得比较好了，应该减少对它的关注，即降低其学习率；对于不常更新的参数，模型学习的信息过少，应该增加对它的关注，即增大其学习率。</p>
<p>a. ✒️<strong>公式</strong>
$$w_{t+1}=w_t-\alpha \cdot g_t / \sqrt{V_t}=w_t-\alpha \cdot g_t / \sqrt{\sum_{\tau=1}^t g_\tau^2}$$
其中Vt是二阶动量，为累计梯度值的平方和，与参数更新频繁程度成正比。</p>
<p>b. ✒️<strong>优点</strong>：稀疏数据场景下表现好；自适应调节学习率。</p>
<p>c. ✒️<strong>缺点</strong>：Vt单调递增，使得学习率单调递减为0，从而使得训练过程过早结束。</p>
<p>(4) ⭐<strong>RMSProp</strong>：AdaGrad的改进版，不累计所有历史梯度，而是过去一段时间窗口内的历史梯度。</p>
<p>a. ✒️<strong>公式</strong>
$$\begin{aligned} w_{t+1} &amp; =w_t-\alpha \cdot g_t / \sqrt{V_t} \ &amp; =w_t-\alpha \cdot g_t / \sqrt{\beta_2 \cdot V_{t-1}+\left(1-\beta_2\right) g_t^2}\end{aligned}$$
即把Vt换成指数移动平均。</p>
<p>b. ✒️<strong>优点</strong>：避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p>
<p>(5) ⭐<strong>Adam</strong>：=Adaptive + momentum，即结合了momentum一阶动量+RMSProp二阶动量。</p>
<p>a. ✒️<strong>公式</strong>
$$\begin{aligned} w_{t+1} &amp; =w_t-\alpha \cdot m_t / \sqrt{V_t} \ &amp; =w_t-\alpha \cdot\left(\beta_1 \cdot m_{t-1}+\left(1-\beta_1\right) \cdot g_t\right) / \sqrt{\beta_2 \cdot V_{t-1}+\left(1-\beta_2\right) g_t^2}\end{aligned}$$</p>
<p>b. ✒️<strong>优点</strong>：通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和在鞍点的静止。</p>
<p>c. ✒️<strong>缺点</strong>：二阶动量是固定历史时间窗口的累积，窗口的变化可能导致Vt振荡，而不是单调变化，从而导致训练后期学习率的振荡，模型不收敛，可通过</p>
<center>

</center>
来修正，保证学习率单调递减；自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果，从而错过全局最优解。
<hr>
<p>整理这篇文章不易，喜欢的话可以关注我&ndash;&gt;<strong>无名氏，某乎和小红薯同名，WX：无名氏的胡言乱语。</strong> 定期分享算法笔试、面试干货。</p>
]]></description></item><item><title>强化学习笔记 [19] | AlphaGo Zero强化学习原理</title><link>https://jianye0428.github.io/posts/rl_learning_note_19/</link><pubDate>Sun, 25 Feb 2024 19:53:22 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_19/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。</p>
<p>本篇主要参考了AlphaGo Zero的<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, <a href="https://www.hhyz.me/2018/08/08/2018-08-08-AlphaGO-Zero/"target="_blank" rel="external nofollow noopener noreferrer">AlphaGo Zero综述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和AlphaGo Zero Cheat Sheet。</p>
<h1 id="1-alphago-zero模型基础">1. AlphaGo Zero模型基础</h1>
<p>AlphaGo Zero不需要学习人类的棋谱，通过自我对弈完成棋力提高。主要使用了两个模型，第一个就是我们上一节介绍MCTS树结构，另一个是一个神经网络。MCTS上一篇已经有基本介绍了，对于神经网络，它的输入是当前的棋局状态，输出两部分，第一部分输出是在当前棋局状态下各个可能的落子动作对应的获胜概率p，可以简单理解为Actor-Critic策略函数部分。另一部分输出为获胜或者失败的评估[-1,1]，可以简单理解为Actor-Critic价值函数部分。</p>
<p>AlphaGo Zero的行棋主要是由MCTS指导完成的，但是在MCTS搜索的过程中，由于有一些不在树中的状态需要仿真，做局面评估，因此需要一个简单的策略来帮助MCTS评估改进策略，这个策略改进部分由前面提到的神经网络完成。</p>
<p>这两部分的关系如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">AlphaGo Zero 中的MCTS和NN</div>
</center>
<br>
<p>具体AlphaGo Zero的MCTS如何搜索，神经网络如何训练，如何指导MCTS搜索我们在后面再讲。</p>
<h1 id="2-alphago-zero的训练过程简介">2. AlphaGo Zero的训练过程简介</h1>
<p>在讨论AlphaGo Zero的MCTS如何搜索，神经网络如何训练等细节之前，我们先看看AlphaGo Zero的训练过程是什么样的。</p>
<p>AlphaGo Zero训练过程主要分为三个阶段：自我对战学习阶段，训练神经网络阶段和评估网络阶段。</p>
<p>自我对战学习阶段主要是AlphaGo Zero自我对弈，产生大量棋局样本的过程，由于AlphaGo Zero并不使用围棋大师的棋局来学习，因此需要自我对弈得到训练数据用于后续神经网络的训练。在自我对战学习阶段，每一步的落子是由MCTS搜索来完成的。在MCTS搜索的过程中，遇到不在树中的状态，则使用神经网络的结果来更新MCTS树结构上保存的内容。在每一次迭代过程中，在每个棋局当前状态 $s$ 下，每一次移动使用1600次MCTS搜索模拟。最终MCTS给出最优的落子策略 $π$ ,这个策略 $π$ 和神经网络的输出 $p$ 是不一样的。当每一局对战结束后，我们可以得到最终的胜负奖励 $z$ ,1或者-1. 这样我们可以得到非常多的样本 $(s,π,z)$,这些数据可以训练神经网络阶段。</p>
<p>在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合(s,π,z)(�,�,�),训练我们神经网络的模型参数。训练的目的是对于每个输入 $s$, 神经网络输出的 $p,v$和我们训练样本中的 $π$, $z$差距尽可能的少。这个损失函数 $L$ 其实是很简单的：</p>
<p>$$L=(z-v)^2-\pi^Tlog(p)+c||\theta||^2$$</p>
<p>损失函数由三部分组成，第一部分是均方误差损失函数，用于评估神经网络预测的胜负结果和真实结果之间的差异。第二部分是交叉熵损失函数，用于评估神经网络的输出策略和我们MCTS输出的策略的差异。第三部分是L2正则化项。</p>
<p>通过训练神经网络，我们可以优化神经网络的参数 $θ$,用于后续指导我们的MCTS搜索过程。</p>
<p>当神经网络训练完毕后，我们就进行了评估阶段，这个阶段主要用于确认神经网络的参数是否得到了优化，这个过程中，自我对战的双方各自使用自己的神经网络指导MCTS搜索，并对战若干局，检验AlphaGo Zero在新神经网络参数下棋力是否得到了提高。除了神经网络的参数不同，这个过程和第一阶段的自我对战学习阶段过程是类似的。</p>
<h1 id="3-alphago-zero的神经网络结构">3. AlphaGo Zero的神经网络结构</h1>
<p>在第二节我们已经讨论了AlphaGo Zero的主要训练过程，但是还有两块没有讲清楚，一是AlphaGo Zero的MCTS搜索过程是怎么样的，二是AlphaGo Zero的神经网络的结构具体是什么样的。这一节我们来看看AlphaGo Zero的神经网络的细节。</p>
<p>首先我们看看AlphaGo Zero的输入，当前的棋局状态。由于围棋是19x19的361个点组成的棋局，每个点的状态有二种：如果当前是黑方行棋，则当前有黑棋的点取值1，有白棋或者没有棋子的位置取值为0，反过来，如果当前是白方行棋，则当前有白棋的点取值1，有黑棋或者没有棋子的位置取值为0。同时，为了提供更多的信息，输入的棋局状态不光只有当前的棋局状态，包括了黑棋白棋各自前8步对应的棋局状态。除了这16个棋局状态，还有一个单独的棋局状态用于标识当前行棋方，如果是当前黑棋行棋，则棋局状态上标全1，白棋则棋局状态上标全0。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Game State</div>
</center>
<br>
<p>最终神经网络的输入是一个19x19x17的张量。里面包含黑棋和白棋的最近8步行棋状态和当前行棋方的信息。</p>
<p>接着我们看看神经网络的输出，神经网络的输出包括策略部分和价值部分。对于策略部分，它预测当前各个行棋点落子的概率。由于围棋有361个落子点，加上还可以Pass一手，因此一共有362个策略端概率输出。对于价值端，输出就简单了，就是当前局面胜负的评估值，在[-1,1]之间。</p>
<p>看完了神经网络的输入和输出，我们再看看神经网络的结构，主要是用CNN组成的深度残差网络。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>在19x19x17的张量做了一个基本的卷积后，使用了19层或者39层的深度残差网络，这个是ResNet的经典结构。理论上这里也可以使用DenseNet等其他流行的网络结构。神经网络的损失函数部分我们在第二节已经将了。整个神经网络就是为了当MCTS遇到没有见过的局面时，提供的当前状态下的局面评估和落子概率参考。这部分信息会被MCTS后续综合利用。</p>
<h1 id="4-alphago-zero的mcts搜索">4. AlphaGo Zero的MCTS搜索</h1>
<p>　　　　现在我们来再看看AlphaGo Zero的MCTS搜索过程，在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们已经介绍了MCTS的基本原理，和4个主要的搜索阶段：选择，扩展，仿真和回溯。和上一篇的内容相比，这里MCTS的不同主要体现在树结构上保存的信息不同，进而UCT的计算公式也稍有不同。最后MCTS搜索完毕后，AlphaGo Zero也有自己选择真正落子点的策略。</p>
<p>　　　　在上一篇里，我们的MCTS上保存的数据很简单，就是下的总盘数和赢的总盘数。在AlphaGo Zero这里，我们保存的信息会多一些。主要包括下面的4部分：</p>
<ul>
<li>$N(s,a)$:记录边的访问次数</li>
<li>$W(s,a)$: 合计行动价值</li>
<li>$Q(s,a)$:平均行动价值</li>
<li>$P(s,a)$:选择该条边的先验概率</li>
</ul>
<p>其中 $s$ 为当前棋局状态，$a$ 为某一落子选择对应的树分支。</p>
<p>有了MCTS上的数据结构，我们看看AlphaGo Zero的MCTS搜索的4个阶段流程：</p>
<p>首先是选择，在MCTS内部，出现过的局面，我们会使用UCT选择子分支。子分支的UCT原理和上一节一样。但是具体的公式稍有不同，如下：</p>
<p>$$\begin{gathered}
U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)} \\
a_t=\arg\max_a(Q(s_t,a)+U(s_t,a))
\end{gathered}$$</p>
<p>最终我们会选择 $Q+U$最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数,上一篇已讲过。</p>
<p>如果到了没有到终局的MCTS叶子节点，那么我们就需要进入MCTS的第二步，扩展阶段,以及后续的第三步仿真阶段。我们这里一起讲。对于叶子节点状态s�，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL��落子的概率p�和对应的价值v�,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为：</p>
<p>$$\{N(s_L,a)=0,W(s_L,a)=0,Q(s_L,a)=0,P(s_L,a)=P_a\}$$</p>
<p>这个过程如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>这样扩展后，之前的叶子节点 $s$，现在就是内部节点了。做完了扩展和仿真后，我们需要进行回溯，将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点 $L$ 依次向根节点回溯，并依次更新上层分支数据结构如下：</p>
<p>$$\begin{gathered}
N(s_t,a_t)=N(s_t,a_t)+1 \\
W(s_t,a_t)=W(s_t,a_t)+v \\
Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}
\end{gathered}$$</p>
<p>这个MCTS搜索过程在一次真正行棋前，一般会进行约1600次搜索，每次搜索都会进行上述4个阶段。</p>
<p>这上千次MCTS搜索完毕后，AlphaGo Zero就可以在MCTS的根节点 $s$ 基于以下公式选择行棋的MCTS分支了:</p>
<p>$$\pi(a|s)=\frac{N(s,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}$$</p>
<p>其中，$τ$ 为温度参数，控制探索的程度，$τ$ 越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。每一次完整的自我对弈的前30步，参数 $τ=1$，这是早期鼓励探索的设置。游戏剩下的步数，该参数将逐渐降低至0。如果是比赛，则直接为0.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>同时在随后的时间步中，这个MCTS搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 。</p>
<p>以上就是AlphaGo Zero MCTS搜索的过程。</p>
<h1 id="5-alphago-zero小结与强化学习系列小结">5. AlphaGo Zero小结与强化学习系列小结</h1>
<p>AlphaGo Zero巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。两者一主一辅，非常优雅的解决了这类状态完全可见，信息充分的棋类问题。</p>
<p>当然这类强化学习算法只对特定的这类完全状态可见，信息充分的问题有效，遇到信息不对称的强化学习问题，比如星际，魔兽之类的对战游戏问题，这个算法就不那么有效了。要推广AlphaGo Zero的算法到大多数普通强化学习问题还是很难的。因此后续强化学习算法应该还有很多发展的空间。</p>
<p>至此强化学习系列就写完了，之前预计的是写三个月，结果由于事情太多，居然花了大半年。但是总算还是完成了，没有烂尾。生活不易，继续努力！</p>
]]></description></item><item><title>强化学习笔记 [18] | 基于模拟的搜索与蒙特卡罗树搜索(MCTS)</title><link>https://jianye0428.github.io/posts/rl_learning_note_18/</link><pubDate>Sun, 25 Feb 2024 19:53:18 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_18/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。</p>
<p>本篇主要参考了UCL强化学习课程的第八讲，第九讲部分。</p>
<h1 id="1-基于模拟的搜索概述">1. 基于模拟的搜索概述</h1>
<p>什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这是数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。</p>
<p>那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点 $S_t$ 开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以 $S_t$ 为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到 $S_t$状态最应该采用的动作 $A_t$。前向搜索的sub-MDP如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">forward search sub-MDP</div>
</center>
<br>
<p>前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。</p>
<h1 id="2-简单蒙特卡罗搜索">2. 简单蒙特卡罗搜索</h1>
<p>首先我们看看基于模拟的搜索中比较简单的一种方法：简单蒙特卡罗搜索。</p>
<p>简单蒙特卡罗搜索基于一个强化学习模型 $M_v$ 和一个模拟策略 $π$.在此基础上，对于当前我们要选择动作的状态 $S_t$, 对每一个可能采样的动作 $a∈A$,都进行 $K$ 轮采样，这样每个动作 $a$ 都会得到 $K$ 组经历完整的状态序列(episode)。即：</p>
<p>$$\{S_t,a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>现在对于每个 $(S_t,a)$ 组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1K\sum_{k=1}^KG_t\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别,那么简单蒙特卡罗搜索也太慢了。同时，由于使用蒙特卡罗法计算其动作价值函数，模拟采样得到的一些中间状态和对应行为的价值就被忽略了，这部分数据能不能利用起来呢？</p>
<p>下面我们看看蒙特卡罗树搜索(Monte-Carlo Tree Search，以下简称MCTS)怎么优化这个问题的解决方案。</p>
<h1 id="3-mcts的原理">3. MCTS的原理</h1>
<p>MCTS摒弃了简单蒙特卡罗搜索里面对当前状态 $S_t$ 每个动作都要进行K次模拟采样的做法，而是总共对当前状态 $S_t$进行K次采样，这样采样到的动作只是动作全集 $A$ 中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。</p>
<p>在MCTS中，基于一个强化学习模型 $M_v$和一个模拟策略$π$，当前状态 $S_t$ 对应的完整的状态序列(episode)是这样的:</p>
<p>$$\{S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算 $Q(st,a)$和最大 $Q(s_t,a)$对应的动作。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1{N(S_t,a)}\sum_{k=1}^K\sum_{u=t}^T1(S_{uk}=S_t,A_{uk}=a)G_u\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>MCTS搜索的策略分为两个阶段：第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使 $ϵ−$贪婪策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。</p>
<p>这里讲到的是最经典的强化学习终MCTS的用户，每一步都有延时奖励，但是在棋类之类的零和问题中，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。</p>
<h1 id="4-上限置信区间算法uct">4. 上限置信区间算法UCT</h1>
<p>在讨论棋类游戏的MCTS搜索之前，我们先熟悉下上限置信区间算法(Upper Confidence Bound Applied to Trees, 以下简称UCT)。它是一种策略算法，我们之前最常用的是 $ϵ−$贪婪策略。但是在棋类问题中，UCT更常使用。</p>
<p>在棋类游戏中，经常有这样的问题，我们发现在某种棋的状态下，有2个可选动作，第一个动作历史棋局中是0胜1负，第二个动作历史棋局中是8胜10负，那么我们应该选择哪个动作好呢？如果按 $ϵ−$贪婪策略，则第二个动作非常容易被选择到。但是其实虽然第一个动作胜利0%，但是很可能是因为这个动作的历史棋局少，数据不够导致的，很可能该动作也是一个不错的动作。那么我们如何在最优策略和探索度达到一个选择平衡呢？ $ϵ−$贪婪策略可以用，但是UCT是一个更不错的选择。</p>
<p>UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度吗，一个常用的公式如下：</p>
<p>$$\text{score}=\left.\frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}\right.$$</p>
<p>其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$ 是所有模拟次数，c 是探索常数，理论值为$√2$，可根据经验调整，$c$ 越大就越偏向于广度搜索，$c$ 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。</p>
<p>比如对于下面的棋局，对于根节点来说，有3个选择，第一个选择7胜3负，第二个选择5胜3负，第三个选择0胜3负。</p>
<p>如果我们取c=10,则第一个节点的分数为：$$score(7,10)=7/10+C\cdot\sqrt{\frac{\log(21)}{10}}\approx6.2$$</p>
<p>第二个节点的分数为：$$score(5,8)=5/8+C\cdot\sqrt{\frac{\log(21)}8}\approx6.8$$</p>
<p>第三个节点的分数为：$$score(0,3)=0/3+C\cdot\sqrt{\frac{\log(21)}3}\approx10$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>可见，由于我们把探索率 $c$ 设置的比较大，第三个节点是被UCT选中要执行的动作节点。当然如果我们把c设置的比较小的话，第一个或者第二个可能就变成最大的分数了。</p>
<h1 id="5-棋类游戏mcts搜索">5. 棋类游戏MCTS搜索</h1>
<p>在像中国象棋，围棋这样的零和问题中，一个动作只有在棋局结束才能拿到真正的奖励，因此我们对MCTS的搜索步骤和树结构上需要根据问题的不同做一些细化。</p>
<p>对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步，如下图(图来自维基百科)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。</p>
<p>第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。</p>
<p>第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。</p>
<p>第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。</p>
<p>以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。</p>
<h1 id="6-mcts小结">6. MCTS小结</h1>
<p>MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。</p>
]]></description></item><item><title>强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架</title><link>https://jianye0428.github.io/posts/rl_learning_note_17/</link><pubDate>Sun, 25 Feb 2024 19:53:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_17/</guid><description><![CDATA[<h1 id="强化学习十七-基于模型的强化学习与dyna算法框架httpswwwcnblogscompinardp10384424html"><a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h1>
<p>在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。</p>
<p>本篇主要参考了UCL强化学习课程的第8讲和Dyna-2的<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-基于模型的强化学习简介">1. 基于模型的强化学习简介</h1>
<p>基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率 $P^a_{ss&rsquo;}$。</p>
<p>而基于模型的强化学习则会尝试从环境的模型去学习，一般是下面两个相互独立的模型：</p>
<ul>
<li>一个是状态转化预测模型，输入当前状态 $s$和动作 $a$，预测下一个状态 $s&rsquo;$。</li>
<li>另一个是奖励预测模型，输入当前状态 $s$和动作 $a$，预测环境的奖励 $r$。</li>
</ul>
<p>即模型可以描述为下面两个式子：</p>
<p>$$\begin{gathered}S_{t+1}\sim P(S_{t+1}|S_t,A_t)\\R_{t+1}\sim R(R_{t+1}|S_t,A_t)\end{gathered}$$</p>
<p>如果模型 $P$, $R$ 可以准确的描述真正的环境的转化模型，那么我们就可以基于模型来预测，当有一个新的状态 $S$ 和动作 $A$到来时，我们可以直接基于模型预测得到新的状态和动作奖励，不需要和环境交互。当然如果我们的模型不好，那么基于模型预测的新状态和动作奖励可能错的离谱。</p>
<p>从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的主要区别：即基于模型的强化学习是从模型中学习，而不基于模型的强化学习是从和环境交互的经历去学习。</p>
<p>下面这张图描述了基于模型的强化学习的思路：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Model-based RL</div>
</center>
<br>
<h1 id="2-基于模型的强化学习算法训练流程">2. 基于模型的强化学习算法训练流程</h1>
<p>这里我们看看基于模型的强化学习算法训练流程，其流程和我们监督学习算法是非常类似的。</p>
<p>假设训练数据是若干组这样的经历：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,R_2,\ldots,S_T$$</p>
<p>对于每组经历，我们可以将其转化为 $T−1$ 组训练样本，即：</p>
<p>$$\begin{gathered}
S_1,A_1\to S_2,S_1,A_1\to R_2 \\
S_2,A_2\to S_3,S_2,A_2\to R_3 \\
&hellip;&hellip; \\
S_{T-1},A_{T-1}\rightarrow S_T,~S_{T_1},A_{T-1}\rightarrow R_T
\end{gathered}$$</p>
<p>右边的训练样本一起组成了一个分类模型或密度估计模型，输入状态和动作，输出下一个状态。 右边的训练样本一起组成了一个回归模型训练集，输入状态和动作，输出动作奖励值。</p>
<p>至此我们的强化学习求解过程和传统的监督学习算法没有太多区别了，可以使用传统的监督学习算法来求解这两个模型。</p>
<p>当然还可以更简单，即通过对训练样本进行查表法进行统计，直接得到 $P(S_{t+1}|S_t,A_t)$ 的概率和 $R(R_{t+1}|S_t,A_t)$ 的平均值，这样就可以直接预测。比使用模型更简单。</p>
<p>此外，还有其他的方法可以用来得到 $P(S_{t+1}|S_t,A_t)$和 $R(R_{t+1}|S_t,A_t)$，这个我们后面再讲。</p>
<p>虽然基于模型的强化学习思路很清晰，而且还有不要和环境持续交互优化的优点，但是用于实际产品还是有很多差距的。主要是我们的模型绝大多数时候不能准确的描述真正的环境的转化模型，那么使用基于模型的强化学习算法得到的解大多数时候也不是很实用。那么是不是基于模型的强化学习就不能用了呢？也不是，我们可以将基于模型的强化学习和不基于模型的强化学习集合起来，取长补短，这样做最常见的就是Dyna算法框架。</p>
<h1 id="3-dyna算法框架">3. Dyna算法框架</h1>
<p>Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。如果用和第一节类似的图，可以表示如下图，和第一节的图相比，多了一个“Direct RL“的箭头，这正是不基于模型的强化学习的思路。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna算法示意图</div>
</center>
<br>
<p>Dyna算法框架和不同的具体的不基于模型的强化学习一起，可以得到具体的不同算法。如果我们使用基于价值函数的Q-Learning，那么我们就得到了Dyna-Q算法。我们基于Dyna-Q来看看Dyna算法框架的一般流程.</p>
<h1 id="4-dyna-q算法流程">4. Dyna-Q算法流程</h1>
<p>这里我们给出基于价值函数的Dyna-Q算法的概要流程。假设模型使用的是查表法。</p>
<ul>
<li>(1). 初始化任意一个状态 $s$,和任意一个动作 $a$ 对应的状态价值 $Q(s,a)$, 初始化奖励模型 $R(s,a)$和状态模型 $P(s,a)$</li>
<li>(2). for $i=1$ to 最大迭代次数T：
<ul>
<li>(a) $S \leftarrow \text{current state}$</li>
<li>(b) $A \leftarrow \text{ϵ−greedy(S,Q)}$</li>
<li>(c) 执行动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>(d) 使用Q-Learning更新价值函数：$Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
<li>(e) 使用 $S,A,S^{\prime}$ 更新状态模型 $P(s,a)$，使用 $S,A,R$ 更新状态模型 $R(s,a)$</li>
<li>(f) $\text{for} \space \space j=1 \space \space \text{to} \text{最大次数}n$：
<ul>
<li>(i) 随机选择一个之前出现过的状态 $S$ , 在状态 $S$ 上出现过的动作中随机选择一个动作 $A$</li>
<li>(ii) 基于模型 $P(S,A)$ 得到 $S&rsquo;$, 基于模型 $R(S,A)$ 得到 $R$</li>
<li>(iii) 使用Q-Learning更新价值函数: $Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>从上面的流程可以看出，Dyna框架在每个迭代轮中，会先和环境交互，并更新价值函数和（或）策略函数，接着进行n次模型的预测，同样更新价值函数和（或）策略函数。这样同时利用上了和环境交互的经历以及模型的预测。</p>
<h1 id="5-dyna-2算法框架">5. Dyna-2算法框架</h1>
<p>在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为<strong>永久性记忆</strong>（permanent memory）和<strong>瞬时记忆</strong>（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。</p>
<p>永久性记忆的Q函数定义为：</p>
<p>$$Q(S,A)=\phi(S,A)^T\theta $$</p>
<p>瞬时记忆的Q函数定义为：</p>
<p>$$Q^{\prime}(S,A)=\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>组合起来后记忆的Q函数定义为：</p>
<p>$$\overline{Q}(S,A)=\phi(S,A)^T\theta+\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似于 $SARSA(λ)$</p>
<p>以下是Dyna-2的算法流程：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna-2 算法流程</div>
</center>
<br>
<h1 id="6-基于模型的强化学习总结">6. 基于模型的强化学习总结</h1>
<p>基于模型的强化学习一般不单独使用，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。对于模型部分，我们可以用查表法和监督学习法等方法，预测或者采样得到模拟的经历。而对于非模型部分，使用前面的Q-Learning系列的价值函数近似，或者基于Actor-Critic的策略函数的近似都是可以的。</p>
<p>除了Dyna算法框架，我们还可以使用基于模拟的搜索(simulation-based search)来结合基于模型的强化学习和不基于模型的强化学习,并求解问题。这部分我们在后面再讨论。</p>
]]></description></item><item><title>强化学习笔记 [16] | 深度确定性策略梯度(DDPG)</title><link>https://jianye0428.github.io/posts/rl_learning_note_16/</link><pubDate>Sun, 25 Feb 2024 19:53:12 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_16/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10334127.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十五) A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DDPG)。</p>
<p>本篇主要参考了DDPG的<a href="https://arxiv.org/pdf/1509.02971.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-从随机策略到确定性策略">1. 从随机策略到确定性策略</h1>
<p>从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。PG(Policy Gradient)我们在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里已经讨论过。那什么是确定性策略梯度(Deterministic Policy Gradient，以下简称DPG)呢？</p>
<p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成：</p>
<p>$$\pi_\theta(s)=a$$</p>
<h1 id="2-从dpg到ddpg">2. 从DPG到DDPG</h1>
<p>在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>其中状态的采样空间为$\rho^\pi$, $\nabla_\theta log\pi_\theta(s,a)$是分值函数，可见随机性策略梯度需要在整个动作的空间$\pi_\mathrm{\theta}$进行采样。</p>
<p>而DPG基于Q值的确定性策略梯度的梯度计算公式是：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi}[\nabla_\theta\pi_\theta(s)\nabla_aQ_\pi(s,a)|<em>{a=\pi</em>\theta(s)}]$$</p>
<p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p>
<p>而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？</p>
<h1 id="3-ddpg的原理">3. DDPG的原理</h1>
<p>DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。可以复习<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>DDQN的当前Q网络负责对当前状态 $S$ 使用 $ϵ$−贪婪法选择动作 $A$，执行动作 $A$,获得新状态 $S&rsquo;$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态 $S&rsquo;$使用贪婪法选择动作 $A&rsquo;$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。</p>
<p>DDQN的目标Q网络则负责基于经验回放池计算目标Q值, 提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。</p>
<p>现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要 $ϵ$−贪婪法这样的选择方法，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态 $S&rsquo;$ 使用贪婪法选择动作 $A&rsquo;$，这部分工作由于用来估计目标Q值，因此可以放到Actor目标网络完成。</p>
<p>基于经验回放池和目标Actor网络提供的 $S&rsquo;$, $A&rsquo;$ 计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。</p>
<p>此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。</p>
<p>有了上面的思路，我们总结下DDPG 4个网络的功能定位：</p>
<ul>
<li>
<p>(1). <strong>Actor当前网络</strong>: 负责策略网络参数 $θ$的迭代更新，负责根据当前状态 $S$选择当前动作 $A$，用于和环境交互生成 $S&rsquo;$, $R$。</p>
</li>
<li>
<p>(2). <strong>Actor目标网络</strong>: 负责根据经验回放池中采样的下一状态 $S&rsquo;$ 选择最优下一动作$A&rsquo;$。网络参数 $θ&rsquo;$定期从 $θ$复制。</p>
</li>
<li>
<p>(3). <strong>Critic当前网络</strong>: 负责价值网络参数 $w$的迭代更新，负责计算负责计算当前Q值 $Q(S,A,w)$。目标Q值$y_i=R+γQ&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$</p>
</li>
<li>
<p>(4). <strong>Critic目标网络</strong>: 负责计算目标Q值中的 $Q&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$部分。网络参数 $w&rsquo;$ 定期从 $w$复制。</p>
</li>
</ul>
<p>DDPG除了这4个网络结构，还用到了经验回放，这部分用于计算目标Q值，和DQN没有什么区别，这里就不展开了。</p>
<p>此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w$′=$w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：</p>
<p>$$\begin{gathered}
w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</p>
<p>其中 $τ$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。</p>
<p>同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作 $A$会增加一定的噪声 $N$, 即最终和环境交互的动作 $A$ 的表达式是：</p>
<p>$$A=\pi_\theta(S)+\mathcal{N}$$</p>
<p>最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：</p>
<p>$$J(w)=\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>而对于 Actor当前网络，其损失函数就和之前讲的PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：</p>
<p>$$\nabla_J(\theta)=\frac1m\sum_{j=1}^m[\nabla_aQ_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>\theta(s)}\nabla_\theta\pi_{\theta(s)}|_{s=s_i}]$$</p>
<p>这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作 $a_1$和$a_2$，从Critic当前网络得到了两个反馈的 $Q$ 值，分别是 $Q_1$,$Q_2$，假设 $Q_1&gt;Q_2$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加 $a_1$的概率，降低$a_2$的概率，也就是说，Actor想要尽可能的得到更大的Q值。所以我们的Actor的损失可以简单的理解为得到的反馈Q值越大损失越小，得到的反馈Q值越小损失越大，因此只要对状态估计网络返回的Q值取个负号即可，即：</p>
<p>$$J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,w)$$</p>
<h1 id="4-ddpg算法流程">4. DDPG算法流程</h1>
<p>这里我们总结下DDPG的算法流程</p>
<p>输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为 $θ$,$θ&rsquo;$,$w$,$w&rsquo;$,衰减因子 $γ$, 软更新系数 $τ$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$。最大迭代次数 $T$。随机噪音函数 $\mathcal{N}$</p>
<p>输出：最优Actor当前网络参数 $θ$,Critic当前网络参数 $w$</p>
<ul>
<li>(1). 随机初始化$θ$,$w$, $w$′=$w$,$θ$′=$θ$。清空经验回放的集合$D$</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor当前网络基于状态 $S$ 得到动作 $A=π_θ(ϕ(S))+\mathcal{N}$</li>
<li>c) 执行动作$A$,得到新状态$S$′,奖励$R$,是否终止状态%is_end$</li>
<li>d) 将 ${ϕ(S), A, R, ϕ(S&rsquo;), is_end}$ 这个五元组存入经验回放集合$D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本${\phi(S_j),A_j,R_j,\phi(S_j^{\prime}),is_end_j},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\pi_{\theta^{\prime}}(\phi(S_j^{\prime})),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Critic当前网络的所有参数 $w$</li>
<li>h) 使用 $\begin{aligned}J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,\theta)\end{aligned}$，通过神经网络的梯度反向传播来更新Actor当前网络的所有参数 $θ$</li>
<li>i) 如果 i%C=1, 则更新Critic目标网络和Actor目标网络参数：
<ul>
<li>$$\begin{gathered} w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</li>
</ul>
</li>
<li>j) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤(b)</li>
</ul>
</li>
</ul>
<p>以上就是DDPG算法的主流程，要注意的是上面2.f中的 $\pi_{\theta^{\prime}}(\phi(S_j^{\prime}))$ 是通过Actor目标网络得到，而 $Q^{\prime}(\phi(S_i^{\prime}),\pi_{\theta^{\prime}}(\phi(S_i^{\prime})),w^{\prime})$ 则是通过Critic目标网络得到的。</p>
<h1 id="5-ddpg实例">5. DDPG实例</h1>
<p>这里我们给出DDPG第一个算法实例，代码主要参考自莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG_update.py"target="_blank" rel="external nofollow noopener noreferrer">Github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。增加了测试模型效果的部分，优化了少量参数。代码详见：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py</p>
<p>这里我们没有用之前的CartPole游戏，因为它不是连续动作。我们使用了Pendulum-v0这个游戏。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a href="https://github.com/openai/gym/wiki/Pendulum-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。输入状态是角度的sin，cos值，以及角速度。一共三个值。动作是一个连续的力矩值。</p>
<p>两个Actor网络和两个Critic网络的定义参见：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_bound</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;scaled_a&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_c</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_l1</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_s</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_s&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_a&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">w1_s</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">w1_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>  <span class="c1"># Q(s,a)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor当前网络和Critic当前网络损失函数的定义参见：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">q_target</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">td_error</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ce_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">a_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>    <span class="c1"># maximize the q</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">atrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">a_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ae_params</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor目标网络和Critic目标网络参数软更新，Actor当前网络和Critic当前网络反向传播更新部分的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># soft target replacement</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">soft_replace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">MEMORY_CAPACITY</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">bt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">ba</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">br</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs_</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">:</span> <span class="n">ba</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">:</span> <span class="n">br</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_</span><span class="p">:</span> <span class="n">bs_</span><span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余的可以对照算法和代码一起学习，应该比较容易理解。</p>
<h1 id="6-ddpg总结">6. DDPG总结</h1>
<p>DDPG参考了DDQN的算法思想吗，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p>
<p>到此，我们的Policy Based RL系列也讨论完了，而在更早我们讨论了Value Based RL系列，至此，我们还剩下Model Based RL没有讨论。后续我们讨论Model Based RL的相关算法。</p>
]]></description></item><item><title>强化学习笔记 [15] | A3C</title><link>https://jianye0428.github.io/posts/rl_learning_note_15/</link><pubDate>Sun, 25 Feb 2024 15:36:01 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_15/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C的算法原理和算法流程。</p>
<p>本文主要参考了A3C的<a href="http://proceedings.mlr.press/v48/mniha16.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，以及ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-a3c的引入">1. A3C的引入</h1>
<p>上一篇Actor-Critic算法的代码，其实很难收敛，无论怎么调参，最后的CartPole都很难稳定在200分，这是Actor-Critic算法的问题。但是我们还是有办法去有优化这个难以收敛的问题的。</p>
<p>回忆下之前的DQN算法，为了方便收敛使用了经验回放的技巧。那么我们的Actor-Critic是不是也可以使用经验回放的技巧呢？当然可以！不过A3C更进一步，还克服了一些经验回放的问题。经验回放有什么问题呢？ 回放池经验数据相关性太强，用于训练的时候效果很可能不佳。举个例子，我们学习下棋，总是和同一个人下，期望能提高棋艺。这当然没有问题，但是到一定程度就再难提高了，此时最好的方法是另寻高手切磋。</p>
<p>A3C的思路也是如此，它<font color=green>利用多线程的方法，同时在多个线程里面分别和环境进行交互学习，每个线程都把学习的成果汇总起来，整理保存在一个公共的地方</font>。并且，定期从公共的地方把大家的齐心学习的成果拿回来，指导自己和环境后面的学习交互。</p>
<p>通过这种方法，A3C避免了经验回放相关性过强的问题，同时做到了异步并发的学习模型。</p>
<h1 id="2-a3c的算法优化">2. A3C的算法优化</h1>
<p>现在我们来看看相比Actor-Critic，A3C到底做了哪些具体的优化。</p>
<p>相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p>我们首先来看这个异步训练框架，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">异步训练框架</div>
</center>
<br>
<p>图中上面的Global Network就是上一节说的共享的公共部分，主要是一个公共的神经网络模型，这个神经网络包括Actor网络和Critic网络两部分的功能。下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。</p>
<p>每个线程和环境交互到一定量的数据后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度却并不更新自己线程里的神经网络，而是去更新公共的神经网络。也就是n个线程会独立的使用累积的梯度分别更新公共部分的神经网络模型参数。每隔一段时间，线程会将自己的神经网络的参数更新为公共神经网络的参数，进而指导后面的环境交互。</p>
<p>可见，公共部分的网络模型就是我们要学习的模型，而线程里的网络模型主要是用于和环境交互使用的，这些线程里的模型可以帮助线程更好的和环境交互，拿到高质量的数据帮助模型更快收敛。</p>
<p>现在我们来看看第二个优化，网络结构的优化。之前在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们使用了两个不同的网络Actor和Critic。在A3C这里，我们把两个网络放到了一起，即输入状态 $S$,可以输出状态价值 $V$,和对应的策略 $π$, 当然，我们仍然可以把Actor和Critic看做独立的两块，分别处理，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">把Actor和Critic看做独立的两块，分别处理</div>
</center>
<br>
<p>第三个优化点是Critic评估点的优化，在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节中，我们讨论了不同的Critic评估点的选择，其中d部分讲到了使用优势函数 $A$ 来做Critic评估点，优势函数 $A$ 在时刻t不考虑参数的默认表达式为：</p>
<p>$$A(S,A,t)=Q(S,A)-V(S)$$</p>
<p>$Q(S,A)$的值一般可以通过单步采样近似估计，即：</p>
<p>$$Q(S,A)=R+\gamma V(S^{\prime})$$</p>
<p>这样优势函数去掉动作可以表达为：</p>
<p>$$A(S,t)=R+\gamma V(S^{\prime})-V(S)$$</p>
<p>其中 $V(S)$的值需要通过Critic网络来学习得到。</p>
<p>在A3C中，采样更进一步，使用了N步采样，以加速收敛。这样A3C中使用的优势函数表达为：</p>
<p>$$A(S,t)=R_t++\gamma R_{t+1}+\ldots\gamma^{n-1}R_{t+n-1}+\gamma^nV(S^{\prime})-V(S)$$</p>
<p>对于Actor和Critic的损失函数部分，和Actor-Critic基本相同。有一个小的优化点就是在Actor-Critic策略函数的损失函数中，加入了策略 $π$ 的熵项,系数为 $c$, 即策略参数的梯度更新和Actor-Critic相比变成了这样：</p>
<p>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,t)+c\nabla_\theta H(\pi(S_t,\theta))$$</p>
<p>以上就是A3C和Actor-Critic相比有优化的部分。下面我们来总价下A3C的算法流程。</p>
<h1 id="3-a3c算法流程">3. A3C算法流程</h1>
<p>这里我们对A3C算法流程做一个总结，由于A3C是异步多线程的，我们这里给出任意一个线程的算法流程。</p>
<ul>
<li>
<p>输入：公共部分的A3C神经网络结构，对应参数位 $θ$ , $w$，本线程的A3C神经网络结构，对应参数 $θ&rsquo;$, $w&rsquo;$, 全局共享的迭代轮数 $T$，全局最大迭代次数 $T_{max}$, 线程内单次迭代时间序列最大长度 $T_{local}$,状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，熵系数 $c$, 衰减因子 $γ$</p>
</li>
<li>
<p>输出：公共部分的A3C神经网络参数 $θ$, $w$</p>
<ul>
<li>(1). 更新时间序列 $t=1$</li>
<li>(2). 重置Actor和Critic的梯度更新量: $dθ←0$,$dw←0$</li>
<li>(3). 从公共部分的A3C神经网络同步参数到本线程的神经网络：$θ&rsquo;=θ,w&rsquo;=w$</li>
<li>(4). $t_{start}=t$，初始化状态 $s_t$</li>
<li>(5). 基于策略 $π(at|st;θ)$ 选择出动作 $a_t$</li>
<li>(6). 执行动作 $a_t$得到奖励 $r_t$ 和新状态 $s_{t+1}$</li>
<li>(7). $t←t+1$, $T←T+1$</li>
<li>(8). 如果 $s_t$是终止状态，或 $t − t_{start}==t_{local}$,则进入步骤(9)，否则回到步骤(5)</li>
<li>(9). 计算最后一个时间序列位置 $s_t$的 $Q(s,t)$:
<ul>
<li>$$\left.Q(s,t)=\left\{\begin{array}{ll}0&amp;terminal<del>state\\V(s_t,w^{\prime})&amp;none</del>terminal~state,bootstrapping\end{array}\right.\right.$$</li>
</ul>
</li>
<li>(10). for $i∈(t−1,t−2,&hellip;t_{start})$:
<ul>
<li>1). 计算每个时刻的$Q(s,i)$： $Q(s,i)=r_i+\gamma Q(s,i+1)$</li>
<li>2). 累计Actor的本地梯度更新：
<ul>
<li>$$d\theta\leftarrow d\theta+\nabla_{\theta^{\prime}}log\pi_{\theta^{\prime}}(s_i,a_i)(Q(s,i)-V(S_i,w^{\prime}))+c\nabla_{\theta^{\prime}}H(\pi(s_i,\theta^{\prime}))$$</li>
</ul>
</li>
<li>3). 累计Critic的本地梯度更新：
<ul>
<li>$$\begin{aligned}dw&amp;\leftarrow dw+\frac{\partial(Q(s,i)-V(S_i,w^{\prime}))^2}{\partial w^{\prime}}\end{aligned}$$</li>
</ul>
</li>
</ul>
</li>
<li>(11). 更新全局神经网络的模型参数：
<ul>
<li>$$\theta=\theta+\alpha d\theta,~w=w-\beta dw$$</li>
</ul>
</li>
<li>(12). 如果 $T&gt;T_{max}$,则算法结束，输出公共部分的A3C神经网络参数 $θ$, $w$,否则进入步骤(3)</li>
</ul>
</li>
</ul>
<p>以上就是A3C算法单个线程的算法流程。</p>
<h1 id="4-a3c算法实例">4. A3C算法实例</h1>
<p>下面我们基于上述算法流程给出A3C算法实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法代码大部分参考了莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/10_A3C/A3C_discrete_action.py"target="_blank" rel="external nofollow noopener noreferrer">A3C代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，增加了模型测试部分的代码并调整了部分模型参数。完整的代码参见我的Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py</p>
<p>整个算法的Actor和Critic的网络结构都定义在这里， 所有的线程中的网络结构，公共部分的网络结构都在这里定义。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;actor&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;la&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_a</span><span class="p">,</span> <span class="n">N_A</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ap&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;critic&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lc&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">)</span>  <span class="c1"># state value</span>
</span></span><span class="line"><span class="cl">  <span class="n">a_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/actor&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">c_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/critic&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">a_prob</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">a_params</span><span class="p">,</span> <span class="n">c_params</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所有线程初始化部分，以及本线程和公共的网络结构初始化部分如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;/cpu:0&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropA&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">GLOBAL_AC</span> <span class="o">=</span> <span class="n">ACNet</span><span class="p">(</span><span class="n">GLOBAL_NET_SCOPE</span><span class="p">)</span>  <span class="c1"># we only need its params</span>
</span></span><span class="line"><span class="cl">  <span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Create worker</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">i_name</span> <span class="o">=</span> <span class="s1">&#39;W_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>   <span class="c1"># worker name</span>
</span></span><span class="line"><span class="cl">    <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker</span><span class="p">(</span><span class="n">i_name</span><span class="p">,</span> <span class="n">GLOBAL_AC</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本线程神经网络将本地的梯度更新量用于更新公共网络参数的逻辑在update_global函数中，而从公共网络把参数拉回到本线程神经网络的逻辑在pull_global中。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">update_a_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_c_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>  <span class="c1"># local grads applies to global net</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">pull_global</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_a_params_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_c_params_op</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>详细的内容大家可以对照代码和算法流程一起看。在主函数里我新加了一个测试模型效果的过程，大家可以试试看看最后的模型效果如何。</p>
<h1 id="5-a3c小结">5. A3C小结</h1>
<p>A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，提供了一种通用的异步的并发的强化学习框架，也就是说，这个并发框架不光可以用于A3C，还可以用于其他的强化学习算法。这是A3C最大的贡献。目前，已经有基于GPU的A3C框架，这样A3C的框架训练速度就更快了。</p>
<p>除了A3C, DDPG算法也可以改善Actor-Critic难收敛的问题。它使用了Nature DQN，DDQN类似的思想，用两个Actor网络，两个Critic网络，一共4个神经网络来迭代更新模型参数。在下一篇我们讨论DDPG算法。</p>
]]></description></item><item><title>强化学习笔记 [14] | Actor-Critic</title><link>https://jianye0428.github.io/posts/rl_learning_note_14/</link><pubDate>Sun, 25 Feb 2024 15:35:58 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_14/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。</p>
<p>在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。</p>
<p>本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。</p>
<h1 id="1-actor-critic算法简介">1. Actor-Critic算法简介</h1>
<p>Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。</p>
<p>回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。</p>
<p>也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：</p>
<p>$$
\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)
$$</p>
<p>第二组是价值函数的近似，对于状态价值和动作价值函数分别是：</p>
<p>$$
\hat{v}(s,w)\approx v_\pi(s)
$$</p>
<p>$$
\hat{q}(s,a,w)\approx q_\pi(s,a)
$$</p>
<p>对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：</p>
<p>$$
\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t
$$</p>
<p>梯度更新部分中，$\nabla_\theta log\pi_\theta(s_t,a_t)$是我们的分值函数，不用动，要变成Actor的话改动的是$v_t$，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。</p>
<p>而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。</p>
<p>现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值$v_t$,而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$,在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{te}$</p>
<h1 id="2-actor-critic算法可选形式">2. Actor-Critic算法可选形式</h1>
<p>在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值 $v_t$实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：</p>
<ul>
<li>
<p>a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)V(s,w)$$</li>
</ul>
</li>
<li>
<p>b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)Q(s,a,w)$$</li>
</ul>
</li>
<li>
<p>c) 基于TD误差：在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了TD误差，它的表达式是 $\delta(t)=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 或者 $\delta(t)=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t)$, 这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)$$</li>
</ul>
</li>
<li>
<p>d) 基于优势函数：在<a href="https://www.cnblogs.com/pinard/p/9923859.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十二) Dueling DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到过优势函数A的定义：$A(S,A,w,\beta)=Q(S,A,w,\alpha,\beta)-V(S,w,\alpha)$, 即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,A,w,\beta)$$</li>
</ul>
</li>
<li>
<p>e) 基于 $TD(λ)$ 误差：一般都是基于后向 $TD(λ)$误差, 在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)E(t)$</li>
</ul>
</li>
</ul>
<p>对于Critic本身的模型参数 $w$ ，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如 $Q(s,a,w)=ϕ(s,a)^Tw$,则Critic本身的模型参数 $w$的更新公式可以表示为：</p>
<p>$$\begin{gathered}
\delta=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t) \\
w=w+\beta\delta\phi(s,a)
\end{gathered}$$</p>
<p>通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。</p>
<h1 id="3-actor-critic算法流程">3. Actor-Critic算法流程</h1>
<p>这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数　　</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，衰减因子 $γ$, 探索率 $ϵ$, Critic网络结构和Actor网络结构。</p>
<p>输出：Actor 网络参数 $θ$, Critic网络参数 $w$</p>
<ul>
<li>(1). 随机初始化所有的状态和动作对应的价值Q�. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor网络中使用 $ϕ(S)$ 作为输入，输出动作 $A$,基于动作 $A$得到新的状态 $S&rsquo;$,反馈 $R$。</li>
<li>c) 在Critic网络中分别使用 $ϕ(S)$，$ϕ(S&rsquo;)$ 作为输入，得到Q值输出 $V(S)$，$V(S&rsquo;)$</li>
<li>d) 计算TD误差 $\delta=R+\gamma V(S^{\prime})-V(S)$</li>
<li>e) 使用均方差损失函数 $\sum(R+\gamma V(S^{\prime})-V(S,w))^2$ 作Critic网络参数 $w$的梯度更新</li>
<li>f) 更新Actor网络参数 $θ$:
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(S_t,A)\delta $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对于Actor的分值函数 $∇_θlogπ_θ(S_t,A)$,可以选择softmax或者高斯分值函数。</p>
<p>上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。</p>
<h1 id="4-actor-critic算法实例">4. Actor-Critic算法实例</h1>
<p>下面我们用一个具体的例子来演示上面的Actor-Critic算法。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法流程可以参考上面的第三节，这里的分值函数我们使用的是softmax函数，和上一片的类似。完整的代码参见Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py</p>
<p>代码主要分为两部分，第一部分是Actor，第二部分是Critic。对于Actor部分，大家可以和上一篇策略梯度的代码对比，改动并不大，主要区别在于梯度更新部分，策略梯度使用是蒙特卡罗法计算出的价值 $v(t)$,则我们的actor使用的是TD误差。</p>
<p>在策略梯度部分，对应的位置如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而我们的Actor对应的位置的代码是：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">td_error</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处要注意的是，由于使用的是TD误差，而不是价值 $v(t)$,此处需要最大化<code>self.exp</code>,而不是最小化它，这点和策略梯度不同。对应的Actor代码为：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除此之外，Actor部分的代码和策略梯度的代码区别并不大。</p>
<p>对于Critic部分，我们使用了类似于DQN的三层神经网络。不过我们简化了这个网络的输出，只有一维输出值，而不是之前DQN使用的有多少个可选动作，就有多少维输出值。网络结构如下:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">],</span> <span class="s2">&#34;state&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layerq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layerq</span><span class="p">,</span> <span class="n">W2q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2q</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和之前的DQN相比，这里还有一个区别就是我们的critic没有使用DQN的经验回放，只是使用了反馈和当前网络在下一个状态的输出来拟合当前状态。</p>
<p>对于算法中Actor和Critic交互的逻辑，在main函数中：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># e-greedy action for train</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">critic</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>  <span class="c1"># gradient = grad[r + gamma * V(s_) - V(s)]</span>
</span></span><span class="line"><span class="cl">  <span class="n">actor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">td_error</span><span class="p">)</span>  <span class="c1"># true_gradient = grad[logPi(s,a) * td_error]</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大家对照第三节的算法流程和代码应该可以比较容易理清这个过程。但是这个程序很难收敛。因此大家跑了后发现分数总是很低的话是可以理解的。我们需要优化这个问题。</p>
<h1 id="5-actor-critic算法小结">5. Actor-Critic算法小结</h1>
<p>基本版的Actor-Critic算法虽然思路很好，但是由于难收敛的原因，还需要做改进。</p>
<p>目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法我们在从DQN到Nature DQN的过程中已经用过一次了。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p>
<p>在后面的文章中，我们会继续讨论DDPG和A3C。</p>
<p>　</p>
]]></description></item><item><title>强化学习笔记 [13] | 策略梯度(Policy Gradient)</title><link>https://jianye0428.github.io/posts/rl_learning_note_13/</link><pubDate>Sun, 25 Feb 2024 15:35:55 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_13/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradient)，它是Policy Based强化学习方法，基于策略来学习。</p>
<p>本文参考了Sutton的强化学习书第13章和策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-value-based强化学习方法的不足">1. Value Based强化学习方法的不足</h1>
<p>DQN系列强化学习算法主要的 <strong><font color=red>问题</font></strong> 主要有三点。</p>
<ul>
<li>
<p>第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。</p>
</li>
<li>
<p>第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。</p>
</li>
<li>
<p>第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。</p>
</li>
</ul>
<p>由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述类别问题的方法，比如Policy Based强化学习方法。</p>
<h1 id="2-policy-based强化学习方法引入">2. Policy Based强化学习方法引入</h1>
<p>回想我们在Value Based强化学习方法里，我们对价值函数进行了近似表示，引入了一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到近似的动作价值，即：</p>
<p>$$\hat{q}\left(s,a,w\right)\approx q_\pi(s,a)$$</p>
<p>在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略 $π$可以被被描述为一个包含参数 $θ$ 的函数,即：</p>
<p>$$\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)$$</p>
<p>将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？</p>
<h1 id="3-策略梯度的优化目标">3. 策略梯度的优化目标</h1>
<p>我们要用梯度上升来寻找最优的梯度，首先就要找到一个可以优化的函数目标。</p>
<p>最简单的优化目标就是初始状态收获的期望，即优化目标为：</p>
<p>$$J_1(\theta)=V_{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}(G_1)$$</p>
<p>但是有的问题是没有明确的初始状态的，那么我们的优化目标可以定义平均价值，即：
$$J_{avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$$</p>
<p>其中，$d_πθ(s)$ 是基于策略 $π_θ$生成的马尔科夫链关于状态的静态分布。</p>
<p>或者定义为每一时间步的平均奖励，即：</p>
<p>$$J_{avR}(\theta)==\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$$</p>
<p>无论我们是采用 $J_1$, $J_{av}V$, 还是 $J_{av}R$ 来表示优化目标，最终对 $θ$求导的梯度都可以表示为：</p>
<p>$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>具体的证明过程这里就不再列了，如果大家感兴趣，可以去看策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的附录1，里面有详细的证明。</p>
<p>当然我们还可以采用很多其他可能的优化目标来做梯度上升，此时我们的梯度式子里面的 $\nabla_\theta log\pi_\theta(s,a)$ 部分并不改变，变化的只是后面的 $Q_\pi(s,a)$ 部分。对于 $\nabla_\theta log\pi_\theta(s,a)$,我们一般称为<strong>分值函数</strong>(score function)。</p>
<p>现在梯度的式子已经有了，后面剩下的就是策略函数 $\pi_\theta(s,a)$的设计了。</p>
<h1 id="4-策略函数的设计">4. 策略函数的设计</h1>
<p>现在我们回头看一下策略函数 $\pi_\theta(s,a)$ 的设计，在前面它一直是一个数学符号。</p>
<p>最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征 $ϕ(s,a)$ 与参数 $θ$的线性组合来权衡一个行为发生的几率,即:</p>
<p>$$\pi_\theta(s,a)=\frac{e^{\phi(s,a)^T\theta}}{\sum_be^{\phi(s,b)^T\theta}}$$</p>
<p>则通过求导很容易求出对应的分值函数为：</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)=\phi(s,a)-\mathbb{E}_{\pi_\theta}[\phi(s,.)]$$</p>
<p>另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布 $\mathbb{N}(\phi(\mathrm{s})^{\mathbb{T}}\theta,\sigma^2)$中产生。高斯策略对应的分值函数求导可以得到为:</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)==\frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}$$</p>
<p>有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。</p>
<h1 id="5-蒙特卡罗策略梯度reinforce算法">5. 蒙特卡罗策略梯度reinforce算法</h1>
<p>这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数 $v(s)$ 来近似代替策略梯度公式里面的 $Q_π(s,a)$。算法的流程很简单，如下所示:</p>
<ul>
<li>输入：N个蒙特卡罗完整序列,训练步长 $α$</li>
<li>输出：策略函数的参数 $θ$
<ul>
<li>(1). for 每个蒙特卡罗序列:
<ul>
<li>a. 用蒙特卡罗法计算序列每个时间位置t的状态价值 $v_t$</li>
<li>b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数 $θ$：
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t$$</li>
</ul>
</li>
</ul>
</li>
<li>(2).返回策略函数的参数 $θ$</li>
</ul>
</li>
</ul>
<p>　　这里的策略函数可以是softmax策略，高斯策略或者其他策略。</p>
<h1 id="6-策略梯度实例">6. 策略梯度实例</h1>
<p>这里给出第5节的蒙特卡罗策略梯度reinforce算法的一个实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py</p>
<p>这里我们采用softmax策略作为我们的策略函数，同时，softmax的前置部分，也就是我们的策略模型用一个三层的softmax神经网络来表示。这样好处就是梯度的更新可以交给神经网络来做。</p>
<p>我们的softmax神经网络的结构如下，注意这个网络不是价值Q网络，而是策略网络：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_softmax_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_num&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_value&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># softmax layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#softmax output</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">all_act_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;act_prob&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意我们的损失函数是softmax交叉熵损失函数和状态价值函数的乘积，这样TensorFlow后面可以自动帮我们做梯度的迭代优化。</p>
<p>另一个要注意的点就是蒙特卡罗法里面价值函数的计算，一般是从后向前算，这样前面的价值的计算可以利用后面的价值作为中间结果，简化计算，对应代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">      <span class="n">running_add</span> <span class="o">=</span> <span class="n">running_add</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="n">discounted_ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分和之前的DQN的代码类似。</p>
<h1 id="7-策略梯度小结">7. 策略梯度小结</h1>
<p>策略梯度提供了和DQN之类的方法不同的新思路，但是我们上面的蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，我们需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，我们的参数更新的方向很可能不是策略梯度的最优方向。</p>
<p>因此，Policy Based的强化学习方法还需要改进，注意到我们之前有Value Based强化学习方法，那么两者能不能结合起来一起使用呢？下一篇我们讨论Policy Based+Value Based结合的策略梯度方法Actor-Critic。</p>
<p>　　　　</p>
]]></description></item><item><title>强化学习笔记 [12] | Dueling DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_12/</link><pubDate>Sun, 25 Feb 2024 11:16:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_12/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9797695.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十一) Prioritized Replay DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。</p>
<h1 id="1-dueling-dqn的优化点考虑">1. Dueling DQN的优化点考虑</h1>
<p>在前面讲到的DDQN中，我们通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，我们通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，我们尝试通过<font color=red>优化神经网络的结构</font>来优化算法。</p>
<p>具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态 $S$有关，与具体要采用的动作 $A$无关，这部分我们叫做<strong>价值函数部分</strong>，记做 $V(S,w,α)$,第二部分同时与状态状态 $S$ 和动作 $A$有关，这部分叫做**优势函数(Advantage Function)**部分,记为 $A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+A(S,A,w,\beta)$$</p>
<p>其中，$w$ 是公共部分的网络参数，而 $α$ 是价值函数独有部分的网络参数，而 $β$ 是优势函数独有部分的网络参数。</p>
<h1 id="2-dueling-dqn网络结构">2. Dueling DQN网络结构</h1>
<p>由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。为了简化算法描述，这里不使用原论文的CNN网络结构，而是使用前面文中用到的最简单的三层神经网络来描述。是否使用CNN对Dueling DQN算法本身无影响。</p>
<p>在前面讲到的DDQN等DQN算法中，我使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络与Dueling DQN</div>
</center>
<br>
<p>而在Dueling DQN中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。</p>
<p>我们可以直接使用上一节的价值函数的组合公式得到我们的动作价值，但是这个式子无法辨识最终输出里面 $V(S,w,α)$ 和 $A(S,A,w,β)$各自的作用，为了可以体现这种可辨识性(identifiability),实际使用的组合公式如下：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+(A(S,A,w,\beta)-\frac1{\mathcal{A}}\sum_{a^{\prime}\in\mathcal{A}}A(S,a^{\prime},w,\beta))$$</p>
<p>其实就是对优势函数部分做了中心化的处理。以上就是Dueling DQN的主要算法思路。由于它仅仅涉及神经网络的中间结构的改进，现有的DQN算法可以在使用Duel DQN网络结构的基础上继续使用现有的算法。由于算法主流程和其他算法没有差异，这里就不单独讲Duel DQN的算法流程了。</p>
<h1 id="3-dueling-dqn实例">3. Dueling DQN实例</h1>
<p>下面我们用一个具体的例子来演示Dueling DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>这个实例代基于Nature DQN，并将网络结构改为上图中右边的Dueling DQN网络结构，完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Dueling DQN和Nature DQN的代码的不同之处。也就是网络结构定义部分，主要的代码如下，一共有两个相同结构的Q网络，每个Q网络都有状态函数和优势函数的定义，以及组合后的Q网络输出，如代码红色部分：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W21</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b21</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W21</span><span class="p">)</span> <span class="o">+</span> <span class="n">b21</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W22</span><span class="p">)</span> <span class="o">+</span> <span class="n">b22</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2v</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2a</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分代码和Nature DQN基本相同。当然，我们可以也在前面DDQN，Prioritized Replay DQN代码的基础上，把网络结构改成上面的定义，这样Dueling DQN也可以起作用。</p>
<h1 id="4-dqn总结">4. DQN总结</h1>
<p>DQN系列我花了5篇来讲解，一共5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。</p>
<p>当然DQN家族的算法远远不止这些，还有一些其他的DQN算法我没有详细介绍，比如使用一些较复杂的CNN和RNN网络来提高DQN的表达能力，又比如改进探索状态空间的方法等，主要是在DQN的基础上持续优化。</p>
<p>DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。虽然NAF DQN可以解决这个问题，但是方法过于复杂了。而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题，从下一篇我们开始讨论Policy-Based深度强化学习。</p>
]]></description></item><item><title>强化学习笔记 [11] | Prioritized Replay DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_11/</link><pubDate>Sun, 25 Feb 2024 11:16:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_11/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay DQN。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Prioritized Replay DQN的论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<h1 id="1-prioritized-replay-dqn之前算法的问题">1. Prioritized Replay DQN之前算法的问题</h1>
<p>在Prioritized Replay DQN之前，我们已经讨论了很多种DQN，比如Nature DQN， DDQN等，他们都是通过经验回放来采样，进而做目标Q值的计算的。在采样的时候，我们是一视同仁，在经验回放池里面的所有的样本都有相同的被采样到的概率。</p>
<p>但是注意到在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，由于TD误差小，对反向梯度的计算影响不大。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。</p>
<p>这样如果TD误差的绝对值 $|δ(t)|$较大的样本更容易被采样，则我们的算法会比较容易收敛。下面我们看看Prioritized Replay DQN的算法思路。</p>
<h1 id="2-prioritized-replay-dqn算法的建模">2. Prioritized Replay DQN算法的建模</h1>
<p>Prioritized Replay DQN根据每个样本的TD误差绝对值 $|δ(t)|$，给定该样本的优先级正比于 $|δ(t)|$，将这个优先级的值存入经验回放池。回忆下之前的DQN算法，我们仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。</p>
<p>由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。</p>
<p>具体的SumTree树结构如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">sum_tree 结构图</div>
</center>
<br>
<p>所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。</p>
<p>这样保存有什么好处呢？主要是方便采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样，采样到哪个区间，就是哪个样本。比如我们采样到了26， 在（25-29）这个区间，那么就是第四个叶子节点被采样到。而注意到第三个叶子节点优先级最高，是12，它的区间13-25也是最长的，会比其他节点更容易被采样到。</p>
<p>如果要采样两个样本，我们可以在[0,21],[21,42]两个区间做均匀采样，方法和上面采样一个样本类似。</p>
<p>类似的采样算法思想我们在<a href="https://www.cnblogs.com/pinard/p/7249903.html"target="_blank" rel="external nofollow noopener noreferrer">word2vec原理(三) 基于Negative Sampling的模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第四节中也有讲到。</p>
<p>除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：</p>
<p>$$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>现在我们新的考虑了样本优先级的损失函数是</p>
<p>$$\frac1m\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>其中 $w_j$是第j个样本的优先级权重，由TD误差 $|δ(t)|$归一化得到。</p>
<p>第三个要注意的点就是当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。</p>
<p>除了以上三个部分，Prioritized Replay DQN和DDQN的算法流程相同。</p>
<h1 id="3-prioritized-replay-dqn算法流程">3. Prioritized Replay DQN算法流程</h1>
<p>下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，采样权重系数 $β$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$, SumTree的叶子节点数 $S$。</li>
<li>输出：Q网络参数。</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级 $p_j$为1。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S&rsquo;),is_end}$这个五元组存入SumTree</li>
<li>e) $S=S'$</li>
<li>f) 从SumTree中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，每个样本被采样的概率基于 $P(j)=\frac{p_j}{\sum_i(p_i)}$，损失函数权重 $w_j=(N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left\\{\begin{matrix}R_j&amp;is_end_j\textit{is true}\\\\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{is false}\end{matrix}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\begin{aligned}\frac{1}{m}\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2\end{aligned}$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 重新计算所有样本的TD误差 $\delta_j=y_j-Q(\phi(S_j),A_j,w)$，更新SumTree中所有节点的优先级 $p_j=|\delta_j|$</li>
<li>i) 如果i%C=1,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>j) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-prioritized-replay-ddqn算法流程">4. Prioritized Replay DDQN算法流程</h1>
<p>下面我们给出Prioritized Replay DDQN算法的实例代码。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 代码中的SumTree的结构和经验回放池的结构参考了morvanzhou的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py"target="_blank" rel="external nofollow noopener noreferrer">github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>这里重点讲下和第三节中算法描述不同的地方，主要是 $w_j$的计算。注意到：</p>
<p>$$w_j=\frac{(N<em>P(j))^{-\beta}}{\max_i(w_i)}=\frac{(N</em>P(j))^{-\beta}}{\max_i((N*P(i))^{-\beta})}=\frac{(P(j))^{-\beta}}{\max_i((P(i))^{-\beta})}=(\frac{P_j}{\min_iP(i)})^{-\beta}$$</p>
<p>因此代码里面$w_j$，即ISWeights的计算代码是这样的：</p>
<p><a href="javascript:void%280%29;"></a></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">pri_seg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span> <span class="o">/</span> <span class="n">n</span>       <span class="c1"># priority segment</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_increment_per_sampling</span><span class="p">])</span>  <span class="c1"># max = 1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">min_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">capacity</span><span class="p">:])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>     <span class="c1"># for later calculate ISweight</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">min_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_prob</span> <span class="o">=</span> <span class="mf">0.00001</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">get_leaf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prob</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>
</span></span><span class="line"><span class="cl">    <span class="n">ISWeights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prob</span><span class="o">/</span><span class="n">min_prob</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">b_memory</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上述代码的采样在第二节已经讲到。根据树的优先级的和total_p和采样数n，将要采样的区间划分为n段，每段来进行均匀采样，根据采样到的值落到的区间，决定被采样到的叶子节点。当我们拿到第i段的均匀采样值v以后，就可以去SumTree中找对应的叶子节点拿样本数据，样本叶子节点序号以及样本优先级了。代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree structure and array storage:
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree index:
</span></span></span><span class="line"><span class="cl"><span class="s2">        0         -&gt; storing priority sum
</span></span></span><span class="line"><span class="cl"><span class="s2">      / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">    1     2
</span></span></span><span class="line"><span class="cl"><span class="s2">    / \   / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">  3   4 5   6    -&gt; storing priority for transitions
</span></span></span><span class="line"><span class="cl"><span class="s2">  Array type for storing:
</span></span></span><span class="line"><span class="cl"><span class="s2">  [0,1,2,3,4,5,6]
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">parent_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>     <span class="c1"># the while loop is faster than the method in the reference code</span>
</span></span><span class="line"><span class="cl">    <span class="n">cl_idx</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">parent_idx</span> <span class="o">+</span> <span class="mi">1</span>         <span class="c1"># this leaf&#39;s left and right kids</span>
</span></span><span class="line"><span class="cl">    <span class="n">cr_idx</span> <span class="o">=</span> <span class="n">cl_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cl_idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">):</span>        <span class="c1"># reach bottom, end search</span>
</span></span><span class="line"><span class="cl">      <span class="n">leaf_idx</span> <span class="o">=</span> <span class="n">parent_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>       <span class="c1"># downward search, always search for a higher priority node</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cl_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cr_idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">data_idx</span> <span class="o">=</span> <span class="n">leaf_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">leaf_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">leaf_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了采样部分，要注意的就是当梯度更新完毕后，我们要去更新SumTree的权重，代码如下，注意叶子节点的权重更新后，要向上回溯，更新所有祖先节点的权重。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">batch_update</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">)</span>  <span class="c1"># update priority</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">batch_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">abs_errors</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>  <span class="c1"># convert to abs and avoid 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">clipped_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">abs_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs_err_upper</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">clipped_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">ps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ti</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">change</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># then propagate the change through tree</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">tree_idx</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>    <span class="c1"># this method is faster than the recursive loop in the reference code</span>
</span></span><span class="line"><span class="cl">      <span class="n">tree_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">change</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，和DDQN比，TensorFlow的网络结构流程中多了一个TD误差的计算节点，以及损失函数多了一个ISWeights系数。此外，区别不大。</p>
<h1 id="5-prioritized-replay-dqn小结">5. Prioritized Replay DQN小结</h1>
<p>Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。</p>
<p>下一篇我们讨论DQN家族的另一个优化算法Duel DQN，它将价值Q分解为两部分，第一部分是仅仅受状态但不受动作影响的部分，第二部分才是同时受状态和动作影响的部分，算法的效果也很好。</p>
]]></description></item><item><title>强化学习笔记 [10] | Double DQN (DDQN)</title><link>https://jianye0428.github.io/posts/rl_learning_note_10/</link><pubDate>Fri, 23 Feb 2024 13:17:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_10/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9756075.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（九）Deep Q-Learning进阶之Nature DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称DDQN）。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和DDQN的论文(Deep Reinforcement Learning with Double Q-learning)。</p>
<h1 id="1-dqn的目标q值计算问题">1. DQN的目标Q值计算问题</h1>
<p>在DDQN之前，基本上所有的目标Q值都是通过<strong>贪婪法</strong>直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算如下式:</p>
<p>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</p>
<h1 id="2-ddqn的算法建模">2. DDQN的算法建模</h1>
<p>DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。</p>
<p>在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：</p>
<p>$$y_j=R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})$$</p>
<p>在DDQN(Double DQN)这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即:</p>
<p>$$a^{max}(S_j^{\prime},w)=\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w)$$</p>
<p>然后利用这个选择出来的动作 $\begin{aligned}&amp;a^{max}(S_j^{\prime},w)\end{aligned}$ 在目标网络里面去计算目标Q值。即：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),a^{max}(S_j^{\prime},w),w^{\prime})$$</p>
<p>综合起来写就是：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})$$</p>
<p>除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。</p>
<h1 id="3-ddqn算法流程">3. DDQN算法流程</h1>
<p>这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本 $m$,目标Q网络参数更新频 $C$。</li>
<li>输出：Q网络参数</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;的参数 $w′=w$ 。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S′),is_end} $,这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$, 计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数w�</li>
<li>h) 如果 $i%C=1$,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-ddqn算法实例">4. DDQN算法实例　</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注DDQN和上一节的Nature DQN的代码的不同之处。代码只有一个地方不一样，就是计算目标Q值的时候，如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">current_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_action_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">current_Q_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">target_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">target_Q_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">max_action_next</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">target_Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而之前的Nature DQN这里的目标Q值计算是如下这样的：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，两个算法的代码完全相同。</p>
<h1 id="5-ddqn小结">5. DDQN小结</h1>
<p>DDQN算法出来以后，取得了比较好的效果，因此得到了比较广泛的应用。不过我们的DQN仍然有其他可以优化的点，如上一篇最后讲到的: 随机采样的方法好吗？按道理经验回放里不同样本的重要性是不一样的，TD误差大的样本重要程度应该高。针对这个问题，我们在下一节的Prioritised Replay DQN中讨论。</p>
]]></description></item><item><title>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_9/</link><pubDate>Fri, 23 Feb 2024 13:17:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_9/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9714655.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（八）价值函数的近似表示与Deep Q-Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Nature DQN的论文。</p>
<h1 id="1-dqnnips-2013的问题">1. DQN(NIPS 2013)的问题</h1>
<p>在上一篇我们已经讨论了DQN(NIPS 2013)的算法原理和代码实现，虽然它可以训练像CartPole这样的简单游戏，但是有很多问题。这里我们先讨论第一个问题。</p>
<p>注意到DQN(NIPS 2013)里面，我们使用的目标 $Q$值的计算方式：</p>
<p>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>这里目标Q值的计算使用到了当前要训练的Q网络参数来计算$Q(\phi(S_j^{\prime}),A_j^{\prime},w)$，而实际上，我们又希望通过 $y_j$来后续更新 $Q$网络参数。这样两者循环依赖，迭代起来两者的相关性就太强了。不利于算法的收敛。</p>
<p>因此，一个改进版的DQN: Nature DQN尝试<strong>用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系</strong>。下面我们来看看Nature DQN是怎么做的。</p>
<h1 id="2-nature-dqn的建模">2. Nature DQN的建模</h1>
<p>Nature DQN的两个Q网络分别命名为当前Q网络和目标Q网络。</p>
<p>Nature DQN使用了两个Q网络，一个<strong>当前Q网络</strong>$Q$用来选择动作，更新模型参数，另一个<strong>目标Q网络</strong> $Q&rsquo;$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。</p>
<p>要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。</p>
<p>Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。</p>
<h1 id="3-nature-dqn的算法流程">3. Nature DQN的算法流程</h1>
<p>下面我们来总结下Nature DQN的算法流程， 基于DQN NIPS 2015：</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率$C$。</p>
<p>输出：$Q$网络参数</p>
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S′),is_end\\}$这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，计算当前目标Q值 $y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果 $i%C=1$, 则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$ 需要随着迭代的进行而变小。</p>
<h1 id="4-nature-dqn算法实例">4. Nature DQN算法实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Nature DQN和上一节的NIPS 2013 DQN的代码的不同之处。</p>
<p>首先是Q网络，上一篇的DQN是一个三层的神经网络，而这里我们有两个一样的三层神经网络，一个是当前Q网络，一个是目标Q网络，网络的定义部分如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2t</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对于定期将目标Q网络的参数更新的代码如下面两部分：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">t_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;target_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">e_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;current_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;soft_replacement&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">t_params</span><span class="p">,</span> <span class="n">e_params</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_target_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># update target Q netowrk</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">REPLACE_TARGET_FREQ</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#print(&#39;episode &#39;+str(episode) +&#39;, target Q network params replaced!&#39;)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此外，注意下我们计算目标Q值的部分，这里使用的目标Q网络的参数，而不是当前Q网络的参数：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分基本和上一篇DQN的代码相同。这里给出我跑的某一次的结果:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 9.6
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.0
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 34.8
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 177.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 198.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 193.2
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>注意，由于DQN不保证稳定的收敛，所以每次跑的结果会不同，如果你跑的结果后面仍然收敛的不好，可以把代码多跑几次，选择一个最好的训练结果。</p>
<h1 id="5-nature-dqn总结">5. Nature DQN总结</h1>
<p>Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：</p>
<ul>
<li>1） 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？</li>
<li>2） 随机采样的方法好吗？按道理不同样本的重要性是不一样的。</li>
<li>3） Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？</li>
</ul>
<p>第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN，这三个DQN的改进版我们在下一篇来讨论。</p>
]]></description></item><item><title>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_8/</link><pubDate>Fri, 23 Feb 2024 13:17:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_8/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在强化学习系列的<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">前七篇<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。</p>
<p>Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。</p>
<h1 id="1-为何需要价值函数的近似表示">1. 为何需要价值函数的近似表示</h1>
<p>在之前讲到了强化学习求解方法，无论是动态规划DP，蒙特卡罗方法MC，还是时序差分TD，使用的状态都是离散的有限个状态集合 $S$。此时问题的规模比较小，比较容易求解。但是假如我们遇到复杂的状态集合呢？甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。　　　　</p>
<p>比如经典的冰球世界(PuckWorld)强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间步时长的一个大小固定的力，借此来改变大圆的速度。环境会在每一个时间步内告诉个体当前的水平与垂直坐标、当前的速度在水平和垂直方向上的分量以及目标的水平和垂直坐标共6项数据，奖励值为个体与目标两者中心距离的负数，也就是距离越大奖励值越低且最高奖励值为0。</p>
<p>在这个问题中，状态是一个6维的向量，并且是连续值。没法直接用之前离散集合的方法来描述状态。当然，你可以说，我们可以把连续特征离散化。比如把这个冰球场100x100的框按1x1的格子划分成10000个格子，那么对于运动员的坐标和冰球的坐标就有$10^4∗10^4=10^8$次种，如果再加上个体速度的分量就更是天文数字了，此时之前讲过的强化学习方法都会因为问题的规模太大而无法使用。怎么办呢？必须要对问题的建模做修改了，而价值函数的近似表示就是一个可行的方法。</p>
<h1 id="2-价值函数的近似表示方法">2. 价值函数的近似表示方法</h1>
<p>由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数 $\hat{v}$, 这个函数由参数 $w$ 描述，并接受状态 $s$ 作为输入，计算后得到状态 $s$ 的价值，即我们期望：</p>
<p>$$\hat{v}(s,w)\approx v_\pi(s)$$</p>
<p>类似的，引入一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到动作价值，即我们期望：</p>
<p>$$\hat{q}(s,a,w)\approx q_\pi(s,a)$$</p>
<p>价值函数近似的方法很多，比如最简单的线性表示法，用 $ϕ(s)$表示状态 $s$ 的特征向量，则此时我们的状态价值函数可以近似表示为：</p>
<p>$$\hat{v}(s,w)=\phi(s)^Tw$$</p>
<p>当然，除了线性表示法，我们还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。因此后面我们的近似表达方法如果没有特别提到，都是指的神经网络的近似表示。</p>
<p>对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络拟合价值函数</div>
</center>
<br>
<p>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值 $\hat{v}(s,w)$。对于动作价值函数，有两种方法，一种是输入状态 $s$ 的特征向量和动作 $a$，输出对应的动作价值 $\hat{q}(s,a,w)$，另一种是只输入状态 $s$ 的特征向量，动作集合有多少个动作就有多少个输出 $\hat{q}(s,ai,w)$。这里隐含了我们的动作是有限个的离散动作。</p>
<p>对于我们前一篇讲到的Q-Learning算法，我们现在就价值函数的近似表示来将其改造，采用上面右边的第三幅图的动作价值函数建模思路来做，现在我们叫它Deep Q-Learning。</p>
<h1 id="3-deep-q-learning算法思路">3. Deep Q-Learning算法思路</h1>
<p>Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值s和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p>
<p>DQN的输入是我们的状态s对应的状态向量 $ϕ(s)$， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p>
<p>DQN主要使用的技巧是经验回放(experience replay), 即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是<strong>经验回放</strong>。</p>
<p>通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数 $w$，当 $w$ 收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。</p>
<p>下面我们总结下DQN的算法流程，基于NIPS 2013 DQN。　　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, Q网络结构, 批量梯度下降的样本数 $m$。</li>
<li>输出：Q网络参数
<ul>
<li>
<ol>
<li>随机初始化$Q$网络的所有参数 $w$，基于 $w$初始化所有的状态和动作对应的价值 $Q$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S&rsquo;),is_end\\}$这个五元组存入经验回放集合D</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\mathrm{<del>}is\mathrm{</del>}true\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\mathrm{<del>}is\mathrm{</del>}false\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{i=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的 $f$步和 $g$步的 $Q$值计算也都需要通过 $Q$网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-deep-q-learning实例">4. Deep Q-Learning实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。这里使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码参考了知乎上的一个<a href="https://zhuanlan.zhihu.com/p/21477488"target="_blank" rel="external nofollow noopener noreferrer">DQN实例<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，修改了代码中的一些错误，并用最新的Python3.6+Tensorflow1.8.0运行。要跑代码需要安装OpenAI的Gym库，使用<code>pip install gym</code>即可。</p>
<p>代码使用了一个三层的神经网络，输入层，一个隐藏层和一个输出层。下面我们看看关键部分的代码。</p>
<p>算法第2步的步骤b通过$ϵ−$贪婪法选择动作的代码如下，注意每次我们$ϵ−$贪婪法后都会减小$ϵ$值。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">egreedy_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:[</span><span class="n">state</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">})[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤c在状态S�执行当前动作A�的代码如下，这个交互是由Gym完成的。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Define reward for agent</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mf">0.1</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤d保存经验回放数据的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">perceive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span><span class="n">one_hot_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">REPLAY_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤f,g计算目标Q值，并更新Q网络的代码如下：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 1: obtain random minibatch from replay memory</span>
</span></span><span class="line"><span class="cl">  <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">y_input</span><span class="p">:</span><span class="n">y_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">action_input</span><span class="p">:</span><span class="n">action_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">state_batch</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们在每100轮迭代完后会去玩10次交互测试，计算10次的平均奖励。运行了代码后，我的3000轮迭代的输出如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 12.2
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.4
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 10.4
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.5
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 11.6
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 12.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 29.6
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 48.1
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 85.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 169.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>大概到第1000次迭代后，算法已经收敛，达到最高的200分。当然由于是$ϵ−$探索，每次前面的输出可能不同，但最后应该都可以收敛到200的分数。当然由于DQN不保证绝对的收敛，所以可能到了200分后还会有抖动。</p>
<h1 id="5-deep-q-learning小结">5. Deep Q-Learning小结　　　　</h1>
<p>DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。但是DQN有个问题，就是它并不一定能保证Q网络的收敛，也就是说，我们不一定可以得到收敛后的Q网络参数。这会导致我们训练出的模型效果很差。</p>
<p>针对这个问题，衍生出了DQN的很多变种，比如Nature DQN(NIPS 2015), Double DQN，Dueling DQN等。这些我们在下一篇讨论。</p>
]]></description></item><item><title>强化学习笔记 [7] | 时序差分离线控制算法Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_7/</link><pubDate>Fri, 23 Feb 2024 13:17:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_7/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。</p>
<p>Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-q-learning算法的引入">1. Q-Learning算法的引入　　　　</h1>
<p>Q-Learning算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解<u>不需要环境的状态转化模型</u>，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新策略，通过策略来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，比如我们上一篇讲到的SARSA, 而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。这一类的经典算法就是Q-Learning。</p>
<p>对于Q-Learning，我们会使用 $ϵ−$贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的 $ϵ−$贪婪法。这一点就是SARSA和Q-Learning本质的区别。</p>
<h1 id="2-q-learning算法概述">2. Q-Learning算法概述</h1>
<p>Q-Learning算法的拓扑图如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Q Learning 拓扑图</div>
</center>
<br>
<p>首先我们基于状态 $S$，用 $ϵ−$贪婪法选择到动作 $A$, 然后执行动作$A$，得到奖励 $R$，并进入状态 $S&rsquo;$，此时，如果是SARSA，会继续基于状态 $S&rsquo;$，用 $ϵ−$贪婪法选择 $A&rsquo;$,然后来更新价值函数。但是Q-Learning则不同。</p>
<p>对于Q-Learning，它基于状态 $S&rsquo;$，没有使用 $ϵ−$贪婪法选择 $A$，而是使用贪婪法选择 $A&rsquo;$，也就是说，选择使 $Q(S&rsquo;,a)$ 最大的 $a$ 作为 $A&rsquo;$来更新价值函数。用数学公式表示就是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</p>
<p>对应到上图中就是在图下方的三个黑圆圈动作中选择一个使 $Q(S&rsquo;,a)$最大的动作作为 $A&rsquo;$。</p>
<p>此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态 $S&rsquo;$，用 $ϵ−$贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的 $A&rsquo;$ 会作为下一阶段开始时候的执行动作。</p>
<p>下面我们对Q-Learning算法做一个总结。</p>
<h1 id="3-q-learning算法流程">3. Q-Learning算法流程</h1>
<p>下面我们总结下Q-Learning算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出: 所有的状态和动作对应的价值 $Q$
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。</li>
<li>b) 用 $ϵ−$贪婪法在当前状态 $S$ 选择出动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$和奖励 $R$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S'$</li>
<li>f) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-q-learning算法实例windy-gridworld">4. Q-Learning算法实例：Windy GridWorld</h1>
<p>我们还是使用和SARSA一样的例子来研究Q-Learning。如果对windy gridworld的问题还不熟悉，可以复习<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第4节的第二段。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>绝大部分代码和SARSA是类似的。这里我们可以重点比较和SARSA不同的部分。区别都在<code>episode()</code>这个函数里面。</p>
<p>首先是初始化的时候，我们只初始化状态 $S$,把 $A$ 的产生放到了while循环里面, 而回忆下SARSA会同时初始化状态 $S$ 和动作 $A$，再去执行循环。下面这段Q-Learning的代码对应我们算法的第二步步骤a和b：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># play for an episode</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">episode</span><span class="p">(</span><span class="n">q_value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># track the total time steps in this episode</span>
</span></span><span class="line"><span class="cl">  <span class="n">time</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="n">state</span> <span class="o">!=</span> <span class="n">GOAL</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>接着我们会去执行动作 $A$,得到 $S&rsquo;$， 由于奖励不是终止就是-1，不需要单独计算。,这部分和SARSA的代码相同。对应我们Q-Learning算法的第二步步骤c：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>后面我们用贪婪法选择出最大的 $Q(S&rsquo;,a)$,并更新价值函数，最后更新当前状态 $S$。对应我们Q-Learning算法的第二步步骤d,e。注意SARSA这里是使用ϵ−�−贪婪法，而不是贪婪法。同时SARSA会同时更新状态S�和动作A�,而Q-Learning只会更新当前状态S�。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span></span></span></code></pre></td></tr></table>
</div>
</div><p>跑完完整的代码，大家可以很容易得到这个问题的最优解，进而得到在每个格子里的最优贪婪策略。</p>
<h1 id="5-sarsa-vs-q-learning">5. SARSA vs Q-Learning</h1>
<p>现在SARSA和Q-Learning算法我们都讲完了，那么作为时序差分控制算法的两种经典方法吗，他们都有说明特点，各自适用于什么样的场景呢？</p>
<p>Q-Learning直接学习的是 <font color=red>最优策略</font>，而SARSA<font color=red>在学习最优策略的同时还在做探索</font>。这导致我们在学习最优策略的时候，如果用SARSA，为了保证收敛，需要制定一个策略，使 $ϵ−$贪婪法的超参数 $ϵ$在迭代的过程中逐渐变小。Q-Learning没有这个烦恼。</p>
<p>另外一个就是Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以<font color=red>受样本数据的影响较大</font>，因此受到训练数据方差的影响很大，甚至会影响Q函数的收敛。Q-Learning的深度强化学习版Deep Q-Learning也有这个问题。</p>
<p>在学习过程中，SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进，导致出现像Q-Learning可能遇到一些特殊的最优“陷阱”。比如经典的强化学习问题&quot;Cliff Walk&quot;。</p>
<p>在实际应用中，如果我们是在模拟环境中训练强化学习模型，推荐使用Q-Learning，如果是 <strong><font color=red>在线生产环境</font></strong> 中训练模型，则推荐使用 <strong><font color=red>SARSA</font></strong>。</p>
<h1 id="6-q-learning结语">6. Q-Learning结语　　　　　　　　</h1>
<p>对于Q-Learning和SARSA这样的时序差分算法，对于小型的强化学习问题是非常灵活有效的，但是在大数据时代，异常复杂的状态和可选动作，使Q-Learning和SARSA要维护的Q表异常的大，甚至远远超出内存，这限制了时序差分算法的应用场景。在深度学习兴起后，基于深度学习的强化学习开始占主导地位，因此从下一篇开始我们开始讨论深度强化学习的建模思路。</p>
]]></description></item></channel></rss>