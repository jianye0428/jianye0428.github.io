<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>RL - 标签 - yejian's blog</title><link>https://jianye0428.github.io/tags/rl/</link><description>RL - 标签 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sun, 25 Feb 2024 19:53:22 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/tags/rl/" rel="self" type="application/rss+xml"/><item><title>强化学习笔记 [19] | AlphaGo Zero强化学习原理</title><link>https://jianye0428.github.io/posts/rl_learning_note_19/</link><pubDate>Sun, 25 Feb 2024 19:53:22 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_19/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。</p>
<p>本篇主要参考了AlphaGo Zero的<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, <a href="https://www.hhyz.me/2018/08/08/2018-08-08-AlphaGO-Zero/"target="_blank" rel="external nofollow noopener noreferrer">AlphaGo Zero综述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和AlphaGo Zero Cheat Sheet。</p>
<h1 id="1-alphago-zero模型基础">1. AlphaGo Zero模型基础</h1>
<p>AlphaGo Zero不需要学习人类的棋谱，通过自我对弈完成棋力提高。主要使用了两个模型，第一个就是我们上一节介绍MCTS树结构，另一个是一个神经网络。MCTS上一篇已经有基本介绍了，对于神经网络，它的输入是当前的棋局状态，输出两部分，第一部分输出是在当前棋局状态下各个可能的落子动作对应的获胜概率p，可以简单理解为Actor-Critic策略函数部分。另一部分输出为获胜或者失败的评估[-1,1]，可以简单理解为Actor-Critic价值函数部分。</p>
<p>AlphaGo Zero的行棋主要是由MCTS指导完成的，但是在MCTS搜索的过程中，由于有一些不在树中的状态需要仿真，做局面评估，因此需要一个简单的策略来帮助MCTS评估改进策略，这个策略改进部分由前面提到的神经网络完成。</p>
<p>这两部分的关系如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">AlphaGo Zero 中的MCTS和NN</div>
</center>
<br>
<p>具体AlphaGo Zero的MCTS如何搜索，神经网络如何训练，如何指导MCTS搜索我们在后面再讲。</p>
<h1 id="2-alphago-zero的训练过程简介">2. AlphaGo Zero的训练过程简介</h1>
<p>在讨论AlphaGo Zero的MCTS如何搜索，神经网络如何训练等细节之前，我们先看看AlphaGo Zero的训练过程是什么样的。</p>
<p>AlphaGo Zero训练过程主要分为三个阶段：自我对战学习阶段，训练神经网络阶段和评估网络阶段。</p>
<p>自我对战学习阶段主要是AlphaGo Zero自我对弈，产生大量棋局样本的过程，由于AlphaGo Zero并不使用围棋大师的棋局来学习，因此需要自我对弈得到训练数据用于后续神经网络的训练。在自我对战学习阶段，每一步的落子是由MCTS搜索来完成的。在MCTS搜索的过程中，遇到不在树中的状态，则使用神经网络的结果来更新MCTS树结构上保存的内容。在每一次迭代过程中，在每个棋局当前状态 $s$ 下，每一次移动使用1600次MCTS搜索模拟。最终MCTS给出最优的落子策略 $π$ ,这个策略 $π$ 和神经网络的输出 $p$ 是不一样的。当每一局对战结束后，我们可以得到最终的胜负奖励 $z$ ,1或者-1. 这样我们可以得到非常多的样本 $(s,π,z)$,这些数据可以训练神经网络阶段。</p>
<p>在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合(s,π,z)(�,�,�),训练我们神经网络的模型参数。训练的目的是对于每个输入 $s$, 神经网络输出的 $p,v$和我们训练样本中的 $π$, $z$差距尽可能的少。这个损失函数 $L$ 其实是很简单的：</p>
<p>$$L=(z-v)^2-\pi^Tlog(p)+c||\theta||^2$$</p>
<p>损失函数由三部分组成，第一部分是均方误差损失函数，用于评估神经网络预测的胜负结果和真实结果之间的差异。第二部分是交叉熵损失函数，用于评估神经网络的输出策略和我们MCTS输出的策略的差异。第三部分是L2正则化项。</p>
<p>通过训练神经网络，我们可以优化神经网络的参数 $θ$,用于后续指导我们的MCTS搜索过程。</p>
<p>当神经网络训练完毕后，我们就进行了评估阶段，这个阶段主要用于确认神经网络的参数是否得到了优化，这个过程中，自我对战的双方各自使用自己的神经网络指导MCTS搜索，并对战若干局，检验AlphaGo Zero在新神经网络参数下棋力是否得到了提高。除了神经网络的参数不同，这个过程和第一阶段的自我对战学习阶段过程是类似的。</p>
<h1 id="3-alphago-zero的神经网络结构">3. AlphaGo Zero的神经网络结构</h1>
<p>在第二节我们已经讨论了AlphaGo Zero的主要训练过程，但是还有两块没有讲清楚，一是AlphaGo Zero的MCTS搜索过程是怎么样的，二是AlphaGo Zero的神经网络的结构具体是什么样的。这一节我们来看看AlphaGo Zero的神经网络的细节。</p>
<p>首先我们看看AlphaGo Zero的输入，当前的棋局状态。由于围棋是19x19的361个点组成的棋局，每个点的状态有二种：如果当前是黑方行棋，则当前有黑棋的点取值1，有白棋或者没有棋子的位置取值为0，反过来，如果当前是白方行棋，则当前有白棋的点取值1，有黑棋或者没有棋子的位置取值为0。同时，为了提供更多的信息，输入的棋局状态不光只有当前的棋局状态，包括了黑棋白棋各自前8步对应的棋局状态。除了这16个棋局状态，还有一个单独的棋局状态用于标识当前行棋方，如果是当前黑棋行棋，则棋局状态上标全1，白棋则棋局状态上标全0。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Game State</div>
</center>
<br>
<p>最终神经网络的输入是一个19x19x17的张量。里面包含黑棋和白棋的最近8步行棋状态和当前行棋方的信息。</p>
<p>接着我们看看神经网络的输出，神经网络的输出包括策略部分和价值部分。对于策略部分，它预测当前各个行棋点落子的概率。由于围棋有361个落子点，加上还可以Pass一手，因此一共有362个策略端概率输出。对于价值端，输出就简单了，就是当前局面胜负的评估值，在[-1,1]之间。</p>
<p>看完了神经网络的输入和输出，我们再看看神经网络的结构，主要是用CNN组成的深度残差网络。如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>在19x19x17的张量做了一个基本的卷积后，使用了19层或者39层的深度残差网络，这个是ResNet的经典结构。理论上这里也可以使用DenseNet等其他流行的网络结构。神经网络的损失函数部分我们在第二节已经将了。整个神经网络就是为了当MCTS遇到没有见过的局面时，提供的当前状态下的局面评估和落子概率参考。这部分信息会被MCTS后续综合利用。</p>
<h1 id="4-alphago-zero的mcts搜索">4. AlphaGo Zero的MCTS搜索</h1>
<p>　　　　现在我们来再看看AlphaGo Zero的MCTS搜索过程，在<a href="https://www.cnblogs.com/pinard/p/10470571.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们已经介绍了MCTS的基本原理，和4个主要的搜索阶段：选择，扩展，仿真和回溯。和上一篇的内容相比，这里MCTS的不同主要体现在树结构上保存的信息不同，进而UCT的计算公式也稍有不同。最后MCTS搜索完毕后，AlphaGo Zero也有自己选择真正落子点的策略。</p>
<p>　　　　在上一篇里，我们的MCTS上保存的数据很简单，就是下的总盘数和赢的总盘数。在AlphaGo Zero这里，我们保存的信息会多一些。主要包括下面的4部分：</p>
<ul>
<li>$N(s,a)$:记录边的访问次数</li>
<li>$W(s,a)$: 合计行动价值</li>
<li>$Q(s,a)$:平均行动价值</li>
<li>$P(s,a)$:选择该条边的先验概率</li>
</ul>
<p>其中 $s$ 为当前棋局状态，$a$ 为某一落子选择对应的树分支。</p>
<p>有了MCTS上的数据结构，我们看看AlphaGo Zero的MCTS搜索的4个阶段流程：</p>
<p>首先是选择，在MCTS内部，出现过的局面，我们会使用UCT选择子分支。子分支的UCT原理和上一节一样。但是具体的公式稍有不同，如下：</p>
<p>$$\begin{gathered}
U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)} \\
a_t=\arg\max_a(Q(s_t,a)+U(s_t,a))
\end{gathered}$$</p>
<p>最终我们会选择 $Q+U$最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数,上一篇已讲过。</p>
<p>如果到了没有到终局的MCTS叶子节点，那么我们就需要进入MCTS的第二步，扩展阶段,以及后续的第三步仿真阶段。我们这里一起讲。对于叶子节点状态s�，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL��落子的概率p�和对应的价值v�,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为：</p>
<p>$$\{N(s_L,a)=0,W(s_L,a)=0,Q(s_L,a)=0,P(s_L,a)=P_a\}$$</p>
<p>这个过程如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>这样扩展后，之前的叶子节点 $s$，现在就是内部节点了。做完了扩展和仿真后，我们需要进行回溯，将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点 $L$ 依次向根节点回溯，并依次更新上层分支数据结构如下：</p>
<p>$$\begin{gathered}
N(s_t,a_t)=N(s_t,a_t)+1 \\
W(s_t,a_t)=W(s_t,a_t)+v \\
Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}
\end{gathered}$$</p>
<p>这个MCTS搜索过程在一次真正行棋前，一般会进行约1600次搜索，每次搜索都会进行上述4个阶段。</p>
<p>这上千次MCTS搜索完毕后，AlphaGo Zero就可以在MCTS的根节点 $s$ 基于以下公式选择行棋的MCTS分支了:</p>
<p>$$\pi(a|s)=\frac{N(s,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}$$</p>
<p>其中，$τ$ 为温度参数，控制探索的程度，$τ$ 越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。每一次完整的自我对弈的前30步，参数 $τ=1$，这是早期鼓励探索的设置。游戏剩下的步数，该参数将逐渐降低至0。如果是比赛，则直接为0.</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>同时在随后的时间步中，这个MCTS搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 。</p>
<p>以上就是AlphaGo Zero MCTS搜索的过程。</p>
<h1 id="5-alphago-zero小结与强化学习系列小结">5. AlphaGo Zero小结与强化学习系列小结</h1>
<p>AlphaGo Zero巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。两者一主一辅，非常优雅的解决了这类状态完全可见，信息充分的棋类问题。</p>
<p>当然这类强化学习算法只对特定的这类完全状态可见，信息充分的问题有效，遇到信息不对称的强化学习问题，比如星际，魔兽之类的对战游戏问题，这个算法就不那么有效了。要推广AlphaGo Zero的算法到大多数普通强化学习问题还是很难的。因此后续强化学习算法应该还有很多发展的空间。</p>
<p>至此强化学习系列就写完了，之前预计的是写三个月，结果由于事情太多，居然花了大半年。但是总算还是完成了，没有烂尾。生活不易，继续努力！</p>
]]></description></item><item><title>强化学习笔记 [18] | 基于模拟的搜索与蒙特卡罗树搜索(MCTS)</title><link>https://jianye0428.github.io/posts/rl_learning_note_18/</link><pubDate>Sun, 25 Feb 2024 19:53:18 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_18/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。</p>
<p>本篇主要参考了UCL强化学习课程的第八讲，第九讲部分。</p>
<h1 id="1-基于模拟的搜索概述">1. 基于模拟的搜索概述</h1>
<p>什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这是数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。</p>
<p>那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点 $S_t$ 开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以 $S_t$ 为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到 $S_t$状态最应该采用的动作 $A_t$。前向搜索的sub-MDP如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">forward search sub-MDP</div>
</center>
<br>
<p>前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。</p>
<h1 id="2-简单蒙特卡罗搜索">2. 简单蒙特卡罗搜索</h1>
<p>首先我们看看基于模拟的搜索中比较简单的一种方法：简单蒙特卡罗搜索。</p>
<p>简单蒙特卡罗搜索基于一个强化学习模型 $M_v$ 和一个模拟策略 $π$.在此基础上，对于当前我们要选择动作的状态 $S_t$, 对每一个可能采样的动作 $a∈A$,都进行 $K$ 轮采样，这样每个动作 $a$ 都会得到 $K$ 组经历完整的状态序列(episode)。即：</p>
<p>$$\{S_t,a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>现在对于每个 $(S_t,a)$ 组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1K\sum_{k=1}^KG_t\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别,那么简单蒙特卡罗搜索也太慢了。同时，由于使用蒙特卡罗法计算其动作价值函数，模拟采样得到的一些中间状态和对应行为的价值就被忽略了，这部分数据能不能利用起来呢？</p>
<p>下面我们看看蒙特卡罗树搜索(Monte-Carlo Tree Search，以下简称MCTS)怎么优化这个问题的解决方案。</p>
<h1 id="3-mcts的原理">3. MCTS的原理</h1>
<p>MCTS摒弃了简单蒙特卡罗搜索里面对当前状态 $S_t$ 每个动作都要进行K次模拟采样的做法，而是总共对当前状态 $S_t$进行K次采样，这样采样到的动作只是动作全集 $A$ 中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。</p>
<p>在MCTS中，基于一个强化学习模型 $M_v$和一个模拟策略$π$，当前状态 $S_t$ 对应的完整的状态序列(episode)是这样的:</p>
<p>$$\{S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots\ldots S_T^k\}_{k=1}^K\sim M_v,\pi $$</p>
<p>采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算 $Q(st,a)$和最大 $Q(s_t,a)$对应的动作。</p>
<p>$$\begin{gathered}Q(S_t,a)=\frac1{N(S_t,a)}\sum_{k=1}^K\sum_{u=t}^T1(S_{uk}=S_t,A_{uk}=a)G_u\\a_t=\arg\max_{a\in A}Q(S_t,a)\end{gathered}$$</p>
<p>MCTS搜索的策略分为两个阶段：第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使 $ϵ−$贪婪策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。</p>
<p>这里讲到的是最经典的强化学习终MCTS的用户，每一步都有延时奖励，但是在棋类之类的零和问题中，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。</p>
<h1 id="4-上限置信区间算法uct">4. 上限置信区间算法UCT</h1>
<p>在讨论棋类游戏的MCTS搜索之前，我们先熟悉下上限置信区间算法(Upper Confidence Bound Applied to Trees, 以下简称UCT)。它是一种策略算法，我们之前最常用的是 $ϵ−$贪婪策略。但是在棋类问题中，UCT更常使用。</p>
<p>在棋类游戏中，经常有这样的问题，我们发现在某种棋的状态下，有2个可选动作，第一个动作历史棋局中是0胜1负，第二个动作历史棋局中是8胜10负，那么我们应该选择哪个动作好呢？如果按 $ϵ−$贪婪策略，则第二个动作非常容易被选择到。但是其实虽然第一个动作胜利0%，但是很可能是因为这个动作的历史棋局少，数据不够导致的，很可能该动作也是一个不错的动作。那么我们如何在最优策略和探索度达到一个选择平衡呢？ $ϵ−$贪婪策略可以用，但是UCT是一个更不错的选择。</p>
<p>UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度吗，一个常用的公式如下：</p>
<p>$$\text{score}=\left.\frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}\right.$$</p>
<p>其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$ 是所有模拟次数，c 是探索常数，理论值为$√2$，可根据经验调整，$c$ 越大就越偏向于广度搜索，$c$ 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。</p>
<p>比如对于下面的棋局，对于根节点来说，有3个选择，第一个选择7胜3负，第二个选择5胜3负，第三个选择0胜3负。</p>
<p>如果我们取c=10,则第一个节点的分数为：$$score(7,10)=7/10+C\cdot\sqrt{\frac{\log(21)}{10}}\approx6.2$$</p>
<p>第二个节点的分数为：$$score(5,8)=5/8+C\cdot\sqrt{\frac{\log(21)}8}\approx6.8$$</p>
<p>第三个节点的分数为：$$score(0,3)=0/3+C\cdot\sqrt{\frac{\log(21)}3}\approx10$$</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div>
</center>
<br>
<p>可见，由于我们把探索率 $c$ 设置的比较大，第三个节点是被UCT选中要执行的动作节点。当然如果我们把c设置的比较小的话，第一个或者第二个可能就变成最大的分数了。</p>
<h1 id="5-棋类游戏mcts搜索">5. 棋类游戏MCTS搜索</h1>
<p>在像中国象棋，围棋这样的零和问题中，一个动作只有在棋局结束才能拿到真正的奖励，因此我们对MCTS的搜索步骤和树结构上需要根据问题的不同做一些细化。</p>
<p>对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步，如下图(图来自维基百科)：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">BP Network</div>
</center>
<br>
<p>第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。</p>
<p>第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。</p>
<p>第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。</p>
<p>第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。</p>
<p>以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。</p>
<h1 id="6-mcts小结">6. MCTS小结</h1>
<p>MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。</p>
]]></description></item><item><title>强化学习笔记 [17] | 基于模型的强化学习与Dyna算法框架</title><link>https://jianye0428.github.io/posts/rl_learning_note_17/</link><pubDate>Sun, 25 Feb 2024 19:53:15 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_17/</guid><description><![CDATA[<h1 id="强化学习十七-基于模型的强化学习与dyna算法框架httpswwwcnblogscompinardp10384424html"><a href="https://www.cnblogs.com/pinard/p/10384424.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十七) 基于模型的强化学习与Dyna算法框架<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h1>
<p>在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。</p>
<p>本篇主要参考了UCL强化学习课程的第8讲和Dyna-2的<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-基于模型的强化学习简介">1. 基于模型的强化学习简介</h1>
<p>基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率 $P^a_{ss&rsquo;}$。</p>
<p>而基于模型的强化学习则会尝试从环境的模型去学习，一般是下面两个相互独立的模型：</p>
<ul>
<li>一个是状态转化预测模型，输入当前状态 $s$和动作 $a$，预测下一个状态 $s&rsquo;$。</li>
<li>另一个是奖励预测模型，输入当前状态 $s$和动作 $a$，预测环境的奖励 $r$。</li>
</ul>
<p>即模型可以描述为下面两个式子：</p>
<p>$$\begin{gathered}S_{t+1}\sim P(S_{t+1}|S_t,A_t)\\R_{t+1}\sim R(R_{t+1}|S_t,A_t)\end{gathered}$$</p>
<p>如果模型 $P$, $R$ 可以准确的描述真正的环境的转化模型，那么我们就可以基于模型来预测，当有一个新的状态 $S$ 和动作 $A$到来时，我们可以直接基于模型预测得到新的状态和动作奖励，不需要和环境交互。当然如果我们的模型不好，那么基于模型预测的新状态和动作奖励可能错的离谱。</p>
<p>从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的主要区别：即基于模型的强化学习是从模型中学习，而不基于模型的强化学习是从和环境交互的经历去学习。</p>
<p>下面这张图描述了基于模型的强化学习的思路：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Model-based RL</div>
</center>
<br>
<h1 id="2-基于模型的强化学习算法训练流程">2. 基于模型的强化学习算法训练流程</h1>
<p>这里我们看看基于模型的强化学习算法训练流程，其流程和我们监督学习算法是非常类似的。</p>
<p>假设训练数据是若干组这样的经历：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,R_2,\ldots,S_T$$</p>
<p>对于每组经历，我们可以将其转化为 $T−1$ 组训练样本，即：</p>
<p>$$\begin{gathered}
S_1,A_1\to S_2,S_1,A_1\to R_2 \\
S_2,A_2\to S_3,S_2,A_2\to R_3 \\
&hellip;&hellip; \\
S_{T-1},A_{T-1}\rightarrow S_T,~S_{T_1},A_{T-1}\rightarrow R_T
\end{gathered}$$</p>
<p>右边的训练样本一起组成了一个分类模型或密度估计模型，输入状态和动作，输出下一个状态。 右边的训练样本一起组成了一个回归模型训练集，输入状态和动作，输出动作奖励值。</p>
<p>至此我们的强化学习求解过程和传统的监督学习算法没有太多区别了，可以使用传统的监督学习算法来求解这两个模型。</p>
<p>当然还可以更简单，即通过对训练样本进行查表法进行统计，直接得到 $P(S_{t+1}|S_t,A_t)$ 的概率和 $R(R_{t+1}|S_t,A_t)$ 的平均值，这样就可以直接预测。比使用模型更简单。</p>
<p>此外，还有其他的方法可以用来得到 $P(S_{t+1}|S_t,A_t)$和 $R(R_{t+1}|S_t,A_t)$，这个我们后面再讲。</p>
<p>虽然基于模型的强化学习思路很清晰，而且还有不要和环境持续交互优化的优点，但是用于实际产品还是有很多差距的。主要是我们的模型绝大多数时候不能准确的描述真正的环境的转化模型，那么使用基于模型的强化学习算法得到的解大多数时候也不是很实用。那么是不是基于模型的强化学习就不能用了呢？也不是，我们可以将基于模型的强化学习和不基于模型的强化学习集合起来，取长补短，这样做最常见的就是Dyna算法框架。</p>
<h1 id="3-dyna算法框架">3. Dyna算法框架</h1>
<p>Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。如果用和第一节类似的图，可以表示如下图，和第一节的图相比，多了一个“Direct RL“的箭头，这正是不基于模型的强化学习的思路。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna算法示意图</div>
</center>
<br>
<p>Dyna算法框架和不同的具体的不基于模型的强化学习一起，可以得到具体的不同算法。如果我们使用基于价值函数的Q-Learning，那么我们就得到了Dyna-Q算法。我们基于Dyna-Q来看看Dyna算法框架的一般流程.</p>
<h1 id="4-dyna-q算法流程">4. Dyna-Q算法流程</h1>
<p>这里我们给出基于价值函数的Dyna-Q算法的概要流程。假设模型使用的是查表法。</p>
<ul>
<li>(1). 初始化任意一个状态 $s$,和任意一个动作 $a$ 对应的状态价值 $Q(s,a)$, 初始化奖励模型 $R(s,a)$和状态模型 $P(s,a)$</li>
<li>(2). for $i=1$ to 最大迭代次数T：
<ul>
<li>(a) $S \leftarrow \text{current state}$</li>
<li>(b) $A \leftarrow \text{ϵ−greedy(S,Q)}$</li>
<li>(c) 执行动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>(d) 使用Q-Learning更新价值函数：$Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
<li>(e) 使用 $S,A,S^{\prime}$ 更新状态模型 $P(s,a)$，使用 $S,A,R$ 更新状态模型 $R(s,a)$</li>
<li>(f) $\text{for} \space \space j=1 \space \space \text{to} \text{最大次数}n$：
<ul>
<li>(i) 随机选择一个之前出现过的状态 $S$ , 在状态 $S$ 上出现过的动作中随机选择一个动作 $A$</li>
<li>(ii) 基于模型 $P(S,A)$ 得到 $S&rsquo;$, 基于模型 $R(S,A)$ 得到 $R$</li>
<li>(iii) 使用Q-Learning更新价值函数: $Q(S,A)=Q(S,A)+\alpha[R+\gamma\max_aQ(S^{\prime},a)-Q(S,A)]$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>从上面的流程可以看出，Dyna框架在每个迭代轮中，会先和环境交互，并更新价值函数和（或）策略函数，接着进行n次模型的预测，同样更新价值函数和（或）策略函数。这样同时利用上了和环境交互的经历以及模型的预测。</p>
<h1 id="5-dyna-2算法框架">5. Dyna-2算法框架</h1>
<p>在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为<strong>永久性记忆</strong>（permanent memory）和<strong>瞬时记忆</strong>（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。</p>
<p>永久性记忆的Q函数定义为：</p>
<p>$$Q(S,A)=\phi(S,A)^T\theta $$</p>
<p>瞬时记忆的Q函数定义为：</p>
<p>$$Q^{\prime}(S,A)=\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>组合起来后记忆的Q函数定义为：</p>
<p>$$\overline{Q}(S,A)=\phi(S,A)^T\theta+\overline{\phi}(S,A)^T\overline{\theta}$$</p>
<p>Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似于 $SARSA(λ)$</p>
<p>以下是Dyna-2的算法流程：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Dyna-2 算法流程</div>
</center>
<br>
<h1 id="6-基于模型的强化学习总结">6. 基于模型的强化学习总结</h1>
<p>基于模型的强化学习一般不单独使用，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。对于模型部分，我们可以用查表法和监督学习法等方法，预测或者采样得到模拟的经历。而对于非模型部分，使用前面的Q-Learning系列的价值函数近似，或者基于Actor-Critic的策略函数的近似都是可以的。</p>
<p>除了Dyna算法框架，我们还可以使用基于模拟的搜索(simulation-based search)来结合基于模型的强化学习和不基于模型的强化学习,并求解问题。这部分我们在后面再讨论。</p>
]]></description></item><item><title>强化学习笔记 [16] | 深度确定性策略梯度(DDPG)</title><link>https://jianye0428.github.io/posts/rl_learning_note_16/</link><pubDate>Sun, 25 Feb 2024 19:53:12 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_16/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10334127.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十五) A3C<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Policy Gradient，以下简称DDPG)。</p>
<p>本篇主要参考了DDPG的<a href="https://arxiv.org/pdf/1509.02971.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-从随机策略到确定性策略">1. 从随机策略到确定性策略</h1>
<p>从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。PG(Policy Gradient)我们在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里已经讨论过。那什么是确定性策略梯度(Deterministic Policy Gradient，以下简称DPG)呢？</p>
<p>确定性策略是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。</p>
<p>作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成：</p>
<p>$$\pi_\theta(s)=a$$</p>
<h1 id="2-从dpg到ddpg">2. 从DPG到DDPG</h1>
<p>在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>其中状态的采样空间为$\rho^\pi$, $\nabla_\theta log\pi_\theta(s,a)$是分值函数，可见随机性策略梯度需要在整个动作的空间$\pi_\mathrm{\theta}$进行采样。</p>
<p>而DPG基于Q值的确定性策略梯度的梯度计算公式是：</p>
<p>$$\nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi}[\nabla_\theta\pi_\theta(s)\nabla_aQ_\pi(s,a)|<em>{a=\pi</em>\theta(s)}]$$</p>
<p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p>
<p>而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？</p>
<h1 id="3-ddpg的原理">3. DDPG的原理</h1>
<p>DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。可以复习<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>DDQN的当前Q网络负责对当前状态 $S$ 使用 $ϵ$−贪婪法选择动作 $A$，执行动作 $A$,获得新状态 $S&rsquo;$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态 $S&rsquo;$使用贪婪法选择动作 $A&rsquo;$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。</p>
<p>DDQN的目标Q网络则负责基于经验回放池计算目标Q值, 提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。</p>
<p>现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要 $ϵ$−贪婪法这样的选择方法，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态 $S&rsquo;$ 使用贪婪法选择动作 $A&rsquo;$，这部分工作由于用来估计目标Q值，因此可以放到Actor目标网络完成。</p>
<p>基于经验回放池和目标Actor网络提供的 $S&rsquo;$, $A&rsquo;$ 计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。</p>
<p>此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。</p>
<p>有了上面的思路，我们总结下DDPG 4个网络的功能定位：</p>
<ul>
<li>
<p>(1). <strong>Actor当前网络</strong>: 负责策略网络参数 $θ$的迭代更新，负责根据当前状态 $S$选择当前动作 $A$，用于和环境交互生成 $S&rsquo;$, $R$。</p>
</li>
<li>
<p>(2). <strong>Actor目标网络</strong>: 负责根据经验回放池中采样的下一状态 $S&rsquo;$ 选择最优下一动作$A&rsquo;$。网络参数 $θ&rsquo;$定期从 $θ$复制。</p>
</li>
<li>
<p>(3). <strong>Critic当前网络</strong>: 负责价值网络参数 $w$的迭代更新，负责计算负责计算当前Q值 $Q(S,A,w)$。目标Q值$y_i=R+γQ&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$</p>
</li>
<li>
<p>(4). <strong>Critic目标网络</strong>: 负责计算目标Q值中的 $Q&rsquo;(S&rsquo;,A&rsquo;,w&rsquo;)$部分。网络参数 $w&rsquo;$ 定期从 $w$复制。</p>
</li>
</ul>
<p>DDPG除了这4个网络结构，还用到了经验回放，这部分用于计算目标Q值，和DQN没有什么区别，这里就不展开了。</p>
<p>此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w$′=$w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：</p>
<p>$$\begin{gathered}
w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</p>
<p>其中 $τ$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。</p>
<p>同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作 $A$会增加一定的噪声 $N$, 即最终和环境交互的动作 $A$ 的表达式是：</p>
<p>$$A=\pi_\theta(S)+\mathcal{N}$$</p>
<p>最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：</p>
<p>$$J(w)=\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>而对于 Actor当前网络，其损失函数就和之前讲的PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：</p>
<p>$$\nabla_J(\theta)=\frac1m\sum_{j=1}^m[\nabla_aQ_(s_i,a_i,w)|<em>{s=s_i,a=\pi</em>\theta(s)}\nabla_\theta\pi_{\theta(s)}|_{s=s_i}]$$</p>
<p>这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作 $a_1$和$a_2$，从Critic当前网络得到了两个反馈的 $Q$ 值，分别是 $Q_1$,$Q_2$，假设 $Q_1&gt;Q_2$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加 $a_1$的概率，降低$a_2$的概率，也就是说，Actor想要尽可能的得到更大的Q值。所以我们的Actor的损失可以简单的理解为得到的反馈Q值越大损失越小，得到的反馈Q值越小损失越大，因此只要对状态估计网络返回的Q值取个负号即可，即：</p>
<p>$$J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,w)$$</p>
<h1 id="4-ddpg算法流程">4. DDPG算法流程</h1>
<p>这里我们总结下DDPG的算法流程</p>
<p>输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为 $θ$,$θ&rsquo;$,$w$,$w&rsquo;$,衰减因子 $γ$, 软更新系数 $τ$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$。最大迭代次数 $T$。随机噪音函数 $\mathcal{N}$</p>
<p>输出：最优Actor当前网络参数 $θ$,Critic当前网络参数 $w$</p>
<ul>
<li>(1). 随机初始化$θ$,$w$, $w$′=$w$,$θ$′=$θ$。清空经验回放的集合$D$</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor当前网络基于状态 $S$ 得到动作 $A=π_θ(ϕ(S))+\mathcal{N}$</li>
<li>c) 执行动作$A$,得到新状态$S$′,奖励$R$,是否终止状态%is_end$</li>
<li>d) 将 ${ϕ(S), A, R, ϕ(S&rsquo;), is_end}$ 这个五元组存入经验回放集合$D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本${\phi(S_j),A_j,R_j,\phi(S_j^{\prime}),is_end_j},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\pi_{\theta^{\prime}}(\phi(S_j^{\prime})),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Critic当前网络的所有参数 $w$</li>
<li>h) 使用 $\begin{aligned}J(\theta)=-\frac1m\sum_{j=1}^mQ_(s_i,a_i,\theta)\end{aligned}$，通过神经网络的梯度反向传播来更新Actor当前网络的所有参数 $θ$</li>
<li>i) 如果 i%C=1, 则更新Critic目标网络和Actor目标网络参数：
<ul>
<li>$$\begin{gathered} w&rsquo;\leftarrow\tau w+(1-\tau)w&rsquo; \
\theta&rsquo;\leftarrow\tau\theta+(1-\tau)\theta'
\end{gathered}$$</li>
</ul>
</li>
<li>j) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤(b)</li>
</ul>
</li>
</ul>
<p>以上就是DDPG算法的主流程，要注意的是上面2.f中的 $\pi_{\theta^{\prime}}(\phi(S_j^{\prime}))$ 是通过Actor目标网络得到，而 $Q^{\prime}(\phi(S_i^{\prime}),\pi_{\theta^{\prime}}(\phi(S_i^{\prime})),w^{\prime})$ 则是通过Critic目标网络得到的。</p>
<h1 id="5-ddpg实例">5. DDPG实例</h1>
<p>这里我们给出DDPG第一个算法实例，代码主要参考自莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG/DDPG_update.py"target="_blank" rel="external nofollow noopener noreferrer">Github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。增加了测试模型效果的部分，优化了少量参数。代码详见：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py</p>
<p>这里我们没有用之前的CartPole游戏，因为它不是连续动作。我们使用了Pendulum-v0这个游戏。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a href="https://github.com/openai/gym/wiki/Pendulum-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。输入状态是角度的sin，cos值，以及角速度。一共三个值。动作是一个连续的力矩值。</p>
<p>两个Actor网络和两个Critic网络的定义参见：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_bound</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;scaled_a&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_c</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">trainable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_l1</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_s</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_s&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;w1_a&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_l1</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">w1_s</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">w1_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>  <span class="c1"># Q(s,a)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor当前网络和Critic当前网络损失函数的定义参见：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">q_target</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">td_error</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ce_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">a_loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>    <span class="c1"># maximize the q</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">atrain</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">a_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ae_params</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Actor目标网络和Critic目标网络参数软更新，Actor当前网络和Critic当前网络反向传播更新部分的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># soft target replacement</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">soft_replace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">MEMORY_CAPACITY</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">bt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">ba</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">br</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">bs_</span> <span class="o">=</span> <span class="n">bt</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">s_dim</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">atrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctrain</span><span class="p">,</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">S</span><span class="p">:</span> <span class="n">bs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">:</span> <span class="n">ba</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">:</span> <span class="n">br</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_</span><span class="p">:</span> <span class="n">bs_</span><span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余的可以对照算法和代码一起学习，应该比较容易理解。</p>
<h1 id="6-ddpg总结">6. DDPG总结</h1>
<p>DDPG参考了DDQN的算法思想吗，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p>
<p>到此，我们的Policy Based RL系列也讨论完了，而在更早我们讨论了Value Based RL系列，至此，我们还剩下Model Based RL没有讨论。后续我们讨论Model Based RL的相关算法。</p>
]]></description></item><item><title>强化学习笔记 [15] | A3C</title><link>https://jianye0428.github.io/posts/rl_learning_note_15/</link><pubDate>Sun, 25 Feb 2024 15:36:01 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_15/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C的算法原理和算法流程。</p>
<p>本文主要参考了A3C的<a href="http://proceedings.mlr.press/v48/mniha16.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，以及ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-a3c的引入">1. A3C的引入</h1>
<p>上一篇Actor-Critic算法的代码，其实很难收敛，无论怎么调参，最后的CartPole都很难稳定在200分，这是Actor-Critic算法的问题。但是我们还是有办法去有优化这个难以收敛的问题的。</p>
<p>回忆下之前的DQN算法，为了方便收敛使用了经验回放的技巧。那么我们的Actor-Critic是不是也可以使用经验回放的技巧呢？当然可以！不过A3C更进一步，还克服了一些经验回放的问题。经验回放有什么问题呢？ 回放池经验数据相关性太强，用于训练的时候效果很可能不佳。举个例子，我们学习下棋，总是和同一个人下，期望能提高棋艺。这当然没有问题，但是到一定程度就再难提高了，此时最好的方法是另寻高手切磋。</p>
<p>A3C的思路也是如此，它<font color=green>利用多线程的方法，同时在多个线程里面分别和环境进行交互学习，每个线程都把学习的成果汇总起来，整理保存在一个公共的地方</font>。并且，定期从公共的地方把大家的齐心学习的成果拿回来，指导自己和环境后面的学习交互。</p>
<p>通过这种方法，A3C避免了经验回放相关性过强的问题，同时做到了异步并发的学习模型。</p>
<h1 id="2-a3c的算法优化">2. A3C的算法优化</h1>
<p>现在我们来看看相比Actor-Critic，A3C到底做了哪些具体的优化。</p>
<p>相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p>我们首先来看这个异步训练框架，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">异步训练框架</div>
</center>
<br>
<p>图中上面的Global Network就是上一节说的共享的公共部分，主要是一个公共的神经网络模型，这个神经网络包括Actor网络和Critic网络两部分的功能。下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。</p>
<p>每个线程和环境交互到一定量的数据后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度却并不更新自己线程里的神经网络，而是去更新公共的神经网络。也就是n个线程会独立的使用累积的梯度分别更新公共部分的神经网络模型参数。每隔一段时间，线程会将自己的神经网络的参数更新为公共神经网络的参数，进而指导后面的环境交互。</p>
<p>可见，公共部分的网络模型就是我们要学习的模型，而线程里的网络模型主要是用于和环境交互使用的，这些线程里的模型可以帮助线程更好的和环境交互，拿到高质量的数据帮助模型更快收敛。</p>
<p>现在我们来看看第二个优化，网络结构的优化。之前在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们使用了两个不同的网络Actor和Critic。在A3C这里，我们把两个网络放到了一起，即输入状态 $S$,可以输出状态价值 $V$,和对应的策略 $π$, 当然，我们仍然可以把Actor和Critic看做独立的两块，分别处理，如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">把Actor和Critic看做独立的两块，分别处理</div>
</center>
<br>
<p>第三个优化点是Critic评估点的优化，在<a href="https://www.cnblogs.com/pinard/p/10272023.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十四) Actor-Critic<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节中，我们讨论了不同的Critic评估点的选择，其中d部分讲到了使用优势函数 $A$ 来做Critic评估点，优势函数 $A$ 在时刻t不考虑参数的默认表达式为：</p>
<p>$$A(S,A,t)=Q(S,A)-V(S)$$</p>
<p>$Q(S,A)$的值一般可以通过单步采样近似估计，即：</p>
<p>$$Q(S,A)=R+\gamma V(S^{\prime})$$</p>
<p>这样优势函数去掉动作可以表达为：</p>
<p>$$A(S,t)=R+\gamma V(S^{\prime})-V(S)$$</p>
<p>其中 $V(S)$的值需要通过Critic网络来学习得到。</p>
<p>在A3C中，采样更进一步，使用了N步采样，以加速收敛。这样A3C中使用的优势函数表达为：</p>
<p>$$A(S,t)=R_t++\gamma R_{t+1}+\ldots\gamma^{n-1}R_{t+n-1}+\gamma^nV(S^{\prime})-V(S)$$</p>
<p>对于Actor和Critic的损失函数部分，和Actor-Critic基本相同。有一个小的优化点就是在Actor-Critic策略函数的损失函数中，加入了策略 $π$ 的熵项,系数为 $c$, 即策略参数的梯度更新和Actor-Critic相比变成了这样：</p>
<p>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,t)+c\nabla_\theta H(\pi(S_t,\theta))$$</p>
<p>以上就是A3C和Actor-Critic相比有优化的部分。下面我们来总价下A3C的算法流程。</p>
<h1 id="3-a3c算法流程">3. A3C算法流程</h1>
<p>这里我们对A3C算法流程做一个总结，由于A3C是异步多线程的，我们这里给出任意一个线程的算法流程。</p>
<ul>
<li>
<p>输入：公共部分的A3C神经网络结构，对应参数位 $θ$ , $w$，本线程的A3C神经网络结构，对应参数 $θ&rsquo;$, $w&rsquo;$, 全局共享的迭代轮数 $T$，全局最大迭代次数 $T_{max}$, 线程内单次迭代时间序列最大长度 $T_{local}$,状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，熵系数 $c$, 衰减因子 $γ$</p>
</li>
<li>
<p>输出：公共部分的A3C神经网络参数 $θ$, $w$</p>
<ul>
<li>(1). 更新时间序列 $t=1$</li>
<li>(2). 重置Actor和Critic的梯度更新量: $dθ←0$,$dw←0$</li>
<li>(3). 从公共部分的A3C神经网络同步参数到本线程的神经网络：$θ&rsquo;=θ,w&rsquo;=w$</li>
<li>(4). $t_{start}=t$，初始化状态 $s_t$</li>
<li>(5). 基于策略 $π(at|st;θ)$ 选择出动作 $a_t$</li>
<li>(6). 执行动作 $a_t$得到奖励 $r_t$ 和新状态 $s_{t+1}$</li>
<li>(7). $t←t+1$, $T←T+1$</li>
<li>(8). 如果 $s_t$是终止状态，或 $t − t_{start}==t_{local}$,则进入步骤(9)，否则回到步骤(5)</li>
<li>(9). 计算最后一个时间序列位置 $s_t$的 $Q(s,t)$:
<ul>
<li>$$\left.Q(s,t)=\left\{\begin{array}{ll}0&amp;terminal<del>state\\V(s_t,w^{\prime})&amp;none</del>terminal~state,bootstrapping\end{array}\right.\right.$$</li>
</ul>
</li>
<li>(10). for $i∈(t−1,t−2,&hellip;t_{start})$:
<ul>
<li>1). 计算每个时刻的$Q(s,i)$： $Q(s,i)=r_i+\gamma Q(s,i+1)$</li>
<li>2). 累计Actor的本地梯度更新：
<ul>
<li>$$d\theta\leftarrow d\theta+\nabla_{\theta^{\prime}}log\pi_{\theta^{\prime}}(s_i,a_i)(Q(s,i)-V(S_i,w^{\prime}))+c\nabla_{\theta^{\prime}}H(\pi(s_i,\theta^{\prime}))$$</li>
</ul>
</li>
<li>3). 累计Critic的本地梯度更新：
<ul>
<li>$$\begin{aligned}dw&amp;\leftarrow dw+\frac{\partial(Q(s,i)-V(S_i,w^{\prime}))^2}{\partial w^{\prime}}\end{aligned}$$</li>
</ul>
</li>
</ul>
</li>
<li>(11). 更新全局神经网络的模型参数：
<ul>
<li>$$\theta=\theta+\alpha d\theta,~w=w-\beta dw$$</li>
</ul>
</li>
<li>(12). 如果 $T&gt;T_{max}$,则算法结束，输出公共部分的A3C神经网络参数 $θ$, $w$,否则进入步骤(3)</li>
</ul>
</li>
</ul>
<p>以上就是A3C算法单个线程的算法流程。</p>
<h1 id="4-a3c算法实例">4. A3C算法实例</h1>
<p>下面我们基于上述算法流程给出A3C算法实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法代码大部分参考了莫烦的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/10_A3C/A3C_discrete_action.py"target="_blank" rel="external nofollow noopener noreferrer">A3C代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，增加了模型测试部分的代码并调整了部分模型参数。完整的代码参见我的Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py</p>
<p>整个算法的Actor和Critic的网络结构都定义在这里， 所有的线程中的网络结构，公共部分的网络结构都在这里定义。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_build_net</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">w_init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;actor&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;la&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_a</span><span class="p">,</span> <span class="n">N_A</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ap&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;critic&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lc&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">l_c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;v&#39;</span><span class="p">)</span>  <span class="c1"># state value</span>
</span></span><span class="line"><span class="cl">  <span class="n">a_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/actor&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">c_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span> <span class="o">+</span> <span class="s1">&#39;/critic&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">a_prob</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">a_params</span><span class="p">,</span> <span class="n">c_params</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所有线程初始化部分，以及本线程和公共的网络结构初始化部分如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;/cpu:0&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_A</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropA&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">OPT_C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LR_C</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RMSPropC&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">GLOBAL_AC</span> <span class="o">=</span> <span class="n">ACNet</span><span class="p">(</span><span class="n">GLOBAL_NET_SCOPE</span><span class="p">)</span>  <span class="c1"># we only need its params</span>
</span></span><span class="line"><span class="cl">  <span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Create worker</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">i_name</span> <span class="o">=</span> <span class="s1">&#39;W_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span>   <span class="c1"># worker name</span>
</span></span><span class="line"><span class="cl">    <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker</span><span class="p">(</span><span class="n">i_name</span><span class="p">,</span> <span class="n">GLOBAL_AC</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本线程神经网络将本地的梯度更新量用于更新公共网络参数的逻辑在update_global函数中，而从公共网络把参数拉回到本线程神经网络的逻辑在pull_global中。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">update_a_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_c_op</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>  <span class="c1"># local grads applies to global net</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">pull_global</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># run by a local</span>
</span></span><span class="line"><span class="cl">  <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_a_params_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull_c_params_op</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>详细的内容大家可以对照代码和算法流程一起看。在主函数里我新加了一个测试模型效果的过程，大家可以试试看看最后的模型效果如何。</p>
<h1 id="5-a3c小结">5. A3C小结</h1>
<p>A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，提供了一种通用的异步的并发的强化学习框架，也就是说，这个并发框架不光可以用于A3C，还可以用于其他的强化学习算法。这是A3C最大的贡献。目前，已经有基于GPU的A3C框架，这样A3C的框架训练速度就更快了。</p>
<p>除了A3C, DDPG算法也可以改善Actor-Critic难收敛的问题。它使用了Nature DQN，DDQN类似的思想，用两个Actor网络，两个Critic网络，一共4个神经网络来迭代更新模型参数。在下一篇我们讨论DDPG算法。</p>
]]></description></item><item><title>强化学习笔记 [14] | Actor-Critic</title><link>https://jianye0428.github.io/posts/rl_learning_note_14/</link><pubDate>Sun, 25 Feb 2024 15:35:58 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_14/</guid><description><![CDATA[<ul>
<li></li>
</ul>
<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/10137696.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十三) 策略梯度(Policy Gradient)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。</p>
<p>在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。</p>
<p>本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。</p>
<h1 id="1-actor-critic算法简介">1. Actor-Critic算法简介</h1>
<p>Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。</p>
<p>回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。</p>
<p>也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：</p>
<p>$$
\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)
$$</p>
<p>第二组是价值函数的近似，对于状态价值和动作价值函数分别是：</p>
<p>$$
\hat{v}(s,w)\approx v_\pi(s)
$$</p>
<p>$$
\hat{q}(s,a,w)\approx q_\pi(s,a)
$$</p>
<p>对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：</p>
<p>$$
\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t
$$</p>
<p>梯度更新部分中，$\nabla_\theta log\pi_\theta(s_t,a_t)$是我们的分值函数，不用动，要变成Actor的话改动的是$v_t$，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。</p>
<p>而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。</p>
<p>现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值$v_t$,而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$,在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{te}$</p>
<h1 id="2-actor-critic算法可选形式">2. Actor-Critic算法可选形式</h1>
<p>在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值 $v_t$实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：</p>
<ul>
<li>
<p>a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)V(s,w)$$</li>
</ul>
</li>
<li>
<p>b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)Q(s,a,w)$$</li>
</ul>
</li>
<li>
<p>c) 基于TD误差：在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了TD误差，它的表达式是 $\delta(t)=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$ 或者 $\delta(t)=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t)$, 这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)$$</li>
</ul>
</li>
<li>
<p>d) 基于优势函数：在<a href="https://www.cnblogs.com/pinard/p/9923859.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十二) Dueling DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到过优势函数A的定义：$A(S,A,w,\beta)=Q(S,A,w,\alpha,\beta)-V(S,w,\alpha)$, 即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)A(S,A,w,\beta)$$</li>
</ul>
</li>
<li>
<p>e) 基于 $TD(λ)$ 误差：一般都是基于后向 $TD(λ)$误差, 在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：</p>
<ul>
<li>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)\delta(t)E(t)$</li>
</ul>
</li>
</ul>
<p>对于Critic本身的模型参数 $w$ ，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如 $Q(s,a,w)=ϕ(s,a)^Tw$,则Critic本身的模型参数 $w$的更新公式可以表示为：</p>
<p>$$\begin{gathered}
\delta=R_{t+1}+\gamma Q(S_{t+1}\text{,}A_{t+1})-Q(S_t,A_t) \\
w=w+\beta\delta\phi(s,a)
\end{gathered}$$</p>
<p>通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。</p>
<h1 id="3-actor-critic算法流程">3. Actor-Critic算法流程</h1>
<p>这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数　　</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$, $β$，衰减因子 $γ$, 探索率 $ϵ$, Critic网络结构和Actor网络结构。</p>
<p>输出：Actor 网络参数 $θ$, Critic网络参数 $w$</p>
<ul>
<li>(1). 随机初始化所有的状态和动作对应的价值Q�. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。</li>
<li>(2). for i from 1 to T，进行迭代。
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Actor网络中使用 $ϕ(S)$ 作为输入，输出动作 $A$,基于动作 $A$得到新的状态 $S&rsquo;$,反馈 $R$。</li>
<li>c) 在Critic网络中分别使用 $ϕ(S)$，$ϕ(S&rsquo;)$ 作为输入，得到Q值输出 $V(S)$，$V(S&rsquo;)$</li>
<li>d) 计算TD误差 $\delta=R+\gamma V(S^{\prime})-V(S)$</li>
<li>e) 使用均方差损失函数 $\sum(R+\gamma V(S^{\prime})-V(S,w))^2$ 作Critic网络参数 $w$的梯度更新</li>
<li>f) 更新Actor网络参数 $θ$:
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(S_t,A)\delta $$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对于Actor的分值函数 $∇_θlogπ_θ(S_t,A)$,可以选择softmax或者高斯分值函数。</p>
<p>上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。</p>
<h1 id="4-actor-critic算法实例">4. Actor-Critic算法实例</h1>
<p>下面我们用一个具体的例子来演示上面的Actor-Critic算法。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>算法流程可以参考上面的第三节，这里的分值函数我们使用的是softmax函数，和上一片的类似。完整的代码参见Github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py</p>
<p>代码主要分为两部分，第一部分是Actor，第二部分是Critic。对于Actor部分，大家可以和上一篇策略梯度的代码对比，改动并不大，主要区别在于梯度更新部分，策略梯度使用是蒙特卡罗法计算出的价值 $v(t)$,则我们的actor使用的是TD误差。</p>
<p>在策略梯度部分，对应的位置如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而我们的Actor对应的位置的代码是：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">td_error</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处要注意的是，由于使用的是TD误差，而不是价值 $v(t)$,此处需要最大化<code>self.exp</code>,而不是最小化它，这点和策略梯度不同。对应的Actor代码为：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除此之外，Actor部分的代码和策略梯度的代码区别并不大。</p>
<p>对于Critic部分，我们使用了类似于DQN的三层神经网络。不过我们简化了这个网络的输出，只有一维输出值，而不是之前DQN使用的有多少个可选动作，就有多少维输出值。网络结构如下:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">],</span> <span class="s2">&#34;state&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layerq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layerq</span><span class="p">,</span> <span class="n">W2q</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2q</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和之前的DQN相比，这里还有一个区别就是我们的critic没有使用DQN的经验回放，只是使用了反馈和当前网络在下一个状态的输出来拟合当前状态。</p>
<p>对于算法中Actor和Critic交互的逻辑，在main函数中：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># e-greedy action for train</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">td_error</span> <span class="o">=</span> <span class="n">critic</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>  <span class="c1"># gradient = grad[r + gamma * V(s_) - V(s)]</span>
</span></span><span class="line"><span class="cl">  <span class="n">actor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">td_error</span><span class="p">)</span>  <span class="c1"># true_gradient = grad[logPi(s,a) * td_error]</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大家对照第三节的算法流程和代码应该可以比较容易理清这个过程。但是这个程序很难收敛。因此大家跑了后发现分数总是很低的话是可以理解的。我们需要优化这个问题。</p>
<h1 id="5-actor-critic算法小结">5. Actor-Critic算法小结</h1>
<p>基本版的Actor-Critic算法虽然思路很好，但是由于难收敛的原因，还需要做改进。</p>
<p>目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法我们在从DQN到Nature DQN的过程中已经用过一次了。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p>
<p>在后面的文章中，我们会继续讨论DDPG和A3C。</p>
<p>　</p>
]]></description></item><item><title>强化学习笔记 [13] | 策略梯度(Policy Gradient)</title><link>https://jianye0428.github.io/posts/rl_learning_note_13/</link><pubDate>Sun, 25 Feb 2024 15:35:55 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_13/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradient)，它是Policy Based强化学习方法，基于策略来学习。</p>
<p>本文参考了Sutton的强化学习书第13章和策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<h1 id="1-value-based强化学习方法的不足">1. Value Based强化学习方法的不足</h1>
<p>DQN系列强化学习算法主要的 <strong><font color=red>问题</font></strong> 主要有三点。</p>
<ul>
<li>
<p>第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。</p>
</li>
<li>
<p>第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。</p>
</li>
<li>
<p>第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。</p>
</li>
</ul>
<p>由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述类别问题的方法，比如Policy Based强化学习方法。</p>
<h1 id="2-policy-based强化学习方法引入">2. Policy Based强化学习方法引入</h1>
<p>回想我们在Value Based强化学习方法里，我们对价值函数进行了近似表示，引入了一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到近似的动作价值，即：</p>
<p>$$\hat{q}\left(s,a,w\right)\approx q_\pi(s,a)$$</p>
<p>在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略 $π$可以被被描述为一个包含参数 $θ$ 的函数,即：</p>
<p>$$\pi_\theta(s,a)=P(a|s,\theta)\approx\pi(a|s)$$</p>
<p>将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？</p>
<h1 id="3-策略梯度的优化目标">3. 策略梯度的优化目标</h1>
<p>我们要用梯度上升来寻找最优的梯度，首先就要找到一个可以优化的函数目标。</p>
<p>最简单的优化目标就是初始状态收获的期望，即优化目标为：</p>
<p>$$J_1(\theta)=V_{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}(G_1)$$</p>
<p>但是有的问题是没有明确的初始状态的，那么我们的优化目标可以定义平均价值，即：
$$J_{avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$$</p>
<p>其中，$d_πθ(s)$ 是基于策略 $π_θ$生成的马尔科夫链关于状态的静态分布。</p>
<p>或者定义为每一时间步的平均奖励，即：</p>
<p>$$J_{avR}(\theta)==\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$$</p>
<p>无论我们是采用 $J_1$, $J_{av}V$, 还是 $J_{av}R$ 来表示优化目标，最终对 $θ$求导的梯度都可以表示为：</p>
<p>$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_\pi(s,a)]$$</p>
<p>具体的证明过程这里就不再列了，如果大家感兴趣，可以去看策略梯度的<a href="https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf"target="_blank" rel="external nofollow noopener noreferrer">论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的附录1，里面有详细的证明。</p>
<p>当然我们还可以采用很多其他可能的优化目标来做梯度上升，此时我们的梯度式子里面的 $\nabla_\theta log\pi_\theta(s,a)$ 部分并不改变，变化的只是后面的 $Q_\pi(s,a)$ 部分。对于 $\nabla_\theta log\pi_\theta(s,a)$,我们一般称为<strong>分值函数</strong>(score function)。</p>
<p>现在梯度的式子已经有了，后面剩下的就是策略函数 $\pi_\theta(s,a)$的设计了。</p>
<h1 id="4-策略函数的设计">4. 策略函数的设计</h1>
<p>现在我们回头看一下策略函数 $\pi_\theta(s,a)$ 的设计，在前面它一直是一个数学符号。</p>
<p>最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征 $ϕ(s,a)$ 与参数 $θ$的线性组合来权衡一个行为发生的几率,即:</p>
<p>$$\pi_\theta(s,a)=\frac{e^{\phi(s,a)^T\theta}}{\sum_be^{\phi(s,b)^T\theta}}$$</p>
<p>则通过求导很容易求出对应的分值函数为：</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)=\phi(s,a)-\mathbb{E}_{\pi_\theta}[\phi(s,.)]$$</p>
<p>另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布 $\mathbb{N}(\phi(\mathrm{s})^{\mathbb{T}}\theta,\sigma^2)$中产生。高斯策略对应的分值函数求导可以得到为:</p>
<p>$$\nabla_\theta log\pi_\theta(s,a)==\frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}$$</p>
<p>有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。</p>
<h1 id="5-蒙特卡罗策略梯度reinforce算法">5. 蒙特卡罗策略梯度reinforce算法</h1>
<p>这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数 $v(s)$ 来近似代替策略梯度公式里面的 $Q_π(s,a)$。算法的流程很简单，如下所示:</p>
<ul>
<li>输入：N个蒙特卡罗完整序列,训练步长 $α$</li>
<li>输出：策略函数的参数 $θ$
<ul>
<li>(1). for 每个蒙特卡罗序列:
<ul>
<li>a. 用蒙特卡罗法计算序列每个时间位置t的状态价值 $v_t$</li>
<li>b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数 $θ$：
<ul>
<li>$$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t,a_t)v_t$$</li>
</ul>
</li>
</ul>
</li>
<li>(2).返回策略函数的参数 $θ$</li>
</ul>
</li>
</ul>
<p>　　这里的策略函数可以是softmax策略，高斯策略或者其他策略。</p>
<h1 id="6-策略梯度实例">6. 策略梯度实例</h1>
<p>这里给出第5节的蒙特卡罗策略梯度reinforce算法的一个实例。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github：https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py</p>
<p>这里我们采用softmax策略作为我们的策略函数，同时，softmax的前置部分，也就是我们的策略模型用一个三层的softmax神经网络来表示。这样好处就是梯度的更新可以交给神经网络来做。</p>
<p>我们的softmax神经网络的结构如下，注意这个网络不是价值Q网络，而是策略网络：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_softmax_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_num&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;actions_value&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">  <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># softmax layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#softmax output</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">all_act_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;act_prob&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax_input</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tf_acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_log_prob</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tf_vt</span><span class="p">)</span>  <span class="c1"># reward guided loss</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意我们的损失函数是softmax交叉熵损失函数和状态价值函数的乘积，这样TensorFlow后面可以自动帮我们做梯度的迭代优化。</p>
<p>另一个要注意的点就是蒙特卡罗法里面价值函数的计算，一般是从后向前算，这样前面的价值的计算可以利用后面的价值作为中间结果，简化计算，对应代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">running_add</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">      <span class="n">running_add</span> <span class="o">=</span> <span class="n">running_add</span> <span class="o">*</span> <span class="n">GAMMA</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="n">discounted_ep_rs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_add</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">discounted_ep_rs</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">discounted_ep_rs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分和之前的DQN的代码类似。</p>
<h1 id="7-策略梯度小结">7. 策略梯度小结</h1>
<p>策略梯度提供了和DQN之类的方法不同的新思路，但是我们上面的蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，我们需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，我们的参数更新的方向很可能不是策略梯度的最优方向。</p>
<p>因此，Policy Based的强化学习方法还需要改进，注意到我们之前有Value Based强化学习方法，那么两者能不能结合起来一起使用呢？下一篇我们讨论Policy Based+Value Based结合的策略梯度方法Actor-Critic。</p>
<p>　　　　</p>
]]></description></item><item><title>强化学习笔记 [12] | Dueling DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_12/</link><pubDate>Sun, 25 Feb 2024 11:16:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_12/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9797695.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习(十一) Prioritized Replay DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Dueling DQN的论文(Dueling Network Architectures for Deep Reinforcement Learning)(ICML 2016)。</p>
<h1 id="1-dueling-dqn的优化点考虑">1. Dueling DQN的优化点考虑</h1>
<p>在前面讲到的DDQN中，我们通过优化目标Q值的计算来优化算法，在Prioritized Replay DQN中，我们通过优化经验回放池按权重采样来优化算法。而在Dueling DQN中，我们尝试通过<font color=red>优化神经网络的结构</font>来优化算法。</p>
<p>具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分是仅仅与状态 $S$有关，与具体要采用的动作 $A$无关，这部分我们叫做<strong>价值函数部分</strong>，记做 $V(S,w,α)$,第二部分同时与状态状态 $S$ 和动作 $A$有关，这部分叫做**优势函数(Advantage Function)**部分,记为 $A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+A(S,A,w,\beta)$$</p>
<p>其中，$w$ 是公共部分的网络参数，而 $α$ 是价值函数独有部分的网络参数，而 $β$ 是优势函数独有部分的网络参数。</p>
<h1 id="2-dueling-dqn网络结构">2. Dueling DQN网络结构</h1>
<p>由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。为了简化算法描述，这里不使用原论文的CNN网络结构，而是使用前面文中用到的最简单的三层神经网络来描述。是否使用CNN对Dueling DQN算法本身无影响。</p>
<p>在前面讲到的DDQN等DQN算法中，我使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络与Dueling DQN</div>
</center>
<br>
<p>而在Dueling DQN中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。</p>
<p>我们可以直接使用上一节的价值函数的组合公式得到我们的动作价值，但是这个式子无法辨识最终输出里面 $V(S,w,α)$ 和 $A(S,A,w,β)$各自的作用，为了可以体现这种可辨识性(identifiability),实际使用的组合公式如下：</p>
<p>$$Q(S,A,w,\alpha,\beta)=V(S,w,\alpha)+(A(S,A,w,\beta)-\frac1{\mathcal{A}}\sum_{a^{\prime}\in\mathcal{A}}A(S,a^{\prime},w,\beta))$$</p>
<p>其实就是对优势函数部分做了中心化的处理。以上就是Dueling DQN的主要算法思路。由于它仅仅涉及神经网络的中间结构的改进，现有的DQN算法可以在使用Duel DQN网络结构的基础上继续使用现有的算法。由于算法主流程和其他算法没有差异，这里就不单独讲Duel DQN的算法流程了。</p>
<h1 id="3-dueling-dqn实例">3. Dueling DQN实例</h1>
<p>下面我们用一个具体的例子来演示Dueling DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>这个实例代基于Nature DQN，并将网络结构改为上图中右边的Dueling DQN网络结构，完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Dueling DQN和Nature DQN的代码的不同之处。也就是网络结构定义部分，主要的代码如下，一共有两个相同结构的Q网络，每个Q网络都有状态函数和优势函数的定义，以及组合后的Q网络输出，如代码红色部分：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W21</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b21</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W21</span><span class="p">)</span> <span class="o">+</span> <span class="n">b21</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b22</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1</span><span class="p">,</span> <span class="n">W22</span><span class="p">)</span> <span class="o">+</span> <span class="n">b22</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">h_layer_1t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for state value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2v</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden layer  for action value</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;Advantage&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer_1t</span><span class="p">,</span> <span class="n">W2a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2a</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">AT</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分代码和Nature DQN基本相同。当然，我们可以也在前面DDQN，Prioritized Replay DQN代码的基础上，把网络结构改成上面的定义，这样Dueling DQN也可以起作用。</p>
<h1 id="4-dqn总结">4. DQN总结</h1>
<p>DQN系列我花了5篇来讲解，一共5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。</p>
<p>当然DQN家族的算法远远不止这些，还有一些其他的DQN算法我没有详细介绍，比如使用一些较复杂的CNN和RNN网络来提高DQN的表达能力，又比如改进探索状态空间的方法等，主要是在DQN的基础上持续优化。</p>
<p>DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。虽然NAF DQN可以解决这个问题，但是方法过于复杂了。而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题，从下一篇我们开始讨论Policy-Based深度强化学习。</p>
]]></description></item><item><title>强化学习笔记 [11] | Prioritized Replay DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_11/</link><pubDate>Sun, 25 Feb 2024 11:16:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_11/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9778063.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（十）Double DQN (DDQN)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay DQN。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Prioritized Replay DQN的论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<h1 id="1-prioritized-replay-dqn之前算法的问题">1. Prioritized Replay DQN之前算法的问题</h1>
<p>在Prioritized Replay DQN之前，我们已经讨论了很多种DQN，比如Nature DQN， DDQN等，他们都是通过经验回放来采样，进而做目标Q值的计算的。在采样的时候，我们是一视同仁，在经验回放池里面的所有的样本都有相同的被采样到的概率。</p>
<p>但是注意到在经验回放池里面的不同的样本由于TD误差的不同，对我们反向传播的作用是不一样的。TD误差越大，那么对我们反向传播的作用越大。而TD误差小的样本，由于TD误差小，对反向梯度的计算影响不大。在Q网络中，TD误差就是目标Q网络计算的目标Q值和当前Q网络计算的Q值之间的差距。</p>
<p>这样如果TD误差的绝对值 $|δ(t)|$较大的样本更容易被采样，则我们的算法会比较容易收敛。下面我们看看Prioritized Replay DQN的算法思路。</p>
<h1 id="2-prioritized-replay-dqn算法的建模">2. Prioritized Replay DQN算法的建模</h1>
<p>Prioritized Replay DQN根据每个样本的TD误差绝对值 $|δ(t)|$，给定该样本的优先级正比于 $|δ(t)|$，将这个优先级的值存入经验回放池。回忆下之前的DQN算法，我们仅仅只保存和环境交互得到的样本状态，动作，奖励等数据，没有优先级这个说法。</p>
<p>由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。</p>
<p>具体的SumTree树结构如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">sum_tree 结构图</div>
</center>
<br>
<p>所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。对于内部节点每个节点只保存自己的儿子节点的优先级值之和，如图中内部节点上显示的数字。</p>
<p>这样保存有什么好处呢？主要是方便采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样，采样到哪个区间，就是哪个样本。比如我们采样到了26， 在（25-29）这个区间，那么就是第四个叶子节点被采样到。而注意到第三个叶子节点优先级最高，是12，它的区间13-25也是最长的，会比其他节点更容易被采样到。</p>
<p>如果要采样两个样本，我们可以在[0,21],[21,42]两个区间做均匀采样，方法和上面采样一个样本类似。</p>
<p>类似的采样算法思想我们在<a href="https://www.cnblogs.com/pinard/p/7249903.html"target="_blank" rel="external nofollow noopener noreferrer">word2vec原理(三) 基于Negative Sampling的模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第四节中也有讲到。</p>
<p>除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：</p>
<p>$$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>现在我们新的考虑了样本优先级的损失函数是</p>
<p>$$\frac1m\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$$</p>
<p>其中 $w_j$是第j个样本的优先级权重，由TD误差 $|δ(t)|$归一化得到。</p>
<p>第三个要注意的点就是当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。</p>
<p>除了以上三个部分，Prioritized Replay DQN和DDQN的算法流程相同。</p>
<h1 id="3-prioritized-replay-dqn算法流程">3. Prioritized Replay DQN算法流程</h1>
<p>下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文(Prioritized Experience Replay)(ICLR 2016)。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，采样权重系数 $β$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率 $C$, SumTree的叶子节点数 $S$。</li>
<li>输出：Q网络参数。</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级 $p_j$为1。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S&rsquo;),is_end}$这个五元组存入SumTree</li>
<li>e) $S=S'$</li>
<li>f) 从SumTree中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，每个样本被采样的概率基于 $P(j)=\frac{p_j}{\sum_i(p_i)}$，损失函数权重 $w_j=(N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left\\{\begin{matrix}R_j&amp;is_end_j\textit{is true}\\\\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{is false}\end{matrix}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\begin{aligned}\frac{1}{m}\sum_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2\end{aligned}$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 重新计算所有样本的TD误差 $\delta_j=y_j-Q(\phi(S_j),A_j,w)$，更新SumTree中所有节点的优先级 $p_j=|\delta_j|$</li>
<li>i) 如果i%C=1,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>j) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-prioritized-replay-ddqn算法流程">4. Prioritized Replay DDQN算法流程</h1>
<p>下面我们给出Prioritized Replay DDQN算法的实例代码。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见我的github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 代码中的SumTree的结构和经验回放池的结构参考了morvanzhou的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py"target="_blank" rel="external nofollow noopener noreferrer">github代码<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p>这里重点讲下和第三节中算法描述不同的地方，主要是 $w_j$的计算。注意到：</p>
<p>$$w_j=\frac{(N<em>P(j))^{-\beta}}{\max_i(w_i)}=\frac{(N</em>P(j))^{-\beta}}{\max_i((N*P(i))^{-\beta})}=\frac{(P(j))^{-\beta}}{\max_i((P(i))^{-\beta})}=(\frac{P_j}{\min_iP(i)})^{-\beta}$$</p>
<p>因此代码里面$w_j$，即ISWeights的计算代码是这样的：</p>
<p><a href="javascript:void%280%29;"></a></p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">pri_seg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span> <span class="o">/</span> <span class="n">n</span>       <span class="c1"># priority segment</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_increment_per_sampling</span><span class="p">])</span>  <span class="c1"># max = 1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">min_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">capacity</span><span class="p">:])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>     <span class="c1"># for later calculate ISweight</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">min_prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_prob</span> <span class="o">=</span> <span class="mf">0.00001</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="n">pri_seg</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">get_leaf</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prob</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total_p</span>
</span></span><span class="line"><span class="cl">    <span class="n">ISWeights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">prob</span><span class="o">/</span><span class="n">min_prob</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_idx</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">b_memory</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">idx</span><span class="p">,</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">b_idx</span><span class="p">,</span> <span class="n">b_memory</span><span class="p">,</span> <span class="n">ISWeights</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上述代码的采样在第二节已经讲到。根据树的优先级的和total_p和采样数n，将要采样的区间划分为n段，每段来进行均匀采样，根据采样到的值落到的区间，决定被采样到的叶子节点。当我们拿到第i段的均匀采样值v以后，就可以去SumTree中找对应的叶子节点拿样本数据，样本叶子节点序号以及样本优先级了。代码如下：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree structure and array storage:
</span></span></span><span class="line"><span class="cl"><span class="s2">  Tree index:
</span></span></span><span class="line"><span class="cl"><span class="s2">        0         -&gt; storing priority sum
</span></span></span><span class="line"><span class="cl"><span class="s2">      / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">    1     2
</span></span></span><span class="line"><span class="cl"><span class="s2">    / \   / </span><span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="s2">  3   4 5   6    -&gt; storing priority for transitions
</span></span></span><span class="line"><span class="cl"><span class="s2">  Array type for storing:
</span></span></span><span class="line"><span class="cl"><span class="s2">  [0,1,2,3,4,5,6]
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">parent_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>     <span class="c1"># the while loop is faster than the method in the reference code</span>
</span></span><span class="line"><span class="cl">    <span class="n">cl_idx</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">parent_idx</span> <span class="o">+</span> <span class="mi">1</span>         <span class="c1"># this leaf&#39;s left and right kids</span>
</span></span><span class="line"><span class="cl">    <span class="n">cr_idx</span> <span class="o">=</span> <span class="n">cl_idx</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cl_idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">):</span>        <span class="c1"># reach bottom, end search</span>
</span></span><span class="line"><span class="cl">      <span class="n">leaf_idx</span> <span class="o">=</span> <span class="n">parent_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">break</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>       <span class="c1"># downward search, always search for a higher priority node</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cl_idx</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">cl_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">cr_idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">data_idx</span> <span class="o">=</span> <span class="n">leaf_idx</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">leaf_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">leaf_idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了采样部分，要注意的就是当梯度更新完毕后，我们要去更新SumTree的权重，代码如下，注意叶子节点的权重更新后，要向上回溯，更新所有祖先节点的权重。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">batch_update</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">)</span>  <span class="c1"># update priority</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">batch_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">abs_errors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">abs_errors</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>  <span class="c1"># convert to abs and avoid 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">clipped_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">abs_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs_err_upper</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">clipped_errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">ti</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tree_idx</span><span class="p">,</span> <span class="n">ps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ti</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">change</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># then propagate the change through tree</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">tree_idx</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>    <span class="c1"># this method is faster than the recursive loop in the reference code</span>
</span></span><span class="line"><span class="cl">      <span class="n">tree_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="p">[</span><span class="n">tree_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">change</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，和DDQN比，TensorFlow的网络结构流程中多了一个TD误差的计算节点，以及损失函数多了一个ISWeights系数。此外，区别不大。</p>
<h1 id="5-prioritized-replay-dqn小结">5. Prioritized Replay DQN小结</h1>
<p>Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。</p>
<p>下一篇我们讨论DQN家族的另一个优化算法Duel DQN，它将价值Q分解为两部分，第一部分是仅仅受状态但不受动作影响的部分，第二部分才是同时受状态和动作影响的部分，算法的效果也很好。</p>
]]></description></item><item><title>强化学习笔记 [10] | Double DQN (DDQN)</title><link>https://jianye0428.github.io/posts/rl_learning_note_10/</link><pubDate>Fri, 23 Feb 2024 13:17:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_10/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9756075.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（九）Deep Q-Learning进阶之Nature DQN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称DDQN）。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和DDQN的论文(Deep Reinforcement Learning with Double Q-learning)。</p>
<h1 id="1-dqn的目标q值计算问题">1. DQN的目标Q值计算问题</h1>
<p>在DDQN之前，基本上所有的目标Q值都是通过<strong>贪婪法</strong>直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算如下式:</p>
<p>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。</p>
<h1 id="2-ddqn的算法建模">2. DDQN的算法建模</h1>
<p>DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。</p>
<p>在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：</p>
<p>$$y_j=R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})$$</p>
<p>在DDQN(Double DQN)这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即:</p>
<p>$$a^{max}(S_j^{\prime},w)=\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w)$$</p>
<p>然后利用这个选择出来的动作 $\begin{aligned}&amp;a^{max}(S_j^{\prime},w)\end{aligned}$ 在目标网络里面去计算目标Q值。即：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),a^{max}(S_j^{\prime},w),w^{\prime})$$</p>
<p>综合起来写就是：</p>
<p>$$y_j=R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})$$</p>
<p>除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。</p>
<h1 id="3-ddqn算法流程">3. DDQN算法流程</h1>
<p>这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本 $m$,目标Q网络参数更新频 $C$。</li>
<li>输出：Q网络参数</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;的参数 $w′=w$ 。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态 <code>is_end</code></li>
<li>d) 将 ${ϕ(S),A,R,ϕ(S′),is_end} $,这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$, 计算当前目标Q值 $y_j$:
<ul>
<li>$$\left.y_j=\left{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\R_j+\gamma Q^{\prime}(\phi(S_j^{\prime}),\arg\max_{a^{\prime}}Q(\phi(S_j^{\prime}),a,w),w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数w�</li>
<li>h) 如果 $i%C=1$,则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-ddqn算法实例">4. DDQN算法实例　</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注DDQN和上一节的Nature DQN的代码的不同之处。代码只有一个地方不一样，就是计算目标Q值的时候，如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">current_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_action_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">current_Q_batch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">target_Q_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span> <span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">target_Q_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">max_action_next</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">target_Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>而之前的Nature DQN这里的目标Q值计算是如下这样的：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>除了上面这部分的区别，两个算法的代码完全相同。</p>
<h1 id="5-ddqn小结">5. DDQN小结</h1>
<p>DDQN算法出来以后，取得了比较好的效果，因此得到了比较广泛的应用。不过我们的DQN仍然有其他可以优化的点，如上一篇最后讲到的: 随机采样的方法好吗？按道理经验回放里不同样本的重要性是不一样的，TD误差大的样本重要程度应该高。针对这个问题，我们在下一节的Prioritised Replay DQN中讨论。</p>
]]></description></item><item><title>强化学习笔记 [9] | Deep Q-Learning进阶之Nature DQN</title><link>https://jianye0428.github.io/posts/rl_learning_note_9/</link><pubDate>Fri, 23 Feb 2024 13:17:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_9/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9714655.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（八）价值函数的近似表示与Deep Q-Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 2015)。</p>
<p>本章内容主要参考了ICML 2016的<a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf"target="_blank" rel="external nofollow noopener noreferrer">deep RL tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>和Nature DQN的论文。</p>
<h1 id="1-dqnnips-2013的问题">1. DQN(NIPS 2013)的问题</h1>
<p>在上一篇我们已经讨论了DQN(NIPS 2013)的算法原理和代码实现，虽然它可以训练像CartPole这样的简单游戏，但是有很多问题。这里我们先讨论第一个问题。</p>
<p>注意到DQN(NIPS 2013)里面，我们使用的目标 $Q$值的计算方式：</p>
<p>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</p>
<p>这里目标Q值的计算使用到了当前要训练的Q网络参数来计算$Q(\phi(S_j^{\prime}),A_j^{\prime},w)$，而实际上，我们又希望通过 $y_j$来后续更新 $Q$网络参数。这样两者循环依赖，迭代起来两者的相关性就太强了。不利于算法的收敛。</p>
<p>因此，一个改进版的DQN: Nature DQN尝试<strong>用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系</strong>。下面我们来看看Nature DQN是怎么做的。</p>
<h1 id="2-nature-dqn的建模">2. Nature DQN的建模</h1>
<p>Nature DQN的两个Q网络分别命名为当前Q网络和目标Q网络。</p>
<p>Nature DQN使用了两个Q网络，一个<strong>当前Q网络</strong>$Q$用来选择动作，更新模型参数，另一个<strong>目标Q网络</strong> $Q&rsquo;$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。</p>
<p>要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。</p>
<p>Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。</p>
<h1 id="3-nature-dqn的算法流程">3. Nature DQN的算法流程</h1>
<p>下面我们来总结下Nature DQN的算法流程， 基于DQN NIPS 2015：</p>
<p>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, 当前Q网络 $Q$，目标Q网络 $Q&rsquo;$, 批量梯度下降的样本数 $m$,目标Q网络参数更新频率$C$。</p>
<p>输出：$Q$网络参数</p>
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 随机初始化当前Q网络的所有参数 $w$,初始化目标Q网络 $Q&rsquo;$的参数 $w&rsquo;=w$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 对应的特征向量 $ϕ(S&rsquo;)$ 和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S′),is_end\\}$这个五元组存入经验回放集合 $D$</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(S_j),A_j,R_j,ϕ(S&rsquo;_j),is_end_j},j=1,2.,,,m$，计算当前目标Q值 $y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\textit{ is true}\\\\R_j+\gamma\max_{a^{\prime}}Q^{\prime}(\phi(S_j^{\prime}),A_j^{\prime},w^{\prime})&amp;is_end_j\textit{ is false}\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数 $\frac1m\sum_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果 $i%C=1$, 则更新目标Q网络参数 $w&rsquo;=w$</li>
<li>i) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$ 需要随着迭代的进行而变小。</p>
<h1 id="4-nature-dqn算法实例">4. Nature DQN算法实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。仍然使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>这里我们重点关注Nature DQN和上一节的NIPS 2013 DQN的代码的不同之处。</p>
<p>首先是Q网络，上一篇的DQN是一个三层的神经网络，而这里我们有两个一样的三层神经网络，一个是当前Q网络，一个是目标Q网络，网络的定义部分如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">create_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input layer</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s2">&#34;float&#34;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># network weights</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;current_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;target_net&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">W1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b1t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">W2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_variable</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="n">b2t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_variable</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1"># hidden layers</span>
</span></span><span class="line"><span class="cl">      <span class="n">h_layer_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">,</span><span class="n">W1t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># Q Value layer</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_layer</span><span class="p">,</span><span class="n">W2t</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2t</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对于定期将目标Q网络的参数更新的代码如下面两部分：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">t_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;target_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">e_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;current_net&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;soft_replacement&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">t_params</span><span class="p">,</span> <span class="n">e_params</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_target_q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># update target Q netowrk</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">REPLACE_TARGET_FREQ</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_replace_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="c1">#print(&#39;episode &#39;+str(episode) +&#39;, target Q network params replaced!&#39;)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此外，注意下我们计算目标Q值的部分，这里使用的目标Q网络的参数，而不是当前Q网络的参数：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>其余部分基本和上一篇DQN的代码相同。这里给出我跑的某一次的结果:</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.8
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 9.6
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.0
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 34.8
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 177.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 198.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 193.2
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>注意，由于DQN不保证稳定的收敛，所以每次跑的结果会不同，如果你跑的结果后面仍然收敛的不好，可以把代码多跑几次，选择一个最好的训练结果。</p>
<h1 id="5-nature-dqn总结">5. Nature DQN总结</h1>
<p>Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：</p>
<ul>
<li>1） 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？</li>
<li>2） 随机采样的方法好吗？按道理不同样本的重要性是不一样的。</li>
<li>3） Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？</li>
</ul>
<p>第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN，这三个DQN的改进版我们在下一篇来讨论。</p>
]]></description></item><item><title>强化学习笔记 [8] | 价值函数的近似表示与Deep Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_8/</link><pubDate>Fri, 23 Feb 2024 13:17:44 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_8/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在强化学习系列的<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">前七篇<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。</p>
<p>Deep Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。</p>
<h1 id="1-为何需要价值函数的近似表示">1. 为何需要价值函数的近似表示</h1>
<p>在之前讲到了强化学习求解方法，无论是动态规划DP，蒙特卡罗方法MC，还是时序差分TD，使用的状态都是离散的有限个状态集合 $S$。此时问题的规模比较小，比较容易求解。但是假如我们遇到复杂的状态集合呢？甚至很多时候，状态是连续的，那么就算离散化后，集合也很大，此时我们的传统方法，比如Q-Learning，根本无法在内存中维护这么大的一张Q表。　　　　</p>
<p>比如经典的冰球世界(PuckWorld)强化学习问题，具体的动态demo见<a href="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间步时长的一个大小固定的力，借此来改变大圆的速度。环境会在每一个时间步内告诉个体当前的水平与垂直坐标、当前的速度在水平和垂直方向上的分量以及目标的水平和垂直坐标共6项数据，奖励值为个体与目标两者中心距离的负数，也就是距离越大奖励值越低且最高奖励值为0。</p>
<p>在这个问题中，状态是一个6维的向量，并且是连续值。没法直接用之前离散集合的方法来描述状态。当然，你可以说，我们可以把连续特征离散化。比如把这个冰球场100x100的框按1x1的格子划分成10000个格子，那么对于运动员的坐标和冰球的坐标就有$10^4∗10^4=10^8$次种，如果再加上个体速度的分量就更是天文数字了，此时之前讲过的强化学习方法都会因为问题的规模太大而无法使用。怎么办呢？必须要对问题的建模做修改了，而价值函数的近似表示就是一个可行的方法。</p>
<h1 id="2-价值函数的近似表示方法">2. 价值函数的近似表示方法</h1>
<p>由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数 $\hat{v}$, 这个函数由参数 $w$ 描述，并接受状态 $s$ 作为输入，计算后得到状态 $s$ 的价值，即我们期望：</p>
<p>$$\hat{v}(s,w)\approx v_\pi(s)$$</p>
<p>类似的，引入一个动作价值函数 $\hat{q}$，这个函数由参数 $w$ 描述，并接受状态 $s$ 与动作 $a$ 作为输入，计算后得到动作价值，即我们期望：</p>
<p>$$\hat{q}(s,a,w)\approx q_\pi(s,a)$$</p>
<p>价值函数近似的方法很多，比如最简单的线性表示法，用 $ϕ(s)$表示状态 $s$ 的特征向量，则此时我们的状态价值函数可以近似表示为：</p>
<p>$$\hat{v}(s,w)=\phi(s)^Tw$$</p>
<p>当然，除了线性表示法，我们还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。因此后面我们的近似表达方法如果没有特别提到，都是指的神经网络的近似表示。</p>
<p>对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">神经网络拟合价值函数</div>
</center>
<br>
<p>对于状态价值函数，神经网络的输入是状态s的特征向量，输出是状态价值 $\hat{v}(s,w)$。对于动作价值函数，有两种方法，一种是输入状态 $s$ 的特征向量和动作 $a$，输出对应的动作价值 $\hat{q}(s,a,w)$，另一种是只输入状态 $s$ 的特征向量，动作集合有多少个动作就有多少个输出 $\hat{q}(s,ai,w)$。这里隐含了我们的动作是有限个的离散动作。</p>
<p>对于我们前一篇讲到的Q-Learning算法，我们现在就价值函数的近似表示来将其改造，采用上面右边的第三幅图的动作价值函数建模思路来做，现在我们叫它Deep Q-Learning。</p>
<h1 id="3-deep-q-learning算法思路">3. Deep Q-Learning算法思路</h1>
<p>Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值s和动作来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。</p>
<p>DQN的输入是我们的状态s对应的状态向量 $ϕ(s)$， 输出是所有动作在该状态下的动作价值函数Q。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。</p>
<p>DQN主要使用的技巧是经验回放(experience replay), 即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是<strong>经验回放</strong>。</p>
<p>通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数 $w$，当 $w$ 收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。</p>
<p>下面我们总结下DQN的算法流程，基于NIPS 2013 DQN。　　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态特征维度 $n$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$, Q网络结构, 批量梯度下降的样本数 $m$。</li>
<li>输出：Q网络参数
<ul>
<li>
<ol>
<li>随机初始化$Q$网络的所有参数 $w$，基于 $w$初始化所有的状态和动作对应的价值 $Q$。清空经验回放的集合 $D$。</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量 $ϕ(S)$</li>
<li>b) 在Q网络中使用 $ϕ(S)$ 作为输入，得到Q网络的所有动作对应的Q值输出。用 $ϵ−$贪婪法在当前Q值输出中选择对应的动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$对应的特征向量 $ϕ(S&rsquo;)$和奖励 $R$,是否终止状态<code>is_end</code></li>
<li>d) 将 $\\{ϕ(S),A,R,ϕ(S&rsquo;),is_end\\}$这个五元组存入经验回放集合D</li>
<li>e) $S=S'$</li>
<li>f) 从经验回放集合 $D$ 中采样 $m$ 个样本 ${ϕ(Sj),Aj,Rj,ϕ(S′j),is_endj},j=1,2.,,,m$，计算当前目标Q值$y_j$：
<ul>
<li>$$\left.y_j=\left\\{\begin{array}{ll}R_j&amp;is_end_j\mathrm{<del>}is\mathrm{</del>}true\\\\R_j+\gamma\max_{a^{\prime}}Q(\phi(S_j^{\prime}),A_j^{\prime},w)&amp;is_end_j\mathrm{<del>}is\mathrm{</del>}false\end{array}\right.\right.$$</li>
</ul>
</li>
<li>g) 使用均方差损失函数$\frac1m\sum_{i=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数 $w$</li>
<li>h) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意，上述第二步的 $f$步和 $g$步的 $Q$值计算也都需要通过 $Q$网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率 $ϵ$需要随着迭代的进行而变小。</p>
<h1 id="4-deep-q-learning实例">4. Deep Q-Learning实例</h1>
<p>下面我们用一个具体的例子来演示DQN的应用。这里使用了OpenAI Gym中的CartPole-v0游戏来作为我们算法应用。CartPole-v0游戏的介绍参见<a href="https://github.com/openai/gym/wiki/CartPole-v0"target="_blank" rel="external nofollow noopener noreferrer">这里<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。它比较简单，基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>代码参考了知乎上的一个<a href="https://zhuanlan.zhihu.com/p/21477488"target="_blank" rel="external nofollow noopener noreferrer">DQN实例<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，修改了代码中的一些错误，并用最新的Python3.6+Tensorflow1.8.0运行。要跑代码需要安装OpenAI的Gym库，使用<code>pip install gym</code>即可。</p>
<p>代码使用了一个三层的神经网络，输入层，一个隐藏层和一个输出层。下面我们看看关键部分的代码。</p>
<p>算法第2步的步骤b通过$ϵ−$贪婪法选择动作的代码如下，注意每次我们$ϵ−$贪婪法后都会减小$ϵ$值。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">egreedy_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:[</span><span class="n">state</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">})[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="n">INITIAL_EPSILON</span> <span class="o">-</span> <span class="n">FINAL_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_value</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤c在状态S�执行当前动作A�的代码如下，这个交互是由Gym完成的。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Define reward for agent</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="mf">0.1</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤d保存经验回放数据的代码如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">perceive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">one_hot_action</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span><span class="n">one_hot_action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">REPLAY_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">train_Q_network</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法第2步的步骤f,g计算目标Q值，并更新Q网络的代码如下：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_Q_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 1: obtain random minibatch from replay memory</span>
</span></span><span class="line"><span class="cl">  <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">reward_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_state_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># Step 2: calculate y</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_batch</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Q_value_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_value</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">next_state_batch</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">done</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">y_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_value_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">y_input</span><span class="p">:</span><span class="n">y_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">action_input</span><span class="p">:</span><span class="n">action_batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">state_input</span><span class="p">:</span><span class="n">state_batch</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们在每100轮迭代完后会去玩10次交互测试，计算10次的平均奖励。运行了代码后，我的3000轮迭代的输出如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">episode: <span class="m">0</span> Evaluation Average Reward: 12.2
</span></span><span class="line"><span class="cl">episode: <span class="m">100</span> Evaluation Average Reward: 9.4
</span></span><span class="line"><span class="cl">episode: <span class="m">200</span> Evaluation Average Reward: 10.4
</span></span><span class="line"><span class="cl">episode: <span class="m">300</span> Evaluation Average Reward: 10.5
</span></span><span class="line"><span class="cl">episode: <span class="m">400</span> Evaluation Average Reward: 11.6
</span></span><span class="line"><span class="cl">episode: <span class="m">500</span> Evaluation Average Reward: 12.4
</span></span><span class="line"><span class="cl">episode: <span class="m">600</span> Evaluation Average Reward: 29.6
</span></span><span class="line"><span class="cl">episode: <span class="m">700</span> Evaluation Average Reward: 48.1
</span></span><span class="line"><span class="cl">episode: <span class="m">800</span> Evaluation Average Reward: 85.0
</span></span><span class="line"><span class="cl">episode: <span class="m">900</span> Evaluation Average Reward: 169.4
</span></span><span class="line"><span class="cl">episode: <span class="m">1000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">1900</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2000</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2100</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2200</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2300</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2400</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2500</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2600</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2700</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2800</span> Evaluation Average Reward: 200.0
</span></span><span class="line"><span class="cl">episode: <span class="m">2900</span> Evaluation Average Reward: 200.0</span></span></code></pre></td></tr></table>
</div>
</div><p>大概到第1000次迭代后，算法已经收敛，达到最高的200分。当然由于是$ϵ−$探索，每次前面的输出可能不同，但最后应该都可以收敛到200的分数。当然由于DQN不保证绝对的收敛，所以可能到了200分后还会有抖动。</p>
<h1 id="5-deep-q-learning小结">5. Deep Q-Learning小结　　　　</h1>
<p>DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。但是DQN有个问题，就是它并不一定能保证Q网络的收敛，也就是说，我们不一定可以得到收敛后的Q网络参数。这会导致我们训练出的模型效果很差。</p>
<p>针对这个问题，衍生出了DQN的很多变种，比如Nature DQN(NIPS 2015), Double DQN，Dueling DQN等。这些我们在下一篇讨论。</p>
]]></description></item><item><title>强化学习笔记 [7] | 时序差分离线控制算法Q-Learning</title><link>https://jianye0428.github.io/posts/rl_learning_note_7/</link><pubDate>Fri, 23 Feb 2024 13:17:35 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_7/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。</p>
<p>Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-q-learning算法的引入">1. Q-Learning算法的引入　　　　</h1>
<p>Q-Learning算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解<u>不需要环境的状态转化模型</u>，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新策略，通过策略来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，比如我们上一篇讲到的SARSA, 而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。这一类的经典算法就是Q-Learning。</p>
<p>对于Q-Learning，我们会使用 $ϵ−$贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的 $ϵ−$贪婪法。这一点就是SARSA和Q-Learning本质的区别。</p>
<h1 id="2-q-learning算法概述">2. Q-Learning算法概述</h1>
<p>Q-Learning算法的拓扑图如下图所示：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Q Learning 拓扑图</div>
</center>
<br>
<p>首先我们基于状态 $S$，用 $ϵ−$贪婪法选择到动作 $A$, 然后执行动作$A$，得到奖励 $R$，并进入状态 $S&rsquo;$，此时，如果是SARSA，会继续基于状态 $S&rsquo;$，用 $ϵ−$贪婪法选择 $A&rsquo;$,然后来更新价值函数。但是Q-Learning则不同。</p>
<p>对于Q-Learning，它基于状态 $S&rsquo;$，没有使用 $ϵ−$贪婪法选择 $A$，而是使用贪婪法选择 $A&rsquo;$，也就是说，选择使 $Q(S&rsquo;,a)$ 最大的 $a$ 作为 $A&rsquo;$来更新价值函数。用数学公式表示就是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</p>
<p>对应到上图中就是在图下方的三个黑圆圈动作中选择一个使 $Q(S&rsquo;,a)$最大的动作作为 $A&rsquo;$。</p>
<p>此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态 $S&rsquo;$，用 $ϵ−$贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的 $A&rsquo;$ 会作为下一阶段开始时候的执行动作。</p>
<p>下面我们对Q-Learning算法做一个总结。</p>
<h1 id="3-q-learning算法流程">3. Q-Learning算法流程</h1>
<p>下面我们总结下Q-Learning算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出: 所有的状态和动作对应的价值 $Q$
<ul>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。</li>
<li>b) 用 $ϵ−$贪婪法在当前状态 $S$ 选择出动作 $A$</li>
<li>c) 在状态 $S$执行当前动作 $A$,得到新状态 $S&rsquo;$和奖励 $R$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)+\alpha(R+\gamma\max_aQ(S^{\prime},a)-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S'$</li>
<li>f) 如果$S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-q-learning算法实例windy-gridworld">4. Q-Learning算法实例：Windy GridWorld</h1>
<p>我们还是使用和SARSA一样的例子来研究Q-Learning。如果对windy gridworld的问题还不熟悉，可以复习<a href="https://www.cnblogs.com/pinard/p/9614290.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（六）时序差分在线控制算法SARSA<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第4节的第二段。</p>
<p>完整的代码参见github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>绝大部分代码和SARSA是类似的。这里我们可以重点比较和SARSA不同的部分。区别都在<code>episode()</code>这个函数里面。</p>
<p>首先是初始化的时候，我们只初始化状态 $S$,把 $A$ 的产生放到了while循环里面, 而回忆下SARSA会同时初始化状态 $S$ 和动作 $A$，再去执行循环。下面这段Q-Learning的代码对应我们算法的第二步步骤a和b：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># play for an episode</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">episode</span><span class="p">(</span><span class="n">q_value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># track the total time steps in this episode</span>
</span></span><span class="line"><span class="cl">  <span class="n">time</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="n">state</span> <span class="o">!=</span> <span class="n">GOAL</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>接着我们会去执行动作 $A$,得到 $S&rsquo;$， 由于奖励不是终止就是-1，不需要单独计算。,这部分和SARSA的代码相同。对应我们Q-Learning算法的第二步步骤c：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>后面我们用贪婪法选择出最大的 $Q(S&rsquo;,a)$,并更新价值函数，最后更新当前状态 $S$。对应我们Q-Learning算法的第二步步骤d,e。注意SARSA这里是使用ϵ−�−贪婪法，而不是贪婪法。同时SARSA会同时更新状态S�和动作A�,而Q-Learning只会更新当前状态S�。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span></span></span></code></pre></td></tr></table>
</div>
</div><p>跑完完整的代码，大家可以很容易得到这个问题的最优解，进而得到在每个格子里的最优贪婪策略。</p>
<h1 id="5-sarsa-vs-q-learning">5. SARSA vs Q-Learning</h1>
<p>现在SARSA和Q-Learning算法我们都讲完了，那么作为时序差分控制算法的两种经典方法吗，他们都有说明特点，各自适用于什么样的场景呢？</p>
<p>Q-Learning直接学习的是 <font color=red>最优策略</font>，而SARSA<font color=red>在学习最优策略的同时还在做探索</font>。这导致我们在学习最优策略的时候，如果用SARSA，为了保证收敛，需要制定一个策略，使 $ϵ−$贪婪法的超参数 $ϵ$在迭代的过程中逐渐变小。Q-Learning没有这个烦恼。</p>
<p>另外一个就是Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以<font color=red>受样本数据的影响较大</font>，因此受到训练数据方差的影响很大，甚至会影响Q函数的收敛。Q-Learning的深度强化学习版Deep Q-Learning也有这个问题。</p>
<p>在学习过程中，SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进，导致出现像Q-Learning可能遇到一些特殊的最优“陷阱”。比如经典的强化学习问题&quot;Cliff Walk&quot;。</p>
<p>在实际应用中，如果我们是在模拟环境中训练强化学习模型，推荐使用Q-Learning，如果是 <strong><font color=red>在线生产环境</font></strong> 中训练模型，则推荐使用 <strong><font color=red>SARSA</font></strong>。</p>
<h1 id="6-q-learning结语">6. Q-Learning结语　　　　　　　　</h1>
<p>对于Q-Learning和SARSA这样的时序差分算法，对于小型的强化学习问题是非常灵活有效的，但是在大数据时代，异常复杂的状态和可选动作，使Q-Learning和SARSA要维护的Q表异常的大，甚至远远超出内存，这限制了时序差分算法的应用场景。在深度学习兴起后，基于深度学习的强化学习开始占主导地位，因此从下一篇开始我们开始讨论深度强化学习的建模思路。</p>
]]></description></item><item><title>RL学习笔记 [6] | 时序差分在线控制算法SARSA</title><link>https://jianye0428.github.io/posts/rl_learning_note_6/</link><pubDate>Thu, 22 Feb 2024 16:29:33 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_6/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。</p>
<p>SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。</p>
<h1 id="1-sarsa算法的引入">1. SARSA算法的引入</h1>
<p>SARSA算法是一种使用时序差分求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$。</p>
<p>这一类强化学习的问题求解不需要环境的状态转化模型，是<strong>不基于模型的强化学习问题</strong>求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。</p>
<p>再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作。而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。</p>
<p>我们的SARSA算法，属于在线控制这一类，即一直使用一个策略来更新价值函数和选择新的动作，而这个策略是 $ϵ−$贪婪法，在<a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（四）用蒙特卡罗法（MC）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们对于 $ϵ−$贪婪法有详细讲解，即通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 m 个可选行为中选择行为。用公式可以表示为：</p>
<p>$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</p>
<p>π(a|s)={ϵ/m+1−ϵifa∗=argmaxa∈AQ(s,a)ϵ/melse�(�|�)={�/�+1−����∗=arg⁡max�∈��(�,�)�/�����</p>
<h1 id="2-sarsa算法概述">2. SARSA算法概述</h1>
<p>作为SARSA算法的名字本身来说，它实际上是由 $S,A,R,S,A$ 几个字母组成的。而 $S,A,R$ 分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">SARSA Transition</div>
</center>
<br>
<p>在迭代的时候，我们首先基于 $ϵ−$贪婪法在当前状态 $S$ 选择一个动作 $A$ ，这样系统会转到一个新的状态 $S′$, 同时给我们一个即时奖励 $R$ , 在新的状态 $S′$，我们会基于 $ϵ−$贪婪法在状态 $S′$ 选择一个动作 $A′$，但是注意这时候我们并不执行这个动作 $A′$，只是用来更新的我们的价值函数，价值函数的更新公式是：</p>
<p>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</p>
<p>其中，$γ$ 是衰减因子，$α$ 是迭代步长。这里和蒙特卡罗法求解在线控制问题的迭代公式的区别主要是，收获 $G_t$的表达式不同，对于时序差分，收获 $G_t$的表达式是 $R+\gamma Q(S&rsquo;,A&rsquo;)$ 。这个价值函数更新的贝尔曼公式我们在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>第2节有详细讲到。</p>
<p>除了收获 $G_t$的表达式不同，SARSA算法和蒙特卡罗在线控制算法基本类似。</p>
<h1 id="3-sarsa算法流程">3. SARSA算法流程</h1>
<p>下面我们总结下SARSA算法的流程。</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率 $ϵ$,</li>
<li>输出：所有的状态和动作对应的价值 $Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值Q�. 对于终止状态其Q�值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化 $S$ 为当前状态序列的第一个状态。设置 $A$ 为 $ϵ−$贪婪法在当前状态$S$ 选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$ ,得到新状态 $S′$ 和 奖励 $R$</li>
<li>c) 用 $\epsilon-$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新价值函数 $Q(S,A)$:
<ul>
<li>$$Q(S,A)=Q(S,A)+\alpha(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))$$</li>
</ul>
</li>
<li>e) $S=S′$, $A=A′$</li>
<li>f) 如果 $S′$ 是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>这里有一个要注意的是，步长 $α$一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数 $Q$ 可以收敛。当 $Q$ 收敛时，我们的策略 $ϵ−$贪婪法也就收敛了。</p>
<h1 id="4-sarsa算法实例windy-gridworld">4. SARSA算法实例：Windy GridWorld</h1>
<p>下面我们用一个著名的实例Windy GridWorld来研究SARSA算法。</p>
<p>如下图一个10×7的长方形格子世界，标记有一个起始位置 S 和一个终止目标位置 G，格子下方的数字表示对应的列中一定强度的风。当个体进入该列的某个格子时，会按图中箭头所示的方向自动移动数字表示的格数，借此来模拟世界中风的作用。同样格子世界是有边界的，个体任意时刻只能处在世界内部的一个格子中。个体并不清楚这个世界的构造以及有风，也就是说它不知道格子是长方形的，也不知道边界在哪里，也不知道自己在里面移动移步后下一个格子与之前格子的相对位置关系，当然它也不清楚起始位置、终止目标的具体位置。但是个体会记住曾经经过的格子，下次在进入这个格子时，它能准确的辨认出这个格子曾经什么时候来过。格子可以执行的行为是朝上、下、左、右移动一步，每移动一步只要不是进入目标位置都给予一个 -1 的惩罚，直至进入目标位置后获得奖励 0 同时永久停留在该位置。现在要求解的问题是个体应该遵循怎样的策略才能尽快的从起始位置到达目标位置。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Windy GridWorld</div>
</center>
<br>
<p>逻辑并不复杂，完整的代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py"target="_blank" rel="external nofollow noopener noreferrer">我的github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。这里我主要看一下关键部分的代码。</p>
<p>算法中第2步步骤a,初始化 $S$,使用 $ϵ−$贪婪法在当前状态 $S$ 选择的动作的过程：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># initialize state</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">START</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># choose an action based on epsilon-greedy algorithm</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤b,在状态S�执行当前动作A�,得到新状态S′�′的过程，由于奖励不是终止就是-1，不需要单独计算：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_UP</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_DOWN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">WORLD_HEIGHT</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_LEFT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="n">ACTION_RIGHT</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">WIND</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WORLD_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="kc">False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤c,用 $ϵ−$贪婪法在状态 $S&rsquo;$选择新的动作 $A′$的过程：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">values_</span> <span class="o">=</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">  <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values_</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values_</span><span class="p">)])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>算法中第2步步骤d,e, 更新价值函数 $Q(S,A)$ 以及更新当前状态动作的过程：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Sarsa update</span>
</span></span><span class="line"><span class="cl"><span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> \
</span></span><span class="line"><span class="cl">  <span class="n">ALPHA</span> <span class="o">*</span> <span class="p">(</span><span class="n">REWARD</span> <span class="o">+</span> <span class="n">q_value</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_value</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
</span></span><span class="line"><span class="cl"><span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span></span></span></code></pre></td></tr></table>
</div>
</div><p>代码很简单，相信大家对照算法，跑跑代码，可以很容易得到这个问题的最优解，进而搞清楚SARSA算法的整个流程。</p>
<h1 id="5-sarsaλ">5. SARSA(λ)</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9529828.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（五）用时序差分法（TD）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中我们讲到了多步时序差分 $TD(λ)$ 的价值函数迭代方法，那么同样的，对应的多步时序差分在线控制算法，就是我们的 $SARSA(λ)$。</p>
<p>$TD(\lambda)$有前向和后向两种价值函数迭代方式，当然它们是等价的。在控制问题的求解时，基于反向认识的 $SARSA(\lambda)$算法将可以有效地在线学习，数据学习完即可丢弃。因此 $SARSA(\lambda)$算法默认都是基于反向来进行价值函数迭代。</p>
<p>在上一篇我们讲到了$TD(\lambda)$状态价值函数的反向迭代，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\\\\V(S_t)=V(S_t)+\alpha\delta_tE_t(S)\end{gathered}$$</p>
<p>对应的动作价值函数的迭代公式可以找样写出，即：</p>
<p>$$\begin{gathered}\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\\\Q(S_t,A_t)=Q(S_t,A_t)+\alpha\delta_tE_t(S,A)\end{gathered}$$</p>
<p>除了状态价值函数 $Q(S,A)$ 的更新方式，多步参数 $λ$ 以及反向认识引入的效用迹 $E(S,A)$ ，其余算法思想和 $SARSA$ 类似。这里我们总结下 $SARSA(λ)$的算法流程。　　　</p>
<ul>
<li>算法输入：迭代轮数 $T$，状态集 $S$, 动作集 $A$, 步长 $α$，衰减因子 $γ$, 探索率$ϵ$, 多步参数$λ$</li>
<li>输出：所有的状态和动作对应的价值$Q$</li>
<li>
<ol>
<li>随机初始化所有的状态和动作对应的价值 $Q$. 对于终止状态其 $Q$值初始化为0.</li>
</ol>
</li>
<li>
<ol start="2">
<li>for i from 1 to T，进行迭代。</li>
</ol>
<ul>
<li>a) 初始化所有状态动作的效用迹 $E$ 为0，初始化S为当前状态序列的第一个状态。设置$A$为 $ϵ−$贪婪法在当前状态 $S$选择的动作。</li>
<li>b) 在状态 $S$ 执行当前动作 $A$,得到新状态 $S&rsquo;$ 和奖励 $R$</li>
<li>c) 用$ϵ−$贪婪法在状态 $S&rsquo;$ 选择新的动作 $A'$</li>
<li>d) 更新效用迹函数 $E(S,A)$和TD误差 $δ$:
<ul>
<li>$$\begin{gathered}E(S,A)=E(S,A)+1\\\\\delta=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\end{gathered}$$</li>
</ul>
</li>
<li>e) 对当前序列所有出现的状态s和对应动作 $a$, 更新价值函数 $Q(s,a)$和效用迹函数 $E(s,a)$:
<ul>
<li>$$\begin{gathered}Q(s,a)=Q(s,a)+\alpha\delta E(s,a)\\\\E(s,a)=\gamma\lambda E(s,a)\end{gathered}$$</li>
</ul>
</li>
<li>f) $S=S&rsquo;$, $A=A'$</li>
<li>g) 如果 $S&rsquo;$是终止状态，当前轮迭代完毕，否则转到步骤b)</li>
</ul>
</li>
</ul>
<p>对于步长$α$，和SARSA一样，一般也需要随着迭代的进行逐渐变小才能保证动作价值函数$Q$收敛。</p>
<h1 id="6-sarsa小结">6. SARSA小结</h1>
<p>SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和蒙特卡罗法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。</p>
<p>但是SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在 SARSA 算法中，$Q(S,A)$ 的值使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。</p>
<p>下一篇我们讨论SARSA的姊妹算法，时序差分离线控制算法Q-Learning。</p>
]]></description></item><item><title>RL学习笔记 [4] | 用蒙特卡罗法（MC）求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_4/</link><pubDate>Thu, 22 Feb 2024 13:00:24 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_4/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（三）用动态规划（DP）求解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型 $P$ 都无法知道，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？本文要讨论的蒙特卡罗(Monte-Calo, MC)就是一种可行的方法。</p>
<p>蒙特卡罗法这一篇对应Sutton书的第五章和UCL强化学习课程的第四讲部分，第五讲部分。</p>
<h1 id="1-不基于模型的强化学习问题定义">1. 不基于模型的强化学习问题定义</h1>
<p>在动态规划法中，强化学习的两个问题是这样定义的:</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的6个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵 $P$, 即时奖励 $R$，衰减因子 $γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>可见, 模型状态转化概率矩阵 $P$ 始终是已知的，即MDP已知，对于这样的强化学习问题，我们一般称为<mark>基于模型的强化学习</mark>问题。</p>
<p>不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵 $P$ ，这时如果仍然需要我们求解强化学习问题，那么这就是<mark>不基于模型的强化学习</mark>问题了。它的两个问题一般的定义是：</p>
<ul>
<li>
<p><strong>预测问题</strong>，即给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$ , 给定策略 $π$， 求解该策略的状态价值函数 $v(π)$</p>
</li>
<li>
<p><strong>控制问题</strong>，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率 $ϵ$, 求解最优的动作价值函数 $q∗$ 和最优策略 $π∗$　</p>
</li>
</ul>
<p>本文要讨论的蒙特卡罗法就是上述不基于模型的强化学习问题。</p>
<h1 id="2-蒙特卡罗法求解特点">2. 蒙特卡罗法求解特点</h1>
<p>蒙特卡罗这个词之前的博文也讨论过，尤其是在之前的<a href="https://www.cnblogs.com/pinard/p/MCMC%28%e4%b8%80%29%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%96%b9%e6%b3%95"target="_blank" rel="external nofollow noopener noreferrer">MCMC系列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中。它是一种通过采样近似求解问题的方法。这里的蒙特卡罗法虽然和MCMC不同，但是采样的思路还是一致的。那么如何采样呢？</p>
<p>蒙特卡罗法通过采样若干经历完整的状态序列(episode)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。</p>
<p>从特卡罗法法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。</p>
<h1 id="3-蒙特卡罗法求解强化学习预测问题">3. 蒙特卡罗法求解强化学习预测问题</h1>
<p>这里我们先来讨论蒙特卡罗法求解强化学习预测问题的方法，即策略评估。一个给定策略 $π$ 的完整有T个状态的状态序列如下：</p>
<p>$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</p>
<p>回忆下<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中对于价值函数 $v_π(s)$的定义:</p>
<p>$$v_\pi(s)=\mathbb{E}_\pi(G_t|S_t=s)=\mathbb{E}_\pi(R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots|S_t=s)$$</p>
<p>可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解，也就是：</p>
<p>$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T$$</p>
<p>$$v_\pi(s)\approx average(G_t),s.t.S_t=s$$</p>
<p>可以看出，预测问题的求解思路还是很简单的。不过有几个点可以优化考虑。</p>
<ul>
<li>
<p>第一个点是: 同样一个状态可能在一个完整的状态序列中重复出现，那么该状态的收获该如何计算？有两种解决方法。第一种是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：首次访问(first visit) 和每次访问(every visit) 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p>
</li>
<li>
<p>第二个点是累进更新平均值(incremental mean)。在上面预测问题的求解公式里，我们有一个average的公式，意味着要保存所有该状态的收获值之和最后取平均。这样浪费了太多的存储空间。一个较好的方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。通过下面的公式就很容易理解这个过程：</p>
</li>
</ul>
<p>$$\mu_k=\frac1k\sum_{j=1}^kx_j=\frac1k(x_k+\sum_{j=1}^{k-1}x_j)=\frac1k(x_k+(k-1)\mu_{k-1})=\mu_{k-1}+\frac1k(x_k-\mu_{k-1})$$</p>
<p>这样上面的状态价值公式就可以改写成：</p>
<p>$$N(S_t)=N(S_t)+1$$</p>
<p>$$V(S_t)=V(S_t)+\frac1{N(S_t)}(G_t-V(S_t))$$</p>
<p>这样我们无论数据量是多还是少，算法需要的内存基本是固定的 。</p>
<p>有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数 $N(S_t)$,这时我们可以用一个系数 $α$ 来代替，即：</p>
<p>$$V(S_t)=V(S_t)+\alpha(G_t-V(S_t))$$</p>
<p>对于动作价值函数 $Q(S_t,A_t)$,也是类似的，比如对上面最后一个式子，动作价值函数版本为：</p>
<p>$$Q(S_t,A_t)=Q(S_t,A_t)+\alpha(G_t-Q(S_t,A_t))$$</p>
<p>以上就是蒙特卡罗法求解预测问题的整个过程，下面我们来看控制问题求解。</p>
<h1 id="4-蒙特卡罗法求解强化学习控制问题">4. 蒙特卡罗法求解强化学习控制问题</h1>
<p>蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。回忆下动态规划价值迭代的的思路， 每轮迭代先做策略评估，计算出价值 $v_k(s)$ ，然后基于据一定的方法（比如贪婪法）更新当前策略 $π$。最后得到最优价值函数 $v∗$ 和最优策略 $π∗$。</p>
<p>和动态规划比，蒙特卡罗法不同之处体现在三点:</p>
<ul>
<li>一是预测问题策略评估的方法不同，这个第三节已经讲了。</li>
<li>第二是蒙特卡罗法一般是优化最优动作价值函数 $q∗$，而不是状态价值函数 $v∗$。</li>
<li>三是动态规划一般基于贪婪法更新策略。而蒙特卡罗法一般采用 $ϵ−$贪婪法更新。这个 $ϵ$ 就是我们在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中讲到的第8个模型要素 $ϵ$。$ϵ−$贪婪法通过设置一个较小的 $ϵ$ 值，使用 $1−ϵ$ 的概率贪婪地选择目前认为是最大行为价值的行为，而用 $ϵ$ 的概率随机的从所有 $m$ 个可选行为中选择行为。用公式可以表示为：
$$\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;if\mathrm{~}a^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.$$</li>
</ul>
<p>在实际求解控制问题时，为了使算法可以收敛，一般 $ϵ$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Mento Carlo 搜索过程示意</div>
</center>
<br>
<h1 id="5-蒙特卡罗法控制问题算法流程">5. 蒙特卡罗法控制问题算法流程</h1>
<p>在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。在线和离线的区别我们在后续的文章里面会讲。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。</p>
<p>在线蒙特卡罗法求解强化学习控制问题的算法流程如下:</p>
<ul>
<li>输入：状态集 $S$, 动作集 $A$, 即时奖励 $R$，衰减因子 $γ$, 探索率$ϵ$</li>
<li>输出：最优的动作价值函数 $q∗$ 和最优策略 $π∗$</li>
<li>
<ol>
<li>初始化所有的动作价值 $Q(s,a)=0$ ， 状态次数 $N(s,a)=0$，采样次数 $k=0$，随机初始化一个策略 $π$</li>
</ol>
</li>
<li>
<ol start="2">
<li>$k=k+1$, 基于策略 $π$ 进行第k次蒙特卡罗采样，得到一个完整的状态序列:
$$S_1,A_1,R_2,S_2,A_2,\ldots S_t,A_t,R_{t+1},\ldots R_T,S_T$$</li>
</ol>
</li>
<li>
<ol start="3">
<li>对于该状态序列里出现的每一状态行为对 $(S_t,A_t)$，计算其收获 $G_t$, 更新其计数 $N(s,a)$ 和行为价值函数 $Q(s,a)$：
$$\begin{gathered}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots\gamma^{T-t-1}R_T \\\\
N(S_t,A_t)=N(S_t,A_t)+1 \\\\
Q(S_t,A_t)=Q(S_t,A_t)+\frac1{N(S_t,A_t)}(G_t-Q(S_t,A_t))
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="4">
<li>基于新计算出的动作价值，更新当前的 $ϵ−$贪婪策略：
$$\begin{gathered}
\epsilon=\frac1k \\\\
\left.\pi(a|s)=\left\\{\begin{array}{ll}\epsilon/m+1-\epsilon&amp;ifa^*=\arg\max_{a\in A}Q(s,a)\\\\\epsilon/m&amp;else\end{array}\right.\right.
\end{gathered}$$</li>
</ol>
</li>
<li>
<ol start="5">
<li>如果所有的 $Q(s,a)$ 收敛，则对应的所有 $Q(s,a)$ 即为最优的动作价值函数 $q∗$。对应的策略 $π(a|s)$ 即为最优策略 $π∗$。否则转到第二步。</li>
</ol>
</li>
</ul>
<h1 id="6-蒙特卡罗法求解强化学习问题小结">6. 蒙特卡罗法求解强化学习问题小结</h1>
<p>蒙特卡罗法是我们第二个讲到的求解强化问题的方法，也是第一个不基于模型的强化问题求解方法。它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点，这就是它每次采样都需要一个完整的状态序列。如果我们没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了， 也就是说，我们还需要寻找其他的更灵活的不基于模型的强化问题求解方法。</p>
<p>下一篇我们讨论用时序差分方法来求解强化学习预测和控制问题的方法。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9492980.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9492980.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [3] | 用动态规划(DP)求解</title><link>https://jianye0428.github.io/posts/rl_learning_note_3/</link><pubDate>Thu, 22 Feb 2024 08:59:02 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_3/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。</p>
<p>动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲。</p>
<h1 id="1-动态规划和强化学习问题的联系">1. 动态规划和强化学习问题的联系</h1>
<p>对于动态规划，相信大家都很熟悉，很多使用算法的地方都会用到。就算是机器学习相关的算法，使用动态规划的也很多，比如之前讲到的<a href="https://www.cnblogs.com/pinard/p/6955871.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="https://www.cnblogs.com/pinard/p/6991852.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>， 都是动态规划的典型例子。</p>
<p>动态规划的关键点有两个：一是问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解。第二是可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题的状态。而强化学习的问题恰好是满足这两个条件的。</p>
<p>我们先看看强化学习的两个基本问题。</p>
<p>第一个问题是预测，即给定强化学习的6个要素：状态集 $S$, 动作集$A$, 模型状态转化概率矩阵$P$, 即时奖励$R$，衰减因子$γ$, 给定策略$π$， 求解该策略的状态价值函数$v(π)$</p>
<p>第二个问题是控制，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 模型状态转化概率矩阵$P$, 即时奖励$R$，衰减因子$γ$, 求解最优的状态价值函数 $v∗$ 和最优策略 $π∗$　</p>
<p>那么如何找到动态规划和强化学习这两个问题的关系呢？</p>
<p>回忆一下上一篇<a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（二）马尔科夫决策过程(MDP)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中状态价值函数的贝尔曼方程：</p>
<p>$$v_\pi(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_\pi(s&rsquo;))$$</p>
<p>从这个式子我们可以看出，我们可以定义出子问题求解每个状态的状态价值函数，同时这个式子又是一个递推的式子, 意味着利用它，我们可以使用上一个迭代周期内的状态价值来计算更新当前迭代周期某状态 $s$ 的状态价值。可见，使用动态规划来求解强化学习问题是比较自然的。</p>
<h1 id="2-策略评估求解预测问题">2. 策略评估求解预测问题</h1>
<p>首先，我们来看如何使用动态规划来求解强化学习的预测问题，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p>
<p>策略评估的基本思路是从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。</p>
<p>假设我们在第k轮迭代已经计算出了所有的状态的状态价值，那么在第 $k+1$ 轮我们可以利用第k轮计算出的状态价值计算出第k+1+1轮的状态价值。这是通过贝尔曼方程来完成的，即：</p>
<p>$$v_{k+1}(s)=\sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_k(s&rsquo;))$$</p>
<p>和上一节的式子唯一的区别是由于我们的策略 $π$ 已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变很小(收敛)，那么我们就得出了预测问题的解，即给定策略的状态价值函数 $v(π)$。</p>
<p>下面我们用一个具体的例子来说明策略评估的过程。</p>
<h1 id="3-策略评估求解实例">3. 策略评估求解实例</h1>
<p>这是一个经典的Grid World的例子。我们有一个4x4的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为γ=1=1。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率P=1=1。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Grid World</div>
</center>
<br>
<p>首先我们初始化所有格子的状态价值为0，如上图 $k=0$ 的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。在 $k=1$ 的时候，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：</p>
<p>$$v_1^{(21)}=\frac14[(-1+0)+(-1+0)+(-1+0)+(-1+0)]=-1$$</p>
<p>第二行第二个格子的价值是：</p>
<p>$$v_1^{(22)}=\frac14[(-1+0)+(-1+0)+(-1+0)+(-1+0)]=-1$$</p>
<p>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图 $k=1$ 的时候。现在我们第一轮迭代完了。开始动态规划迭代第二轮了。还是看第二行第一个格子的价值：</p>
<p>$$v_2^{(21)}=\frac14[(-1+0)+(-1-1)+(-1-1)+(-1-1)]=-1.75$$</p>
<p>第二行第二个格子的价值是：</p>
<p>$$v_2^{(22)}=\frac14[(-1-1)+(-1-1)+(-1-1)+(-1-1)]=-2$$</p>
<p>最终得到的结果是上图 $k=2$ 的时候。第三轮的迭代如下：</p>
<p>$$v_3^{(21)}=\frac14[(-1-1.7)+(-1-2)+(-1-2)+(-1+0)]=-2.425$$</p>
<p>$$v_3^{(22)}=\frac14[(-1-1.7)+(-1-1.7)+(-1-2)+(-1-2)]=-2.85$$</p>
<p>最终得到的结果是上图 $k=3$ 的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h1 id="4-策略迭代求解控制问题">4. 策略迭代求解控制问题</h1>
<p>上面我们讲了使用策略评估求解预测问题，现在我们再来看如何使用动态规划求解强化学习的第二个问题控制问题。一种可行的方法就是根据我们之前基于任意一个给定策略评估得到的状态价值来及时调整我们的动作策略，这个方法我们叫做策略迭代(Policy Iteration)。</p>
<p>如何调整呢？最简单的方法就是贪婪法。考虑一种如下的贪婪策略：个体在某个状态下选择的行为是其能够到达后续所有可能的状态中状态价值最大的那个状态。还是以第三节的例子为例，如上面的图右边。当我们计算出最终的状态价值后，我们发现，第二行第一个格子周围的价值分别是0,-18,-20，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-14,-14,-20,-20。那么我们整行动策略为向状态价值为-14的方向移动，也就是图中的向左向上。</p>
<p>如果用一副图来表示策略迭代的过程的话，如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">Policy Iteration</div>
</center>
<br>
<p>在策略迭代过程中，我们循环进行两部分工作，第一步是使用当前策略 $π∗$ 评估计算当前策略的最终状态价值 $v∗$，第二步是根据状态价值 $v∗$ 根据一定的方法（比如贪婪法）更新策略 $π∗$，接着回到第一步，一直迭代下去，最终得到收敛的策略 $π∗$ 和状态价值 $v∗$。</p>
<h1 id="5-价值迭代求解控制问题">5. 价值迭代求解控制问题</h1>
<p>观察第三节的图发现，我们如果用贪婪法调整动作策略，那么当k=3=3的时候，我们就已经得到了最优的动作策略。而不用一直迭代到状态价值收敛才去调整策略。那么此时我们的策略迭代优化为价值迭代。</p>
<p>还是以第三节的例子为例，如上面的图右边。比如当k=2=2时，第二行第一个格子周围的价值分别是0,-2,-2，此时我们用贪婪法，则我们调整行动策略为向状态价值为0的方向移动，而不是随机移动。也就是图中箭头向上。而此时第二行第二个格子周围的价值分别是-1.7,-1.7,-2, -2。那么我们整行动策略为向状态价值为-1.7的方向移动，也就是图中的向左向上。</p>
<p>和上一节相比，我们没有等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时我们的状态价值的更新方法也和策略迭代不同。现在的贝尔曼方程迭代式子如下：</p>
<p>$$v_{k+1}(s)=\max_{a\in A}(R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^av_k(s&rsquo;))$$</p>
<p>可见由于策略调整，我们现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。</p>
<h1 id="6-异步动态规划算法">6. 异步动态规划算法</h1>
<p>在前几节我们讲的都是同步动态规划算法，即每轮迭代我会计算出所有的状态价值并保存起来，在下一轮中，我们使用这些保存起来的状态价值来计算新一轮的状态价值。</p>
<p>另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。</p>
<p>常见的异步动态规划算法有三种：</p>
<p>第一种是原位动态规划 (in-place dynamic programming)， 此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。</p>
<p>第二种是优先级动态规划 (prioritised sweeping)：该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。</p>
<p>第三种是实时动态规划 (real-time dynamic programming)：实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢。</p>
<h1 id="7-动态规划求解强化学习问题小结">7. 动态规划求解强化学习问题小结</h1>
<p>动态规划是我们讲到的第一个系统求解强化学习预测和控制问题的方法。它的算法思路比较简单，主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。</p>
<p>动态规划算法使用全宽度（full-width）的回溯机制来进行状态价值的更新，也就是说，无论是同步还是异步动态规划，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此我们还需要寻找其他的针对复杂问题的强化学习问题求解方法。</p>
<p>下一篇我们讨论用蒙特卡罗方法来求解强化学习预测和控制问题的方法。</p>
<p>ref:
<a href="https://www.cnblogs.com/pinard/p/9463815.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9463815.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [2] | 马尔科夫决策过程(MDP)</title><link>https://jianye0428.github.io/posts/rl_learning_note_2/</link><pubDate>Wed, 21 Feb 2024 10:38:11 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_2/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>在<a href="https://www.cnblogs.com/pinard/p/9385570.html"target="_blank" rel="external nofollow noopener noreferrer">强化学习（一）模型基础<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。</p>
<p>MDP这一篇对应Sutton书的第三章和UCL强化学习课程的第二讲。</p>
<h1 id="1-强化学习引入mdp的原因">1. 强化学习引入MDP的原因</h1>
<p>对于马尔科夫性本身，我之前讲过的<a href="http://www.cnblogs.com/pinard/p/6945257.html"target="_blank" rel="external nofollow noopener noreferrer">隐马尔科夫模型HMM（一）HMM模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，<a href="http://www.cnblogs.com/pinard/p/7048333.html"target="_blank" rel="external nofollow noopener noreferrer">条件随机场CRF(一)从随机场到线性链条件随机场<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>以及<a href="http://www.cnblogs.com/pinard/p/6632399.html"target="_blank" rel="external nofollow noopener noreferrer">MCMC(二)马尔科夫链<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>都有讲到。它本身是一个比较简单的假设，因此这里就不专门对“马尔可夫性”做专门的讲述了。</p>
<p>除了对于环境的状态转化模型这个因素做马尔科夫假设外，我们还对强化学习第四个要素个体的策略(policy) $π$ 也做了马尔科夫假设。即在状态 $s$ 时采取动作 $a$ 的概率仅与当前状态 $s$ 有关，与其他的要素无关。用公式表示就是</p>
<p>$$\pi(a\mid s)=P(A_{t}=a\mid S_{t}=s)$$</p>
<p>对于第五个要素，价值函数 $v_π(s)$ 也是一样, $v_π(s)$ 现在仅仅依赖于当前状态了，那么现在价值函数 $v_π(s)$ 表示为:</p>
<p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(G_{t}|S_{t}=s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)$$</p>
<p>其中，$G_t$ 代表收获(return), 是一个MDP中从某一个状态 $S_t$ 开始采样直到终止状态时所有奖励的有衰减的之和。</p>
<h1 id="2-mdp的价值函数与贝尔曼方程">2. MDP的价值函数与贝尔曼方程</h1>
<p>对于MDP，我们在第一节里已经讲到了它的价值函数 $v_π(s)$ 的表达式。但是这个表达式没有考虑到所采用的动作$a$带来的价值影响，因此我们除了 $v_π(s)$ 这个<strong>状态价值函数</strong>外，还有一个<strong>动作价值函数</strong> $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=\operatorname{E}_{\pi}(G_{t}|S_{t}=s,A_{t}=a)=\operatorname{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s,A_{t}=a)$$</p>
<p>根据价值函数的表达式，我们可以推导出价值函数基于状态的递推关系，比如对于状态价值函数 $v_π(s)$，可以发现：</p>
<p>$$\begin{aligned}
V_{\pi}(s)&amp; =\mathrm{E}_{\pi}(R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots\mid S_{t}=s)  \
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\ldots)|S_{t}=s) \
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma G_{t+1}|S_{t}=s) \
&amp;=\mathrm{E}_{\pi}(R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s)
\end{aligned}$$</p>
<p>也就是说，在 $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 是满足递推关系的，即：</p>
<p>$$v_{\pi}(s)=\mathrm{E}_{\pi}(R_{t+1}+\gamma\nu_{\pi}(S_{t+1})\mid S_{t}=s)$$
　　　　
这个递推式子我们一般将它叫做<strong>贝尔曼方程</strong>。这个式子告诉我们，一个状态的价值由该状态的奖励以及后续状态价值按一定的衰减比例联合组成。</p>
<p>同样的方法，我们可以得到动作价值函数 $q_π(s,a)$ 的贝尔曼方程：</p>
<p>$$q_{\pi}(s,a)=\mathrm{E}_{\pi}(R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})\mid S_{t}=s,A_{t}=a)$$</p>
<h1 id="3-状态价值函数与动作价值函数的递推关系">3. 状态价值函数与动作价值函数的递推关系</h1>
<p>根据动作价值函数 $q_π(s,a)$ 和状态价值函数 $v_π(s)$ 的定义，我们很容易得到他们之间的转化关系公式：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$</p>
<p>也就是说，状态价值函数是所有动作价值函数基于策略 $π$ 的期望。通俗说就是某状态下所有状态动作价值乘以该动作出现的概率，最后求和，就得到了对应的状态价值。</p>
<p>反过来，利用上贝尔曼方程，我们也很容易从状态价值函数 $v_π(s)$ 表示动作价值函数 $q_π(s,a)$，即：</p>
<p>$$q_{\pi}(s,a)=R_{s}^{a}+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s&rsquo;)$$</p>
<p>通俗说就是状态动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。</p>
<p>这两个转化过程也可以从下图中直观的看出：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">状态价值函数</div>
</center>
<br>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">动作价值函数</div>
</center>
<br>
<p>把上面两个式子互相结合起来，我们可以得到：</p>
<p>$$\nu_{\pi}(s)=\sum_{a\in A}\pi(a\mid s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\nu_{\pi}(s^{&rsquo;}))$$</p>
<p>$$q_\pi(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\sum_{a&rsquo; \in A}\pi(a&rsquo; \mid s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)$$</p>
<h1 id="4-最优价值函数">4. 最优价值函数</h1>
<p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略我们可以用 $π^*$表示。一旦找到这个最优策略$π^∗$，那么我们就解决了这个强化学习问题。一般来说，比较难去找到一个最优策略，但是可以通过比较若干不同策略的优劣来确定一个较好的策略，也就是局部最优解。</p>
<p>如何比较策略的优劣呢？一般是通过对应的价值函数来比较的，也就是说，寻找较优策略可以通过寻找较优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者，即：</p>
<p>$$v^{\ast}(s)=\max_{\pi} v_{\pi}(s)$$</p>
<p>同理也可以定义最优动作价值函数是所有策略下产生的众多动作状态价值函数中的最大者，即：</p>
<p>$$q^{\ast}(s,a)=\max_\pi q_\pi(s,a)$$</p>
<p>对于最优的策略，基于动作价值函数我们可以定义为：</p>
<p>$$\pi ^{\ast} (a|s)=\begin{cases}1&amp;\mathrm{if~}a=\mathrm{arg~}\max_{a\in A}q*(s,a)\\\\0&amp;\mathrm{else}&amp;\end{cases}$$</p>
<p>只要我们找到了最大的状态价值函数或者动作价值函数，那么对应的策略 $π^*$ 就是我们强化学习问题的解。同时，利用状态价值函数和动作价值函数之间的关系，我们也可以得到:</p>
<p>$$v^{\ast}(s)=\max_a q ^{\ast} (s,a)$$</p>
<p>反过来的最优价值函数关系也很容易得到：</p>
<p>$$q_{<em>}(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss}^{a}{}_{</em>}(\mathrm{s&rsquo;})$$</p>
<p>利用上面的两个式子也可以得到和第三节末尾类似的式子：</p>
<p>$$v_<em>(s)=\max_a(R_s^a+\gamma\sum_{s^{\prime}\in S}P_{ss&rsquo;}^a\nu_</em>(s&rsquo;))$$</p>
<p>$$q_<em>(s,a)=R_s^a+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^a\max_{a&rsquo;}q_</em>(s&rsquo;,a&rsquo;)$$</p>
<h1 id="5-mdp实例">5. MDP实例</h1>
<p>上面的公式有点多，需要一些时间慢慢消化，这里给出一个UCL讲义上实际的例子，首先看看具体我们如何利用给定策略来计算价值函数。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP 举例</div>
</center>
<br>
<p>例子是一个学生学习考试的MDP。里面左下那个圆圈位置是起点，方框那个位置是终点。上面的动作有study, pub, facebook, quit, sleep，每个状态动作对应的即时奖励R已经标出来了。我们的目标是找到最优的动作价值函数或者状态价值函数，进而找出最优的策略。</p>
<p>为了方便，我们假设衰减因子 $γ=1$, $π(a|s)=0.5$。</p>
<p>对于终点方框位置，由于其没有下一个状态，也没有当前状态的动作，因此其状态价值函数为0。对于其余四个状态，我们依次定义其价值为<em>v</em>1,<em>v</em>2,<em>v</em>3,<em>v</em>4， 分别对应左上，左下，中下，右下位置的圆圈。我们基于$\nu_{\pi}(s)=\sum_{a\in A}\pi(a|s)(R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}v_{\pi}(s&rsquo;))$计算所有的状态价值函数。可以列出一个方程组。</p>
<ul>
<li>
<p>对于<em>v</em>1位置，我们有：$v_1=0.5*(-1+v_1)+0.5*(0+v_2)$</p>
</li>
<li>
<p>对于<em>v</em>2位置，我们有：$v_2=0.5*(-1+v_1)+0.5*(-2+v_3)$</p>
</li>
<li>
<p>对于<em>v</em>3位置，我们有：$v_3=0.5*(0+0)+0.5*(-2+v_4)$</p>
</li>
<li>
<p>对于<em>v</em>4位置，我们有：$v_4=0.5*(10+0)+0.5*(1+0.2<em>v_2+0.4</em>v_3+0.4*v_4)$</p>
</li>
</ul>
<p>解出这个方程组可以得到 $v_1=−2.3$, $v_2=−1.3$, $v_3=2.7$, $v_4=7.4$, 即每个状态的价值函数如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>上面我们固定了策略$ π(a|s)$, 虽然求出了每个状态的状态价值函数，但是却并不一定是最优价值函数。那么如何求出最优价值函数呢？这里由于状态机简单，求出最优的状态价值函数 $v*(s)$ 或者动作价值函数 $q*(s,a)$ s比较容易。</p>
<p>我们这次以动作价值函数 $q*(s,a)$ 来为例求解。首先终点方框处的好求。</p>
<p>$$q*(s_3,sleep)=0,q*(s_4,study)=10$$</p>
<p>接着我们就可利用 $q*(s,a)=R_{s}^{a}+\gamma\sum_{s&rsquo; \in S}P_{ss&rsquo;}^{a}\max_{a&rsquo;}q*(s&rsquo;,a&rsquo;)$ 列方程组求出所有的 $q∗(s,a)$ 。有了所有的 $q^{\ast}(s,a)$,利用 $v_{<em>}(s)=\max_{a}q</em>(s,a)$ 就可以求出所有的 $v∗(s)$。最终求出的所有 $v∗(s)$ 和 $q∗(s,a)$ 如下图：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">MDP</div>
</center>
<br>
<p>从而我们的最优决策路径是走6-&gt;6-&gt;8-&gt;10-&gt;结束。　　　　</p>
<h1 id="6-mdp小结">6. MDP小结</h1>
<p>MDP是强化学习入门的关键一步，如果这部分研究的比较清楚，后面的学习就会容易很多。因此值得多些时间在这里。虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。</p>
<p>下一篇讨论用动态规划的方法来求解强化学习的问题。</p>
<h1 id="7-ref">7. ref</h1>
<p><a href="https://www.cnblogs.com/pinard/p/9426283.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/pinard/p/9426283.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>RL学习笔记 [1] | 模型基础</title><link>https://jianye0428.github.io/posts/rl_learning_note_1/</link><pubDate>Wed, 21 Feb 2024 10:38:07 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_learning_note_1/</guid><description><![CDATA[<h1 id="0-引言">0. 引言</h1>
<p>　从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。</p>
<p>　第一篇会从强化学习的基本概念讲起，对应Sutton书的第一章和UCL课程的第一讲。</p>
<h1 id="1-强化学习在机器学习中的位置">1. 强化学习在机器学习中的位置</h1>
<p>强化学习的学习思路和人比较类似，是在实践中学习，比如学习走路，如果摔倒了，那么我们大脑后面会给一个负面的奖励值，说明走的姿势不好。然后我们从摔倒状态中爬起来，如果后面正常走了一步，那么大脑会给一个正面的奖励值，我们会知道这是一个好的走路姿势。那么这个过程和之前讲的机器学习方法有什么区别呢？</p>
<p>强化学习是和监督学习，非监督学习并列的第三种机器学习方法，从下图我们可以看出来。</p>
  <br>
  <center>
    
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">RL、SL、UL与ML的区别联系</div>
  </center>
  <br>
<p>与监督学习相比，强化学习最大的区别是它没有监督学习已经准备好的训练数据输出值。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的，比如上面的例子里走路摔倒了才得到大脑的奖励值。同时，强化学习的每一步与时间顺序前后关系紧密。而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。</p>
<p>再来看看强化学习和非监督学习的区别。也还是在奖励值这个地方。非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。</p>
<h1 id="2-强化学习的建模">2. 强化学习的建模</h1>
<p>我们现在来看看强化学习这样的问题我们怎么来建模，简单的来说，是下图这样的：</p>
  <br>
  <center>
    
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">大脑与环境的交互</div>
  </center>
  <br>
<p>上面的大脑代表我们的算法执行个体，我们可以操作个体来做决策，即选择一个合适的动作（Action）$A_t$。下面的地球代表我们要研究的环境,它有自己的状态模型，我们选择了动作 $A_t$ 后，环境的状态(State)会变，我们会发现环境状态已经变为 $S_{t+1}$,同时我们得到了我们采取动作 $A_t$ 的延时奖励(Reward) $R_{t+1}$。然后个体可以继续选择下一个合适的动作，然后环境的状态又会变，又有新的奖励值&hellip;这就是强化学习的思路。</p>
<p>那么我们可以整理下这个思路里面出现的强化学习要素。</p>
<ul>
<li>
<p>第一个是环境的状态 $S$, $t$ 时刻环境的状态 $S_t$ 是它的环境状态集中某一个状态。</p>
</li>
<li>
<p>第二个是个体的动作 $A$, $t$ 时刻个体采取的动作 $A_t$ 是它的动作集中某一个动作。</p>
</li>
<li>
<p>第三个是环境的奖励 $R$, $t$ 时刻个体在状态 $S_t$ 采取的动作 $A_t$ 对应的奖励 $R_{t+1}$ 会在 $t+1$ 时刻得到。</p>
</li>
<li>
<p>第四个是个体的策略(policy) $π$,它代表个体采取动作的依据，即个体会依据策略 $π$ 来选择动作。最常见的策略表达方式是一个条件概率分布 $π(a|s)$, 即在状态 $s$ 时采取动作 $a$ 的概率。即 $π(a|s)=P(A_t=a|S_t=s)$.此时概率大的动作被个体选择的概率较高。</p>
</li>
<li>
<p>第五个是个体在策略 $π$ 和状态 $s$ 时，采取行动后的价值(value)，一般用 $v_π(s)$ 表示。这个价值一般是一个期望函数。虽然当前动作会给一个延时奖励 $R_{t+1}$,但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表到了 $t+1$, $t+2$,&hellip;时刻的后续奖励也高。比如下象棋，我们可以某个动作可以吃掉对方的车，这个延时奖励是很高，但是接着后面我们输棋了。此时吃车的动作奖励值高但是价值并不高。因此我们的价值要综合考虑当前的延时奖励和后续的延时奖励。价值函数 $v_{\pi}(s)$ 一般可以表示为下式，不同的算法会有对应的一些价值函数变种，但思路相同。
$$v_{\pi}(s)=\mathbb{E}_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+&hellip;|S_t=s)$$</p>
</li>
<li>
<p>其中 $γ$ 是第六个模型要素，即奖励衰减因子，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。大多数时候，我们会取一个0到1之间的数字，即当前延时奖励的权重比后续奖励的权重大。</p>
</li>
<li>
<p>第七个是环境的状态转化模型，可以理解为一个概率状态机，它可以表示为一个概率模型，即在状态 $s$ 下采取动作 $a$,转到下一个状态 $s&rsquo;$ 的概率，表示为 $P^a_{ss&rsquo;}$。</p>
</li>
<li>
<p>第八个是探索率 $ϵ$，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。因此我们在训练选择最优动作时，会有一定的概率 $ϵ$ 不选择使当前轮迭代价值最大的动作，而选择其他的动作。</p>
</li>
</ul>
<p>以上8个就是强化学习模型的基本要素了。当然，在不同的强化学习模型中，会考虑一些其他的模型要素，或者不考虑上述要素的某几个，但是这8个是大多数强化学习模型的基本要素。</p>
<h1 id="3-强化学习的简单实例">3. 强化学习的简单实例</h1>
<p>这里给出一个简单的强化学习例子Tic-Tac-Toe。这是一个简单的游戏，在一个3x3的九宫格里，两个人轮流下，直到有个人的棋子满足三个一横一竖或者一斜，赢得比赛游戏结束，或者九宫格填满也没有人赢，则和棋。</p>
<p>这个例子的完整代码在<a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/introduction.py"target="_blank" rel="external nofollow noopener noreferrer">github<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。例子只有一个文件，很简单，代码首先会用两个电脑选手训练模型，然后可以让人和机器对战。当然，由于这个模型很简单，所以只要你不乱走，最后的结果都是和棋，当然想赢电脑也是不可能的。</p>
<p>我们重点看看这个例子的模型，理解上面第二节的部分。如何训练强化学习模型可以先不管。代码部分大家可以自己去看，只有300多行。</p>
<ul>
<li>
<p>首先看第一个要素环境的状态 $S$。这是一个九宫格，每个格子有三种状态，即没有棋子(取值0)，有第一个选手的棋子(取值1)，有第二个选手的棋子(取值-1)。那么这个模型的状态一共有$3^9=19683$个。</p>
</li>
<li>
<p>接着我们看个体的动作 $A$，这里只有9个格子，每次也只能下一步，所以最多只有9个动作选项。实际上由于已经有棋子的格子是不能再下的，所以动作选项会更少。实际可以选择动作的就是那些取值为0的格子。</p>
</li>
<li>
<p>第三个是环境的奖励 $R$，这个一般是我们自己设计。由于我们的目的是赢棋，所以如果某个动作导致的改变到的状态可以使我们赢棋，结束游戏，那么奖励最高，反之则奖励最低。其余的双方下棋动作都有奖励，但奖励较少。特别的，对于先下的棋手，不会导致结束的动作奖励要比后下的棋手少。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># give reward to two players</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">giveReward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">currentState</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p1Symbol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">currentState</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">p2Symbol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p1</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">p2</span><span class="o">.</span><span class="n">feedReward</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第四个是个体的策略(policy) $π$，这个一般是学习得到的，我们会在每轮以较大的概率选择当前价值最高的动作，同时以较小的概率去探索新动作，在这里AI的策略如下面代码所示。里面的exploreRate就是我们的第八个要素探索率 $ϵ$。即策略是以 $1−ϵ$ 的概率选择当前最大价值的动作，以 $ϵ$ 的概率随机选择新动作。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># determine next action</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">takeAction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">nextStates</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="n">nextPositions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BOARD_ROWS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BOARD_COLS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextPositions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">nextState</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span><span class="o">.</span><span class="n">getHash</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploreRate</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">nextPositions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Not sure if truncating is the best way to deal with exploratory step</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Maybe it&#39;s better to only skip this step rather than forget all the history</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span> <span class="o">=</span> <span class="n">nextPositions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">action</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="nb">hash</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nextStates</span><span class="p">,</span> <span class="n">nextPositions</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="nb">hash</span><span class="p">],</span> <span class="n">pos</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">values</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">symbol</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">action</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第五个是价值函数，代码里用value表示。价值函数的更新代码里只考虑了当前动作的现有价值和得到的奖励两部分，可以认为我们的第六个模型要素衰减因子 $γ$ 为0。具体的代码部分如下，价值更新部分的代码加粗。具体为什么会这样更新价值函数我们以后会讲。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># update estimation according to reward</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">feedReward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="o">.</span><span class="n">getHash</span><span class="p">()</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">latestState</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">stepSize</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">estimations</span><span class="p">[</span><span class="n">latestState</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">      <span class="n">target</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>第七个是环境的状态转化模型, 这里由于每一个动作后，环境的下一个模型状态是确定的，也就是九宫格的每个格子是否有某个选手的棋子是确定的，因此转化的概率都是1，不存在某个动作后会以一定的概率到某几个新状态，比较简单。</p>
</li>
</ul>
<p>从这个例子，相信大家对于强化学习的建模会有一个初步的认识了。　　　　　　　　</p>
<p>以上就是强化学习的模型基础，下一篇会讨论马尔科夫决策过程。</p>
]]></description></item><item><title>RL | 强化学习 -- 简介</title><link>https://jianye0428.github.io/posts/rl_introduction/</link><pubDate>Fri, 14 Jul 2023 08:21:32 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/rl_introduction/</guid><description><![CDATA[<h2 id="1-强化学习">1. 强化学习</h2>
<p>Reinforcement Learning (RL): 强化学习</br>
强化学习是人工智能（AI）和机器学习（ML）领域的一个重要子领域，不同于<code>监督学习</code>和<code>无监督学习</code>，强化学习通过智能体与环境的不断交互(即采取动作)，进而获得奖励，从而不断优化自身动作策略，以期待最大化其长期收益(奖励之和)。强化学习特别适合序贯决策问题(涉及一系列有序的决策问题)。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">ML Categories</div>
</center>
<br>
<p>在实际应用中，针对某些任务，我们往往无法给每个数据或者状态贴上准确的标签，但是能够知道或评估当前情况或数据是好还是坏，可以采用强化学习来处理。例如，下围棋(Go)，星际争霸II(Starcraft II)等游戏。</p>
<h4 id="11-强化学习的定义">1.1 强化学习的定义</h4>
<p>Agent interacts with its surroundings known as the environment. Agent will get a reward from the environemnt once it takes an action in the current enrivonment. Meanwhile, the environment evolves to the next state. The goal of the agent is to maximize its total reward (the Return) in the long run.</p>
<p>智能体与环境的不断交互(即在给定状态采取动作)，进而获得奖励，此时环境从一个状态转移到下一个状态。智能体通过不断优化自身动作策略，以期待最大化其长期回报或收益(奖励之和)。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">强化学习流程图</div>
</center>
<br>
<h3 id="12-强化学习的相关概念">1.2 强化学习的相关概念</h3>
<p>(1) <font color=red>状态 State ($S$)</font>: agent’s observation of its environment;</br></p>
<p>(2) <font color=red>动作 Action ($A$)</font>: the approaches that agent interacts with the environment;</br></p>
<p>(3) <font color=red>奖励 Reward ($R_t$)</font>: the bonus that agent get once it takes an action in the environment at the given time step t.回报(Return)为Agent所获得的奖励之和。</br></p>
<p>(4) <font color=red>转移概率 Transistion Probability ($P$)</font>: the transition possibility that environment evolves from one state to another. 环境从一个状态转移到另一个状态，可以是确定性转移过程，例如，$S_{t+1} = f(S_t, A_t)$, 也可以是随机性转移过程，例如 $S_{t+1} \sim p\left( S_{t+1}|S_t, A_t \right)$</br></p>
<p>(5) <font color=red>折扣因子 Discount factor ( $\gamma$ )</font>: to measure the importance of future reward to agent at the current state.</br></p>
<p>(6) <font color=red>轨迹(Trajectory)</font>:是一系列的状态、动作、和奖励，可以表述为：</p>
<p>$$\tau = (S_0, A_0, R_0, S_1, A_1, R_1, &hellip; )$$</p>
<p>用轨迹$\tau$来记录Agent如何和环境交互。轨迹的初始状态是从起始状态分布中随机采样得到的。一条轨迹有时候也称为片段(Episode)或者回合，是一个从初始状态(Initial State，例如游戏的开局)到最终状态(Terminal State，如游戏中死亡或者胜利)的序列。</br></p>
<p>(7) <font color=red>探索-利用的折中(Exploration-Exploitation Tradeoff)</font>:
这里，探索是指Agent通过与环境的交互来获取更多的信息，而利用是指使用当前已知信息来使得Agent的表现达到最佳，例如，贪心(greedy)策略。同一时间，只能二者选一。因此，如何平衡探索和利用二者，以实现长期回报(Long-term Return)最大，是强化学习中非常重要的问题。</br></p>
<p>因此，可以用$ (S，A，P，R，\gamma) $来描述强化学习过程。</p>
<h3 id="13-强化学习的数学建模">1.3 强化学习的数学建模</h3>
<p>(1) 马尔可夫过程 (Markov Process，MP) 是一个具备马尔可夫性质的离散随机过程。</p>
<p>马尔可夫性质是指下一状态 $ S_{t+1} $ 只取决于当前状态 $S_t$.</p>
<p>$$p(S_{t+1}|S_{t}) = p(S_{t+1} | S_0, S_1, S_2, &hellip;, S_t)$$</p>
<p>可以用有限状态集合 $\mathcal{S}$ 和状态转移矩阵 $\mathbf{P}$ 表示MP过程为 $&lt;\mathcal{S}, \mathbf{P}&gt;$。</p>
<p>为了能够刻画环境对Agent的反馈奖励，马尔可夫奖励过程将上述MP从 $&lt;\mathcal{S}, \mathbf{P}&gt;$ 扩展到了$ &lt;\mathcal{S}, \mathbf{P}, R, \gamma&gt;$。这里，$R$表示奖励函数，而 $\gamma$ 表示奖励折扣因子。</p>
<p>$$R_t = R(S_t)$$</p>
<p>回报(Return)是Agent在一个轨迹上的累计奖励。折扣化回报定义如下：</p>
<p>$$G_{t=0:T} = R(\tau) = \sum_{t=0}^{T}\gamma^{t}R_t$$</p>
<p>价值函数(Value Function) $V(s)$是Agent在状态$s$的期望回报(Expected Return)。</p>
<p>$$V^{\pi} (s) = \mathbb{E}[R(\tau) | S_0 = s]$$</p>
<p>(3) 马尔可夫决策过程 (Markov Decision Process，MDP)</br></p>
<p>MDP被广泛应用于经济、控制论、排队论、机器人、网络分析等诸多领域。
马尔可夫决策过程的立即奖励(Reward，$R$)与状态和动作有关。MDP可以用$&lt;\mathcal{S},\mathcal{A}, \mathbf{P}, R, \gamma&gt;$来刻画。
$\mathcal{A}$表示有限的动作集合，此时，立即奖励变为</p>
<p>$$R_t = R(S_t, A_t)$$</p>
<p>策略(Policy)用来刻画Agent根据环境观测采取动作的方式。Policy是从一个状态 $s \in \mathcal{S}$ 到动作 $a \in \mathcal{A}$的概率分布$\pi(a|s)$ 的映射，$\pi(a|s)$ 表示在状态$s$下，采取动作 $a$ 的概率。</p>
<p>$$\pi (a|s) = p (A_t = a | S_t = s), \exist{t} $$</p>
<p>期望回报(Expected Return)是指在一个给定策略下所有可能轨迹的回报的期望值，可以表示为：</p>
<p>$$J(\pi) = \int_{\tau} p(\tau | \pi) R(\tau) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]$$</p>
<p>这里, $p(\tau|\pi)$表示给定初始状态分布 $\rho_0$ 和策略 $\pi$，马尔可夫决策过程中一个 $T$ 步长的轨迹 $\tau$ 的发生概率，如下：</p>
<p>$$p(\tau | \pi) = \rho_0(s_0)\prod \limits_{t=0}^{T-1} p(S_{t+1} | S_t, A_t) \pi (A_t | S_t)$$</p>
<p>强化学习优化问题通过优化方法来提升策略，以最大化期望回报。最优策略$\pi^*$ 可以表示为:</p>
<p>$$\pi ^ * = \argmax_{\pi} J(\pi)$$</p>
<p>给定一个策略 $\pi$，价值函数$V(s)$，即给定状态下的期望回报，可以表示为:</p>
<p>$$V^{\pi}(s) = \mathbb{E}_{\tau \sim \pi} [R(\tau) | S_0 = s] = \mathbb{E}_{A_t \sim \pi(\cdot | S_t)} [\sum_{t=0}^{\infin}\gamma^t R(S_t, A_t) | S_0 = s]$$</p>
<p>在MDP中，给定一个动作，就有动作价值函数(Action-Value Function)，是基于状态和动作的期望回报。其定义如下：</p>
<p>$$Q^{\pi}(s, a) = \mathbb{E}_{\tau \sim \pi}[R(\tau) | S_0 = s, A_0 = a] = \mathbb{E}_{A_t \sim \pi(\cdot | S_t)}[\sum_{t=0}^{\infin}\gamma^t R(S_t, A_t)|S_0 = s, A_0 = a]$$</p>
<p>根据上述定义，可以得到：</p>
<p>$$V^{\pi}(s) = \mathbb{E}_{a \sim \pi}[Q^{\pi}(s,a)]$$</p>
<h2 id="2-深度强化学习">2. 深度强化学习</h2>
<p>Deep Learning + Reinforcement Learning = Deep Reinforcement Learning (DRL)
深度学习DL有很强的抽象和表示能力，特别适合建模RL中的值函数，例如: 动作价值函数 $Q^\pi \left(s, a \right)$。
二者结合，极大地拓展了RL的应用范围。</p>
<h2 id="3-常见深度强化学习算法">3. 常见深度强化学习算法</h2>
<p>深度强化学习的算法比较多，常见的有：DQN，DDPG，PPO，TRPO，A3C，SAC 等等。</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">常见深度强化学习算法</div>
</center>
<br>
<h3 id="31-deep-q-networks-dqn">3.1 Deep Q-Networks （DQN）</h3>
<p>DQN网路将Q-Learning和深度学习结合起来，并引入了两种新颖的技术来解决以往采用神经网络等非线性函数逼近器表示动作价值函数 <code>Q(s,a)</code> 所产生的不稳定性问题：</p>
<ul>
<li>技术1: 经验回放缓存（Replay Buffer）：将Agent获得的经验存入缓存中，然后从该缓存中均匀采用（也可考虑基于优先级采样）小批量样本用于Q-Learning的更新；</li>
<li>技术2: 目标网络（Target Network）：引入独立的网络，用来代替所需的Q网络来生成Q-Learning的目标，进一步提高神经网络稳定性。</li>
</ul>
<p>其中, 技术1 能够提高样本使用效率，降低样本间相关性，平滑学习过程；技术2 能够是目标值不受最新参数的影响，大大较少发散和震荡。</p>
<p>DQN算法具体描述如下：
<br></p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">DQN 伪代码</div>
</center>
<br>
</br>
注意：这里随机动作选择概率$\epsilon$一般是随着迭代Episode和Time Step的增加，而逐渐降低，目的是降低随机策略的影响，逐步提高Q网络对Agent动作选择的影响。
<p>该算法中，Line 14 具体更新方式如下：</p>
<p>$$\theta^Q\leftarrow\theta^Q+\beta\sum_{i\in\mathcal{N}}\frac{\partial Q(s,a|\theta^Q)}{\partial\theta^Q}\left[y_i-Q(s,a|\theta^Q)\right]$$</p>
<p>其中，集合$N$中为<code>minibatch</code>的$N$个$(S_t,A_t,R_t,S_{t+1})$经验样本集合，$\beta$表示一次梯度迭代中的迭代步长。</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>参考文献<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">[1] V. Mnih et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.</div>
    </div>
  </div>
<h3 id="32-deep-deterministic-policy-gradientddpg">3.2 Deep Deterministic Policy Gradient（DDPG）</h3>
<p>DDPG算法可以看作Deterministic Policy Gradient（DPG）算法和深度神经网络的结合，是对上述深度Q网络（DQN）在连续动作空间的扩展。</p>
<p>DDPG同时建立Q值函数（Critic）和策略函数（Actor）。这里，Critic与DQN相同，采用TD方法进行更新；而Actor利用Critic的估计，通过策略梯度方法进行更新。</p>
<p>DDPG算法具体描述如下：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">DDPG 伪代码</div>
</center>
<br>
<p>原论文中采用Ornstein-Uhlenbeck过程（O-U过程）作为添加噪声项N \mathcal{N}N，也可以采用时间不相关的零均值高斯噪声（相关实践表明，其效果也很好）。</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>参考文献<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">[1] Lillicrap, Timothy P., et al. “Continuous control with deep reinforcement learning”，arXiv preprint, 2015, online: <a href="https://arxiv.org/pdf/1509.02971.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/1509.02971.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></div>
    </div>
  </div>
<h3 id="33-proximal-policy-optimizationppo">3.3 Proximal Policy Optimization（PPO）</h3>
<p>PPO算法是对信赖域策略优化算法(Trust Region Policy Optimization, TRPO) 的一个改进，用一个更简单有效的方法来强制策略$\pi_\theta$与$\pi_{\theta}^{\prime}$相似。</p>
<p>具体来说，TRPO中的优化问题如下：</p>
<p>$$\begin{gathered}\max_{\pi_{\theta}^{\prime}}\mathcal{L}_{\pi_{\theta}}(\pi_{\theta}^{\prime})\\s.t.\mathbb{E}_{s\sim\rho_{\pi_\theta}}[D_{KL}\left(\pi_\theta\left|\left|\pi_\theta^{\prime}\right.\right)\right]\leq\delta \end{gathered}$$</p>
<p>而PPO算法直接优化上述问题的正则版本，即：</p>
<p>$$\max_{\pi_{\theta}^{\prime}}\mathcal{L}_{\pi_{\theta}}\left(\pi_{\theta}^{\prime}\right)-\lambda\mathbb{E}_{s\sim\rho_{\pi_{\theta}}}\quad[D_{KL}\left(\pi_{\theta}||\pi_{\theta}^{\prime}\right)]$$</p>
<p>这里，入为正则化系数，对应TRPO优化问题中的每一个$\delta$,都存在一个相应的$\lambda$,使得上述两个优化问题有相同的解。然而，入的值依赖于$\pi_\theta$,因此，在PPO中，需要使用一个可动态调整的$\lambda$。具体来说有两种方法：
(1) 通过检验KL散度值来决定$\lambda$是增大还是减小，该版本的PPO算法称为PPO-Penalty;
(2) 直接截断用于策略梯度的目标函数，从而得到更保守的更新，该方法称为PPO-Clip。</p>
<p>PPO-Clip算法具体描述如下：</p>
<br>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;">PPO 伪代码</div>
</center>
<br>
<p>$$f(\theta&rsquo;)=\min\left(\ell_t\left(\theta&rsquo;\right)A^{\pi_{\theta_{dd}}}(S_t,A_t),clip(\ell_t\left(\theta&rsquo;\right),1-\epsilon,1+\epsilon)A^{\pi_{\theta_{dd}}}(S_t,A_t)\right)$$</p>
<p>这里，$clip(x,1-\epsilon,1+\epsilon)$表示将$x$截断在$[1-\epsilon,1+\epsilon]$中。</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-quote-right fa-fw" aria-hidden="true"></i>参考文献<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">[1] Schulman, J. , et al. “Proximal Policy Optimization Algorithms”，arXiv preprint, 2017, online: <a href="https://arxiv.org/pdf/1707.06347.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/1707.06347.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2] Schulman J, Levine S, Abbeel P, et al. “Trust region policy optimization”, International conference on machine learning. PMLR, 2015: 1889-1897, online: <a href="http://proceedings.mlr.press/v37/schulman15.pdf"target="_blank" rel="external nofollow noopener noreferrer">http://proceedings.mlr.press/v37/schulman15.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></div>
    </div>
  </div>
<h2 id="4-深度强化学习算法分类">4. 深度强化学习算法分类</h2>
<h3 id="41-根据agent训练与测试所采用的策略是否一致">4.1 根据Agent训练与测试所采用的策略是否一致</h3>
<h4 id="411-off-policy-离轨策略离线策略">4.1.1 off-policy (离轨策略、离线策略)</h4>
<p>Agent在训练(产生数据)时所使用的策略 $\pi_1$与 agent测试(方法评估与实际使用&ndash;目标策略)时所用的策略 $\pi_2$ 不一致。</p>
<p>例如，在DQN算法中，训练时，通常采用 $\epsilon-greedy$ 策略；而在测试性能或者实际使用时，采用 $ a^* = arg \max\limits_{a} Q^{\pi}\left( s, a \right) $ 策略。</p>
<p>常见算法有：DDPG，TD3，Q-learning，DQN等。</p>
<h4 id="412-on-policy-同轨策略在线策略">4.1.2 on-policy (同轨策略、在线策略)</h4>
<p>Agent在训练时(产生数据)所使用的策略与其测试(方法评估与提升)时使用的策略为同一个策略 $\pi$。</p>
<p>常见算法有：Sarsa，Policy Gradient，TRPO，PPO，A3C等。</p>
<h3 id="42-策略优化的方式不同">4.2 策略优化的方式不同</h3>
<h4 id="421-value-based-algorithms基于价值的算法">4.2.1 Value-based algorithms(基于价值的算法)</h4>
<p>基于价值的方法通常意味着对动作价值函数 $Q^{\pi}(s,a)$的优化，最优策略通过选取该函数 $Q^{\pi}(s,a)$ 最大值所对应的动作，即 $\pi^* \approx \arg \max\limits_{\pi}Q^{\pi}(s,a)$，这里，$\approx$ 由函数近似误差导致。</p>
<p>基于价值的算法具有采样效率相对较高，值函数估计方差小，不易陷入局部最优等优点，缺点是通常不能处理连续动作空间问题，最终策略通常为确定性策略。</p>
<p>常见算法有 Q-learning，DQN，Double DQN，等，适用于 Discrete action space。其中，DQN算法是基于state-action function $Q(s,a)$ 来进行选择最优action的。</p>
<h4 id="422-policy-based-algorithms基于策略的算法">4.2.2 Policy-based algorithms(基于策略的算法)</h4>
<p>基于策略的方法直接对策略进行优化，通过对策略迭代更新，实现累计奖励(回报)最大化。其具有策略参数化简单、收敛速度快的优点，而且适用于连续或者高维动作空间。</p>
<p>**策略梯度方法(Policy Gradient Method，PGM)**是一类直接针对期望回报通过梯度下降(Gradient Descent，针对最小化问题)进行策略优化的强化学习方法。其不需要在动作空间中求解价值最大化的优化问题，从而比较适用于 continuous and high-Dimension action space，也可以自然地对随机策略进行建模。</p>
<p>PGM方法通过梯度上升的方法直接在神经网络的参数上优化Agent的策略。</p>
<p>根据相关理论，期望回报 $J(\pi_{\theta})$ 关于参数 $\theta$ 的梯度可以表示为：</p>
<p>$$\nabla_\theta J(\pi_\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^TR_t\nabla_\theta\sum_{t^{\prime}=0}^T\log\pi_\theta(A_{t^{\prime}}|S_{t^{\prime}})\right]=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t^{\prime}=0}^T\nabla_\theta\log\pi_\theta\left(A_{t^{\prime}}|S_{t^{\prime}}\right)\sum_{t=0}^TR_t\right]$$</p>
<p>当$T \rightarrow \infin$ 时，上式可以表示为：</p>
<p>$$\nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t&rsquo;=0}^{\infin}\nabla_{\theta} \log \pi_{\theta}(A_{t&rsquo;} | S_{t&rsquo;}) \gamma^{t&rsquo;}\sum_{t=t&rsquo;}^{\infin} \gamma^{t-t&rsquo;}R_t]$$</p>
<p>在实际中，经常去掉 $ \gamma^{t^{\prime}} $，从而避免过分强调轨迹早期状态的问题。</p>
<p>上述方法往往对梯度的估计有较大的方法(奖励 $R_t$ 的随机性可能对轨迹长度L呈指数级增长)。为此，常用的方法是引进一个基准函数 $b(S_i)$，仅是状态 $S_i$ 的函数。可将上述梯度修改为：</p>
<p>$$\nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t&rsquo;=0}^{\infin}\nabla_{\theta} \log \pi_{\theta}(A_{t&rsquo;} | S_{t&rsquo;}) (\sum_{t=t&rsquo;}^{\infin} \gamma^{t-t&rsquo;}R_t - b(S_{t&rsquo;}))]$$</p>
<p>常见的PGM算法有REINFORCE，PG，PPO，TRPO 等。</p>
<h4 id="423-actor-critic-algorithms-演员-评论家方法">4.2.3 Actor-Critic algorithms (演员-评论家方法)</h4>
<p>Actor-Critic方法结合了上述 <font color=red>基于价值</font> 的方法和 <font color=red>基于策略</font> 的方法，利用基于价值的方法学习Q值函数或状态价值函数V来提高采样效率(Critic)，并利用基于策略的方法学习策略函数(Actor)，从而适用于连续或高维动作空间。其缺点也继承了二者的缺点，例如，Critic存在过估计问题，而Actor存在探索不足的问题等。</p>
<p>常见算法有 DDPG, A3C，TD3，SAC等，适用于 continuous and high-Dimension action space</p>
<h3 id="43-参数更新的方式不同">4.3 参数更新的方式不同</h3>
<p>Parameters updating methods</p>
<h4 id="431-monte-carlo-method蒙特卡罗方法">4.3.1 Monte Carlo method(蒙特卡罗方法)</h4>
<p>蒙特卡罗方法：必须等待一条轨迹 $\tau_k$ 生成(真实值)后才能更新。</p>
<p>常见算法有：Policy Gradient，TRPO，PPO等。</p>
<h4 id="432-temporal-difference-method时间差分方法">4.3.2 Temporal Difference method(时间差分方法)</h4>
<p>时间差分方法：在每一步动作执行都可以通过自举法(Bootstrapping)(估计值)及时更新。</p>
<p>常见算法有：DDPG，Q-learning，DQN等。</p>
<h2 id="参考">参考</h2>
<p>[1]. <a href="https://blog.csdn.net/b_b1949/article/details/128997146"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/b_b1949/article/details/128997146<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></br>
[2]. <a href="https://blog.csdn.net/magicyangjay111/article/details/132645347"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/magicyangjay111/article/details/132645347<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>