<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Horovod - 标签 - yejian's blog</title><link>https://jianye0428.github.io/tags/horovod/</link><description>Horovod - 标签 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Thu, 13 Jul 2023 08:18:52 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/tags/horovod/" rel="self" type="application/rss+xml"/><item><title>Horovod and Openmpi</title><link>https://jianye0428.github.io/posts/horovod_and_openmpi/</link><pubDate>Thu, 13 Jul 2023 08:18:52 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/horovod_and_openmpi/</guid><description><![CDATA[<h2 id="horovod-介绍">Horovod 介绍</h2>
<p>Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; 与百度 &ldquo;Ring Allreduce&rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不同深度学习框架的支持，使用起来对开发人员比较的友好，算是分布式训练方向的标杆项目了。</p>
<h2 id="集合通信库">集合通信库</h2>
<p>集合通信库，这个词可能听起来会比较的陌生，不过如果我再提几个关键字，可能大家多少都会有所耳闻。资历比较老的是 MPI (<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Message_Passing_Interface"target="_blank" rel="external nofollow noopener noreferrer">Message Passing Interface<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 及其实现 <a href="https://link.zhihu.com/?target=https%3A//www.open-mpi.org/"target="_blank" rel="external nofollow noopener noreferrer">OpenMPI<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 和 <a href="https://link.zhihu.com/?target=https%3A//www.mpich.org/"target="_blank" rel="external nofollow noopener noreferrer">MPICH<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，年轻一点的会是 Nvidia 针对其显卡开源的 NCCL，或者是 facebook 开源的 gloo，或者是像华为针对其高性能硬件提供的HCCL，大体上都可以归入到<strong>集合通信库</strong>的类别。他们相同的地方是大体上会遵照 MPI 提供的接口规定，实现了包括<font color=red><em>点对点通信</em></font>（SEND,RECV等），<font color=red><em>集合通信</em></font>（ REDUCE，BROADCAST，ALLREDUCE等）等相关接口，然后根据自己硬件或者是系统的需要，在底层实现上进行了相应的改动，保证接口的稳定和性能。</p>
<h3 id="点对点通信-point-to-point-communication">点对点通信: Point-to-Point Communication</h3>
<p><strong>Send/Recv:</strong></p>
<p></p>
<h3 id="集合通信">集合通信</h3>
<p><strong>Scatter/Gather</strong></p>
<p></p>
<p><strong>reduce/allreduce</strong></p>
<p></p>
<p><strong>boradcast/all-gather</strong></p>
<p></p>
<p>这里在机器学习训练中使用比较多的是 <strong>all-reduce</strong>，场景类似在不同的 node 上跑不同 batch 的数据，然后更新梯度需要从各个汇总之后平均再回传到各自的 node 中。而这部分，有很多种实现的方式，比较直观和简单的是把所有的梯度都汇总到的某一个 node 上（如下图 node d 所示），然后再把汇总的信息重新分发到不同的 node 上 ，这样可以计算通信量，如下：对于 P 个节点，每个节点消息大小为 M，node d 节点的通信量为 2*(P-1)M，这里假设节点之间互联互通，带宽为B。</p>
<p></p>
<p>不过这种情况下，很容易导致 <strong>node d</strong> 会成为性能瓶颈，因为 <strong>node d</strong> 需要跟其他所有 <strong>node</strong> 通信所以它的通信量是其他节点的 <strong>P</strong> 倍。假设节点间的带宽还是一样，<strong>node d</strong> 完成所有通信所需要的时间是 <em><em>2</em>(P-1)M/B</em>*。所以现在很多的集合通信框架不会使用这种方式，更多的是<strong>通过树状或者是环状(ring) 去实现 all-reduce</strong>。</p>
<p>如果只是做成树状的可以做成如下图所示，虽然传递的步数增多了，不过消除了node d 的通信瓶颈，完成所有的通信的时间大概是 <em><em>2log_2N</em>(M/B)</em>*，随着节点数目 P 的增加，树形结构的效果会越来越明显。</p>
<p></p>
<p>业界用得最多一种优化的方式是，每次只传一部分，这部分是百度提出的 ring-allreduce 的方案，具体的介绍可以参考这篇博客<a href="https://link.zhihu.com/?target=https%3A//andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"target="_blank" rel="external nofollow noopener noreferrer">Bringing HPC Techniques to Deep Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，这边就不赘述了。整体上就是每次不会像上面这样整份数据传递，而是一部分一部分传，优化后，所有节点需要传输的数据量的传输 <strong>2(N−1)M/N</strong> 比较平均，所需要的时间可以大概是 <strong>2(N−1)M/(NB)</strong>，horovod 也是基于这种 all-reduce 的形式实现的。</p>
<h3 id="实践">实践:</h3>
<h4 id="pytorchdistributed">pytorch.distributed</h4>
<p>尝试使用 pytorch 自带的分布式工具包 <a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/distributed.html"target="_blank" rel="external nofollow noopener noreferrer">torch.distributed<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，进行一些概念性的尝试。</p>
<p>为了方便尝试，我这里提供了一个简单的 demo，大家如果安装了 gpu 版本的 pytorch &gt;= 1.3，应该都可以尝试下面的例子尝试使用多进程模拟分布式（单机上可以跑）。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">argparse</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;PyTorch MNIST Example&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-m&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--mode&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;one_device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute mode, distributed/one_device&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--function&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;p2p&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;function to run (p2p/all_reduce/gpu_all_reduce)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-b&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--backend&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s2">&#34;nccl&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute backend (gloo/nccl)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Initialize the distributed environment. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data before send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Send the tensor to process 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Receive tensor from process 0</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data after send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">backend</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">backend</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;distributed&#34;</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;RANK&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in distribute mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;RANK&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in one device mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>可以简单地运行上面的例子：</p>
<p><strong>send/recv:</strong></p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>上面是演示的是通过 pytorch 的 multiprocessing 包，模拟一次分布式的 send/recv 过程，这里是 rank0 的进程往 rank1 的进程发送一个 tensor，可以看到 rank 1 tensor 初始化为 0，是接收到 rank 0 的tensor 后变为 1 的。（注意：这里特别设置了 backend 为 gloo 是因为 nccl 不支持 point2point 的传输，具体不同 backend 支持什么形式的原语，参考文档backend部分 ）</p>
<p><strong>all_reduce</strong></p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对应函数</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># use rank 0 and rank 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里也很浅白，主要就是对两个进程上的 tensor 进行一次 allreduce，可以看到两个 rank 上的结果都为 2了。</p>
<p><strong>gpu_all_reduce</strong></p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="c1">#in one device mode</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;), tensor([1.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;), tensor([1.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:2&#39;), tensor([4.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:0&#39;), tensor([4.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  0  has data tensor[0]: tensor([8.], device=&#39;cuda:0&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:1&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  1  has data tensor[0]: tensor([8.], device=&#39;cuda:2&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:3&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对应函数</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<blockquote>
<p>all_reduce_multigpu: 相当于将多个gpu内的多进程的值进行相加;
all_reduce: 相当于单个gpu内的多进程的值相加</p>
</blockquote>
</blockquote>
<p>这里演示的是尝试对不同进程下多个 gpu (这里是 4 个) 进行 reduce，具体逻辑就是：</p>
<pre><code>- 对不同的进程分别把 tensor 初始化在不同的 gpu 上，rank0 初始化在 0，1 gpu 上，rank 1 在 2，3上。
- 进行一次 all_reduce_multigpu （这个函数跟 all_reduce 不同，是把不同的 node 上不同的gpu 上的tensor 都放到一个 list 中，进行reduce），这时所有 gpu 上的值都是4，作为对比，我们对 tensor_list[0] 的tensor 做一次all_reduce，得到的结果在 gpu 0,2 上的 tensor 进行了all_reduce 结果是 8，在 gpu 1,3 的 tensor 没有任何变化。
</code></pre>
<p><strong>多terminal尝试</strong></p>
<p>在验证分布式逻辑的时候，其实我们不一定需要多台机子才可以，对一些不涉及网络性能的验证，可以尝试在一台机子上开多个 terminal 进行验证。可以使用上面的例子，在多个 terminal 下跑以下命令。</p>
<p><em>terminal0:</em></p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">0</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p><em>terminal1:</em></p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">1</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里是通过本地机子上的回送地址进行模拟，结果是分别在不同的 terminal 呈现，当然可以用上面的demo，在多台机子上跑，不过需要修改一下 init_process 函数中的 os.environ[&lsquo;MASTER_ADDR&rsquo;] = &lsquo;127.0.0.1&rsquo; 为 rank 0 机子的 IP，这里就不演示了。具体 pytorch distributed 工具相关的内容可以参考<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/intermediate/dist_tuto.html"target="_blank" rel="external nofollow noopener noreferrer">官方博客<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>练习： 如果大概理解了上面的一些集合通信的原语，可以尝试着用上面 pytorch 提供的 send/recv 尝试去实现一下上面的树状 allreduce。</p>
<h3 id="mpi">MPI</h3>
<p>更深入的尝试，可以尝试了解一下 mpi 的知识，这个<a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/"target="_blank" rel="external nofollow noopener noreferrer">mpi<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>教程 算是写得比较系统的，大家可以参考一下来练习，特别是对底层不是很了解的同学，可以多看看 <a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/running-an-mpi-cluster-within-a-lan/"target="_blank" rel="external nofollow noopener noreferrer">Running an MPI cluster within a LAN<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 的部分，实操一下通过 ssh 跑起一个分布式的 demo。集合通信库的基础大概先到这里，如果要深入的可以再去看看 <a href="https://link.zhihu.com/?target=https%3A//github.com/open-mpi/ompi/blob/98afc838aa53da88cba339f6dcbab256806a5745/ompi/mca/coll/tuned/coll_tuned_allreduce_decision.c"target="_blank" rel="external nofollow noopener noreferrer">openMPI<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，和 <a href="https://github.com/NVIDIA/nccl"target="_blank" rel="external nofollow noopener noreferrer">nccl<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 的实现。</p>
<h2 id="horovod流程分析">Horovod流程分析</h2>
<p>下面我会以一个简单的 pytorch horovod 的 demo 尝试去理解一下 horovod 的工作机理，demo 如下（省略了一些不关键的代码段）。为了准确起见，我们是根据 horovod v0.20.3 的版本进行阅读的，如果是其他版本，可能会跟这里的内容有一些出入。</p>
<h3 id="pytorch-demo">pytorch demo</h3>
<p>一般的 horovod 训练程序都会包含以下几个关键步骤：</p>
<pre><code>1. hvd.init: 对 horovod
2. 初始化。初始化模型，数据集，优化器，初始化不同 node 的模型权重。
3. 使用 hvd.DistributedOptimizer 包装优化器。
4. 进入训练流程，进行优化迭代。
</code></pre>
<p>我们会着重介绍第 1 和 4 步，因为主要也是1，4步会跟 c++ 后端进行信息交换。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">timeit</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">...</span> <span class="c1"># some argparse</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">lr_scaler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl"><span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: wrap optimizer with DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">named_parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">op</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">Adasum</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_adasum</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Average</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: broadcast parameters &amp; optimizer state.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up fixed fake data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">()</span> <span class="o">%</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#... some log configuration</span>
</span></span><span class="line"><span class="cl"><span class="n">img_secs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="n">benchmark_step</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span> <span class="o">/</span> <span class="n">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_secs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img_sec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Results</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span></span></span></code></pre></td></tr></table>
</div>
</div><p>然后下图是我对 horovod 整体流程的梳理，把一些不是很关键的部分隐藏了，可能有一些细节的地方和实现有出入，不过我待会会有详细的说明。这里先解释一下，下面几个大的部分:</p>
<ul>
<li>main.py： 表示训练脚本，一般是 使用 horovod 提供的函数跟特定的训练框架相互合作完成分布式训练（下文称前端）</li>
<li>C++ interface：是指 horovod python 函数调用 C++ 的接口</li>
<li>GlobalState：在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问，在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）后面会提到，主要会在下图 backgroundThreadLoop 中完成 globalstate 不同元素初始化，比较重要的有 controller 管理总体通信控制流，tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等）。</li>
<li>BackgroundThreadLoop：是训练过程中的后台线程，主要负责跟其他节点的通信，和处理前端过来的通信需求（request），会轮询调用 RunLoopOnce，不断查看 tensor_queue 中有没有需要通信的tensor，如果有跟其他节点同步更新，然后执行通信操作。</li>
</ul>
<p></p>
<h3 id="流程分析">流程分析</h3>
<p>下面使用 mpi_controller 进行 allreduce 操作进行分析。</p>
<p><strong>1.hvd.init()-&gt;InitializeHorovodOnce</strong></p>
<p>首先，hvd.init() 会通过一系列的调用和配置最终调用 horovod/common/http://operations.cc 下的 InitializeHorovodOnce 函数，这个函数会根据加载的<strong>集合通讯库</strong>（<em>mpi</em> 或者 <em>gloo</em>）为 globalstate 创建对应的 controller，然后使用 BackgroundThreadLoop 启动一个后台线程。</p>
<p>horovod/common/http://operations.cc #628</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">InitializeHorovodOnce</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ... some envParse
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Enable mpi is it&#39;s used either i[n cpu data transfer or controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="p">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建一个 MPIController 对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Reset initialization flag
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 启动后台线程
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>2.BackgroundThreadLoop</strong></p>
<p>BackgroundThreadLoop 会为 GlobalState 初始化一系列包括初始化 mpi_context， controller的元素，然后轮询调用 RunLoopOnce，还有一些对 RunLoopOnce 结束后的后处理。</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">BackgroundThreadLoop</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize mpi context
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">mpi_ctx_manager</span> <span class="o">=</span> <span class="n">MPIContextManager</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// mpi_context 会根据前端和环境变量传过来的信息，创建 mpi 线程，和一些 mpiOps
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">mpi_context</span><span class="p">.</span><span class="n">Initialize</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRanks</span><span class="p">(),</span> <span class="n">mpi_ctx_manager</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 会同步不同 node 的 global_size, local_size, rank, is_coordinator 等信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">Initialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set background thread affinity
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">parse_and_set_affinity</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">getenv</span><span class="p">(</span><span class="n">HOROVOD_THREAD_AFFINITY</span><span class="p">),</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GPU
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="p">...</span> <span class="c1">// 设置 gpu_context 的 stream 数目等初始化动作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// 下面是设置 parameter_manager 这里为了节省篇幅直接给出，设置的语句，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 原来这里会读取对应的环境变量的，去设置 parameter_manager。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 后面也会有篇幅介绍 parameter_manager，这里先不展开。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetTensorFusionThresholdBytes</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCycleTimeMs</span><span class="p">(</span><span class="mi">5</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCacheEnabled</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">response_cache</span><span class="p">.</span><span class="n">set_capacity</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">CacheEnabled</span><span class="p">()</span> <span class="o">*</span> <span class="n">state</span><span class="p">.</span><span class="n">cache_capacity</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetHierarchicalAllgather</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetAutoTuning</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 其他一些初始化设置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 设置op_manager，这里主要是注册不同的集合通信库的 ops
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//（ 如：NCCLAllreduce, MPI_GPUAllgather 等）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_manager</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">CreateOperationManager</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 初始化完成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate until shutdown.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="p">(</span><span class="n">RunLoopOnce</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Horovod background loop uncaught exception: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// 其他一些后处理函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>3.Optimizer.step()-&gt;DoAllReduce</strong>
这里我们先不急着看 RunLoopOnce 函数，先回到 InitializeHorovodOnce ，因为上面的 initialization_done = True，所以 InitializeHorovodOnce 可以退出了，就是前端的 hvd.init() 可以进行下一步了。这里 main.py 走完前向 loss = model(data,target)，后向逻辑 loss.backward()，调用 optimizer.step() 进行梯度同步。optimizer.step() 会通过一系列的调用和处理（如：compression 等操作）最终会调用 C++ interface 的 DoAllReduce 函数。</p>
<p><em><strong>DoAllReduce</strong></em> 函数会调用 EnqueueTensorAllreduce 函数会把需要 reduce 的 tensor 组装成一个Request 往 GlobalState 的 tensor_queue 里面塞。这里注意每个 tensor 会创建对应 TensorTableEntry，用于保存tensor 的权重，message 主要是一些 元信息 metadata。然后就等后台线程去读取这些allreduce 的请求了。</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="nf">EnqueueTensorAllreduce</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">OpContext</span><span class="o">&gt;</span> <span class="n">context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyEvent</span><span class="o">&gt;</span> <span class="n">ready_event</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">StatusCallback</span> <span class="n">callback</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">ReduceOp</span> <span class="n">reduce_op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">postscale_factor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span> <span class="c1">// some config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Request</span> <span class="n">message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_request_rank</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_name</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_type</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_prescale_factor</span><span class="p">(</span><span class="n">prescale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_postscale_factor</span><span class="p">(</span><span class="n">postscale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">reduce_op</span> <span class="o">==</span> <span class="n">ReduceOp</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ALLREDUCE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dims</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">add_tensor_shape</span><span class="p">((</span><span class="kt">int64_t</span><span class="p">)</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dim_size</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorTableEntry</span> <span class="n">e</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">ready_event</span> <span class="o">=</span> <span class="n">ready_event</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">shut_down</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">SHUT_DOWN_ERROR</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">status</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">AddToTensorQueue</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">TRACE</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Enqueued &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>4.RunLoopOnce</strong></p>
<p>回到后台线程 BackgroundThreadLoop，后面会轮询调用 RunLoopOnce。 RunLoopOnce会首先调用 ComputeResponseList 函数，其主要工作是同步不同 worker 之间的需要 allreduce 的 tensors，为后面 allreduce 的执行做好准备。</p>
<p>？？？为什么会在执行 tensor 的 allreduce 之前执行这样一步工作呢？而不是直接执行 allreduce 呢？我自己的猜测是，因为分布式训练是运行在不同的机子上的，因为 <u>horovod 没有引入类似参数服务器（parameter server）的节点，而是采取 master-worker</u> 的形式 进行 allreduce的。所以 allreduce 的时候必须确保所有的节点都是走到了同一句 allreduce 上，然后传输的 tensors 也要求是一致的，否则传输的 tensors 有可能没有匹配起来就执行allreduce，导致一些不可预知的错误。另外这部分引入了一些提高性能的 tricks，如对之前 reduce 过的 tensor 通过一个 bitmap 进行缓存，每次调用看一下是不是都是之前的 tensor，如果不是再 update 一下，不需要每次都全量更新。？？？（不是很确定）</p>
<p><strong>ComputeResponseList</strong>具体的流程是(可以对照上面流程图看):</p>
<ul>
<li>从自己进程的 GlobalState 读取 tensor_queue 的信息，如果有新的元素，会通过图中 popMessagesFromQueue pop 出来，然后经过一系列处理缓存到 message_queue_tmp 中。</li>
<li>当 worker 到达了前端 all_reduce 这句的时候，会用 message_queue_tmp 整理成一个 message_list通过流程图中的 SendReadyTensors 函数往主节点( coordinator ) 发送一个请求表明我打算reduce，然后会把准备 reduce 的 tensor 信息通过 message_list 迭代地送过去，最后有一个 Done 的请求</li>
<li>coordinator 会接收通过图中 RecvReadyTensors 这些 requests，然后保存在 ready_to_reduce 中，coordinator 会持续接收这些信息，直到获取的 Done 的数目等于 global_size。</li>
<li>coordinator 会找到所有准备好 reduce 的 tensors，通过 SendFinalTensors 返回一个 response 给所有的 worker，如果信息有误会返回一个 error，发送完成也会发送一个 Done。</li>
<li>worker 会通过 RecvFinalTensors 监听 response 的信息，整理出需要 reduce 的 tensor，当收到 Done，会尝试调用 performation 去进行 reduce 。</li>
<li>coordinator 和 worker 都会把同步的信息整理成一个 responses 的数组给到后面的 PerformOperation 操作。</li>
</ul>
<p>这里说一下mpi是怎么实现的，就是<u>对应的 coordinator 和 worker 会阻塞地到同一条指令</u>：</p>
<p>SendReadyTensors 和 RecvReadyTensors 阻塞到 MPI_Gather，SendFinalTensors 和 RecvFinalTensors 到 MPI_Bcast ，可以这样分辨：<font color=red><em>如果是 coordinator 发送的就是 MPI_Bcast，如果是worker 发送的是 MPI_Gather</font></em>。通信都是先同步需要通信message的大小 length，再同步message，代码如下：</p>
<p>horovod/common/mpi/http://mpi_controller.cc</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">SendReadyTensors</span><span class="p">(</span><span class="n">RequestList</span><span class="o">&amp;</span> <span class="n">message_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">encoded_message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">RequestList</span><span class="o">::</span><span class="n">SerializeToString</span><span class="p">(</span><span class="n">message_list</span><span class="p">,</span> <span class="n">encoded_message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">encoded_message_length</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 先 gather 这个 message 的大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gather</span><span class="p">(</span><span class="o">&amp;</span><span class="n">encoded_message_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Gather failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 再 gather 这个 message
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gatherv</span><span class="p">((</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">encoded_message_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvReadyTensors</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span> <span class="n">ready_to_reduce</span><span class="p">,</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">RequestList</span><span class="o">&gt;&amp;</span> <span class="n">ready_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gather</span><span class="p">(</span><span class="n">MPI_IN_PLACE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gatherv</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="n">displcmnts</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvFinalTensors</span><span class="p">(</span><span class="n">ResponseList</span><span class="o">&amp;</span> <span class="n">response_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">msg_length</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="o">&amp;</span><span class="n">msg_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;MPI_Broadcast failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">buffer</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">uint8_t</span><span class="p">[</span><span class="n">msg_length</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">  <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">msg_length</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p><strong>5.PerformOperation</strong></p>
<p>从 ComputeResponseList 继续跑 RunLoopOnce， 不同 node 下面会根据前面 ComputeResponseList 返回的 response_list 对每个 response 轮询调用 PerformOperation 完成对应的 reduce 工作。</p>
<p>PerformOperation 流程：</p>
<p><code>horovod/common/http://operations.cc</code></p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">PerformOperation</span><span class="p">(</span><span class="n">Response</span> <span class="n">response</span><span class="p">,</span> <span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;</span> <span class="n">entries</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">timeline</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">response_type</span><span class="p">()</span> <span class="o">!=</span> <span class="n">Response</span><span class="o">::</span><span class="n">JOIN</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">GetTensorEntriesFromResponse</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">entries</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                             <span class="n">state</span><span class="p">.</span><span class="n">joined</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 对数据预处理和 buffer 初始化
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 执行 all_reduce 等操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">op_manager</span><span class="o">-&gt;</span><span class="n">ExecuteOperation</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">response</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">Status</span><span class="o">::</span><span class="n">UnknownError</span><span class="p">(</span><span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 调用 callback 函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>PerformOperation 会从 horovod_global.tensor_queue 通过函数 <code>GetTensorEntriesFromResponse</code> 取出对应的 TensorEntry</li>
<li>如果还没初始化buffer，调用 horovod_global.fusion_buffer.InitializeBuffer 初始化</li>
<li>然后 status = op_manager-&gt;ExecuteOperation(entries, response) 会调用不同的 op-&gt;Execute(entries, response) 执行reduce 运算</li>
</ul>
<p>下面以 <strong>MPIAllreduce::Execute</strong> 为例：
<code>horovod/common/ops/http://mpi_operations.cc</code></p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="n">MPIAllreduce</span><span class="o">::</span><span class="n">Execute</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;&amp;</span> <span class="n">entries</span><span class="p">,</span> <span class="k">const</span> <span class="n">Response</span><span class="o">&amp;</span> <span class="n">response</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 一些变量声明
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 把 tensor copy 到 buffer 中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_IN_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyInFusionBuffer</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">fused_input_data</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span> <span class="n">buffer_len</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">fused_input_data</span> <span class="o">=</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_data</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">size_t</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Do allreduce
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">sendbuf</span> <span class="o">=</span> <span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">fused_input_data</span> <span class="o">==</span> <span class="n">buffer_data</span>
</span></span><span class="line"><span class="cl">                        <span class="o">?</span> <span class="nl">MPI_IN_PLACE</span> <span class="p">:</span> <span class="n">fused_input_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">op</span> <span class="o">=</span> <span class="n">MPI_Allreduce</span><span class="p">(</span><span class="n">sendbuf</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">num_elements</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPIDataType</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPISumOp</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPICommunicator</span><span class="p">(</span><span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Allreduce failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Copy memory out of the fusion buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 把 allreduce 后的 tensor copy 会 entries
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_OUT_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyOutFusionBuffer</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">,</span> <span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>然后调用不同 entries 的 callback，这里 callback 一般是给前端作相应的。</li>
</ul>
<p><strong>6.parameter_manager.update</strong></p>
<p>完成上述步骤之后，如果设置了 state.parameter_manager.IsAutoTuning()，RunLoopOnce 还会调用相关的逻辑，调整传输的参数，然后返回 BackgroundThreadLoop 重新调用。_重新调用时会睡一定时间再继续_上述第 3 - 5 步的工作。</p>
<h3 id="其他关键模块">其他关键模块</h3>
<p>上面只是介绍了 horovod 主流程工作原理，不过 horovod 还有其他一些模块协同主流程工作的，下面会对其中的一些我认为可以值得一说的模块说一下。</p>
<p><strong>Parameter_manager:</strong> Parameter_manager 主要是 GlobalState 的一个用于管理一些调节 horovod 性能的参数的管理器，在 BackgroundThreadLoop 中跟其他的 GlobalState 的元素一同初始化，然后会读取下面这些对应的环境变量，然后进行设置。</p>
<p><strong>HOROVOD_FUSION_THRESHOLD</strong>：指传输数据切片的大小，默认是64M，如果切片太大，传输的时候就不能很好地 pipeline 传输，如果太小，一个 tensor 需要传输多次，增加 IO 的 overhead。</p>
<p><strong>HOROVOD_CYCLE_TIME</strong>：指 <u>RunLoopOnce 的睡眠时长</u>，默认是 <strong>5ms</strong>，我自己的猜测（还没进行验证）比较理想的睡眠时间应该是 RunLoopOnce 其余逻辑处理的时间 + HOROVOD_CYCLE_TIME 刚好等于一次前向传播和后向传播所用的时间，因为睡太久前端会在等 RunLoopOnce 睡醒；如果睡太短，不断地跑一次 RunLoopOnce，tensor_queue 也不会有新的元素，只是白跑。</p>
<p><strong>HOROVOD_CACHE_CAPACITY</strong>：指 cache 的大小，这个可能跟 model 层数参数量相关了。</p>
<p><strong>HOROVOD_HIERARCHICAL_ALLGATHER</strong>：是否使用分层的allgather的方式等</p>
<p>Parameter_manager也提供了对这些参数自动调节的功能。通过Parameter_manager.SetAutoTuning进行设置，设置后会在初始的几个batch尝试不同的参数组合进行通信，后面会收敛到一组最优的参数值。</p>
<h3 id="mpicontext">MPIContext</h3>
<p>mpi_context 是在加载 C++ 的代码时候就已经创建了，同时创建的还有其他 context（ nccl_context, gpu_context），主要是维护一些节点上 mpi 通信的必要环境信息和设置，如：</p>
<ul>
<li>3 个 MPI communicator，mpi_comm，local_comm，cross_comm 分别负责 horovod mpi 传输，节点内传输，和节点间分层传输（主要用于 hierarchical allreduce）。</li>
<li>mpi_float16_t: horovod 主要以 float16 传输。</li>
<li>mpi_float16_sum: float16 对应的sum 操作。</li>
</ul>
<p>在 horovod 使用 mpi 的时候，都会使用上面的 communicator 进行数据传输。</p>
<h3 id="tensorflow2">Tensorflow2</h3>
<p>TensorFlow2 前端对 horovod 的调用跟 pytorch 类似，只是因为 tensorflow 2 是通过 tape 等级制记录梯度的, 所以会有一些不同。</p>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">applications</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">999</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tf.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl">    <span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: use DistributedGradientTape</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: add Horovod Distributed GradientTape.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tape</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>with tf.GradientTape() as tape</code>这一句会调用 <code>horovod/tensorflow/__init__.py</code> 中<code>_DistributedGradientTape</code> 下 <strong>init</strong> 函数注册 allreduce 的句柄（handle）</li>
<li>然后调用 <code>gradients = tape.gradient(loss, model.trainable_variables)</code> 会调用一系列的跳转最后会调用 <code>tensorflow/mpi_ops.py</code> 下的 _allreduce ，进而调用 `MPI_LIB.horovod_allreduce</li>
<li><code>MPI_LIB.horovod_allreduce</code> 在 <code>horovod/tensorflow/http://mpi_ops.cc</code> 中被 <code>HorovodAllreduceOp</code> 所注册，根据 TensorFlow 的 ops流程，会调用 <code>ops.ComputeAsync</code>，到这里会跟 pytorch 类似会调用 <code>EnqueueTensorAllreduce</code> 把对应的 tensor 和 ops 送到 GlobalState 的 tensor_queue 中。</li>
</ul>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodAllreduceOp</span> <span class="p">:</span> <span class="n">public</span> <span class="n">AsyncOpKernel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="n">public</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">explicit</span> <span class="n">HorovodAllreduceOp</span><span class="p">(</span><span class="n">OpKernelConstruction</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">:</span> <span class="n">AsyncOpKernel</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;reduce_op&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">reduce_op_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;prescale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">prescale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;postscale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">postscale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;ignore_name_scope&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ignore_name_scope_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">void</span> <span class="n">ComputeAsync</span><span class="p">(</span><span class="n">OpKernelContext</span><span class="o">*</span> <span class="n">context</span><span class="p">,</span> <span class="n">DoneCallback</span> <span class="n">done</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">common</span><span class="p">::</span><span class="n">CheckInitialized</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span> <span class="o">//</span> <span class="n">一些变量验证</span><span class="err">，</span><span class="n">初始化</span>
</span></span><span class="line"><span class="cl">    <span class="n">auto</span> <span class="n">enqueue_result</span> <span class="o">=</span> <span class="n">EnqueueTensorAllreduce</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd_context</span><span class="p">,</span> <span class="n">hvd_tensor</span><span class="p">,</span> <span class="n">hvd_output</span><span class="p">,</span> <span class="n">ready_event</span><span class="p">,</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">done</span><span class="p">](</span><span class="n">const</span> <span class="n">common</span><span class="p">::</span><span class="n">Status</span><span class="o">&amp;</span> <span class="n">status</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">context</span><span class="o">-&gt;</span><span class="n">SetStatus</span><span class="p">(</span><span class="n">ConvertStatus</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">          <span class="n">done</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">prescale_factor_</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">postscale_factor_</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">enqueue_result</span><span class="p">),</span> <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">private</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="nb">int</span> <span class="n">reduce_op_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Using</span> <span class="nb">float</span> <span class="n">since</span> <span class="n">TF</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">support</span> <span class="n">double</span> <span class="n">OP</span> <span class="n">attributes</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">prescale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">postscale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">bool</span> <span class="n">ignore_name_scope_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="总结">总结</h2>
<p>horovod 的流程分析大概就是这样，没有特别复杂，代码的阅读体验也是比较好的，在主流程的关键函数都有比较清晰的注释。对于第三方开发者来说，horovod 本身已经用了很多提高性能的 tricks，可以 custom 优化的地方不多，一些可以动的参数，也已经提供了autotuning，直接使用就可以得到很好的性能。如果尝试优化，可能要从传输上着手，如 BytePS 会尝试使用不同的网络拓扑引入一些 PS 节点提高带宽等，如果有时间我也会聊一下这个。另外上面的分析也有很多是我自己阅读代码时候的一些思考可能不一定准确，如果有不准确或者模糊的地方，也希望大家可以多多斧正。</p>
<p>References:
[1]. <a href="https://zhuanlan.zhihu.com/p/332825987"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/332825987<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://zhuanlan.zhihu.com/p/158584571"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/158584571<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3]. <a href="https://zhuanlan.zhihu.com/p/79030485"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/79030485<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[4]. <a href="https://github.com/zjykzj/pytorch-distributed"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/zjykzj/pytorch-distributed<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[5]. <a href="https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/"target="_blank" rel="external nofollow noopener noreferrer">MPI教程<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<a href="https://blog.csdn.net/qq_47058489/article/details/125980505"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/qq_47058489/article/details/125980505<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&utm_relevant_index=1<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>[5.] <a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1"target="_blank" rel="external nofollow noopener noreferrer">ubuntu20.04 + docker + horovod<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h1 id="horovod-and-distributed-training">Horovod and Distributed Training</h1>
]]></description></item><item><title>深度学习分布式训练框架 horovod[4] -- 网络基础 &amp; Driver</title><link>https://jianye0428.github.io/posts/2022-10-08_horovod_4/</link><pubDate>Mon, 10 Jul 2023 07:53:48 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/2022-10-08_horovod_4/</guid><description><![CDATA[<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。本文是系列第四篇，看看如何获取 host 之间的路由等网络信息。</p>
<h2 id="1-引子">1 引子</h2>
<p>在 horovod/runner/launch.py 文件中，_run_static 函数中使用 <code>driver_service.get_common_interfaces</code> 来获取路由信息等。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_static</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">get_common_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">all_host_names</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">remote_host_names</span><span class="p">,</span> <span class="n">fn_cache</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>因为这部分比较复杂（ Driver 的概念很类似 Spark 之中 Driver 的概念），所以本文我们单独来分析。</p>
<p>本文的分析问题点是：</p>
<ul>
<li>为什么要知道路由信息？</li>
<li>当有多个host时候，horovod如何处理？</li>
<li>如何找到路由信息？</li>
<li>怎么互相交互？</li>
<li>（后文会详细分析）SparkDriverService，SparkTaskService，ElasticDriver, Worker 都有什么区别和联系？</li>
</ul>
<p>本文重点分析 HorovodRunDriverService 和 HorovodRunTaskService 相关。</p>
<p>先给出一个图例，大家可以有些概念。</p>
<p></p>
<h2 id="2-总体架构">2 总体架构</h2>
<p>从注释可知，get_common_interfaces 完成了获得路由信息（所有host之间的共有路由接口集合）的功能，主要是调用 _driver_fn 来完成相关工作。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_common_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">all_host_names</span><span class="p">,</span> <span class="n">remote_host_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fn_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Find the set of common and routed interfaces on all the hosts.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 得到远端host地址</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">remote_host_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">remote_host_names</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">filter_local_addresses</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">remote_host_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">:</span> <span class="c1"># 如果参数有设定网络接口，就使用</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># If args.nics is provided, we will use those interfaces. All the workers</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># must have at least one of those interfaces available.</span>
</span></span><span class="line"><span class="cl">            <span class="n">nics</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Find the set of common, routed interfaces on all the hosts (remote</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># and local) and specify it in the args to be used by NCCL. It is</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># expected that the following function will find at least one interface</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># otherwise, it will raise an exception.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">local_host_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">remote_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 获取其他host的网络接口</span>
</span></span><span class="line"><span class="cl">            <span class="n">nics</span> <span class="o">=</span> <span class="n">_driver_fn</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">fn_cache</span><span class="o">=</span><span class="n">fn_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span> <span class="o">=</span> <span class="n">get_local_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span> <span class="c1"># 获取本地的网络接口</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nics</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="21-get_local_interfaces">2.1 get_local_interfaces</h3>
<p>此函数比较简单，目的是<strong>获取本地的网络接口</strong>。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_local_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># If all the given hosts are local, find the interfaces with address</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 127.0.0.1</span>
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">iface</span><span class="p">,</span> <span class="n">addrs</span> <span class="ow">in</span> <span class="n">net_if_addrs</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span> <span class="ow">and</span> <span class="n">iface</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">addrs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">addr</span><span class="o">.</span><span class="n">family</span> <span class="o">==</span> <span class="n">AF_INET</span> <span class="ow">and</span> <span class="n">addr</span><span class="o">.</span><span class="n">address</span> <span class="o">==</span> <span class="s1">&#39;127.0.0.1&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">nics</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">iface</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nics</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="22-_driver_fn">2.2 _driver_fn</h3>
<p>这是本文重点，获取其他host 的网络接口，_driver_fn 的作用是：</p>
<ul>
<li>启动 service 服务；</li>
<li>使用 driver.addresses() 获取 Driver 服务的地址（使用<code>self._addresses = self._get_local_addresses()</code>完成）；</li>
<li>使用 _launch_task_servers（利用 Driver 服务的地址）在每个 worker 之中启动 task 服务，然后 task 服务会在 service 服务中注册；</li>
<li>因为是一个环形，每个 worker 会探测 worker index + 1 的所有网络接口；</li>
<li>最后 _run_probe 返回一个所有 workers 上的所有路由接口的交集；</li>
</ul>
<p>代码如下：</p>
<p>这里需要注意的一点是：@cache.use_cache() 的使用：当第一次使用过之后，会把结果放入缓存。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@cache.use_cache</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_driver_fn</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    launches the service service, launches the task service on each worker and
</span></span></span><span class="line"><span class="cl"><span class="s2">    have them register with the service service. Each worker probes all the
</span></span></span><span class="line"><span class="cl"><span class="s2">    interfaces of the worker index + 1 (in a ring manner) and only keeps the
</span></span></span><span class="line"><span class="cl"><span class="s2">    routed interfaces. Function returns the intersection of the set of all the
</span></span></span><span class="line"><span class="cl"><span class="s2">    routed interfaces on all the workers.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param all_host_names: list of addresses. for example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;worker-0&#39;,&#39;worker-1&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;10.11.11.11&#39;, &#39;10.11.11.12&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type all_host_names: list(string)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param local_host_names: host names that resolve into a local addresses.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type local_host_names: set
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param settings: the object that contains the setting for running horovod
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type settings: horovod.runner.common.util.settings.Settings
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return: example: [&#39;eth0&#39;, &#39;eth1&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype: list[string]
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Launch a TCP server called service service on the host running horovod</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 启动 service 服务</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_hosts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">driver</span> <span class="o">=</span> <span class="n">HorovodRunDriverService</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Have all the workers register themselves with the service service.</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#（利用 Driver 服务的地址）在每个worker之中启动 task 服务，然后task服务会在 service 服务中注册</span>
</span></span><span class="line"><span class="cl">    <span class="n">_launch_task_servers</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">driver</span><span class="o">.</span><span class="n">addresses</span><span class="p">(),</span> <span class="n">settings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 返回一个所有 workers 上的所有路由接口的交集</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_run_probe</span><span class="p">(</span><span class="n">driver</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="23-获取路由接口">2.3 获取路由接口</h3>
<p>我们对 _run_probe 函数做进一步分析。</p>
<h4 id="231-probe逻辑">2.3.1 probe逻辑</h4>
<p>_run_probe 函数就是当<u>所有 task 都启动，注册，probe 环中下一个worker 邻居完成</u> 之后，得到 接口集合。</p>
<ul>
<li>利用 wait_for_initial_registration 等待所有 task 完成注册；</li>
<li>对于所有 task，完成 task.notify_initial_registration_complete 通知；</li>
<li>利用 driver.wait_for_task_to_task_address_updates 等待 每一个 worker probe 完成；</li>
<li>利用 nics.intersection_update 得到接口集合；</li>
</ul>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_probe</span><span class="p">(</span><span class="n">driver</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">       <span class="c1"># wait for all the hosts to register with the service service.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">driver</span><span class="o">.</span><span class="n">wait_for_initial_registration</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_driver</span><span class="p">(</span><span class="n">index</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_hosts</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Notify all the drivers that the initial registration is complete.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">task</span> <span class="ow">in</span> <span class="n">tasks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">notify_initial_registration_complete</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Each worker should probe the interfaces of the next worker in a ring</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># manner and filter only the routed ones -- it should filter out</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># interfaces that are not really connected to any external networks</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># such as lo0 with address 127.0.0.1.</span>
</span></span><span class="line"><span class="cl">    <span class="n">driver</span><span class="o">.</span><span class="n">wait_for_task_to_task_address_updates</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Determine a set of common interfaces for task-to-task communication.</span>
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_tasks</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span><span class="o">.</span><span class="n">intersection_update</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_tasks</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nics</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="232-等待函数">2.3.2 等待函数</h4>
<p>probe 利用 wait_for_initial_registration 等待所有 task 完成注册，具体等待函数如下：</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">wait_for_initial_registration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">timeout</span><span class="o">.</span><span class="n">remaining</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="n">timeout</span><span class="o">.</span><span class="n">check_time_out_for</span><span class="p">(</span><span class="s1">&#39;tasks to start&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">wait_for_task_to_task_address_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">timeout</span><span class="o">.</span><span class="n">remaining</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="n">timeout</span><span class="o">.</span><span class="n">check_time_out_for</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;tasks to update task-to-task addresses&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-基础网络服务">3 基础网络服务</h2>
<p>前面提到，Horovod Driver 的概念很类似 Spark 之中 Driver 的概念。Spark应用程序运行时主要分为 Driver 和 Executor，Driver负责总体调度及UI展示，Executor负责Task运行。用户的Spark应用程序运行在Driver上（某种程度上说，用户的程序就是Spark Driver程序），经过Spark调度封装成一个个Task，再将这些Task信息发给Executor执行，Task信息包括代码逻辑以及数据信息，Executor不直接运行用户的代码。</p>
<p>对于 Horovod 来说：</p>
<ul>
<li>HorovodRunDriverService 就是 Driver 的实现类。</li>
<li>HorovodRunTaskService 提供了 Task 部分服务功能，这些 task 需要注册到 HorovodRunDriverService 之中。</li>
<li>这套 driver &amp; task 机制的底层由 &ldquo;基础网络服务&rdquo; 支撑。</li>
</ul>
<p>所以我们就仔细分析下基础网络服务。</p>
<h3 id="31-继承关系">3.1 继承关系</h3>
<p>首先给出继承关系，我们下面讲解的 Driver 服务由 HorovodRunDriverService 提供，Task 服务由HorovodRunTaskService 提供。</p>
<p>这两个类最终都继承了 network.BasicService。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">                            <span class="n">network</span><span class="o">.</span><span class="n">BasicService</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                                  <span class="o">^</span>    <span class="o">^</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+-------------------+</span>    <span class="o">+-------------+</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverService</span>       <span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskService</span>
</span></span><span class="line"><span class="cl">              <span class="o">^</span>                                      <span class="o">^</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl">    <span class="n">HorovodRunDriverService</span>                <span class="n">HorovodRunTaskService</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-networkbasicservice">3.2 network.BasicService</h3>
<p>BasicService 提供了一个网络服务器功能。即通过find_port函数构建了一个<code>ThreadingTCPServer</code>对外提供服务。</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BasicService</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">service_name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_service_name</span> <span class="o">=</span> <span class="n">service_name</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wire</span> <span class="o">=</span> <span class="n">Wire</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_nics</span> <span class="o">=</span> <span class="n">nics</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">find_port</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="k">lambda</span> <span class="n">addr</span><span class="p">:</span> <span class="n">socketserver</span><span class="o">.</span><span class="n">ThreadingTCPServer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">addr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_handler</span><span class="p">()))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">_block_on_close</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_port</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">socket</span><span class="o">.</span><span class="n">getsockname</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_addresses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_local_addresses</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">serve_forever</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="321-创建server">3.2.1 创建Server</h4>
<p>创建服务器代码如下，这里是搜索一个随机端口，然后设置：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">find_port</span><span class="p">(</span><span class="n">server_factory</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_port</span> <span class="o">=</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_port</span> <span class="o">=</span> <span class="mi">65536</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_ports</span> <span class="o">=</span> <span class="n">max_port</span> <span class="o">-</span> <span class="n">min_port</span>
</span></span><span class="line"><span class="cl">    <span class="n">start_port</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_ports</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">port_offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_ports</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">port</span> <span class="o">=</span> <span class="n">min_port</span> <span class="o">+</span> <span class="p">(</span><span class="n">start_port</span> <span class="o">+</span> <span class="n">port_offset</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_ports</span>
</span></span><span class="line"><span class="cl">            <span class="n">addr</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">server</span> <span class="o">=</span> <span class="n">server_factory</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">server</span><span class="p">,</span> <span class="n">port</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Unable to find a port to bind to.&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="322-server功能">3.2.2 Server功能</h4>
<p>服务器就是基本的功能，比如获取本server地址，处理 ping，网络交互等。</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_make_handler</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">server</span> <span class="o">=</span> <span class="bp">self</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">class</span> <span class="nc">_Handler</span><span class="p">(</span><span class="n">socketserver</span><span class="o">.</span><span class="n">StreamRequestHandler</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">req</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">resp</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">_handle</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">client_address</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># A tuple is the usual response object followed by a utf8 text stream</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">resp</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span> <span class="o">=</span> <span class="n">resp</span>
</span></span><span class="line"><span class="cl">                    <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">server</span><span class="o">.</span><span class="n">_wire</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">resp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">except</span> <span class="p">(</span><span class="ne">EOFError</span><span class="p">,</span> <span class="ne">BrokenPipeError</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Happens when client is abruptly terminated, don&#39;t want to pollute the logs.</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">_Handler</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">client_address</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">PingRequest</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">PingResponse</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_service_name</span><span class="p">,</span> <span class="n">client_address</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_get_local_addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">intf</span><span class="p">,</span> <span class="n">intf_addresses</span> <span class="ow">in</span> <span class="n">psutil</span><span class="o">.</span><span class="n">net_if_addrs</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nics</span> <span class="ow">and</span> <span class="n">intf</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nics</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">intf_addresses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">addr</span><span class="o">.</span><span class="n">family</span> <span class="o">==</span> <span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">intf</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">                <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">addr</span><span class="o">.</span><span class="n">address</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_port</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_addresses</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_server</span><span class="o">.</span><span class="n">server_close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_port</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_port</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="33-networkbasicclient">3.3 network.BasicClient</h3>
<p>HorovodRunDriverClient 和 HorovodRunTaskClient 这两个类都继承了network.BasicClient。</p>
<p>network.BasicClient 的作用就是连接 network.BasicService，与其交互。即 network.BasicClient 是一个操作接口。</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             <span class="n">network</span><span class="o">.</span><span class="n">BasicClient</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                                <span class="o">^</span>            <span class="o">^</span>
</span></span><span class="line"><span class="cl">                                <span class="o">|</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">+------------------+</span>            <span class="o">+---------------+</span>
</span></span><span class="line"><span class="cl">             <span class="o">|</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">+</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                             <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverClient</span>               <span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskClient</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">             <span class="o">^</span>                                               <span class="o">^</span>
</span></span><span class="line"><span class="cl">             <span class="o">|</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">|</span>                                               <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="o">+</span>                                               <span class="o">+</span>
</span></span><span class="line"><span class="cl">   <span class="n">HorovodRunDriverClient</span>                           <span class="n">HorovodRunTaskClient</span></span></span></code></pre></td></tr></table>
</div>
</div><p>两个主要 API 如下：</p>
<h4 id="331-_probe">3.3.1 _probe</h4>
<p>_probe 获取 server 的网络接口。</p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_probe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">addresses</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">result_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">intf</span><span class="p">,</span> <span class="n">intf_addresses</span> <span class="ow">in</span> <span class="n">addresses</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">addr</span> <span class="ow">in</span> <span class="n">intf_addresses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_probe_one</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">intf</span><span class="p">,</span> <span class="n">addr</span><span class="p">,</span> <span class="n">result_queue</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">intf</span><span class="p">,</span> <span class="n">addr</span> <span class="o">=</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">intf</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span><span class="p">[</span><span class="n">intf</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">addr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="332-发送消息">3.3.2 发送消息</h4>
<p>_send 的作用是给server发送消息。</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_send</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Sends the request and returns the response object.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Streaming data response is transferred to the optional stream parameter.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Since all the addresses were vetted, use the first one.</span>
</span></span><span class="line"><span class="cl">    <span class="n">addr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_addresses</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_one</span><span class="p">(</span><span class="n">addr</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="34-总结">3.4 总结</h3>
<p>我们可以看到，network.BasicService 会提供了一个server，这个 Service 都是通过 network.BasicClient 来访问。基于此，Horovod 的HorovodRunDriverService 和 HorovodRunTaskService 这两个类就可以互相交互，进行沟通。</p>
<h2 id="4-driver-服务">4 Driver 服务</h2>
<p>Driver 服务由 HorovodRunDriverService 提供，其功能主要是维护维护各种 task 地址以及相应关系。具体各种 task 地址 就是 Task 服务 来注册的。</p>
<p>需要注意的是：HorovodRunDriverService 和 HorovodRunTaskService 都最终继承了 network.BasicService，他们之间可以是异地运行交互。</p>
<h3 id="41-horovodrundriverservice">4.1 HorovodRunDriverService</h3>
<p>HorovodRunDriverService 是对 BasicDriverService 的封装。</p>
<p>HorovodRunDriverClient 是 其 访问接口。</p>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunDriverService</span><span class="p">(</span><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">NAME</span> <span class="o">=</span> <span class="s1">&#39;horovod driver service&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunDriverService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                      <span class="n">HorovodRunDriverService</span><span class="o">.</span><span class="n">NAME</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                      <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunDriverClient</span><span class="p">(</span><span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverClient</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">match_intf</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunDriverClient</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">HorovodRunDriverService</span><span class="o">.</span><span class="n">NAME</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver_addresses</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">verbose</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">match_intf</span><span class="o">=</span><span class="n">match_intf</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42-basicdriverservice">4.2 BasicDriverService</h3>
<p>BasicDriverService基类 主要就是 维护各种 task 地址以及相应关系。</p>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BasicDriverService</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">BasicService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_proc</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">BasicDriverService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span> <span class="o">=</span> <span class="n">num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Condition</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里的各种 task 地址就是 Task 服务 注册到 Driver 的数值。</p>
<p>可以看到里面有各种关于地址的变量，为了让大家理解这些变量的作用，对于每一个变量我们举例如下（这里有些变量是专门为 spark 设计，都放到基类里面有点奇怪）：</p>
<h4 id="421-_all_task_addresses">4.2.1 _all_task_addresses</h4>
<p>本变量是记录了所有 task 的地址，变量举例如下：</p>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;lo&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;1.1.1.1&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.01&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;lo&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;2.2.2.2&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.02&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本变量由 task 调用 RegisterTaskRequest 来注册。</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">RegisterTaskRequest</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">req</span><span class="o">.</span><span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_all_task_addresses</span><span class="p">[</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">task_addresses</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="422-_task_addresses_for_driver">4.2.2 _task_addresses_for_driver</h4>
<p>本变量是记录了所有 task 的地址，但是网卡接口有多种，这里选择与 本 driver 地址匹配的地址。</p>
<p>变量举例如下：</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.01&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.02&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本变量由 task 调用 RegisterTaskRequest 来注册。</p>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Just use source address for service for fast probing.</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span><span class="p">[</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_filter_by_ip</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">task_addresses</span><span class="p">,</span> <span class="n">client_address</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体使用举例如下：</p>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_addresses_for_driver</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_driver</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>driver用这个地址来生成 其内部 task 变量。</p>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_driver</span><span class="p">(</span><span class="n">index</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_hosts</span><span class="p">)]</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-_task_addresses_for_tasks">4.2.3 _task_addresses_for_tasks</h4>
<p>该变量举例如下：</p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.01&#39;</span><span class="p">,</span> <span class="mi">12345</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;eth0&#39;</span> <span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;10.10.10.02&#39;</span><span class="p">,</span> <span class="mi">54321</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>本变量由RegisterTaskToTaskAddressesRequest注册。</p>
<div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">RegisterTaskToTaskAddressesRequest</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">register_task_to_task_addresses</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">task_addresses</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">network</span><span class="o">.</span><span class="n">AckResponse</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">register_task_to_task_addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">task_addresses</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_addresses_for_tasks</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">task_addresses</span> <span class="c1"># 这里赋值</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">notify_all</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>该变量被 task 用来获取 某个 task 的一套网络接口，比如：</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Determine a set of common interfaces for task-to-task communication.</span>
</span></span><span class="line"><span class="cl"><span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">driver</span><span class="o">.</span><span class="n">task_addresses_for_tasks</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="424-_task_index_host_hash">4.2.4 _task_index_host_hash</h4>
<p>每一个 task 有一个对应的 host hash，该数值被 MPI 作为 host name 来操作。</p>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-01-dfdsfdsfdsfdsf2&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-02-treterwrtqwer&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体使用如下。这个函数是 spark 相关会使用，具体是逐一通知 spark task 进入下一阶段。</p>
<div class="highlight" id="id-26"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>或者使用如下，是为了获取某一个 host 对应的 <code>host hash name</code>。</p>
<div class="highlight" id="id-27"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_index_host_hash</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proc</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_index_host_hash</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="425-_task_host_hash_indices">4.2.5 _task_host_hash_indices</h4>
<p>该变量举例如下：</p>
<div class="highlight" id="id-28"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-01-dfdsfdsfdsfdsf2&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="s1">&#39;ip-10-10-10-02-treterwrtqwer&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体是在注册 RegisterTaskRequest 时候生成。</p>
<div class="highlight" id="id-29"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span><span class="p">[</span><span class="n">req</span><span class="o">.</span><span class="n">host_hash</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">index</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>使用具体代码是：</p>
<div class="highlight" id="id-30"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">task_host_hash_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_host_hash_indices</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体是被 rsh 使用。rsh 就是在某一个 host 上，让某一个 horovod rank 启动。具体逻辑是：</p>
<ul>
<li>获取某一个 host 上所有的 task indices ；</li>
<li>利用 task_host_hash_indices 取出本进程 local rank 对应的 task index；</li>
<li>取出在 driver 中 task index 对应保持的 task address；</li>
<li>最后依据这个 task addresses 生成一个 SparkTaskClient，进行后续操作。</li>
</ul>
<div class="highlight" id="id-31"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">driver_client</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">SparkDriverClient</span><span class="p">(</span><span class="n">driver_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_indices</span> <span class="o">=</span> <span class="n">driver_client</span><span class="o">.</span><span class="n">task_host_hash_indices</span><span class="p">(</span><span class="n">host_hash</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_index</span> <span class="o">=</span> <span class="n">task_indices</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">task_addresses</span> <span class="o">=</span> <span class="n">driver_client</span><span class="o">.</span><span class="n">all_task_addresses</span><span class="p">(</span><span class="n">task_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_client</span> <span class="o">=</span> <span class="n">task_service</span><span class="o">.</span><span class="n">SparkTaskClient</span><span class="p">(</span><span class="n">task_index</span><span class="p">,</span> <span class="n">task_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_client</span><span class="o">.</span><span class="n">stream_command_output</span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">task_client</span><span class="o">.</span><span class="n">run_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">capture_stdout</span><span class="o">=</span><span class="n">stdout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">capture_stderr</span><span class="o">=</span><span class="n">stderr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">prefix_output_with_timestamp</span><span class="o">=</span><span class="n">prefix_output_with_timestamp</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-总体逻辑">4.3 总体逻辑</h3>
<p>总体逻辑如下：</p>
<div class="highlight" id="id-32"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">                               <span class="n">network</span><span class="o">.</span><span class="n">BasicService</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                                     <span class="o">^</span>    <span class="o">^</span>
</span></span><span class="line"><span class="cl">                                     <span class="o">|</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+-------------------+</span>    <span class="o">+-------------+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl">   <span class="n">driver_service</span><span class="o">.</span><span class="n">BasicDriverService</span>       <span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskService</span>
</span></span><span class="line"><span class="cl">                 <span class="o">^</span>                                      <span class="o">^</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                      <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="o">+----------------+------------------+</span>         <span class="n">HorovodRunTaskService</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">HorovodRunDriverService</span>           <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>        <span class="n">_all_task_addresses</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>    <span class="n">_task_addresses_for_driver</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_addresses_for_tasks</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_index_host_hash</span>       <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>     <span class="n">_task_host_hash_indices</span>       <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----------------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-task-服务">5 Task 服务</h2>
<p>HorovodRunTaskService 提供了 Task 部分服务功能。整体逻辑是由几个函数共同完成。</p>
<h3 id="51-启动具体服务">5.1 启动具体服务</h3>
<p>_launch_task_servers 用来启动具体服务，其主要作用是：多线程运行，在每一个线程中，远程运行 <code>horovod.runner.task_fn</code>。
其中：</p>
<ul>
<li>传入参数中，all_host_names 就是程序启动时候配置的所有host，比如 [&ldquo;1.1.1.1&rdquo;, &ldquo;2.2.2.2&rdquo;]；</li>
<li>使用了我们之前提到的 safe_shell_exec.execute 完成了安全运行保证；</li>
<li>使用我们前文提到的 get_remote_command 完成了远程命令的获取，即在命令之前加上了 ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no等等配置；</li>
<li>最终每个启动的命令举例如下： ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no 1.1.1.1 python -m horovod.runner.task_fn xxxxxxx；</li>
<li>使用 execute_function_multithreaded 在每一个 host 上运行，启动 task 服务；</li>
</ul>
<p>具体代码如下：</p>
<div class="highlight" id="id-33"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_launch_task_servers</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">,</span> <span class="n">local_host_names</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Executes the task server and service client task for registration on the
</span></span></span><span class="line"><span class="cl"><span class="s2">    hosts.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param all_host_names: list of addresses. for example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;worker-0&#39;,&#39;worker-1&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">        [&#39;10.11.11.11&#39;, &#39;10.11.11.12&#39;]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type all_host_names: list(string)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param local_host_names: names that are resolved to one of the addresses
</span></span></span><span class="line"><span class="cl"><span class="s2">    of local hosts interfaces. For example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        set([&#39;localhost&#39;, &#39;127.0.0.1&#39;])
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type local_host_names: set
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param driver_addresses: map of interfaces and their address and port for
</span></span></span><span class="line"><span class="cl"><span class="s2">    the service. For example:
</span></span></span><span class="line"><span class="cl"><span class="s2">        {
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;lo&#39;: [(&#39;127.0.0.1&#39;, 34588)],
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;docker0&#39;: [(&#39;172.122.10.1&#39;, 34588)],
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;eth0&#39;: [(&#39;11.111.33.73&#39;, 34588)]
</span></span></span><span class="line"><span class="cl"><span class="s2">        }
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type driver_addresses: map
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param settings: the object that contains the setting for running horovod
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type settings: horovod.runner.common.util.settings.Settings
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype:
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_exec_command</span><span class="p">(</span><span class="n">command</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_output</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 完成了安全运行保证</span>
</span></span><span class="line"><span class="cl">            <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stdout</span><span class="o">=</span><span class="n">host_output</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stderr</span><span class="o">=</span><span class="n">host_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">host_output</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">exit_code</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">args_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_hosts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_name</span> <span class="o">=</span> <span class="n">all_host_names</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="c1"># all_host_names 就是程序启动时候配置的所有host，比如 [&#34;1.1.1.1&#34;, &#34;2.2.2.2&#34;]</span>
</span></span><span class="line"><span class="cl">        <span class="n">command</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="s1">&#39;</span><span class="si">{python}</span><span class="s1"> -m horovod.runner.task_fn </span><span class="si">{index}</span><span class="s1"> </span><span class="si">{num_hosts}</span><span class="s1"> &#39;</span> \
</span></span><span class="line"><span class="cl">            <span class="s1">&#39;</span><span class="si">{driver_addresses}</span><span class="s1"> </span><span class="si">{settings}</span><span class="s1">&#39;</span> \
</span></span><span class="line"><span class="cl">            <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">python</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">index</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">index</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_hosts</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">num_hosts</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">driver_addresses</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">driver_addresses</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">settings</span><span class="o">=</span><span class="n">codec</span><span class="o">.</span><span class="n">dumps_base64</span><span class="p">(</span><span class="n">settings</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">host_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">local_host_names</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 完成了远程命令的获取，即在命令之前加上了 `ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no`等等配置</span>
</span></span><span class="line"><span class="cl">            <span class="n">command</span> <span class="o">=</span> <span class="n">get_remote_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">host</span><span class="o">=</span><span class="n">host_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">port</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">identity_file</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args_list</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">command</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Each thread will use ssh command to launch the server on one task. If an</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># error occurs in one thread, entire process will be terminated. Otherwise,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># threads will keep running and ssh session -- and the the task server --</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># will be bound to the thread. In case, the horovod process dies, all</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the ssh sessions and all the task servers will die as well.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 使用 execute_function_multithreaded 在每一个 host 上运行，启动 task 服务</span>
</span></span><span class="line"><span class="cl">    <span class="n">threads</span><span class="o">.</span><span class="n">execute_function_multithreaded</span><span class="p">(</span><span class="n">_exec_command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">args_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">block_until_all_done</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="52-具体服务逻辑">5.2 具体服务逻辑</h3>
<p>上段有：<code>{python} -m horovod.runner.task_fn {index} {num_hosts} {driver_addresses} {settings}</code>执行具体服务逻辑，所以我们介绍下 <code>horovod.runner.task_fn</code>。</p>
<p><code>_task_fn</code> 函数完成了</p>
<ul>
<li>生成了 HorovodRunTaskService 实例，赋值给 task；</li>
<li>使用 HorovodRunDriverClient . register_task 来向 Driver 服务注册task（自己）的地址；</li>
<li>使用 HorovodRunDriverClient . register_task_to_task_addresses 来向 Driver 服务注册自己在Ring上 下一个邻居的地址；</li>
<li>每一个 task 都做这个操作，最后就得到了在这个 ring cluster 之中的一个路由接口；</li>
</ul>
<p>具体代码如下：</p>
<div class="highlight" id="id-34"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_task_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">task</span> <span class="o">=</span> <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskService</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">HorovodRunDriverClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 向 Driver 服务注册task（自己）的地址</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">register_task</span><span class="p">(</span><span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                             <span class="n">task</span><span class="o">.</span><span class="n">addresses</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                             <span class="n">host_hash</span><span class="o">.</span><span class="n">host_hash</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">wait_for_initial_registration</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Tasks ping each other in a circular fashion to determine interfaces</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reachable within the cluster.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_hosts</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task_addresses</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">all_task_addresses</span><span class="p">(</span><span class="n">next_task_index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># We request interface matching to weed out all the NAT&#39;ed interfaces.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task</span> <span class="o">=</span> <span class="n">task_service</span><span class="o">.</span><span class="n">HorovodRunTaskClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_task_index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_task_addresses</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">key</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">match_intf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attempts</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 向 Driver 服务注册自己在Ring上 下一个邻居的地址</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver</span><span class="o">.</span><span class="n">register_task_to_task_addresses</span><span class="p">(</span><span class="n">next_task_index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                               <span class="n">next_task</span><span class="o">.</span><span class="n">addresses</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Notify the next task that the address checks are completed.</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_task</span><span class="o">.</span><span class="n">task_to_task_address_check_completed</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Wait to get a notification from previous task that its address checks</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># are completed as well.</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">wait_for_task_to_task_address_check_finish_signal</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">start_timeout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">task</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">index</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_hosts</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">driver_addresses</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">settings</span> <span class="o">=</span> <span class="n">codec</span><span class="o">.</span><span class="n">loads_base64</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_task_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">driver_addresses</span><span class="p">,</span> <span class="n">settings</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="53-horovodruntaskservice">5.3 HorovodRunTaskService</h3>
<p>HorovodRunTaskService 主要的作用是提供了两个等待函数。因为具体路由操作是需要彼此通知，所以需要互相等待。</p>
<div class="highlight" id="id-35"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunTaskService</span><span class="p">(</span><span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">NAME_FORMAT</span> <span class="o">=</span> <span class="s1">&#39;horovod task service #</span><span class="si">%d</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunTaskService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">HorovodRunTaskService</span><span class="o">.</span><span class="n">NAME_FORMAT</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">index</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_task_to_task_address_check_completed</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">req</span><span class="p">,</span> <span class="n">client_address</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">TaskToTaskAddressCheckFinishedSignal</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_task_to_task_address_check_completed</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">            <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">notify_all</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">TaskToTaskAddressCheckFinishedSignalResponse</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunTaskService</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_handle</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">client_address</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">wait_for_task_to_task_address_check_finish_signal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_task_to_task_address_check_completed</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">timeout</span><span class="o">.</span><span class="n">remaining</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">                <span class="n">timeout</span><span class="o">.</span><span class="n">check_time_out_for</span><span class="p">(</span><span class="s1">&#39;Task to task address check&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_wait_cond</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodRunTaskClient</span><span class="p">(</span><span class="n">task_service</span><span class="o">.</span><span class="n">BasicTaskClient</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">task_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">match_intf</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">attempts</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">HorovodRunTaskClient</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">HorovodRunTaskService</span><span class="o">.</span><span class="n">NAME_FORMAT</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">task_addresses</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">match_intf</span><span class="o">=</span><span class="n">match_intf</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">attempts</span><span class="o">=</span><span class="n">attempts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">task_to_task_address_check_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">resp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send</span><span class="p">(</span><span class="n">TaskToTaskAddressCheckFinishedSignal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">resp</span><span class="o">.</span><span class="n">index</span></span></span></code></pre></td></tr></table>
</div>
</div><p>逻辑如下：</p>
<div class="highlight" id="id-36"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">                                                         <span class="n">_driver_fn</span>
</span></span><span class="line"><span class="cl">                                                            <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                                            <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                            <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+---------------------------------------+-------------------------------------</span><span class="n">v</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                                             <span class="n">v</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                                   <span class="n">_launch_task_servers</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                                             <span class="o">+</span>
</span></span><span class="line"><span class="cl">     <span class="n">driver</span> <span class="o">=</span> <span class="n">HorovodRunDriverService</span>                                                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                                                              <span class="o">+--------------+-------------------+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                              <span class="n">v</span>                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------+---------------+</span>                                    <span class="n">horovod</span><span class="o">.</span><span class="na">runner</span><span class="o">.</span><span class="na">task_fn</span>    <span class="o">......</span>     <span class="n">horovod</span><span class="o">.</span><span class="na">runner</span><span class="o">.</span><span class="na">task_fn</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">HorovodRunDriverService</span>           <span class="o">|</span>                                              <span class="o">+</span>                                  <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>        <span class="n">_all_task_addresses</span>        <span class="o">|</span>                                              <span class="o">|</span>                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                              <span class="n">v</span>                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>    <span class="n">_task_addresses_for_driver</span>     <span class="o">|</span>          <span class="n">register_task</span>           <span class="o">+-----------+---------------+</span>          <span class="o">+-------+--------------------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                  <span class="o">|</span> <span class="n">HorovodRunTaskService</span>     <span class="o">|</span>          <span class="o">|</span>  <span class="n">HorovodRunTaskService</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_addresses_for_tasks</span>   <span class="o">|</span> <span class="o">&lt;--------------------------------+</span>                           <span class="o">|</span>          <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                  <span class="o">|</span>                           <span class="o">|</span>   <span class="n">wait</span>   <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">_task_index_host_hash</span>       <span class="o">|</span>                                  <span class="o">|</span>                           <span class="o">|</span> <span class="o">&lt;------&gt;</span> <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span> <span class="o">&lt;--------------------------------+</span>                           <span class="o">|</span>          <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>     <span class="n">_task_host_hash_indices</span>       <span class="o">|</span>  <span class="n">register_task_to_task_addresses</span> <span class="o">|</span>                           <span class="o">|</span>          <span class="o">|</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                   <span class="o">|</span>                                  <span class="o">+---------------------------+</span>          <span class="o">+----------------------------+</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----------------------------------+</span>                                                  <span class="err">`</span></span></span></code></pre></td></tr></table>
</div>
</div><p>图示:</p>
<p></p>
<h2 id="6-总结">6 总结</h2>
<p>本文总结如下：</p>
<ul>
<li>因为 Horovod 分布式训练 涉及到多个 hosts，所以如果要彼此访问，需要知道路由信息；</li>
<li>当所有 task 都启动，注册，probe 环中下一个worker 邻居完成 之后，DriverService 会得到路由信息（所有host之间的共有路由接口集合），返回给 Horovod 主体部分使用；</li>
<li>network.BasicService 提供了网络服务功能；</li>
<li>XXXService 都是通过 XXXClient作为接口才能访问；</li>
<li>HorovodRunDriverService 和 HorovodRunTaskService 都最终继承了 network.BasicService，他们之间可以是异地运行交互。</li>
<li>HorovodRunTaskService 提供了 Task 部分服务功能，这些 task 需要注册到 Driver 之中（和Spark思路类似）。</li>
<li>HorovodRunDriverService 是对 BasicDriverService 的封装。BasicDriverService 就是 维护各种 task 地址以及相应关系，比如：
<ul>
<li>_all_task_addresses ：记录了所有 task 的地址；</li>
<li>_task_addresses_for_driver ：记录了所有 task 的地址，但是因为网卡接口有多种，这里选择与 本driver 地址匹配的地址；</li>
<li>_task_addresses_for_tasks ：用来给某一个 task 分配一个地址，同时获取本 task 的一套网络接口；</li>
<li>_task_index_host_hash ：每一个 task 有一个对应的 host hash。这个函数是 spark 相关会使用，具体是逐一通知 spark task 进入下一阶段。或者是为了获取某一个 host 对应的 host hash name；</li>
<li>_task_host_hash_indices ：具体是被 rsh 使用，由 rank 得到 在 driver 中 task index 对应保持的 task address；</li>
</ul>
</li>
<li>SparkDriverService，SparkTaskService，ElasticDriver, Worker 都有什么区别和联系？
<ul>
<li>HorovodRunDriverService 这里只是用来得到路由信息，记录各种 Task 地址；</li>
<li>SparkDriverService 除了记录路由和地址之外，还提交执行任务（Command），因为具体在哪一个Spark Executor启动之后，SparkDriverService 就需要知道 对应 SparkTaskService 的地址，这样才能知道提交到哪里；</li>
<li>SparkTaskService 负责执行命令（抛弃了Spark Executor的逻辑，自己搞了一套），就是从 SparkDriverService 那里获得训练函数，然后启动 python 进程来执行；</li>
<li>ElasticDriver 做得更多，因为还有弹性，需要容错；</li>
</ul>
</li>
</ul>
<p>references:
[1]. <a href="https://www.cnblogs.com/rossiXYZ/p/14882053.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14882053.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://www.zhihu.com/column/c_1491039346714746880"target="_blank" rel="external nofollow noopener noreferrer">https://www.zhihu.com/column/c_1491039346714746880<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>深度学习分布式训练框架 horovod[3] -- Horovodrun背后做了什么</title><link>https://jianye0428.github.io/posts/2022-10-08_horovod_3/</link><pubDate>Mon, 10 Jul 2023 07:53:45 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/2022-10-08_horovod_3/</guid><description><![CDATA[<p>references:
[1]. <a href="https://www.cnblogs.com/rossiXYZ/p/14881812.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14881812.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。本文是系列第三篇，从 python 开始进入 Horovod 世界，看看 Horovodrun 做了什么。</p>
<p>前两篇链接如下：</p>
<p><a href="https://www.cnblogs.com/rossiXYZ/p/14856464.html"target="_blank" rel="external nofollow noopener noreferrer">深度学习分布式训练框架 Horovod (1) &mdash; 基础知识<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://www.cnblogs.com/rossiXYZ/p/14856543.html"target="_blank" rel="external nofollow noopener noreferrer">深度学习分布式训练框架 horovod (2) &mdash; 从使用者角度切入<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="1-背景知识">1 背景知识</h2>
<p>首先介绍一些相关背景知识。</p>
<h3 id="11-分布式体系">1.1 分布式体系</h3>
<p>在设计并行计算机时，<u>最直接的方式就是<mark>多个计算单元共享一个内存</mark></u>。共享内存的编程在数据交换和访问上有较大的优势，程序编写起来更加简单。<font color=red>但在扩展性上有较大的瓶颈</font>。</p>
<p>另一种方式为<font color=red><strong>分布式内存</strong></font>。即<u>每个计算单元有单独的内存，计算单元之间的数据访问通过互联网络去传输</u>。这一架构在可移植性和扩展上会强很多，但<u>消息的传递</u>会成为程序设计中的难点。</p>
<p>将这两点结合，即是<u><font color=red><strong>分布式共享内存并行计算机的架构</strong></font></u>，也是当今最常用的体系结构。</p>
<h3 id="12-并行任务通信">1.2 并行任务通信</h3>
<p>并行任务通信一般分为<font color=red><strong>P2P</strong>(Point-to-point communication)</font>和 <font color=red><strong>Collective communication</strong></font>。</p>
<ul>
<li>P2P通信这种模式只有一个sender和一个receiver，即点到点通信.</li>
<li>Collective communication含多个sender多个receive</li>
</ul>
<p>Collective communication包含一些常见的原语</p>
<ul>
<li>broadcast</li>
<li>reduce，allreduce</li>
<li>scatter，scatter reduce</li>
<li>gather，allgather</li>
<li>ring-base collectives</li>
<li>ring-allreduce</li>
</ul>
<p>传统Collective communication假设通信节点组成的topology是一颗fat tree，这样通信效率最高。但实际的通信topology可能比较复杂，并不是一个fat tree。因此一般用<mark><strong>ring-based Collective communication</strong></mark>。</p>
<h3 id="13-mpi">1.3 MPI</h3>
<p><mark>MPI(Message Passing Interface)</mark> 是一种可以支持点对点和广播的通信协议，具体实现的库有很多，使用比较流行的包括 Open Mpi， Intel MPI 等等。</p>
<p>MPI 是一种<u>消息传递编程模型</u>。消息传递指用户必须通过显式地发送和接收消息来实现处理器间的数据交换。在这种并行编程中，<font color=red>每个控制流均有自己独立的地址空间，不同的控制流之间不能直接访问彼此的地址空间，必须通过显式的消息传递来实现</font>。这种编程方式是<mark>大规模并行处理机(MPP)</mark>和<mark>机群(Cluster)</mark>采用的主要编程方式。由于消息传递程序设计要求用户很好地分解问题，组织不同控制流间的数据交换，并行计算粒度大，特别适合于大规模可扩展并行算法。</p>
<p>MPI 是<mark>基于进程的并行环境。进程拥有独立的虚拟地址空间和处理器调度，并且执行相互独立</mark>。MPI 设计为支持通过网络连接的机群系统，且通过消息传递来实现通信，消息传递是 MPI 的最基本特色。</p>
<h3 id="14-open-mpi">1.4 Open-MPI</h3>
<p>OpenMPI 是一种高性能消息传递库，最初是作为融合的技术和资源从其他几个项目（FT-MPI， LA-MPI， LAM/MPI， 以及 PACX-MPI），它是 MPI-2 标准的一个开源实现，由一些科研机构和企业一起开发和维护。因此，OpenMPI 能够从高性能社区中获得专业技术、工业技术和资源支持，来创建最好的 MPI 库。OpenMPI 提供给系统和软件供应商、程序开发者和研究人员很多便利。易于使用，并运行本身在各种各样的操作系统，网络互连，以及一批/调度系统。</p>
<h3 id="15-mpi-使用问题">1.5 MPI 使用问题</h3>
<p>因为MPI是<u>分布式内存编程</u>，在后面的开发中涉及节点间信息的传递。往往数据和程序是在多个节点上，所以需要保证执行命令时各节点之间信息的交换。</p>
<p>具体使用之中，就有两个问题:</p>
<ul>
<li>这个多台机器Open-MPI是如何发现并建立连接的呢？</li>
<li>多机多卡在训练过程中，传输环如何建立，这个也是决定了训练效率，那么Open-MPI如何去做呢？</li>
</ul>
<p>关于第一个问题：</p>
<p>设置<font color=red>SSH免密登录</font>可以免去操作中密码的输入。各节点生成私钥和公钥后需要认证，此时可以保证本机免密登录。将各个子节点的公钥文件发送给主节点，然后分别加入到主节点的认证文件中，此时可以保证主节点对各个子节点的免密登录。最后将认证文件传回到每个子节点，从而保证各个子节点对其他节点之间的免密登录。</p>
<p>在 Open-MPI 启动的时候，可以指定<code>--hostfile</code>或者<code>--host</code>去指定要运行任务的 IP 或 Hostname，这样 Open-MPI 就会试图通过 ssh 免秘钥的方式试图去链接对方机器，并执行一系列命令，主要是为了<strong>同步环境变量、当前路径以及下发启动命令</strong>。</p>
<p>当然用户也可以通过其他方式给远程机器下发命令，这个可以通过环境变量<code>OMPI_MCA_plm_rsh_agent</code>指定。</p>
<p>关于第二个问题：</p>
<p>当所有的机器建立好连接，准备开始计算，为了能够最高效的去通信，Open-MPI中集成了组件——<a href="https://github.com/open-mpi/hwloc"target="_blank" rel="external nofollow noopener noreferrer">hwloc<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。该组件主要是<font color=red><strong>为了单机硬件资源拓扑构建，进而构建最短路径通信</strong></font>。</p>
<h2 id="2-入口点">2 入口点</h2>
<p>很多机器学习框架都会采用如下套路：shell脚本（可选），python端 和 C++端。</p>
<ul>
<li>Shell脚本是启动运行的入口，负责解析参数，确认并且调用训练程序；</li>
<li>Python是用户的接口，引入了C++库，封装了API，负责运行时和底层C++交互；</li>
<li>C++实现底层训练逻辑；</li>
</ul>
<p>以我们先看看 hordovodrun 脚本。</p>
<h3 id="21-如何运行">2.1 如何运行</h3>
<p>官方给出的 Hovorod 运行范例之一如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">horovodrun</span> <span class="o">-</span><span class="n">np</span> <span class="mi">2</span> <span class="o">-</span><span class="n">H</span> <span class="n">localhost</span><span class="p">:</span><span class="mi">4</span> <span class="o">--</span><span class="n">gloo</span> <span class="n">python</span> <span class="o">/</span><span class="n">horovod</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">tensorflow2</span><span class="o">/</span><span class="n">tensorflow2_mnist</span><span class="o">.</span><span class="n">py</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里 -np 指的是<strong>进程的数量</strong>，localhost:4<strong>表示localhost节点上4个GPU</strong>。</p>
<p>注意，如果虚拟机只有一个核。想要强行地达到并行的效果，可以使用 -np参数，它会自动帮你把一个核心切成多份处理器，每一个分布式处理就是一个slot。</p>
<p>因此，我们可以从 horovodrun 这个命令入手看看。</p>
<h3 id="22-horovodrun">2.2 horovodrun</h3>
<p>入口文件可以从 setup.py 看到，其就被映射成 horovod.runner.launch:run_commandline。</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">entry_points</span><span class="o">={</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;console_scripts&#39;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;horovodrun = horovod.runner.launch:run_commandline&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>所以我们看看 run_commandline</p>
<h3 id="23-run_commandline">2.3 run_commandline</h3>
<p>该命令位于：horovod-master/horovod/runner/launch.py，我们摘录重要部分。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_commandline</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">_run</span><span class="p">(</span><span class="n">args</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>于是进入到 _run 函数。可以看到，Horovod 会依据是否是弹性训练来选择不同的路径。我们在此系列中，会首先分析 非弹性训练 _run_static。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># if hosts are not specified, either parse from hostfile, or default as</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># localhost</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">hosts</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">host_discovery_script</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">hostfile</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">args</span><span class="o">.</span><span class="n">hosts</span> <span class="o">=</span> <span class="n">hosts</span><span class="o">.</span><span class="n">parse_host_files</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hostfile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Set hosts to localhost if not specified</span>
</span></span><span class="line"><span class="cl">            <span class="n">args</span><span class="o">.</span><span class="n">hosts</span> <span class="o">=</span> <span class="s1">&#39;localhost:</span><span class="si">{np}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Convert nics into set</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span><span class="o">.</span><span class="n">nics</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">nics</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">))</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">nics</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">_is_elastic</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_run_elastic</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_run_static</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="c1"># 我们先看这里</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="24-非弹性训练-_run_static">2.4 非弹性训练 _run_static()</h3>
<p>在 _run_static 之中做了如下操作：</p>
<ul>
<li>首先解析各种参数，得到 settings；</li>
<li>会调用 <code>driver_service.get_common_interfaces</code> 获取网卡以及其他host的信息，依据这些信息会进行slot分配，这部分很复杂，具体我们会有专文讲解（下一篇）。</li>
<li>这里有一个问题：为什么要得到 host, slot, rank 之间的关系信息？由于工程上的考虑，底层 C++ 世界中对于 rank 的角色做了区分：<code>rank 0</code> 是 master，<code>rank n</code> 是 worker，所以这些信息需要决定并且传递给 C++世界；</li>
<li>会根据是否在参数中传递运行函数来决定采取何种路径，一般默认没有运行参数，所以会执行_launch_job 来启动训练 job；</li>
</ul>
<p>具体代码如下：</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_static</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">settings</span> <span class="o">=</span> <span class="n">hvd_settings</span><span class="o">.</span><span class="n">Settings</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">2</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">verbose</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ssh_port</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ssh_identity_file</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">extra_mpi_args</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">mpi_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">tcp_flag</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">tcp_flag</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">binding_args</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">binding_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">key</span><span class="o">=</span><span class="n">secret</span><span class="o">.</span><span class="n">make_secret_key</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">start_timeout</span><span class="o">=</span><span class="n">tmout</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">num_proc</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">hosts</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">output_filename</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">output_filename</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">run_func_mode</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">run_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">nics</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">nics</span><span class="p">,</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	  <span class="c1"># 首先解析各种参数，得到 settings</span>
</span></span><span class="line"><span class="cl">    <span class="n">fn_cache</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">disable_cache</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">+=</span> <span class="n">args</span><span class="o">.</span><span class="n">ssh_identity_file</span>
</span></span><span class="line"><span class="cl">        <span class="n">parameters_hash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">fn_cache</span> <span class="o">=</span> <span class="n">cache</span><span class="o">.</span><span class="n">Cache</span><span class="p">(</span><span class="n">CACHE_FOLDER</span><span class="p">,</span> <span class="n">CACHE_STALENESS_THRESHOLD_MINUTES</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                               <span class="n">parameters_hash</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取网卡以及其他host的信息，依据这些信息会进行slot分配</span>
</span></span><span class="line"><span class="cl">    <span class="n">all_host_names</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hosts</span><span class="o">.</span><span class="n">parse_hosts_and_slots</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">remote_host_names</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">filter_local_addresses</span><span class="p">(</span><span class="n">all_host_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">nics</span> <span class="o">=</span> <span class="n">driver_service</span><span class="o">.</span><span class="n">get_common_interfaces</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">all_host_names</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">remote_host_names</span><span class="p">,</span> <span class="n">fn_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_func</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get the driver IPv4 address</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver_ip</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_driver_ip</span><span class="p">(</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">run_func_server</span> <span class="o">=</span> <span class="n">KVStoreServer</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span> <span class="c1"># 启动内部KV服务器</span>
</span></span><span class="line"><span class="cl">        <span class="n">run_func_server_port</span> <span class="o">=</span> <span class="n">run_func_server</span><span class="o">.</span><span class="n">start_server</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">put_data_into_kvstore</span><span class="p">(</span><span class="n">driver_ip</span><span class="p">,</span> <span class="n">run_func_server_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="s1">&#39;runfunc&#39;</span><span class="p">,</span> <span class="s1">&#39;func&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">run_func</span><span class="p">)</span> <span class="c1"># 把&#39;func&#39;, args.run_func存储成KV</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">command</span> <span class="o">=</span> <span class="p">[</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span><span class="p">,</span> <span class="s1">&#39;-m&#39;</span><span class="p">,</span> <span class="s1">&#39;horovod.runner.run_task&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">driver_ip</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">run_func_server_port</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">_launch_job</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">np</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">np</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_data_from_kvstore</span><span class="p">(</span><span class="n">driver_ip</span><span class="p">,</span> <span class="n">run_func_server_port</span><span class="p">,</span><span class="s1">&#39;runfunc_result&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">results</span>
</span></span><span class="line"><span class="cl">        <span class="k">finally</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">run_func_server</span><span class="o">.</span><span class="n">shutdown_server</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">command</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">command</span>
</span></span><span class="line"><span class="cl">        <span class="n">_launch_job</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span> <span class="c1"># 我们重点讲解这里</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下：</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">              <span class="o">+-----------+</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span><span class="n">horovodrun</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+-----+-----+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">           <span class="o">+--------+--------+</span>
</span></span><span class="line"><span class="cl">           <span class="o">|</span> <span class="n">run_commandline</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="o">+----+------+-----+</span>
</span></span><span class="line"><span class="cl">                <span class="o">|</span>      <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">+---------+</span>      <span class="o">+--------+</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="n">v</span>                         <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----+--------+</span>           <span class="o">+----+--------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">_run_elastic</span> <span class="o">|</span>           <span class="o">|</span> <span class="n">_run_static</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>              <span class="o">|</span>           <span class="o">|</span>             <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+--------------+</span>           <span class="o">+-------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>至此，我们已经分析完成 horovod 的入口，下面会分析具体如何启动 Job。</p>
<h2 id="3-运行训练-job">3 运行训练 Job</h2>
<h3 id="31-_launch_job">3.1 _launch_job</h3>
<p>_launch_job 会根据配置或者安装情况来进行具体调用。我们看到有三种可能：gloo, mpi, js。</p>
<p>jsrun的资料很难找，所以我们重点看看 gloo, mpi 这两种。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_launch_job</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">command</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">config_parser</span><span class="o">.</span><span class="n">set_env_from_args</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">gloo_run_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">driver_ip</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_driver_ip</span><span class="p">(</span><span class="n">nics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">gloo_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">driver_ip</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">mpi_run_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">js_run_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">js_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">run_controller</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">use_gloo</span><span class="p">,</span> <span class="n">gloo_run_fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">args</span><span class="o">.</span><span class="n">use_mpi</span><span class="p">,</span> <span class="n">mpi_run_fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">args</span><span class="o">.</span><span class="n">use_jsrun</span><span class="p">,</span> <span class="n">js_run_fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">args</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-run_controller">3.2 run_controller</h3>
<p>run_controller 依然是一个中介函数，具体导入 gloo 或者 mpi。</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_controller</span><span class="p">(</span><span class="n">use_gloo</span><span class="p">,</span> <span class="n">gloo_run</span><span class="p">,</span> <span class="n">use_mpi</span><span class="p">,</span> <span class="n">mpi_run</span><span class="p">,</span> <span class="n">use_jsrun</span><span class="p">,</span> <span class="n">js_run</span><span class="p">,</span> <span class="n">verbosity</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">use_gloo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">gloo_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">use_mpi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">use_jsrun</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">js_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mpi_built</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">lsf</span><span class="o">.</span><span class="n">LSFUtils</span><span class="o">.</span><span class="n">using_lsf</span><span class="p">()</span> <span class="ow">and</span> <span class="n">is_jsrun_installed</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="n">js_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_run</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">gloo_built</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">gloo_run</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">              <span class="o">+-----------+</span>
</span></span><span class="line"><span class="cl">              <span class="o">|</span><span class="n">horovodrun</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="o">+-----+-----+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">           <span class="o">+--------+--------+</span>
</span></span><span class="line"><span class="cl">           <span class="o">|</span> <span class="n">run_commandline</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="o">+----+------+-----+</span>
</span></span><span class="line"><span class="cl">                <span class="o">|</span>      <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">+---------+</span>      <span class="o">+--------+</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>                         <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="n">v</span>                         <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-----+--------+</span>           <span class="o">+----+--------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">_run_elastic</span> <span class="o">|</span>           <span class="o">|</span> <span class="n">_run_static</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>              <span class="o">|</span>           <span class="o">|</span>             <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+--------------+</span>           <span class="o">+------+------+</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl">                           <span class="o">+------+------+</span>
</span></span><span class="line"><span class="cl">                           <span class="o">|</span> <span class="n">_launch_job</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">                           <span class="o">|</span>             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                           <span class="o">+------+------+</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">v</span>
</span></span><span class="line"><span class="cl">                        <span class="o">+---------+--------+</span>
</span></span><span class="line"><span class="cl">                        <span class="o">|</span>  <span class="n">run_controller</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                        <span class="o">|</span>                  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                        <span class="o">+----+----+-----+--+</span>
</span></span><span class="line"><span class="cl">                             <span class="o">|</span>    <span class="o">|</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">               <span class="o">+-------------+</span>    <span class="o">|</span>     <span class="o">+--------+</span>
</span></span><span class="line"><span class="cl">               <span class="o">|</span>                  <span class="o">|</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl">               <span class="o">|</span>                  <span class="o">|</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl">               <span class="n">v</span>                  <span class="n">v</span>              <span class="n">v</span>
</span></span><span class="line"><span class="cl">        <span class="o">+------+---+</span>       <span class="o">+------+----+</span>     <span class="o">+---+-----+</span>
</span></span><span class="line"><span class="cl">        <span class="o">|</span> <span class="n">gloo_run</span> <span class="o">|</span>       <span class="o">|</span>   <span class="n">mpi_run</span> <span class="o">|</span>     <span class="o">|</span> <span class="n">js_run</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">        <span class="o">|</span>          <span class="o">|</span>       <span class="o">|</span>           <span class="o">|</span>     <span class="o">|</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">        <span class="o">+----------+</span>       <span class="o">+-----------+</span>     <span class="o">+---------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>于是我们下面就分为两个分支介绍：gloo &amp; mpi。</p>
<h2 id="4-gloo-实现">4 Gloo 实现</h2>
<h3 id="41-gloo-简介">4.1 Gloo 简介</h3>
<p>Gloo 是 facebook出品的一个类似MPI的集合通信库（https://github.com/facebookincubator/gloo）。</p>
<p>集合通信库的主要特征是：大体上会遵照 MPI 提供的接口规定，实现了包括<strong>点对点通信</strong>（SEND,RECV等），<strong>集合通信</strong>（ REDUCE，BROADCAST，ALLREDUCE等）等相关接口，然后根据自己硬件或者是系统的需要，在底层实现上进行相应改动，保证接口的稳定和性能。</p>
<p>Gloo 为CPU和GPU提供了集合通信程序的优化实现。 它特别适用于GPU，因为它可以执行通信而无需使用GPUDirect 将数据传输到CPU的内存。 它还能够使用 NCCL 执行快速的节点内通信，并实现其自己的节点间例程计算。你不需要考虑内存数据的拷贝，只需要实现逻辑就可以。</p>
<p>Gloo 支持集合通信（collective Communication），并对其进行了优化。由于 GPU 之间可以直接进行数据交换，而无需经过 CPU 和内存，因此，在 GPU 上使用 <strong>gloo后端</strong>速度更快。</p>
<p>Horovod 为什么会选择 Gloo？个人认为除了其功能的全面性和性能之外，基于它可以二次开发是一个亮点，比如下面我们所说的 Rendezvous 功能就被 Horovod 用来实现弹性训练（我们后文有专门讲解）。</p>
<p>Gloo 和 MPI 都起到了同样类似作用：</p>
<ul>
<li>一方面Horovod内集成了基于 Gloo 的AllReduce，类似于NCCL，都是用作梯度规约；</li>
<li>另一方面，Gloo 可以用来启动多个进程（Hovorod里用Rank表示），实现并行计算；</li>
</ul>
<p>具体如下：</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">   <span class="o">+-----------------------+</span>   <span class="o">+-----------------------+</span>  <span class="o">+------------------------+</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span>  <span class="n">gloo_run</span>      <span class="n">slot</span> <span class="mi">1</span> <span class="o">|</span>   <span class="o">|</span> <span class="n">gloo_run</span>     <span class="n">slot</span> <span class="mi">2</span>   <span class="o">|</span>  <span class="o">|</span>  <span class="n">gloo_run</span>  <span class="n">slot</span> <span class="mi">3</span>      <span class="o">|</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span>                       <span class="o">|</span>   <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span> <span class="o">+-------------------+</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">+------------------+</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">+------------------+</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl">   <span class="o">|</span> <span class="o">|</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>   <span class="o">|</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">|</span>  <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span> <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">|</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>  <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+----+</span>                   <span class="o">+&lt;------+</span>                  <span class="o">+&lt;------+</span>                  <span class="o">+&lt;------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">|</span> <span class="o">|</span>                   <span class="o">|</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">|</span>                  <span class="o">|</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">|</span>                  <span class="o">|</span>   <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">|</span> <span class="o">+-------------------+</span> <span class="o">|</span>   <span class="o">|</span> <span class="o">+------------------+</span>  <span class="o">|</span>  <span class="o">|</span> <span class="o">+------------------+</span>   <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">|</span>                       <span class="o">|</span>   <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>                        <span class="o">|</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>  <span class="o">+-----------------------+</span>   <span class="o">+-----------------------+</span>  <span class="o">+------------------------+</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                                                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                                                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                                                                                      <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">v</span><span class="o">--------------------------------------------------------------------------------------&gt;</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">Ring</span> <span class="n">Allreduce</span> <span class="n">on</span> <span class="n">Gloo</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42-rendezvous-功能">4.2 Rendezvous 功能</h3>
<h4 id="421-rendezvous-概念">4.2.1 Rendezvous 概念</h4>
<p>在 Gloo 的文档中，如此说:</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">The rendezvous process needs to happen exactly once per Gloo context.
</span></span><span class="line"><span class="cl">It makes participating Gloo processes exchange details for setting up their communication channels. For example, when the TCP transport is used, processes exchange IP address and port number details of listening sockets.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Rendezvous can be executed by accessing a key/value store that is accessible by all participating processes. Every process is responsible for setting a number of keys and will wait until their peers have set their keys. The values stored against these keys hold
</span></span><span class="line"><span class="cl">the information that is passed to the transport layer.</span></span></code></pre></td></tr></table>
</div>
</div><p>大致意思是：</p>
<p>Gloo 在每一个 Gloo context 之中有一个 rendezvous process，Gloo 利用它来<strong>交换通讯需要的细节</strong>。</p>
<p>Rendezvous 具体实现是可以依靠访问一个 KVstore 来完成。具体细节就是通过 KVstore 来进行交互。</p>
<p>以 Horovod 为例：</p>
<ul>
<li>Horovod 在进行容错 AllReduce 训练时，除了启动 <strong>worker 进程</strong>外，还会启动一个<strong>driver 进程</strong>。这个 driver 进程用于帮助 worker 调用 gloo 构造 AllReduce 通信环。</li>
<li>driver 进程中会创建一个带有 KVStore 的 RendezvousServer，driver 会将参与通信的 worker 的 ip 等信息存入 KVstore 中。</li>
<li>然后 worker 就可以调用 gloo 来访问 RendezvousServer 构造通信环了。</li>
</ul>
<h4 id="422-rendezvousserver">4.2.2 RendezvousServer</h4>
<p>具体代码如下，可以看到是启动了RendezvousHTTPServer(就是继承拓展了 HTTPServer):</p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RendezvousServer</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_listen_thread</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span> <span class="o">=</span> <span class="n">verbose</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Rendezvous function finds a available port, create http socket,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># and start listening loop to handle request</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># self.httpd.init needs to be called after server start</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handler_cls</span><span class="o">=</span><span class="n">RendezvousHandler</span><span class="p">):</span> <span class="c1"># 下面马上介绍</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="n">find_port</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="k">lambda</span> <span class="n">addr</span><span class="p">:</span> <span class="n">RendezvousHTTPServer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">addr</span><span class="p">,</span> <span class="n">handler_cls</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># start the listening loop</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_listen_thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="o">.</span><span class="n">serve_forever</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">port</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_alloc_plan</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">host_alloc_plan</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_httpd</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_listen_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-kvstore">4.2.3 KVStore</h4>
<p>KVStore 是由 KVStoreHandler 来体现，RendezvousHandler 继承了 KVStoreHandler，进而被 RendezvousServer 作为 handler 使用。</p>
<p>KVStoreHandler 精简版代码如下：</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KVStoreHandler</span><span class="p">(</span><span class="n">SimpleHTTPRequestHandler</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Override PUT handler</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">do_PUT</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">paths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">paths</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Get body length</span>
</span></span><span class="line"><span class="cl">        <span class="n">content_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="p">[</span><span class="s1">&#39;Content-Length&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">content_length</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_put_value</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">send_status_code</span><span class="p">(</span><span class="n">OK</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_put_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">server</span><span class="o">.</span><span class="n">cache_lock</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scope_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">server</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="p">{})</span>
</span></span><span class="line"><span class="cl">            <span class="n">scope_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="424-底层使用">4.2.4 底层使用</h4>
<p>Rendezvous 具体如何使用？简要的说：</p>
<ul>
<li>Python世界构建了一个 RendezvousServer，其地址配置在环境变量（或者其他方式）中。</li>
<li>在 C++ 世界中，比如 horovod/common/gloo/gloo_context.h，horovod/common/gloo/gloo_context.cc 之中有使用。即得到 Python 配置的 RendezvousServer 的地址端口等，然后构建 gloo 所需的 context。</li>
</ul>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#define HOROVOD_HOSTNAME &#34;HOROVOD_HOSTNAME&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_RANK &#34;HOROVOD_RANK&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_SIZE &#34;HOROVOD_SIZE&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_LOCAL_RANK &#34;HOROVOD_LOCAL_RANK&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_LOCAL_SIZE &#34;HOROVOD_LOCAL_SIZE&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_CROSS_RANK &#34;HOROVOD_CROSS_RANK&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_CROSS_SIZE &#34;HOROVOD_CROSS_SIZE&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp">#define HOROVOD_ELASTIC &#34;HOROVOD_ELASTIC&#34;
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">  <span class="n">ctx</span> <span class="o">=</span> <span class="n">Rendezvous</span><span class="p">(</span><span class="n">HOROVOD_GLOO_GLOBAL_PREFIX</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">rendezvous_addr_env</span><span class="p">,</span> <span class="n">rendezvous_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">timeout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">local_ctx</span> <span class="o">=</span> <span class="n">Rendezvous</span><span class="p">(</span><span class="n">HOROVOD_GLOO_LOCAL_PREFIX</span> <span class="o">+</span> <span class="n">hostname</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">rendezvous_addr_env</span><span class="p">,</span> <span class="n">rendezvous_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">local_rank</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">timeout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">cross_ctx</span> <span class="o">=</span> <span class="n">Rendezvous</span><span class="p">(</span><span class="n">HOROVOD_GLOO_CROSS_PREFIX</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">local_rank</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">rendezvous_addr_env</span><span class="p">,</span> <span class="n">rendezvous_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">cross_rank</span><span class="p">,</span> <span class="n">cross_size</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">timeout</span><span class="p">);</span></span></span></code></pre></td></tr></table>
</div>
</div><p>逻辑如下，C++世界会从python世界的获取到RendezvousServer的 IP，port：</p>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">          <span class="o">+---------------------&gt;</span>  <span class="n">System</span> <span class="n">Env</span>  <span class="o">+------------------+</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>  <span class="n">addr</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="o">...</span>                     <span class="n">addr</span><span class="o">,</span> <span class="n">port</span><span class="o">,</span> <span class="o">...</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">+</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>    <span class="n">Python</span>                  <span class="o">|</span>              <span class="n">C</span><span class="o">++</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="o">|</span>
</span></span><span class="line"><span class="cl">          <span class="o">|</span>                            <span class="o">|</span>                          <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+---------+---------------+</span>            <span class="o">|</span>             <span class="o">+------------+--------+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">RendezvousServer</span>        <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span><span class="n">GlooContext</span>          <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>    <span class="n">RendezvousHandler</span>    <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>      <span class="n">Rendezvous</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>                         <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>                     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------------+</span>            <span class="o">|</span>             <span class="o">+---------------------+</span>
</span></span><span class="line"><span class="cl">                                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                       <span class="o">+</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-horovd-的-gloo-入口">4.3 Horovd 的 gloo 入口</h3>
<p>gloo_run 是 horovod 之中，gloo 模块的 相关入口。</p>
<p>注释说的很清楚：每一个 thread 将使用 ssh 命令在远程host之上启动训练job。</p>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gloo_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">,</span> <span class="n">command</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Each thread will use ssh command to launch the job on each remote host. If an</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># error occurs in one thread, entire process will be terminated. Otherwise,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># threads will keep running and ssh session.</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec_command</span> <span class="o">=</span> <span class="n">_exec_command_fn</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">launch_gloo</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">exec_command</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>就是用 launch_gloo 来运行 exec_command。</p>
<p>此时 command 参数类似 <code>&quot;['python', 'train.py']&quot;</code>。</p>
<h3 id="44-构建可执行环境">4.4 构建可执行环境</h3>
<p>gloo_run 的第一部分是 <code>exec_command = _exec_command_fn(settings)</code>，就是基于各种配置来生成可以执行命令环境。如果是远程，就得生成相关远程可运行命令环境（包括切换目录，远程执行等等）。</p>
<h4 id="441-_exec_command_fn">4.4.1 _exec_command_fn</h4>
<p>具体又可以分为两部分：</p>
<ul>
<li>利用 get_remote_command 来生成相关远程可运行环境，比如在训练脚本前面加上 <code>'ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no'</code>；</li>
<li>调整输入输出，利用 safe_shell_exec.execute 来实现安全执行能力；</li>
</ul>
<p>具体如下：</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_exec_command_fn</span><span class="p">(</span><span class="n">settings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    executes the jobs defined by run command on hosts.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param hosts_alloc: list of dict indicating the allocating info.
</span></span></span><span class="line"><span class="cl"><span class="s2">    For example,
</span></span></span><span class="line"><span class="cl"><span class="s2">        [{&#39;Hostname&#39;:&#39;worker-0&#39;, &#39;Rank&#39;: 0, &#39;Local_rank&#39;: 0, &#39;Cross_rank&#39;:0,
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;Size&#39;:2, &#39;Local_size&#39;:1, &#39;Cross_size&#39;:2},
</span></span></span><span class="line"><span class="cl"><span class="s2">        {&#39;Hostname&#39;:&#39;worker-1&#39;, &#39;Rank&#39;: 1, &#39;Local_rank&#39;: 0, &#39;Cross_rank&#39;:1,
</span></span></span><span class="line"><span class="cl"><span class="s2">            &#39;Size&#39;:2, &#39;Local_size&#39;:1, &#39;Cross_size&#39;:2}
</span></span></span><span class="line"><span class="cl"><span class="s2">        ]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type hosts_alloc: list(dict)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param remote_host_names: names that are resolved to one of the addresses
</span></span></span><span class="line"><span class="cl"><span class="s2">    of remote hosts interfaces.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param _run_command: command to execute
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_exec_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">slot_info</span><span class="p">,</span> <span class="n">events</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span> <span class="o">=</span> <span class="n">slot_info</span><span class="o">.</span><span class="n">rank</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_name</span> <span class="o">=</span> <span class="n">slot_info</span><span class="o">.</span><span class="n">hostname</span>
</span></span><span class="line"><span class="cl">        <span class="n">host_address</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">resolve_host_address</span><span class="p">(</span><span class="n">host_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_addresses</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_local_host_addresses</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 需要构建远程命令</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">host_address</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">local_addresses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_command</span> <span class="o">=</span> <span class="n">quote</span><span class="p">(</span><span class="s1">&#39;cd </span><span class="si">{pwd}</span><span class="s1"> &gt; /dev/null 2&gt;&amp;1 ; </span><span class="si">{command}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">                                  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">command</span><span class="o">=</span><span class="n">command</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">command</span> <span class="o">=</span> <span class="n">get_remote_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">host</span><span class="o">=</span><span class="n">host_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">port</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">identity_file</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Redirect output if requested</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 调整输入输出，利用 safe_shell_exec.execute 来实现安全执行能力</span>
</span></span><span class="line"><span class="cl">        <span class="n">stdout</span> <span class="o">=</span> <span class="n">stderr</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">stdout_file</span> <span class="o">=</span> <span class="n">stderr_file</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_rank</span> <span class="o">=</span> <span class="n">_pad_rank</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_dir_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">,</span> <span class="s1">&#39;rank.</span><span class="si">{rank}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">padded_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">stdout_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">,</span> <span class="s1">&#39;stdout&#39;</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">stderr_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir_rank</span><span class="p">,</span> <span class="s1">&#39;stderr&#39;</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">stdout</span> <span class="o">=</span> <span class="n">MultiFile</span><span class="p">([</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stdout_file</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">stderr</span> <span class="o">=</span> <span class="n">MultiFile</span><span class="p">([</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">,</span> <span class="n">stderr_file</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 实现安全执行能力</span>
</span></span><span class="line"><span class="cl">            <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stdout</span><span class="o">=</span><span class="n">stdout</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">stderr</span><span class="o">=</span><span class="n">stderr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">events</span><span class="o">=</span><span class="n">events</span><span class="p">,</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">exit_code</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">_exec_command</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="442-get_remote_command">4.4.2 get_remote_command</h4>
<p>本函数是针对远程 host，获取如何在其上运行的方式。这个函数是比较新加入的，具体和 kubeflow mpi operator 也相关，以后有机会再分析。</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">SSH_COMMAND_PREFIX</span> <span class="o">=</span> <span class="s1">&#39;ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_ssh_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">identity_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timeout_s</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">port_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-p </span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">port</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">identity_file_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-i </span><span class="si">{</span><span class="n">identity_file</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">identity_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeout_arg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-o ConnectTimeout=</span><span class="si">{</span><span class="n">timeout_s</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">timeout_s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">SSH_COMMAND_PREFIX</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">host</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">port_arg</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">identity_file_arg</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">timeout_arg</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">local_command</span><span class="si">}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_remote_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">identity_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timeout_s</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">env_util</span><span class="o">.</span><span class="n">KUBEFLOW_MPI_EXEC</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">host</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">local_command</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">env_util</span><span class="o">.</span><span class="n">is_kubeflow_mpi</span><span class="p">()</span> \
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="n">get_ssh_command</span><span class="p">(</span><span class="n">local_command</span><span class="p">,</span> <span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">identity_file</span><span class="p">,</span> <span class="n">timeout_s</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>大致逻辑如下：</p>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">command</span>  <span class="o">:</span>  <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>  <span class="n">get_remote_command</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">ssh</span> <span class="o">-</span><span class="n">o</span> <span class="o">...</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">            <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="o">|</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+--------------+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span><span class="n">safe_shell_exec</span><span class="o">.</span><span class="na">execute</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="45-使用-gloo-执行命令">4.5 使用 gloo 执行命令</h3>
<p>获取到了可执行环境 exec_command 与 执行命令 command 之后，就可以使用 gloo 来执行命令了。</p>
<p>每个 command 都是被 exec_command 来执行。</p>
<p>launch_gloo 来获取命令，各种配置信息，网卡信息（nics，比如 {&rsquo;lo&rsquo;}），host信息等，然后开始运行，就是开始运行我们的训练代码了，具体是：</p>
<ul>
<li>建立 RendezvousServer，这个会被底层 Gloo C++ 环境使用到;</li>
<li>host_alloc_plan = get_host_assignments 来根据host进行分配slot，就是horovod的哪个rank应该在哪个host上的哪个slot之上运行；</li>
<li>get_run_command 获取到可执行命令；</li>
<li>slot_info_to_command_fn 来得到在slot之上可执行的 slot command；</li>
<li>依据 slot_info_to_command_fn 构建 args_list，这个 list 之中，每一个arg就是一个 slot command；</li>
<li>多线程执行，在每一个 exec_command 之上执行每一个 arg（slot command）；</li>
</ul>
<p>代码如下：</p>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">launch_gloo</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">exec_command</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Launches the given command multiple times using gloo.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Each command is launched via exec_command.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param command: command to launch
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param exec_command: means to execute a single command
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param settings: settings for the distribution
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param nics: common interfaces
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param env: environment to use
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param server_ip: ip to use for rendezvous server
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Make the output directory if it does not exist</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">_mkdir_p</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># start global rendezvous server and get port that it is listening on</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 建立 RendezvousServer，这个会被底层 Gloo C++ 环境使用到</span>
</span></span><span class="line"><span class="cl">    <span class="n">rendezvous</span> <span class="o">=</span> <span class="n">RendezvousServer</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># allocate processes into slots</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 来根据host进行分配slot，就是horovod的哪个rank应该在哪个host上的哪个slot之上运行</span>
</span></span><span class="line"><span class="cl">    <span class="n">hosts</span> <span class="o">=</span> <span class="n">parse_hosts</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">host_alloc_plan</span> <span class="o">=</span> <span class="n">get_host_assignments</span><span class="p">(</span><span class="n">hosts</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># start global rendezvous server and get port that it is listening on</span>
</span></span><span class="line"><span class="cl">    <span class="n">global_rendezv_port</span> <span class="o">=</span> <span class="n">rendezvous</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">rendezvous</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">host_alloc_plan</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取到可执行命令</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_command</span> <span class="o">=</span> <span class="n">get_run_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">global_rendezv_port</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 得到在slot之上可执行的 slot command</span>
</span></span><span class="line"><span class="cl">    <span class="n">slot_info_to_command</span> <span class="o">=</span> <span class="n">_slot_info_to_command_fn</span><span class="p">(</span><span class="n">run_command</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">event</span> <span class="o">=</span> <span class="n">register_shutdown_event</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 依据 slot_info_to_command_fn 构建 args_list，这个 list 之中，每一个arg就是一个 slot command</span>
</span></span><span class="line"><span class="cl">    <span class="n">args_list</span> <span class="o">=</span> <span class="p">[[</span><span class="n">slot_info_to_command</span><span class="p">(</span><span class="n">slot_info</span><span class="p">),</span> <span class="n">slot_info</span><span class="p">,</span> <span class="p">[</span><span class="n">event</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">                 <span class="k">for</span> <span class="n">slot_info</span> <span class="ow">in</span> <span class="n">host_alloc_plan</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># If an error occurs in one thread, entire process will be terminated.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Otherwise, threads will keep running.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 多线程执行，在每一个 exec_command 之上执行每一个 arg（slot command）</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">=</span> <span class="n">threads</span><span class="o">.</span><span class="n">execute_function_multithreaded</span><span class="p">(</span><span class="n">exec_command</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                 <span class="n">args_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                 <span class="n">block_until_all_done</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">exit_code</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">=</span> <span class="n">value</span></span></span></code></pre></td></tr></table>
</div>
</div><p>具体 HostInfo.from_string 信息如下：</p>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HostInfo</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hostname</span><span class="p">,</span> <span class="n">slots</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hostname</span> <span class="o">=</span> <span class="n">hostname</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">slots</span> <span class="o">=</span> <span class="n">slots</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">from_string</span><span class="p">(</span><span class="n">host_string</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">hostname</span><span class="p">,</span> <span class="n">slots</span> <span class="o">=</span> <span class="n">host_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">HostInfo</span><span class="p">(</span><span class="n">hostname</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">slots</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><h5 id="4512-分配方案">4.5.1.2 分配方案</h5>
<p>get_host_assignments 会依据 host 和 process capacities (slots) 来给 Horovod 之中的进程分配，即给出一个 horovod rank 和 slot 的对应关系。设置了几个 np，就有几个 slot。</p>
<p>给出的分配方案类似如下，这样就知道了哪个rank对应于哪个host上的哪个slot：</p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">[
</span></span><span class="line"><span class="cl">  SlotInfo(hostname=&#39;h1&#39;, rank=0, local_rank=0, cross_rank=0, size=2, local_size=2, coress_size=1),
</span></span><span class="line"><span class="cl">	SlotInfo(hostname=&#39;h2&#39;, rank=1, local_rank=0, cross_rank=0, size=2, local_size=2, coress_size=1),
</span></span><span class="line"><span class="cl">]</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_host_assignments</span><span class="p">(</span><span class="n">hosts</span><span class="p">,</span> <span class="n">min_np</span><span class="p">,</span> <span class="n">max_np</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Assign hosts with process capacities (slots) to ranks in the Horovod process.
</span></span></span><span class="line"><span class="cl"><span class="s2">    This function will try to allocate as many as possible processes on the same host to leverage local network.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param hosts: list of HostInfo objects describing host and slot capacity
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type hosts: list[HostInfo]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param min_np: minimum number of processes to be allocated
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param max_np: (optional) maximum number of processes to be allocated
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return: a list of the allocation of process on hosts in a `SlotInfo` object.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype: list[SlotInfo]
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">host_ranks</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">cross_ranks</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 依据 hosts 信息构建 rank, local rank, cross rank(hierarchical allreduce所需要)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">host_info</span> <span class="ow">in</span> <span class="n">hosts</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">local_rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">host_info</span><span class="o">.</span><span class="n">slots</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">max_np</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">cross_ranks_at_local</span> <span class="o">=</span> <span class="n">cross_ranks</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_ranks_at_local</span><span class="p">[</span><span class="n">host_info</span><span class="o">.</span><span class="n">hostname</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cross_ranks_at_local</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">host_ranks</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">host_info</span><span class="p">,</span> <span class="n">ranks</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span> <span class="o">=</span> <span class="n">rank</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 给出一个 horovod rank 和 slot 的对应关系。返回一个alloc_list，每个SlotInfo包括各种rank信息</span>
</span></span><span class="line"><span class="cl">    <span class="n">alloc_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">host_info</span><span class="p">,</span> <span class="n">ranks</span> <span class="ow">in</span> <span class="n">host_ranks</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">local_rank</span><span class="p">,</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ranks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_ranks_at_local</span> <span class="o">=</span> <span class="n">cross_ranks</span><span class="p">[</span><span class="n">local_rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_rank</span> <span class="o">=</span> <span class="n">cross_ranks_at_local</span><span class="p">[</span><span class="n">host_info</span><span class="o">.</span><span class="n">hostname</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">cross_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cross_ranks_at_local</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">alloc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">SlotInfo</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">hostname</span><span class="o">=</span><span class="n">host_info</span><span class="o">.</span><span class="n">hostname</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">local_rank</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">cross_rank</span><span class="o">=</span><span class="n">cross_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">local_size</span><span class="o">=</span><span class="n">local_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">cross_size</span><span class="o">=</span><span class="n">cross_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">alloc_list</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="452-得到运行命令">4.5.2 得到运行命令</h4>
<p>get_run_command 是从环境变量中得到 Gloo 的变量，然后加到 command 之上。此步完成之后，得到类似如下命令：</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>可以把这个格式缩写为：{horovod_gloo_env} command。</p>
<p>代码为：</p>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_run_env_vars</span><span class="p">(</span><span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">elastic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 从环境变量中得到 Gloo 的变量</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_envs</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_GLOO_RENDEZVOUS_ADDR&#39;</span><span class="p">:</span> <span class="n">server_ip</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_GLOO_RENDEZVOUS_PORT&#39;</span><span class="p">:</span> <span class="n">port</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_CONTROLLER&#39;</span><span class="p">:</span> <span class="s2">&#34;gloo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_CPU_OPERATIONS&#39;</span><span class="p">:</span> <span class="s2">&#34;gloo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;HOROVOD_GLOO_IFACE&#39;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">nics</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>   <span class="c1"># TODO: add multiple ifaces in future</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;NCCL_SOCKET_IFNAME&#39;</span><span class="p">:</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nics</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">elastic</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">run_envs</span><span class="p">[</span><span class="s2">&#34;HOROVOD_ELASTIC&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;1&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">run_envs</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_run_command</span><span class="p">(</span><span class="n">command</span><span class="p">,</span> <span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">elastic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">env_vars</span> <span class="o">=</span> <span class="n">create_run_env_vars</span><span class="p">(</span><span class="n">server_ip</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">elastic</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">env_string</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">env_vars</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_command</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{env_string}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{command}</span><span class="s1">&#39;</span>  <span class="c1"># expect a lot of environment variables</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_string</span><span class="o">=</span><span class="n">env_string</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">command</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">quote</span><span class="p">(</span><span class="n">par</span><span class="p">)</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">command</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">run_command</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="453-得到slot运行命令">4.5.3 得到slot运行命令</h4>
<p>得到运行命令之后，这里会结合 horovod env 和 env，以及slot 分配情况 进一步修改为适合 gloo 运行的方式。就是可以在具体每一个slot上运行的命令。</p>
<p>可以把这个格式缩写为：{horovod_gloo_env} {horovod_rendez_env} {env} run_command。</p>
<p>此步完成之后，得到类似如下：</p>
<div class="highlight" id="id-26"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">HOROVOD_HOSTNAME</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_RANK</span><span class="o">=</span><span class="m">1</span> <span class="nv">HOROVOD_SIZE</span><span class="o">=</span><span class="m">2</span> <span class="nv">HOROVOD_LOCAL_RANK</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SHELL</span><span class="o">=</span>/bin/bash <span class="nv">PATH</span><span class="o">=</span>XXXX <span class="nv">USER</span><span class="o">=</span>xxx <span class="nv">PWD</span><span class="o">=</span>xxx <span class="nv">SSH_CONNECTION</span><span class="o">=</span><span class="s2">&#34;1.1.1.1 11 2.2.2.2 22&#34;</span> <span class="nv">HOME</span><span class="o">=</span>xxx <span class="nv">SSH_CLIENZT</span><span class="o">=</span>xxxx
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>lo
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>具体代码如下：</p>
<div class="highlight" id="id-27"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_slot_info_to_command_fn</span><span class="p">(</span><span class="n">run_command</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Workaround for over-buffered outputs. Investigate how mpirun avoids this problem.</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>  <span class="c1"># copy env so we do not leak env modifications</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span><span class="p">[</span><span class="s1">&#39;PYTHONUNBUFFERED&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">slot_info_to_command</span><span class="p">(</span><span class="n">slot_info</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Given a slot_info, creates a command used by gloo to launch a single job.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param slot_info: host and slot to execute the run command on
</span></span></span><span class="line"><span class="cl"><span class="s2">        :return:
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">env_vars</span> <span class="o">=</span> <span class="n">create_slot_env_vars</span><span class="p">(</span><span class="n">slot_info</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_rendez_env</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">env_vars</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{horovod_env}</span><span class="s1"> </span><span class="si">{env}</span><span class="s1"> </span><span class="si">{run_command}</span><span class="s1">&#39;</span> <span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">horovod_env</span><span class="o">=</span><span class="n">horovod_rendez_env</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">quote</span><span class="p">(</span><span class="n">value</span><span class="p">))</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                          <span class="k">if</span> <span class="n">env_util</span><span class="o">.</span><span class="n">is_exportable</span><span class="p">(</span><span class="n">key</span><span class="p">)]),</span>
</span></span><span class="line"><span class="cl">            <span class="n">run_command</span><span class="o">=</span><span class="n">run_command</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">slot_info_to_command</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="454-多线程调用命令">4.5.4 多线程调用命令</h4>
<p>这就是启动了多线程进行调用。gloo_run 的注释说的很清楚：在调用 execute_function_multithreaded 时，每一个thread将使用 ssh 命令在远程host之上启动训练job。</p>
<p>回忆下之前我们在“构建可执行环境” 中提到：利用 get_remote_command 来生成相关远程可运行环境，比如在训练脚本前面加上 &lsquo;ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no&rsquo;。大家就理解了如何在远端执行。</p>
<p>在本地运行，则命令大致为：</p>
<div class="highlight" id="id-28"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> /code directory &gt; /dev/null <span class="m">2</span> &gt;<span class="p">&amp;</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_HOSTNAME</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_RANK</span><span class="o">=</span><span class="m">1</span> <span class="nv">HOROVOD_SIZE</span><span class="o">=</span><span class="m">2</span> <span class="nv">HOROVOD_LOCAL_RANK</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SHELL</span><span class="o">=</span>/bin/bash <span class="nv">PATH</span><span class="o">=</span>XXXX <span class="nv">USER</span><span class="o">=</span>xxx <span class="nv">PWD</span><span class="o">=</span>xxx <span class="nv">SSH_CONNECTION</span><span class="o">=</span><span class="s2">&#34;1.1.1.1 11 2.2.2.2 22&#34;</span> <span class="nv">HOME</span><span class="o">=</span>xxx <span class="nv">SSH_CLIENZT</span><span class="o">=</span>xxxx
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>lo
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>在远端运行，命令就需要加上 ssh 信息，大致为：</p>
<div class="highlight" id="id-29"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ssh -o <span class="nv">PasswordAuthentication</span><span class="o">=</span>no -o <span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no 1.1.1.1
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /code directory &gt; /dev/null <span class="m">2</span> &gt;<span class="p">&amp;</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_HOSTNAME</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_RANK</span><span class="o">=</span><span class="m">1</span> <span class="nv">HOROVOD_SIZE</span><span class="o">=</span><span class="m">2</span> <span class="nv">HOROVOD_LOCAL_RANK</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">SHELL</span><span class="o">=</span>/bin/bash <span class="nv">PATH</span><span class="o">=</span>XXXX <span class="nv">USER</span><span class="o">=</span>xxx <span class="nv">PWD</span><span class="o">=</span>xxx <span class="nv">SSH_CONNECTION</span><span class="o">=</span><span class="s2">&#34;1.1.1.1 11 2.2.2.2 22&#34;</span> <span class="nv">HOME</span><span class="o">=</span>xxx <span class="nv">SSH_CLIENZT</span><span class="o">=</span>xxxx
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>lo
</span></span><span class="line"><span class="cl"><span class="nv">HOROVOD_GLOO_RENDEZVOUS_ADDR</span><span class="o">=</span>1.1.1.1 <span class="nv">HOROVOD_GLOO_RENDEZVOUS_PORT</span><span class="o">=</span><span class="m">2222</span> <span class="nv">HOROVOD_CPU_OPERATIONS</span><span class="o">=</span>gloo <span class="nv">HOROVOD_GLOO_IFACE</span><span class="o">=</span>lo <span class="nv">HOROVOD_CONTROLLER</span><span class="o">=</span>gloo python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>execute_function_multithreaded 具体代码如下，其中：</p>
<ul>
<li><code>fn</code> 就是前面提到的程序运行环境（能力）<code>exec_command</code>。</li>
<li><code>fn(*arg[:-1])</code> 就是在 <code>exec_command</code> 之中运行<code>slot_info_to_command</code>。</li>
</ul>
<div class="highlight" id="id-30"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">execute_function_multithreaded</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">args_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">block_until_all_done</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">max_concurrent_executions</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Executes fn in multiple threads each with one set of the args in the
</span></span></span><span class="line"><span class="cl"><span class="s2">    args_list.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param fn: function to be executed
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type fn:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param args_list:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type args_list: list(list)
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param block_until_all_done: if is True, function will block until all the
</span></span></span><span class="line"><span class="cl"><span class="s2">    threads are done and will return the results of each thread&#39;s execution.
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type block_until_all_done: bool
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param max_concurrent_executions:
</span></span></span><span class="line"><span class="cl"><span class="s2">    :type max_concurrent_executions: int
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return:
</span></span></span><span class="line"><span class="cl"><span class="s2">    If block_until_all_done is False, returns None. If block_until_all_done is
</span></span></span><span class="line"><span class="cl"><span class="s2">    True, function returns the dict of results.
</span></span></span><span class="line"><span class="cl"><span class="s2">        {
</span></span></span><span class="line"><span class="cl"><span class="s2">            index: execution result of fn with args_list[index]
</span></span></span><span class="line"><span class="cl"><span class="s2">        }
</span></span></span><span class="line"><span class="cl"><span class="s2">    :rtype: dict
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">result_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">worker_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args_list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">arg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">worker_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fn_execute</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">arg</span> <span class="o">=</span> <span class="n">worker_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span>
</span></span><span class="line"><span class="cl">            <span class="n">exec_index</span> <span class="o">=</span> <span class="n">arg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># fn 就是前面提到的程序运行环境（能力）exec_command</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># fn(*arg[:-1])是在 exec_command 之中运行 slot_info_to_command</span>
</span></span><span class="line"><span class="cl">            <span class="n">res</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">arg</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">result_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">exec_index</span><span class="p">,</span> <span class="n">res</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">number_of_threads</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_concurrent_executions</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">args_list</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 在多线程中执行 fn_execute</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_threads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">thread</span> <span class="o">=</span> <span class="n">in_thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">fn_execute</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="ow">not</span> <span class="n">block_until_all_done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thread</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Returns the results only if block_until_all_done is set.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果有设置，则 block 等待</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">block_until_all_done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Because join() cannot be interrupted by signal, a single join()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># needs to be separated into join()s with timeout in a while loop.</span>
</span></span><span class="line"><span class="cl">        <span class="n">have_alive_child</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">have_alive_child</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">have_alive_child</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">have_alive_child</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="ow">not</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">item</span> <span class="o">=</span> <span class="n">result_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span><span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">results</span></span></span></code></pre></td></tr></table>
</div>
</div><p>python train.py 就会进入到我们的训练代码。</p>
<p>大致逻辑如下图，可以看到，结合了各种信息之后，构建了一个可以执行的结果，然后多host执行：</p>
<ul>
<li>图左面，是从 参数中获取 host 等信息，然后解析出 slot 信息；</li>
<li>图右边，是从 python train.py 这个待运行的命令，基于各种配置来生成可以执行命令环境。如果是远程，就得生成 相关远程可运行命令环境（包括切换目录，远程执行等等）；</li>
<li>图中间，是从 python train.py 这个待运行的命令，经过添加 env 信息，gloo 信息。然后结合 左面的 slot 信息 和 右面 的可以执行命令环境 之后，得到了可以在多线程上运行，从而在 多slot 运行的命令。</li>
</ul>
<div class="highlight" id="id-31"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">args</span> <span class="o">:</span> <span class="err">&#39;</span><span class="mf">10.11.11.11</span><span class="o">:</span><span class="mi">4</span><span class="o">,</span><span class="mf">10.11.11.12</span><span class="o">:</span><span class="mi">4</span><span class="err">&#39;</span>            <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>                  <span class="n">command</span>  <span class="o">:</span>  <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                 <span class="o">+</span>                                     <span class="o">+</span>                                     <span class="o">+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="n">v</span>                                     <span class="n">v</span>                                     <span class="n">v</span>
</span></span><span class="line"><span class="cl">      <span class="o">+----------+--------+</span>                 <span class="o">+----------+----------+</span>                <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">      <span class="o">|</span>    <span class="n">parse_hosts</span>    <span class="o">|</span>                 <span class="o">|</span>   <span class="n">get_run_command</span>   <span class="o">|</span>                <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">      <span class="o">+----------+--------+</span>                 <span class="o">|</span>                     <span class="o">|</span>                <span class="o">|</span>  <span class="n">get_remote_command</span>   <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                          <span class="o">+----------+----------+</span>                <span class="o">|</span>                       <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                           <span class="o">+---------+-------------+</span>
</span></span><span class="line"><span class="cl">                 <span class="n">v</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">    <span class="o">+------------+-----------+</span>                         <span class="n">v</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">    <span class="o">|</span>  <span class="n">get_host_assignments</span>  <span class="o">|</span>                                                               <span class="n">v</span>
</span></span><span class="line"><span class="cl">    <span class="o">|</span>                        <span class="o">|</span>               <span class="n">gloo</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">    <span class="o">+------------+-----------+</span>                         <span class="o">+</span>                          <span class="n">ssh</span> <span class="o">-</span><span class="n">o</span> <span class="o">...</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="n">v</span>                                     <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">    <span class="n">SlotInfo</span><span class="o">(</span><span class="n">hostname</span><span class="o">=</span><span class="err">&#39;</span><span class="n">h2</span><span class="err">&#39;</span><span class="o">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="o">)</span>                    <span class="n">v</span>                                     <span class="n">v</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+</span>                         <span class="o">+-----------+---------------+</span>           <span class="o">+---------+--------------+</span>
</span></span><span class="line"><span class="cl">                 <span class="o">|</span>                         <span class="o">|</span> <span class="n">_slot_info_to_command_fn</span>  <span class="o">|</span>           <span class="o">|</span><span class="n">safe_shell_exec</span><span class="o">.</span><span class="na">execute</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">                 <span class="o">+-----------------------&gt;</span> <span class="o">|</span>                           <span class="o">|</span>           <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                           <span class="o">+-----------+---------------+</span>           <span class="o">+---------+--------------+</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="n">v</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                          <span class="n">HOROVOD_CONTROLLER</span><span class="o">=</span><span class="n">gloo</span> <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="na">py</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">+</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="o">|</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                       <span class="n">v</span>                                     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">+-------------+-------------------+</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">|</span>                                 <span class="o">|</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">|</span> <span class="n">execute_function_multithreaded</span>  <span class="o">|</span> <span class="o">&lt;---------------+</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">|</span>                                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">+---------------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>图示如下：</p>
<p></p>
<h3 id="46-c举例">4.6 C++举例</h3>
<p>我们给出一个底层代码，大家就进一步了解 Gloo 可以起到什么作用。</p>
<p>这个就是 Horovod 之中，rank 0 最终给其他 rank 发送构建好的 Tensor。</p>
<div class="highlight" id="id-32"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GlooController</span><span class="o">::</span><span class="n">SendFinalTensors</span><span class="p">(</span><span class="n">ResponseList</span><span class="o">&amp;</span> <span class="n">response_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Notify all nodes which tensors we&#39;d like to reduce at this step.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">encoded_response</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">ResponseList</span><span class="o">::</span><span class="n">SerializeToString</span><span class="p">(</span><span class="n">response_list</span><span class="p">,</span> <span class="n">encoded_response</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Boardcast the response length
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int</span> <span class="n">encoded_response_length</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">encoded_response</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">BroadcastOptions</span> <span class="n">opts</span><span class="p">(</span><span class="n">gloo_context_</span><span class="p">.</span><span class="n">ctx</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setOutput</span><span class="p">(</span><span class="o">&amp;</span><span class="n">encoded_response_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setRoot</span><span class="p">(</span><span class="n">RANK_ZERO</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">broadcast</span><span class="p">(</span><span class="n">opts</span><span class="p">);</span> <span class="c1">// 广播给其他rank
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Boardcast the response
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">BroadcastOptions</span> <span class="n">opts</span><span class="p">(</span><span class="n">gloo_context_</span><span class="p">.</span><span class="n">ctx</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setOutput</span><span class="p">((</span><span class="kt">uint8_t</span><span class="o">*</span><span class="p">)(</span><span class="n">encoded_response</span><span class="p">.</span><span class="n">c_str</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                   <span class="n">encoded_response_length</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">opts</span><span class="p">.</span><span class="n">setRoot</span><span class="p">(</span><span class="n">RANK_ZERO</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">gloo</span><span class="o">::</span><span class="n">broadcast</span><span class="p">(</span><span class="n">opts</span><span class="p">);</span> <span class="c1">// 广播给其他rank
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-mpi-实现">5 Mpi 实现</h2>
<h3 id="51-openmpi-库">5.1 openmpi 库</h3>
<p>horovod 这里主要依赖 openmpi。</p>
<ul>
<li>MPI：英文全称是Message Passing Interface，MPI是一个跨语言的通讯协议，用于编写并行计算机。支持点对点和广播。MPI是一个信息传递应用程序接口，包括协议和和语义说明，他们指明其如何在各种实现中发挥其特性。MPI的目标是高性能，大规模性，和可移植性。</li>
<li>openMPI：英文全称是open Message Passing Interface。openMPI是MPI的一种实现，一种库项目。</li>
</ul>
<p>MPI在Hovorod的角色比较特殊：</p>
<ul>
<li>
<p>一方面Horovod内集成了<strong>基于MPI的AllReduce</strong>，类似于NCCL，都是用作梯度规约；</p>
</li>
<li>
<p>另一方面，MPI可以用来在所有机器上<strong>启动多个进程(Hovorod里用Rank表示)，实现并行计算</strong>；</p>
</li>
</ul>
<h3 id="52-mpi_run-函数">5.2 mpi_run 函数</h3>
<p>此部分代码位于：horovod/runner/mpi_run.py。</p>
<p>首先摘录其关键代码如下，可以看出来其核心是运行 mpirun 命令。</p>
<div class="highlight" id="id-33"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 我是下面大段代码中的关键代码！</span>
</span></span><span class="line"><span class="cl"><span class="n">mpirun_command</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;mpirun </span><span class="si">{basic_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;-np </span><span class="si">{num_proc}{ppn_arg}{hosts_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{binding_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_ssh_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{tcp_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{nccl_socket_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{output_filename_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{env}</span><span class="s1"> </span><span class="si">{extra_mpi_args}</span><span class="s1"> </span><span class="si">{command}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">basic_args</span><span class="o">=</span><span class="n">basic_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_proc</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">ppn_arg</span><span class="o">=</span><span class="n">ppn_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">hosts_arg</span><span class="o">=</span><span class="n">hosts_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">binding_args</span><span class="o">=</span><span class="n">binding_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_args</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mpi_impl_flags</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">tcp_intf_arg</span><span class="o">=</span><span class="n">tcp_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">nccl_socket_intf_arg</span><span class="o">=</span><span class="n">nccl_socket_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_ssh_args</span><span class="o">=</span><span class="n">mpi_ssh_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">output_filename_arg</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">env</span><span class="o">=</span><span class="n">env_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_mpi_args</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">command</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">quote</span><span class="p">(</span><span class="n">par</span><span class="p">)</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">command</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Execute the mpirun command.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">run_func_mode</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">mpirun_command</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">stderr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">execve</span><span class="p">(</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="s1">&#39;-c&#39;</span><span class="p">,</span> <span class="n">mpirun_command</span><span class="p">],</span> <span class="n">env</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>就是依据各种配置以及参数来构建 mpirun 命令的所有参数，比如 ssh 的参数，mpi 参数，nccl 参数等等。</p>
<p>最后得到的 mpirun 命令举例如下：</p>
<div class="highlight" id="id-34"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">mpirun --allow-run-as-root --np <span class="m">2</span> -bind-to none -map-by slot <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    -x <span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO -x LD_LIBRARY_PATH -x PATH <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    -mca pml ob1 -mca btl ^openib <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    python train.py</span></span></code></pre></td></tr></table>
</div>
</div><p>具体代码如下，具体是：</p>
<div class="highlight" id="id-35"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 上面代码是我之中的片段</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mpi_run</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">nics</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">command</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Runs mpi_run.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        settings: Settings for running MPI.
</span></span></span><span class="line"><span class="cl"><span class="s2">                  Note: settings.num_proc and settings.hosts must not be None.
</span></span></span><span class="line"><span class="cl"><span class="s2">        nics: Interfaces to include by MPI.
</span></span></span><span class="line"><span class="cl"><span class="s2">        env: Environment dictionary to use for running command.
</span></span></span><span class="line"><span class="cl"><span class="s2">        command: Command and arguments to run as a list of string.
</span></span></span><span class="line"><span class="cl"><span class="s2">        stdout: Stdout of the mpi process.
</span></span></span><span class="line"><span class="cl"><span class="s2">                Only used when settings.run_func_mode is True.
</span></span></span><span class="line"><span class="cl"><span class="s2">        stderr: Stderr of the mpi process.
</span></span></span><span class="line"><span class="cl"><span class="s2">                Only used when settings.run_func_mode is True.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 得到各种配置</span>
</span></span><span class="line"><span class="cl">    <span class="n">mpi_impl_flags</span><span class="p">,</span> <span class="n">impl_binding_args</span><span class="p">,</span> <span class="n">mpi</span> <span class="o">=</span> <span class="n">_get_mpi_implementation_flags</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">tcp_flag</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">impi</span> <span class="o">=</span> <span class="n">_IMPI_IMPL</span> <span class="o">==</span> <span class="n">mpi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理ssh参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">ssh_args</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ssh_args</span> <span class="o">+=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;-p </span><span class="si">{</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_port</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ssh_args</span> <span class="o">+=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;-i </span><span class="si">{</span><span class="n">settings</span><span class="o">.</span><span class="n">ssh_identity_file</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mpi_ssh_args</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">ssh_args</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">joined_ssh_args</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ssh_args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_ssh_args</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;-bootstrap=ssh -bootstrap-exec-args </span><span class="se">\&#34;</span><span class="si">{</span><span class="n">joined_ssh_args</span><span class="si">}</span><span class="se">\&#34;</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="sa">f</span><span class="s1">&#39;-mca plm_rsh_args </span><span class="se">\&#34;</span><span class="si">{</span><span class="n">joined_ssh_args</span><span class="si">}</span><span class="se">\&#34;</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理网络配置，网卡信息等</span>
</span></span><span class="line"><span class="cl">    <span class="n">tcp_intf_arg</span> <span class="o">=</span> <span class="s1">&#39;-mca btl_tcp_if_include </span><span class="si">{nics}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nics</span><span class="p">))</span> <span class="k">if</span> <span class="n">nics</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">nccl_socket_intf_arg</span> <span class="o">=</span> <span class="s1">&#39;-</span><span class="si">{opt}</span><span class="s1"> NCCL_SOCKET_IFNAME=</span><span class="si">{nics}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">opt</span><span class="o">=</span><span class="s1">&#39;genv&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">nics</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nics</span><span class="p">))</span> <span class="k">if</span> <span class="n">nics</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理host信息</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># On large cluster runs (e.g. Summit), we need extra settings to work around OpenMPI issues</span>
</span></span><span class="line"><span class="cl">    <span class="n">host_names</span><span class="p">,</span> <span class="n">host_to_slots</span> <span class="o">=</span> <span class="n">hosts</span><span class="o">.</span><span class="n">parse_hosts_and_slots</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">impi</span> <span class="ow">and</span> <span class="n">host_names</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">host_names</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">_LARGE_CLUSTER_THRESHOLD</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_impl_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;-mca plm_rsh_no_tree_spawn true&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_impl_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;-mca plm_rsh_num_concurrent </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">host_names</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># if user does not specify any hosts, mpirun by default uses local host.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># There is no need to specify localhost.</span>
</span></span><span class="line"><span class="cl">    <span class="n">hosts_arg</span> <span class="o">=</span> <span class="s1">&#39;-</span><span class="si">{opt}</span><span class="s1"> </span><span class="si">{hosts}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt</span><span class="o">=</span><span class="s1">&#39;hosts&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;H&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">hosts</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">host_names</span><span class="p">)</span> <span class="k">if</span> <span class="n">host_names</span> <span class="ow">and</span> <span class="n">impi</span> <span class="k">else</span> <span class="n">settings</span><span class="o">.</span><span class="n">hosts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理ppn配置</span>
</span></span><span class="line"><span class="cl">    <span class="n">ppn_arg</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">host_to_slots</span> <span class="ow">and</span> <span class="n">impi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ppn</span> <span class="o">=</span> <span class="n">host_to_slots</span><span class="p">[</span><span class="n">host_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">h_name</span> <span class="ow">in</span> <span class="n">host_names</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">ppn_arg</span> <span class="o">=</span> <span class="s1">&#39; -ppn </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ppn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理超时配置</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">prefix_output_with_timestamp</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">impi</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_impl_flags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;--timestamp-output&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">binding_args</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">binding_args</span> <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">binding_args</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">impl_binding_args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 配置需要root身份运行</span>
</span></span><span class="line"><span class="cl">    <span class="n">basic_args</span> <span class="o">=</span> <span class="s1">&#39;-l&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;--allow-run-as-root --tag-output&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;-outfile-pattern&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39;--output-filename&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">output_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 构建环境信息列表</span>
</span></span><span class="line"><span class="cl">    <span class="n">env_list</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span> <span class="k">if</span> <span class="n">impi</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;-x </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">env_util</span><span class="o">.</span><span class="n">is_exportable</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 构建最终的 MPI 命令</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Pass all the env variables to the mpirun command.</span>
</span></span><span class="line"><span class="cl">    <span class="n">mpirun_command</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;mpirun </span><span class="si">{basic_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;-np </span><span class="si">{num_proc}{ppn_arg}{hosts_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{binding_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{mpi_ssh_args}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{tcp_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{nccl_socket_intf_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{output_filename_arg}</span><span class="s1"> &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;</span><span class="si">{env}</span><span class="s1"> </span><span class="si">{extra_mpi_args}</span><span class="s1"> </span><span class="si">{command}</span><span class="s1">&#39;</span>  <span class="c1"># expect a lot of environment variables</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">basic_args</span><span class="o">=</span><span class="n">basic_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_proc</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">num_proc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">ppn_arg</span><span class="o">=</span><span class="n">ppn_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">hosts_arg</span><span class="o">=</span><span class="n">hosts_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">binding_args</span><span class="o">=</span><span class="n">binding_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_args</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mpi_impl_flags</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">tcp_intf_arg</span><span class="o">=</span><span class="n">tcp_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">nccl_socket_intf_arg</span><span class="o">=</span><span class="n">nccl_socket_intf_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mpi_ssh_args</span><span class="o">=</span><span class="n">mpi_ssh_args</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">output_filename_arg</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">env</span><span class="o">=</span><span class="n">env_list</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_mpi_args</span><span class="o">=</span><span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">extra_mpi_args</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">command</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">quote</span><span class="p">(</span><span class="n">par</span><span class="p">)</span> <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">command</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># we need the driver&#39;s PATH and PYTHONPATH in env to run mpirun,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># env for mpirun is different to env encoded in mpirun_command</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">,</span> <span class="s1">&#39;PYTHONPATH&#39;</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">var</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">env</span> <span class="ow">and</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># copy env so we do not leak env modifications</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># copy var over from os.environ</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">var</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Execute the mpirun command.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">run_func_mode</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">exit_code</span> <span class="o">=</span> <span class="n">safe_shell_exec</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">mpirun_command</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">stderr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">execve</span><span class="p">(</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/bin/sh&#39;</span><span class="p">,</span> <span class="s1">&#39;-c&#39;</span><span class="p">,</span> <span class="n">mpirun_command</span><span class="p">],</span> <span class="n">env</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="53-mpirun命令">5.3 mpirun命令</h3>
<p>因为 mpi_run 使用的是 mpirun 命令来运行，所以我们介绍一下。</p>
<p>mpirun是MPI程序的启动脚本，它简化了并行进程的启动过程，尽可能屏蔽了底层的实现细节，从而为用户提供了一个通用的MPI并行机制。</p>
<p>在用mpirun命令执行并行程序时，参数-np指明了需要并行运行的进程个数。mpirun首先在本地结点上启动一个进程，然后根据/usr/local/share/machines.LINUX文件中所列出的主机，为每个主机启动一个进程。若进程数比可用的并行节点数多，则多余的进程将重新按照上述规则进行。按这个机制分配好进程后，一般会给每个节点分一个固定的标号，类似于身份证了，后续在消息传递中会用到。</p>
<p>这里需要说明的是，实际运行的</p>
<p>orterun(Open MPI SPMD / MPMD启动器; mpirun / mpiexec只是它的符号链接)</p>
<p>命令举例如下：</p>
<div class="highlight" id="id-36"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="mi">4</span> \
</span></span><span class="line"><span class="cl">    <span class="o">-</span><span class="n">bind</span><span class="o">-</span><span class="n">to</span> <span class="n">none</span> <span class="o">-</span><span class="nb">map</span><span class="o">-</span><span class="n">by</span> <span class="n">slot</span> \
</span></span><span class="line"><span class="cl">    <span class="o">-</span><span class="n">x</span> <span class="n">NCCL_DEBUG</span><span class="o">=</span><span class="n">INFO</span> <span class="o">-</span><span class="n">x</span> <span class="n">LD_LIBRARY_PATH</span> <span class="o">-</span><span class="n">x</span> <span class="n">PATH</span> \
</span></span><span class="line"><span class="cl">    <span class="o">-</span><span class="n">mca</span> <span class="n">pml</span> <span class="n">ob1</span> <span class="o">-</span><span class="n">mca</span> <span class="n">btl</span> <span class="o">^</span><span class="n">openib</span> \
</span></span><span class="line"><span class="cl">    <span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="6-总结">6 总结</h2>
<p>对比 gloo 和 mpi 的实现，我们还是能看出来区别。</p>
<h3 id="61-gloo">6.1 gloo</h3>
<p>gloo 只是一个库，需要 horovod 来完成命令分发功能。</p>
<p>gloo 需要 horovod 自己实现本地运行和远端运行方式，即 get_remote_command 函数 实现 <code>'ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no'</code>。</p>
<p>gloo 需要实现 RendezvousServer，底层会利用 RendezvousServer 进行通讯。</p>
<h3 id="62-mpi">6.2 mpi</h3>
<p>mpi 则功能强大很多，只要把命令配置成被 mpirun 包装，openmpi 就可以自行完成命令分发执行。说到底，horovod 是一个 mpirun 程序，即使运行了 tensor flow，也是一个mpi程序，可以互相交互。</p>
<p>references:
[1]. <a href="https://www.cnblogs.com/rossiXYZ/p/14881812.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14881812.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item><item><title>深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入</title><link>https://jianye0428.github.io/posts/2022-10-08_horovod_2/</link><pubDate>Mon, 10 Jul 2023 07:53:40 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/2022-10-08_horovod_2/</guid><description><![CDATA[<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。</p>
<p>前一篇参见如下：</p>
<p><a href="http://localhost:1313/posts/notes/2022-10-08_horovod_1/"target="_blank" rel="external nofollow noopener noreferrer">深度学习分布式训练框架 Horovod[1] &ndash; 基础知识<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="1-horovod-简介">1 Horovod 简介</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，支持TensorFlow，Keras，PyTorch和MXNet。Horovod 的名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与分布式 TensorFlow 流程使用 Horovod 互相通信的场景很像。</p>
<p>因为各个机器学习框架对于底层集合通信库（ nccl，openmpi，gloo 等等）的利用水平可能各不相同，使得他们无法充分利用这些底层集合通信库的威力。因而，hovorod 就整合这些框架，提供一个易用高效的解决方案。</p>
<p>Uber的工程师就是根据FaceBook的一篇paper：“<a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf"target="_blank" rel="external nofollow noopener noreferrer">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>”和百度的一篇“<a href="https://research.baidu.com/bringing-hpc-techniques-deep-learning/"target="_blank" rel="external nofollow noopener noreferrer">Bringing HPC Techniques to Deep Learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>” 改进并发布了开源框架Horovod。</p>
<p>Horovod 相比于百度的工作，并无学术上的贡献。但是 Horovod 扎实的工程实现，使得它受到了更多的关注。它最大的优势在于对 RingAllReduce 进行了更高层次的抽象，使其支持多种不同的框架。同时引入了 Nvidia NCCL，对 GPU 更加友好。</p>
<p>Horovod依赖于Nvidia的 NCCL2 做 All Reduce，依赖于MPI做进程间通信，简化了同步多 GPU 或多节点分布式训练的开发流程。由于使用了NCCL2，Horovod也可以利用以下功能：NVLINK，RDMA，GPUDirectRDMA，自动检测通信拓扑，能够回退到 PCIe 和 TCP/IP 通信。</p>
<p>我们需要几个问题来引导分析：</p>
<ul>
<li>Hovorod 怎么进行数据分割？</li>
<li>Hovorod 怎么进行训练代码分发？</li>
<li>Hovorod 启动时候，python 和 C++ 都做了什么？</li>
<li>如何确保 Hovorod 启动时候步骤一致；</li>
</ul>
<h2 id="2-hovorod-机制概述">2 Hovorod 机制概述</h2>
<h3 id="21-horovod-机制">2.1 Horovod 机制</h3>
<p>Horovod使用<strong>数据并行化</strong>策略在GPU上分配训练。</p>
<p>在数据并行化中，作业中的每个GPU都会接收其自己的数据批处理的独立切片，即它的“批处理切片”。 每个GPU都使用自己分配到的数据来独立计算，进行梯度更新。</p>
<p>假如使用两个GPU，批处理大小为32，则第一个GPU将处理前16条记录的正向传播和向后传播，以及第二个GPU处理后16条记录的正向传播和向后传播。然后，这些梯度更新将在GPU之间平均在一起，最后应用于模型。</p>
<p>每一个迭代的操作方法如下：</p>
<ol>
<li>
<p>每个 worker 将维护自己的模型权重副本和自己的数据集副本。</p>
</li>
<li>
<p>收到执行信号后，每个工作进程都会从数据集中提取一个不相交的批次，并计算该批次的梯度。</p>
</li>
<li>
<p>Workers 使用ring all-reduce算法来同步彼此的梯度，从而在本地所有节点上计算同样的平均梯度。</p>
<ol>
<li>
<p>将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片，后续每一次通信都将给下一个邻居发送一个自己的分片（同时从上一个邻居接受一个新分片）。</p>
</li>
<li>
<p>ScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和，即每个 device 将有一个块，其中包含所有device 中该块中所有值的总和；具体如下：</p>
</li>
</ol>
<p></p>
<ol start="3">
<li>AllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和 广播到其他 device；最终所有节点都拥有所有tensor分片和。具体如下：
</li>
<li>在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度；</li>
</ol>
</li>
<li>
<p>每个 worker 将 梯度更新 应用于其模型的本地副本。</p>
</li>
<li>
<p>执行下一个batch。</p>
</li>
</ol>
<h2 id="3-示例代码">3 示例代码</h2>
<h3 id="31--摘要代码">3.1  摘要代码</h3>
<p>我们此处给出官网示例代码部分摘要，具体分析参见下面代码中的注释。</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: initialize Horovod.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="c1"># 初始化 Horovod，启动相关线程和MPI线程</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 依据 local rank 为不同的进程分配不同的GPU</span>
</span></span><span class="line"><span class="cl"><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">mnist_images</span><span class="p">,</span> <span class="n">mnist_labels</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;mnist-</span><span class="si">%d</span><span class="s1">.npz&#39;</span> <span class="o">%</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 切分数据</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_images</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="o">......</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: adjust learning rate based on number of GPUs.</span>
</span></span><span class="line"><span class="cl"><span class="n">scaled_lr</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1"># 根据Worker的数量增加学习率的大小</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">scaled_lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 把常规TensorFlow Optimizer通过Horovod包装起来，进而使用 ring-allreduce 来得到平均梯度</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
</span></span><span class="line"><span class="cl"><span class="c1"># uses hvd.DistributedOptimizer() to compute gradients.</span>
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">experimental_run_tf_function</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 广播初始化，将模型的参数从第一个设备传向其他设备，以保证初始化模型参数的一致性</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">MetricAverageCallback</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateWarmupCallback</span><span class="p">(</span><span class="n">initial_lr</span><span class="o">=</span><span class="n">scaled_lr</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them. # 只有设备0需要保存模型参数作为checkpoint</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;./checkpoint-</span><span class="si">{epoch}</span><span class="s1">.h5&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: write logs on worker 0.</span>
</span></span><span class="line"><span class="cl"><span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the model.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: adjust number of steps based on number of GPUs.</span>
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">500</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-horovodrun">3.2 horovodrun</h3>
<p>Horovod训练脚本未作为Python脚本启动。 例如，您不能使用<code>python train.py</code>运行此脚本。 需要采用特殊的CLI命令 <code>horovodrun</code> 来启动（训练代码 train.py 需要手动拷贝到各个节点上，且目录相同）：</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ horovodrun -np 4 -H localhost:4 python train.py</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="4-运行逻辑">4 运行逻辑</h2>
<p>我们按照顺序梳理，看看在程序初始化过程背后都做了什么。</p>
<h3 id="41-引入python文件">4.1 引入python文件</h3>
<p>如下代码会引入各种相关python文件。</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42--初始化-in-python">4.2  初始化 in python</h3>
<p>python 世界的初始化位于 <code>horovod-master/horovod/mxnet/mpi_ops.py</code></p>
<h4 id="421-引入so库">4.2.1 引入SO库</h4>
<h5 id="4211-so库">4.2.1.1 SO库</h5>
<p><code>horovod/tensorflow/mpi_ops.py</code> 之中会引入SO库。
比如 <code>dist-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so</code>。</p>
<p>SO库 就是 horovod 中 C++ 代码编译出来的结果。</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_load_library</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Loads a .so file containing the specified operators.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">filename</span> <span class="o">=</span> <span class="n">resource_loader</span><span class="o">.</span><span class="n">get_path_to_datafile</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">library</span> <span class="o">=</span> <span class="n">load_library</span><span class="o">.</span><span class="n">load_op_library</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">library</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check possible symbol not found error from tensorflow version mismatch</span>
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">MPI_LIB</span> <span class="o">=</span> <span class="n">_load_library</span><span class="p">(</span><span class="s1">&#39;mpi_lib&#39;</span> <span class="o">+</span> <span class="n">get_ext_suffix</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">check_installed_version</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">check_installed_version</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h5 id="4222-so作用">4.2.2.2 SO作用</h5>
<p>引入库的作用是获取到 C++ 的函数，并且用 python 封装一下，这样就可以在 python 世界使用 C++代码了。</p>
<p>由下文可以看出来，python 的 _allreduce 函数就会把功能转发给 C++，由 <code>MPI_LIB.horovod_allreduce</code> 完成。</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">Sum</span><span class="p">,</span> <span class="n">prescale_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">postscale_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">               <span class="n">ignore_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;HorovodAllreduce_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">_normalize_name</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">MPI_LIB</span><span class="o">.</span><span class="n">horovod_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">reduce_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">prescale_factor</span><span class="o">=</span><span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">postscale_factor</span><span class="o">=</span><span class="n">postscale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ignore_name_scope</span><span class="o">=</span><span class="n">ignore_name_scope</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="422-初始化配置">4.2.2 初始化配置</h4>
<p>我们摘录了主要部分，就是初始化 _HorovodBasics，然后从 _HorovodBasics 内获取各种函数，变量和配置，比如是否编译了mpi，gloo等等.</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">horovod.common.basics</span> <span class="kn">import</span> <span class="n">HorovodBasics</span> <span class="k">as</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">_basics</span> <span class="o">=</span> <span class="n">_HorovodBasics</span><span class="p">(</span><span class="vm">__file__</span><span class="p">,</span> <span class="s1">&#39;mpi_lib&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># import basic methods</span>
</span></span><span class="line"><span class="cl"><span class="n">init</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">init</span>
</span></span><span class="line"><span class="cl"><span class="n">size</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl"><span class="n">local_size</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">local_size</span>
</span></span><span class="line"><span class="cl"><span class="n">rank</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">rank</span>
</span></span><span class="line"><span class="cl"><span class="n">local_rank</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">local_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">mpi_built</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">mpi_built</span>
</span></span><span class="line"><span class="cl"><span class="n">gloo_enabled</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">gloo_enabled</span>
</span></span><span class="line"><span class="cl"><span class="o">......</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-hvdinit-初始化">4.2.3 hvd.init() 初始化</h4>
<p>首先需要用 <code>hvd.init()</code> 来初始化，horovod 管理的所有状态都会传到 hvd 对象中。</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: initialize Horovod.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>此处调用的是 HorovodBasics 中的函数，我们看看做了什么。</p>
<p>可以看到，这部分会一直深入到 C++世界，调用了大量的 MPI_LIB_CTYPES 函数，所以我们接下来就要进入到 C++的世界看看。</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">comm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;A function that initializes Horovod.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_built</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_mpi_built</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">MPI</span><span class="o">.</span><span class="n">_sizeof</span><span class="p">(</span><span class="n">MPI</span><span class="o">.</span><span class="n">Comm</span><span class="p">)</span> <span class="o">==</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">sizeof</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">MPI_Comm</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">MPI_Comm</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init_comm</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">MPI_Comm</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">comm_obj</span> <span class="o">=</span> <span class="n">MPI_Comm</span><span class="o">.</span><span class="n">from_address</span><span class="p">(</span><span class="n">MPI</span><span class="o">.</span><span class="n">_addressof</span><span class="p">(</span><span class="n">comm</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init_comm</span><span class="p">(</span><span class="n">comm_obj</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">comm_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">comm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span> <span class="o">*</span> <span class="n">comm_size</span><span class="p">)(</span><span class="o">*</span><span class="n">comm</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">(</span><span class="n">comm_size</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下图：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-JAVA" data-lang="JAVA"><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">python</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">C</span><span class="o">++</span> <span class="n">SO</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Create</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="na">init</span><span class="o">()</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">Python</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+------------------------------------------+</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span><span class="o">++</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-初始化-in-c">4.3 初始化 in C++</h3>
<h4 id="431-horovod_init_comm">4.3.1 horovod_init_comm</h4>
<p>在初始化的时候，Horovod 会：</p>
<ul>
<li>调用 <code>MPI_Comm_dup</code> 获取一个 Communicator，这样就有了和 MPI 协调的基础。</li>
<li>然后调用 <code>InitializeHorovodOnce</code>。</li>
</ul>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">void</span> <span class="n">horovod_init_comm</span><span class="p">(</span><span class="n">MPI_Comm</span> <span class="n">comm</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Comm_dup</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mpi_context</span><span class="o">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="432-initializehorovodonce">4.3.2 InitializeHorovodOnce</h4>
<p>InitializeHorovodOnce 是初始化的主要工作，主要是：</p>
<ul>
<li>依据是否编译了 mpi 或者 gloo，对各自的 context 进行处理，为 globalstate 创建对应的 controller；</li>
<li>启动了后台线程 BackgroundThreadLoop 用来在各个worker之间协调；</li>
</ul>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">void</span> <span class="n">horovod_init</span><span class="p">(</span><span class="n">const</span> <span class="nb">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="nb">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">void</span> <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">const</span> <span class="nb">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="nb">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Ensure</span> <span class="n">background</span> <span class="n">thread</span> <span class="ow">is</span> <span class="n">only</span> <span class="n">started</span> <span class="n">once</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">initialize_flag</span><span class="o">.</span><span class="n">test_and_set</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">=</span> <span class="n">ParseControllerOpsFromEnv</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">=</span> <span class="n">ParseCPUOpsFromEnv</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#if HAVE_MPI // 依据是否编译了MPI进行处理</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Enable</span> <span class="n">mpi</span> <span class="ow">is</span> <span class="n">it</span><span class="s1">&#39;s used either in cpu data transfer or controller</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="o">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="o">//</span> <span class="n">创建一个</span> <span class="n">MPIController</span> <span class="n">对象</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">group_table</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#endif</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#if HAVE_GLOO // 依据是否编译了 GLOO 进行处理</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Enable</span> <span class="n">gloo</span> <span class="ow">is</span> <span class="n">it</span><span class="s1">&#39;s used either in cpu data transfer or controller</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">gloo_context</span><span class="o">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">new</span> <span class="n">GlooController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">group_table</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">gloo_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#endif</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Reset</span> <span class="n">initialization</span> <span class="n">flag</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">启动后台线程</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="n">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="p">::</span><span class="n">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Wait</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">background</span> <span class="n">thread</span> <span class="n">has</span> <span class="n">finished</span> <span class="n">initializing</span> <span class="n">MPI</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="err">!</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="p">::</span><span class="n">this_thread</span><span class="p">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">chrono</span><span class="p">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="433-horovodglobalstate">4.3.3 HorovodGlobalState</h4>
<p>在 C++ 世界，HorovodGlobalState 起到了<font color=red>集中管理各种全局变量</font>的作用。</p>
<p>HorovodGlobalState 在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问。HorovodGlobalState 在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）。</p>
<p>Horovod 主要会在backgroundThreadLoop 中完成 HorovodGlobalState 不同元素初始化，比较重要的有：</p>
<ul>
<li>controller 管理总体通信控制流；</li>
<li>tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等)；</li>
</ul>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// All the Horovod state that must be stored globally per-process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">HorovodGlobalState</span> <span class="n">horovod_global</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">MPIContext</span> <span class="n">mpi_context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">GlooContext</span> <span class="n">gloo_context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="p">....</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">OperationManager</span><span class="o">&gt;</span> <span class="n">op_manager</span><span class="p">;</span></span></span></code></pre></td></tr></table>
</div>
</div><p>HorovodGlobalState 摘要如下：</p>
<div class="highlight" id="id-13"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">HorovodGlobalState</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Background thread running MPI communication.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">background_thread</span><span class="p">;</span> <span class="c1">// 后台线程，用来在各个worker之间协调
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">ParameterManager</span> <span class="n">parameter_manager</span><span class="p">;</span> <span class="c1">// 维护后台总体参数配置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Encapsulates the fusion buffers, handles resizing and auto-tuning of buffer
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// size.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">FusionBufferManager</span> <span class="n">fusion_buffer</span><span class="p">;</span> <span class="c1">// 融合tensor，以便缩减通信开销
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Controller</span><span class="o">&gt;</span> <span class="n">controller</span><span class="p">;</span> <span class="c1">//管理总体通信控制流
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorQueue</span> <span class="n">tensor_queue</span><span class="p">;</span> <span class="c1">//处理从前端过来的通信需求（allreduce，broadcast 等）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Pointer to shared buffer for allgather
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">void</span><span class="o">*</span> <span class="n">shared_buffer</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// LRU cache of Responses
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ResponseCache</span> <span class="n">response_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Information on registered groups.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">GroupTable</span> <span class="n">group_table</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">HorovodGlobalState</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Make sure that the destructor of the background thread is safe to
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// call. If a thread is still joinable (not detached or complete) its
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// destructor cannot be called.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">background_thread</span><span class="p">.</span><span class="n">joinable</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">shut_down</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">background_thread</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span></span></span></code></pre></td></tr></table>
</div>
</div><p>目前具体逻辑如下：</p>
<div class="highlight" id="id-14"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">python</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">C</span><span class="o">++</span> <span class="n">SO</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Create</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="na">init</span><span class="o">()</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">Python</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------------------------------------------------------------------------------------------------+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">++</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                          <span class="o">+-----------------------------+</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">|</span>  <span class="n">HorovodGlobalState</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="n">horovod_init_comm</span>                                                <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                             <span class="o">+------------------+</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span> <span class="n">horovod_global</span> <span class="o">+---------&gt;</span> <span class="o">|</span>        <span class="n">TensorQueue</span>          <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">background_thread</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                  <span class="o">|</span> <span class="n">mpi_context</span>      <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="n">InitializeHorovodOnce</span>   <span class="o">+------------&gt;</span> <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">ParameterManager</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span> <span class="n">gloo_context</span>     <span class="o">|</span>         <span class="o">|</span>        <span class="n">FusionBufferManager</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">Controller</span>           <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                             <span class="o">|</span> <span class="n">op_manager</span>       <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="n">background_threa</span>                     <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">ResponseCache</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                  <span class="o">+------------------+</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">|</span>        <span class="n">shared_buffer</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">+-----------------------------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>如图：</p>
<p></p>
<p>至此，horovod 已经初始化完成，用户代码可以使用了。</p>
<h3 id="43-hvd-概念">4.3 hvd 概念</h3>
<p>在用户代码中，接下来是rank概念。</p>
<div class="highlight" id="id-15"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p>我们介绍下几个相关概念：</p>
<ul>
<li>Horovod为设备上的每个GPU启动了该训练脚本的一个副本。<strong>local rank</strong>就是分配给某一台计算机上每个执行训练的唯一编号（也可以认为是进程号或者GPU设备的ID号），范围是 0 到 n-1，其中 n 是该计算机上GPU设备的数量。</li>
<li>rank 可以认为是代表分布式任务里的一个执行训练的唯一全局编号（<font color=red>用于进程间通讯</font>）。Rank 0 在Horovod中通常具有特殊的意义：<strong>它是负责此同步的设备</strong>。
<ul>
<li>在百度的实现中，不同 Rank 的角色是不一样的，Rank 0 会充当 coordinator 的角色。它会协调来自其他 Rank 的 MPI 请求，是一个工程上的考量。这一设计也被后来的 Horovod 采用。</li>
<li>Rank 0 也用来把参数广播到其他进程 &amp; 存储 checkpoint。</li>
<li>world_size：进程总数量，会等到所有world_size个进程就绪之后才会开始训练。</li>
</ul>
</li>
</ul>
<p>hvd.init 这部分的目的就是让<strong>并行进程</strong>们可以知道自己被分配的 rank / local rank 等信息，于是后续可以根据 local rank（所在节点上的第几张 GPU 卡） 来设置所需的显存分配。</p>
<h3 id="45--数据处理">4.5  数据处理</h3>
<p>接下来是数据处理。</p>
<div class="highlight" id="id-16"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_images</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这里有几点需要说明：</p>
<ul>
<li>
<p>首先，训练的数据需要放置在任何节点都能访问的地方。</p>
</li>
<li>
<p>其次，Horovod 需要对数据进行分片处理，需要在不同机器上按Rank进行切分，以保证每个GPU进程训练的数据集是不一样的。</p>
</li>
<li>
<p>数据集本体需要出于数据并行性的需求而被拆分为多个分片，Horovod的不同工作节点都将分别读取自己的数据集分片。</p>
</li>
</ul>
<p>从 PyTorch 示例脚本看得更加清楚。</p>
<div class="highlight" id="id-17"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: use DistributedSampler to partition the training data.</span>
</span></span><span class="line"><span class="cl"><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">rank</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><code>DataLoader</code>的采样器组件从要绘制的数据集中返回可迭代的索引。 PyTorch中的默认采样器是顺序的，返回序列<code>0, 1, 2, …, n</code> 。 Horovod使用其<code>DistributedSampler</code>覆盖了此行为，该DistributedSampler处理跨计算机的数据集分区。 <code>DistributedSampler</code>本身接受两个参数作为输入： <code>hvd.size()</code> (GPU的总数，例如16)和<code>hvd.rank()</code> (从总体列表中分配给该设备的ID，例如0…15)。</p>
</li>
<li>
<p>Pytorch使用的是<strong>数据分布式训练</strong>，每个进程实际上是独立加载数据的，所以需要加载相同数据集后用一定的规则根据rank来顺序切割获取不同的数据子集，DistributedSampler就是用来确保dataloader只会load到整个数据集的一个特定子集的做法(实际上不用Pytorch提供的DistributedSampler工具，自己做加载数据后切分word_size个子集按rank顺序拿到子集效果也是一样)。</p>
</li>
<li>
<p>同时为了能够按顺序划分数据子集，拿到不同部分数据，所以数据集不能够进行随机打散，所以用了参数 <code>'shuffle': False</code>。</p>
</li>
</ul>
<h3 id="46-广播初始化变量">4.6 广播初始化变量</h3>
<p>以下代码完成广播初始化的功能。</p>
<div class="highlight" id="id-18"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这句代码保证的是 rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</p>
<p>下面就介绍下 Horvod 之中广播的使用。</p>
<h4 id="461-广播定义">4.6.1 广播定义</h4>
<p>广播的具体实现是：</p>
<div class="highlight" id="id-19"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BroadcastGlobalVariablesCallbackImpl</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">BroadcastGlobalVariablesCallbackImpl</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span> <span class="o">=</span> <span class="n">root_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">_executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;variables&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># TensorFlow 2.0 or TensorFlow eager</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">root_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">root_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">bcast_op</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_global_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">bcast_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span> <span class="o">=</span> <span class="kc">True</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="462-broadcast_variables">4.6.2 broadcast_variables</h4>
<p>broadcast_variables 调用了 _make_broadcast_group_fn 完成功能，可以看到对于 执行图 的每个变量，调用了 broadcast。</p>
<div class="highlight" id="id-20"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">broadcast_variables</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Broadcasts variables from root rank to all other processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">        variables: variables for broadcast
</span></span></span><span class="line"><span class="cl"><span class="s2">        root_rank: rank of the process from which global variables will be broadcasted
</span></span></span><span class="line"><span class="cl"><span class="s2">                   to all other processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">broadcast_group</span> <span class="o">=</span> <span class="n">_make_broadcast_group_fn</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>以及</p>
<div class="highlight" id="id-21"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@_cache</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_make_broadcast_group_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Eager mode will parallelize independent control flow</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_make_subgraph</span><span class="p">(</span><span class="n">broadcast_group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Graph mode requires an Op</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                              <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">broadcast_group</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="463-调用-mpi">4.6.3 调用 MPI</h4>
<p>broadcast 就是调用了 MPI 函数真正完成了功能。</p>
<div class="highlight" id="id-22"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;An op which broadcasts the input tensor on root rank to the same input tensor
</span></span></span><span class="line"><span class="cl"><span class="s2">    on all other Horovod processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The broadcast operation is keyed by the name of the op. The tensor type and
</span></span></span><span class="line"><span class="cl"><span class="s2">    shape must be the same on all Horovod processes for a given name. The broadcast
</span></span></span><span class="line"><span class="cl"><span class="s2">    will not start until all processes are ready to send and receive the tensor.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">      A tensor of the same shape and type as `tensor`, with the value broadcasted
</span></span></span><span class="line"><span class="cl"><span class="s2">      from root rank.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;HorovodBroadcast_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">_normalize_name</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">MPI_LIB</span><span class="o">.</span><span class="n">horovod_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="n">root_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ignore_name_scope</span><span class="o">=</span><span class="n">ignore_name_scope</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="464-同步参数">4.6.4 同步参数</h4>
<p>在后台进程中，会<strong>根据情况定期同步参数</strong>。</p>
<div class="highlight" id="id-23"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">bool</span> <span class="nf">RunLoopOnce</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// 业务逻辑功能
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">IsAutoTuning</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">should_sync</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">Update</span><span class="p">(</span><span class="n">tensor_names</span><span class="p">,</span> <span class="n">total_tensor_size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 看看是否需要同步，如果需要，就同步。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">should_sync</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SynchronizeParameters</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">......</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><p>同步参数代码也是调用了 Bcast 功能完成。</p>
<div class="highlight" id="id-24"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Controller</span><span class="o">::</span><span class="n">SynchronizeParameters</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ParameterManager</span><span class="o">::</span><span class="n">Params</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">is_coordinator_</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// rank 0 执行操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">param</span> <span class="o">=</span> <span class="n">parameter_manager_</span><span class="p">.</span><span class="n">GetParams</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">void</span><span class="o">*</span> <span class="n">buffer</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">size_t</span> <span class="n">param_size</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">param_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_coordinator_</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// worker 执行操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">parameter_manager_</span><span class="p">.</span><span class="n">SetParams</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="47-distributedoptimizer">4.7 DistributedOptimizer</h3>
<p>最后需要配置DistributedOptimizer，这就是关键点之一。</p>
<div class="highlight" id="id-25"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>TF Optimizer 是模型训练的关键API，可以获取到每个OP的梯度并用来更新权重。HVD 在原始 TF Optimizer的基础上包装了hvd.DistributedOptimizer。</p>
<p>DistributedOptimizer包装器将原始优化器作为输入，将梯度计算委托给它。 即DistributedOptimizer会调用原始优化器进行梯度计算。这样，在集群中每台机器都会用原始优化器得到自己的梯度（Local Gradient）。</p>
<p><code>Horovod DistributedOptimizer</code>接下来会使用all-reduce或all-gather来完成全局梯度归并，然后将这些平均梯度应用于所有设备。</p>
<p>我们梳理下其中的调用关系：</p>
<ul>
<li>hvd.DistributedOptimizer继承 keras Optimizer，在计算时候，依然由传入的原始优化器做计算。</li>
<li>在得到计算的梯度之后，调用 hvd.allreduce 或者 hvd.allgather 来计算。</li>
<li>最后实施这些平均之后的梯度。从而实现整个集群的梯度归并操作。</li>
</ul>
<p>具体后文会详细介绍。</p>
<h3 id="48-未来可能">4.8 未来可能</h3>
<p>Horovod 目前架构的基础是：机器学习的模型参数在一张 GPU 上可以存下。</p>
<p><strong>未来是否可以把模型分片结合进来，是一个很大的看点。</strong></p>
<p>另外，如果模型的全连接层较多，则全连接层的强耦合性结合 allreduce 类似 bsp 的同步机制，还是会让网络通信时间成为瓶颈。因此，在 ring-allreduce 环境下，同步协议的改造，比如利用 SSP 来替换 BSP，或者利用梯度压缩来加快 allreduce 进程也是值得探索的方向。</p>
<h2 id="5-总结">5 总结</h2>
<p>针对文初提出的几个问题，我们现在回答如下：</p>
<ul>
<li>Hovorod 怎么进行数据分割？
答案：有的框架可以自动做数据分割。如果框架不提供，则需要用户自己进行数据分割，以保证每个GPU进程训练的数据集是不一样的。</li>
<li>Hovorod 怎么进行模型分发？
用户需要手动拷贝训练代码到各个节点上。</li>
<li>Hovorod 启动时候，python 和 C++ 都做了什么？
答案：python 会引入 C++库，初始化各种变量和配置。C++部分会对 MPI，GLOO上下文进行初始化，启动后台进程处理内部通信。</li>
<li>如何确保 Hovorod 启动时候步骤一致；
答案： rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</li>
</ul>
<p>下一篇文章将深入到python世界看看。</p>
<p>reference:
[1].https://www.cnblogs.com/rossiXYZ/p/14856543.html</p>
]]></description></item><item><title>深度学习分布式训练框架 Horovod[1] -- 基础知识</title><link>https://jianye0428.github.io/posts/2022-10-08_horovod_1/</link><pubDate>Mon, 10 Jul 2023 07:45:42 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/2022-10-08_horovod_1/</guid><description><![CDATA[<h2 id="0-摘要">0 摘要</h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第一篇，介绍相关背景知识。</p>
<h2 id="1-分布式并行训练">1 分布式并行训练</h2>
<p>我们首先要介绍下<strong>分布式并行训练</strong>。</p>
<h3 id="11-分布式并行训练的必要">1.1 分布式并行训练的必要</h3>
<p>传统的模型训练中，<font color=red><strong>迭代计算</strong></font>只能利用当前进程所在主机上的所有硬件资源，可是<u>单机扩展性始终有限</u>。而目前的机器学习有如下特点：</p>
<ul>
<li><strong>样本数量大</strong> 目前训练数据越来越多，在大型互联网场景下，每天的样本量可以达到百亿级别。</li>
<li><strong>特征维度多</strong> 因为巨大样本量导致机器学习模型参数越来越多，特征维度可以达到千亿或者万亿级别。</li>
<li><strong>训练性能要求高</strong> 虽然样本量和模型参数巨大，但是业务需要我们在短期内训练出一个优秀的模型来验证。</li>
<li><strong>模型实时上线</strong> 对于推荐资讯类应用，往往要求根据用户最新行为及时调整模型进行预测。</li>
</ul>
<p>因此，单机面对海量数据和巨大模型时是无能为力的，有必要把数据或者模型分割成为多份，在多个机器上借助不同主机上的硬件资源进行训练加速。</p>
<h3 id="12-分布式训练">1.2 分布式训练</h3>
<p>本文所说的训练，指的是<font color=red>利用训练数据通过计算梯度下降的方式迭代地去优化神经网络参数，并最终输出网络模型的过程</font>。在单次模型训练迭代中，会有如下操作：</p>
<ul>
<li>首先利用数据对模型进行前向的计算。所谓的前向计算，就是将模型上一层的输出作为下一层的输入，并计算下一层的输出，从输入层一直算到输出层为止。</li>
<li>其次会根据目标函数，我们将反向计算模型中每个参数的导数，并且结合学习率来更新模型的参数。</li>
</ul>
<p>而并行梯度下降的基本思想便是：<font color=red>多个处理器分别利用自己的数据来计算梯度，最后通过聚合或其他方式来实现并行计算梯度下降以加速模型训练过程</font>。 比如两个处理器分别处理一半数据计算梯度 g_1, g_2，然后把两个梯度结果进行聚合更新，这样就实现了并行梯度下降。</p>
<h3 id="13--训练并行机制">1.3  训练并行机制</h3>
<h4 id="131-三种机制">1.3.1 三种机制</h4>
<p>由于使用小批量算法，可以把宽度$(∝W)$和深度$(∝D)$的前向传播和反向传播分发到并行的处理器上，这样深度训练的并行机制主要有三种：</p>
<ul>
<li>第一个是<font color=red><strong>模型并行机制</strong></font>（按照网络结构分区）。
<ul>
<li>通常是针对一个节点无法存下整个模型的情况下，去对图进行拆分。</li>
<li>将模型参数进行分布式存储。<strong><u>计算机上每个计算可以建模为一个有向无环图（DAG），顶点是计算指令，边是数据依赖（数据流）。</u></strong>&ldquo;基于图去拆分&rdquo; 会根据每一层中的神经元（即四维张量中的C、H或W维）来把一张大的图拆分成很多部分，每个部分都会在很多设备上去计算。</li>
<li>或者可以这么理解：深度学习的计算主要是矩阵运算，有时候矩阵非常大无法放到显存中，就只能把超大矩阵拆分了放到不同卡上计算。</li>
<li>模型较后部分的计算必须等前面计算完成，因此不同节点间的计算实际是串行的。但每个部分计算互不妨碍，更像是流水线结构。</li>
</ul>
</li>
<li>第二个是<font color=red><strong>数据并行机制</strong></font>（按照输入样本分区）。
<ul>
<li>更多场景下我们模型规模不大，在一张 GPU 可以容纳，但是训练数据量会比较大，这时候就采用数据并行机制。</li>
<li>具体就是在多节点上并行分割数据和训练。</li>
</ul>
</li>
<li>第三种不常用的并行机制是 <font color=red><strong>流水线机制</strong></font>（按层分区）。
<ul>
<li>在深度学习中，流水线可以是指重叠的计算，即在一层和下一层之间（当数据准备就绪时）连续计算；或者根据深度划分DNN，将层分配给特定处理器。</li>
<li>流水线可以看作是数据并行的一种形式，因为元素（样本）是通过网络并行处理的，但也可以看作是模型并行，因为流水线的长度是由DNN结构决定的。</li>
</ul>
</li>
</ul>
<p>具体可见下图:
</p>
<h4 id="132-如何使用">1.3.2 如何使用</h4>
<p><u>数据的并行往往意味着<strong>计算性能</strong>的可扩展，而模型的并行往往意味着<strong>内存使用</strong>的可扩展。</u></p>
<p>需要注意的是：<font color=green>数据并行和模型并行也并不冲突，两者可以同时存在，而流水线机制也可以和模型并行一起混用。</font>比如，DistBelief分布式深度学习系统结合了三种并行策略。训练在同时复制的多个模型上训练，每个模型副本在不同的样本上训练（数据并行），每个副本上，依据同一层的神经元（模型并行性）和不同层（流水线）上划分任务，进行分布训练。</p>
<p>另外也需要根据具体问题具体分析，比如现代卷积神经网络主要由两种层构成，他们具有不一样的属性和性能。</p>
<ul>
<li><strong>卷积层</strong>，占据了90% ~ 95% 的计算量，5% 的参数，但是对结果具有很大的表达能力。</li>
<li><strong>全连接层</strong>，占据了 5% ~ 10% 的计算量， 95% 的参数，但是对于结果具有相对较小的表达的能力。</li>
</ul>
<p>综上：卷积层计算量大，所需参数系数 W 少，全连接层计算量小，所需参数系数 W 多。因此对于卷积层适合使用数据并行，对于全连接层适合使用模型并行。</p>
<p></p>
<h3 id="14-数据并行训练">1.4 数据并行训练</h3>
<p>我们本系列主要讨论数据并行训练（其中的一种架构）.</p>
<p>数据并行训练只是一种逻辑架构。我们从沐神的书里面摘录：</p>
<blockquote>
<p>假设机器上有k个GPU。给定要训练的模型，每个GPU将独立地维护一组完整的模型参数，尽管GPU上的参数值是相同且同步的。例如，下图演示了在k=2时使用数据并行的训练。</p>
</blockquote>
<blockquote>
<p></p>
</blockquote>
<blockquote>
<p>一般来说，训练过程如下：</p>
<ul>
<li>在训练的任何迭代中，给定一个随机的小批量，我们将该小批量中的样本分成k个部分，并将它们均匀地分在多个GPU上。</li>
<li>每个GPU根据分配给它的小批量子集计算模型参数的损失和梯度。</li>
<li>将k个GPU中每个GPU的局部梯度聚合以获得当前的小批量随机梯度。</li>
<li>聚合梯度被重新分配到每个GPU。</li>
<li>每个GPU使用这个小批量随机梯度来更新它维护的完整的模型参数集。</li>
</ul>
</blockquote>
<h2 id="2-通信和架构">2 通信和架构</h2>
<p>前面提到并行梯度下降的例子：两个处理器分别处理一般数据计算梯度 $g_1$, $g_2$，然后把两个梯度结果进行聚合，最后再把最新参数发给各个分布计算单元，这种训练算法叫<strong>模型一致性方法</strong>（consistent model methods）。<font color=red>这就涉及到了通信问题，即如何做聚合</font>。</p>
<h3 id="21-方法和架构">2.1 方法和架构</h3>
<p>一般有两种通信方法：<strong>Share memory</strong> 和 <strong>Message passing</strong>。</p>
<ul>
<li><strong>Share memory</strong> 就是<u>所有处理器共享同一块内存</u>，这样通信很容易，<u>但是同一个节点内的处理器之间才可以共享内存，不同节点处理器之间无法共享内存</u>。</li>
</ul>
<p></p>
<ul>
<li><strong>Message passing</strong> 就是<u>不同节点之间用消息</u>（比如基于 TCP/IP 或者 RDMA）进行传递/通信，这样容易扩展，可以进行大规模训练。</li>
</ul>
<p></p>
<p>因此我们知道，Message passing 才是解决方案，于是带来了问题：<font color=red>如何协调这些节点之间的通讯</font>。</p>
<p>有两种架构：</p>
<ul>
<li><font color=red>Client-Server</font>架构: 一个 server 节点协调其他节点工作，其他节点是用来执行计算任务的 worker。</li>
<li><font color=red>Peer-to-Peer</font>架构：每个节点都有邻居，邻居之间可以互相通信。</li>
</ul>
<h3 id="22-异步-vs-同步">2.2 异步 vs 同步</h3>
<p>异步 vs 同步 是通信的另外一个侧面。</p>
<p>在数据并行训练之中，各个计算设备分别根据各自获得的batch，前向计算获得损失，进而反向传播计算梯度。计算好梯度后，就涉及到一个<font color=red><strong>梯度同步的问题</strong></font>：每个计算设备 都有根据自己的数据计算的梯度，如何在不同GPU之间维护模型的不同副本之间的一致性？ 如果不同的模型以某种方式最终获得不同的权重，则权重更新将变得不一致，并且模型训练将有所不同。</p>
<blockquote>
<blockquote>
<p><font color=red><strong>怎么做这个同步就是设计分布式机器学习系统的一个核心问题</strong></font>。</p>
</blockquote>
</blockquote>
<p>分布式训练的梯度同步策略可分为<strong>异步（asynchronous）梯度更新</strong> 和 <strong>同步（synchronous）梯度更新</strong>机制。</p>
<ul>
<li>
<p><font color=red><strong>同步</strong></font>指的是所有的设备都是采用相同的模型参数来训练，<u>等待所有设备的mini-batch训练完成后，收集它们的梯度然后取均值，然后执行模型的一次参数更新</u>。</p>
<ul>
<li>同步训练相当于通过聚合很多设备上的mini-batch形成一个很大的batch来训练模型，Facebook就是这样做的，但是他们发现当batch大小增加时，同时线性增加学习速率会取得不错的效果。</li>
<li>同步训练看起来很不错，但是实际上需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡。</li>
<li>因为每一轮结束时算得快的节点都需等待算得慢的节点算完，再进行下一轮迭代。类似于木桶效应，一个拖油瓶会严重拖慢训练进度，所以同步训练方式相对来说训练速度会慢一些。这个拖油瓶一般就叫做 straggler。(缺点)</li>
</ul>
</li>
<li>
<p><font color=red><strong>异步</strong></font>训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数，这样总体会训练速度会快很多</p>
<ul>
<li>异步训练的一个很严重的问题是<strong>梯度失效问题</strong>（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数其实已经被其它设备更新过了，此时这个梯度就过期了，因为现在的模型参数和训练前采用的参数是不一样的。由于梯度失效问题，异步训练虽然速度快，但是可能陷入次优解（sub-optimal training performance）。</li>
</ul>
</li>
</ul>
<p>具体如图所示:</p>
<p>
</p>
<p>这两种更新方式各有优缺点：</p>
<ul>
<li>异步更新可能会更快速地完成整个梯度计算。</li>
<li>同步更新 可以更快地进行一个收敛。</li>
</ul>
<p>选择哪种方式取决于实际的应用场景。</p>
<h2 id="3-具体架构">3 具体架构</h2>
<p>接下来，我们看看几种具体架构实现，先给出一个总体说明：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>通信</th>
<th>架构</th>
<th>并行性</th>
</tr>
</thead>
<tbody>
<tr>
<td>MapReduce</td>
<td>消息传递</td>
<td>client-server</td>
<td>批同步</td>
</tr>
<tr>
<td>Parameter Server</td>
<td>消息传递</td>
<td>client-server</td>
<td>异步</td>
</tr>
<tr>
<td>Decentralized Network</td>
<td>消息传递</td>
<td>P2P(Peer to Peer)</td>
<td>同步或异步</td>
</tr>
</tbody>
</table>
<h3 id="31-mapreduce">3.1 MapReduce</h3>
<p>MapReduce是Client-Server架构。以 Spark 为例看看是如何进行并行化：</p>
<ul>
<li>Spark Driver 就是 Server，Spark Executor 就是 Worker 节点，每一个梯度下降过程包含一个<font color=red>广播</font>、<font color=red>map</font>和一个 <font color=red>reduce</font> 操作。</li>
<li>Server 定义了 map操作（就是具体的训练），也可以把信息广播到worker节点。</li>
<li>Worker 会执行 map 操作进行训练，在此过程中，数据被分给 worker 进行计算。</li>
<li>计算结束后，worker把计算结果传回 driver 处理，这个叫做reduce。</li>
<li>在 reduce 过程中，Server 节点对 worker 传来的计算结果进行聚合之后，把聚合结果广播到各个worker节点，进行下一次迭代。</li>
</ul>
<h3 id="32-parameter-server-参数服务器">3.2 Parameter Server 参数服务器</h3>
<p>Parameter server 也是一种client-server架构。<u>和MapReduce不同在于 Parameter server 可以是异步的</u>，MapReduce只有等所有map都完成了才能做reduce操作。</p>
<p>参数服务器架构中，计算设备被划分为参数服务器（PS）和worker。</p>
<ul>
<li><strong>参数服务器（server</strong>）。是中心化的组件，主要是负责模型参数的存储，平均梯度和交换更新。参数服务器可以按照不同比例的参数服务器和工作线程进行配置，每个参数服务器都有着不同的配置数据。</li>
<li><strong>工作节点（worker）</strong>。每个工作节点会负责它领域内的数据分片所对应模型参数的更新计算（比如前向和反向传播这类计算密集的运算），同时它们又会向参数服务器去传递它所计算的梯度，由参数服务器来汇总所有的梯度，再进一步反馈到所有节点。</li>
</ul>
<p>具体步骤如下：</p>
<ul>
<li>所有的参数都存储在参数服务器中，而 工作节点（worker） 是万年打工仔。</li>
<li>工作节点 们只负责计算梯度，待所有计算设备完成梯度计算之后，把计算好的梯度发送给参数服务器，这样参数服务器收到梯度之后，执行一定的计算（梯度平均等）之后，就更新其维护的参数，做到了在节点之间对梯度进行平均，利用平均梯度对模型进行更新。</li>
<li>然后参数服务器再把更新好的新参数返回给所有的工作节点，以对每个节点中的模型副本应用一致化更新。</li>
<li>打工仔们会再进行下一轮的前后向计算。</li>
</ul>
<p>逻辑如下：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">     <span class="o">+----------------------------------------------+</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>  <span class="n">Parameter</span> <span class="n">Server</span>                            <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>   <span class="n">Compute</span> <span class="p">:</span> <span class="n">New</span> <span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">+</span> <span class="n">Sum</span><span class="p">(</span><span class="n">Delta</span> <span class="n">P</span> <span class="o">...</span><span class="p">)</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>   <span class="n">Parameter</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Parameter</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Parameter</span> <span class="mi">3</span> <span class="o">...</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">|</span>                                              <span class="o">|</span>
</span></span><span class="line"><span class="cl">     <span class="o">+--+----+----------+--+----------------+--+----+</span>
</span></span><span class="line"><span class="cl">        <span class="o">^</span>    <span class="o">|</span>          <span class="o">^</span>  <span class="o">|</span>                <span class="o">^</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">        <span class="o">|</span>    <span class="o">|</span>          <span class="o">|</span>  <span class="o">|</span>                <span class="o">|</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">Delta</span> <span class="n">P</span> <span class="o">|</span>    <span class="o">|</span>   <span class="n">Delta</span> <span class="n">P</span><span class="o">|</span>  <span class="o">|</span>         <span class="n">Delta</span> <span class="n">P</span><span class="o">|</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+-----+</span>    <span class="o">|</span>          <span class="o">|</span>  <span class="o">|</span>                <span class="o">|</span>  <span class="o">+------+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="o">+-----+</span>          <span class="o">|</span>  <span class="o">|</span>                <span class="o">|</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="o">|</span> <span class="n">New</span> <span class="n">P</span>          <span class="o">|</span>  <span class="o">|</span> <span class="n">New</span> <span class="n">P</span>          <span class="o">+------+</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="o">|</span>                <span class="o">|</span>  <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>  <span class="n">New</span> <span class="n">P</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>    <span class="n">v</span>                <span class="o">|</span>  <span class="o">|</span>                       <span class="o">|</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span>                     <span class="o">|</span>  <span class="n">v</span>                       <span class="o">|</span>  <span class="n">v</span>
</span></span><span class="line"><span class="cl"><span class="o">+-+-----------+</span>   <span class="o">+-----+--+---+</span>             <span class="o">+-----+--+---+</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span> <span class="n">Worker</span>      <span class="o">|</span>   <span class="o">|</span> <span class="n">Worker</span>     <span class="o">|</span>             <span class="o">|</span> <span class="n">Worker</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>             <span class="o">|</span>   <span class="o">|</span>            <span class="o">|</span>             <span class="o">|</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>             <span class="o">|</span>   <span class="o">|</span>            <span class="o">|</span>   <span class="o">......</span>    <span class="o">|</span>            <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">|</span>       <span class="n">Model</span> <span class="o">|</span>   <span class="o">|</span>     <span class="n">Model</span>  <span class="o">|</span>             <span class="o">|</span>     <span class="n">Model</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+------+------+</span>   <span class="o">+------+-----+</span>             <span class="o">+----+-------+</span>
</span></span><span class="line"><span class="cl">       <span class="o">^</span>                 <span class="o">^</span>                        <span class="o">^</span>
</span></span><span class="line"><span class="cl">       <span class="o">|</span>                 <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">       <span class="o">|</span>                 <span class="o">|</span>                        <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+----+----+</span>       <span class="o">+----+-----+</span>               <span class="o">+--+-----+</span>
</span></span><span class="line"><span class="cl">  <span class="o">|</span> <span class="n">Data</span> <span class="mi">1</span>  <span class="o">|</span>       <span class="o">|</span>  <span class="n">Data</span> <span class="mi">2</span>  <span class="o">|</span>               <span class="o">|</span> <span class="n">Data</span> <span class="mi">3</span> <span class="o">|</span>
</span></span><span class="line"><span class="cl">  <span class="o">+---------+</span>       <span class="o">+----------+</span>               <span class="o">+--------+</span></span></span></code></pre></td></tr></table>
</div>
</div><p>如图:
</p>
<p>参数服务器既可以用在数据并行上，也可以被用到模型并行训练上。比如可以将模型切分为多个部分，存储在不同的PS Server节点上，并提供方便的访问服务，这是参数服务器的本质。</p>
<h3 id="33--decentralized-network">3.3  Decentralized Network</h3>
<p>Decentralized Network 就是去中心化网络，其特点如下：</p>
<ul>
<li>去中心化网络没有一个中心节点，属于 Peer-to-Peer 架构。</li>
<li>采用 message passing 进行通信，且节点只和邻居通信。</li>
<li>并行方式可以采用异步或者同步。</li>
<li>去中心化网络的收敛情况取决于网络连接情况：
<ul>
<li>连接越紧密，收敛性越快，当强连接时候，模型可以很快收敛；</li>
<li>如果不是强连接，它可能不收敛；</li>
</ul>
</li>
</ul>
<h2 id="4-allreduce">4 AllReduce</h2>
<p>因为本系列是 Horovod，所以我们要先说说参数服务器的劣势，下一个系列我们再说参数服务器优势。</p>
<h3 id="41-参数服务器劣势">4.1 参数服务器劣势</h3>
<p>尽管参数服务器可以提升表现，但仍然面临几个问题：</p>
<ul>
<li><font color=red>确定工作者与参数服务器的正确比例</font>：如果使用一个参数服务器，它可能会成为网络或计算瓶颈。 如果使用多个参数服务器，则通信模式变为“All-to-All”，这可能使网络饱和。</li>
<li><font color=red>处理程序复杂性</font>：参数服务器的概念较多，这通常导致陡峭的学习曲线和大量的代码重构，压缩了实际建模的时间。</li>
<li><font color=red>硬件成本</font> : 参数服务器的引入也增加了系统的硬件成本。</li>
</ul>
<p>人们发现，MPI_AllReduce 语义也可以很好地满足数据并行训练这一需要。</p>
<p>需要注意的是：AllReduce <strong>既可以是去中心化，也可以是主从式的。</strong></p>
<h3 id="42-并行任务通信分类">4.2 并行任务通信分类</h3>
<p>并行任务的通信一般可以分为 <font color=red><strong>Point-to-point communication</strong></font>和 <font color=red><strong>Collective communication</strong></font>。</p>
<ul>
<li>P2P 这种模式只有一个sender和一个receiver，实现起来比较简单，比如NV GPU Direct P2P技术服务于单机多卡的单机卡间数据通信 。</li>
<li>Collective communication包含多个sender和多个receiver，一般的通信原理包括 broadcast，gather,all-gather，scatter，reduce，all-reduce，reduce-scatter，all-to-all等。</li>
</ul>
<h3 id="43-mpi_allreduce">4.3 MPI_AllReduce</h3>
<p>AllReduce<font color=red>(对m个独立参数进行规约，并将规约结果返回给所有进程)</font>, 其实是最显然和直接的<strong>分布式机器学习抽象</strong>，因为大部分算法的结构都是分布数据。<u>在每个子集上面算出一些局部统计量，然后整合出全局统计量，并且再分配给各个节点去进行下一轮的迭代，这样一个过程就是AllReduce</u>。</p>
<ul>
<li>
<p>可以把每个 Worker 看作是 MPI 概念中的一个进程，比如可以用 4 个 Worker 组成了一个组，该组由 4 个进程组成。我们在这四个进程中对梯度进行一次 MPI_AllReduce。</p>
</li>
<li>
<p>根据 MPI_AllReduce 的语义，所有参与计算的进程都有结果，所以梯度就完成了分发。只要在初始化的时候，我们可以保证每个 Worker 的参数是一致的，那在后续的迭代计算中，参数会一直保持一致，因为梯度信息是一致的。</p>
</li>
<li>
<p>AllReduce 跟 MapReduce 有类似，但后者采用的是<u>面向通用任务处理的多阶段执行任务的方式</u>，而AllReduce则让一个程序在必要的时候占领一台机器，并且在所有迭代的时候一直跑到底，来防止重新分配资源的开销，这更加适合于机器学习的任务处理。</p>
</li>
</ul>
<p>所以，MPI_AllReduce 的语义可以很好地解决深度学习中梯度同步的问题。但是到底能不能使用它，还是要看下层的实现对这一场景是否足够友好。</p>
<h2 id="5--ring-allreduce">5  ring-allreduce</h2>
<p>百度提出使用新算法来平均梯度，取消 Reducer，并让这些梯度在所有节点之间交流，这被称为 ring-allreduce，他们使用 TensorFlow 也实现了这种算法（https://github.com/baidu-research/tensorflow-allreduce）。</p>
<h3 id="51-特点">5.1 特点</h3>
<p><strong>Ring-Allreduce</strong>特点如下：</p>
<ul>
<li>Ring Allreduce 算法使用定义良好的成对消息传递步骤序列在一组进程之间同步状态（在这种情况下为张量）。</li>
<li>Ring-Allreduce 的命名中 Ring 意味着设备之间的拓扑结构为一个逻辑环形，每个设备都应该有一个左邻和一个右邻居，且本设备只会<strong>向它右邻居发送数据，并且从它的左邻居接受数据</strong>。</li>
<li>Ring-Allreduce 的命名中的 Allreduce 则代表着没有中心节点，架构中的每个节点都是梯度的汇总计算节点。</li>
<li>此种算法各个节点之间只与相邻的两个节点通信，并不需要参数服务器。因此，所有节点都参与计算也参与存储，也避免产生中心化的通信瓶颈。</li>
<li>相比PS架构，Ring-Allreduce 架构是<strong>带宽优化</strong>的，因为集群中每个节点的带宽都被充分利用。
<ul>
<li>在 ring-allreduce 算法中，每个 N 节点与其他两个节点进行 2 * (N-1) 次通信。在这个通信过程中，一个节点发送并接收数据缓冲区传来的块。<strong>在第一个N-1迭代中，接收的值被添加到节点缓冲区中的值</strong>。<strong>在第二个N-1迭代中，接收的值代替节点缓冲区中保存的值</strong>。百度的文章证明了这种算法是带宽上最优的，这意味着如果缓冲区足够大，它将最大化地利用可用的网络。</li>
</ul>
</li>
<li>在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于后面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。</li>
<li>Ring架构下的同步算法将参数在通信环中依次传递，往往需要多步才能完成一次参数同步。在大规模训练时会引入很大的通信开销，并且对小尺寸张量（tensor）不够友好。对于小尺寸张量，可以采用批量操作（batch）的方法来减小通信开销。</li>
</ul>
<p>综上所述，Ring-based AllReduce 架构的网络通讯量如果处理适当，不会随着机器增加而增加，而仅仅和模型 &amp; 网络带宽有关，这针对参数服务器是个巨大的提升。</p>
<h3 id="52-策略">5.2 策略</h3>
<p>Ring-based AllReduce 策略包括 <font color=red>Scatter-Reduce</font> 和 <font color=red>AllGather</font> 两个阶段。</p>
<ul>
<li>
<p>首先是scatter-reduce，scatter-reduce 会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分，是最终结果的一个块。</p>
<ul>
<li>假设环中有 N 个 worker，每个 worker 有长度相同的数组，需要将 worker 的数组进行求和。在 Scatter-Reduce 阶段，每个 worker 会将数组分成 N 份数据块，然后 worker 之间进行 N 次数据交换。在第 k 次数据交换时，第 i 个 worker 会将自己的 (i - k) % N 份数据块发送给下一个 worker。接收到上一个 worker 的数据块后，worker 会将其与自己对应的数据块求和。</li>
</ul>
</li>
<li>
<p>然后是allgather。<u>GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的最终融合梯度</u>。</p>
<ul>
<li>在执行完 Scatter-Reduce 后，每个 worker 的数组里都有某个数据块是最终求和的结果，现在需要将各数据块的最后求和结果发送到每个 worker 上。和 Scatter-Reduce 一样，也需要 N 次循环。在第 k 次循环时，第 i 个 worker 会将其第 (i+1-k)%N 个数据块发送给下一个 worker 。接收到前一个 worker 的数据块后，worker 会用接收的数据快覆盖自己对应的数据块。进行 N 次循环后，每个 worker 就拥有了数组各数据块的最终求和结果了。</li>
</ul>
</li>
</ul>
<p>以下部分来自 <a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"target="_blank" rel="external nofollow noopener noreferrer">https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>，这是我能找到最优秀的解读。</p>
<h4 id="521-结构">5.2.1 结构</h4>
<p>环形结构如下，每个 GPU 应该有一个左邻居和一个右邻居；它只会向其右侧邻居发送数据，并从其左侧邻居接收数据。</p>
<p></p>
<h4 id="522-scatter-reduce">5.2.2 scatter reduce</h4>
<p>scatter-reduce：会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分。</p>
<p>为简单起见，我们假设目标是按元素对单个大型浮点数数组的所有元素求和；系统中有 N 个 GPU，每个 GPU 都有一个相同大小的数组，在 allreduce 的最后环节，每个 GPU 都应该有一个相同大小的数组，其中包含原始数组中数字的总和。</p>
<h5 id="5221-分块">5.2.2.1 分块</h5>
<p>首先，GPU 将阵列划分为 N 个较小的块（其中 N 是环中的 GPU 数量）。</p>
<p></p>
<p>接下来，GPU 将进行 N-1 次 scatter-reduce 迭代。</p>
<p>在每次迭代中，GPU 会将其一个块发送到其右邻居，并将从其左邻居接收一个块并累积到该块中。每个 GPU 发送和接收的数据块每次迭代都不同。第 n 个 GPU 通过发送块 n 和接收块 n – 1 开始，然后逐步向后进行，每次迭代发送它在前一次迭代中接收到的块。</p>
<h5 id="5222-第一次迭代">5.2.2.2 第一次迭代</h5>
<p>在第一次迭代中，上图中的五个 GPU 将发送和接收以下块：</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>发送</th>
<th>接收</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>块0</td>
<td>块4</td>
</tr>
<tr>
<td>1</td>
<td>块1</td>
<td>块0</td>
</tr>
<tr>
<td>2</td>
<td>块2</td>
<td>块1</td>
</tr>
<tr>
<td>3</td>
<td>块3</td>
<td>块2</td>
</tr>
<tr>
<td>4</td>
<td>块4</td>
<td>块3</td>
</tr>
</tbody>
</table>
<p>scatter-reduce 的第一次迭代中的数据传输如下：</p>
<p></p>
<p>第一次发送和接收完成后，每个 GPU 都会有一个块，该块由两个不同 GPU 上相同块的总和组成。例如，第二个 GPU 上的第一个块将是该块中来自第二个 GPU 和第一个 GPU 的值的总和。</p>
<p></p>
<h5 id="5222-全部迭代">5.2.2.2 全部迭代</h5>
<p>在后续迭代中，该过程继续直到最后。最终每个 GPU 将有一个块，这个块包含所有 GPU 中该块中所有值的总和。</p>
<p>下面系列图展示了所有数据传输和中间结果，从第一次迭代开始，一直持续到scatter-reduce完成。</p>
<p>iter 1：</p>
<p></p>
<p>iter2：</p>
<p></p>
<p>iter3：</p>
<p></p>
<p>iter4：</p>
<p></p>
<p>所有 scatter-reduce 传输后的最终状态</p>
<p></p>
<h4 id="523-allgather">5.2.3 Allgather</h4>
<p>在 scatter-reduce 步骤完成后，在每个 GPU 的数组中都有某一些值（每个 GPU 有一个块）是最终值，其中包括来自所有 GPU 的贡献。为了完成 allreduce，GPU 必须接下来交换这些块，以便所有 GPU 都具有最终所需的值。</p>
<p>ring allgather 与 scatter-reduce 进行相同的处理（发送和接收的 N-1 次迭代），但是他们这次不是累积 GPU 接收的值，而只是简单地覆盖块。第 n 个 GPU 开始发送第 n+1 个块并接收第 n 个块，然后在以后的迭代中始终发送它刚刚接收到的块。</p>
<h5 id="5231-第一次迭代">5.2.3.1 第一次迭代</h5>
<p>例如，在我们的 5-GPU 设置的第一次迭代中，GPU 将发送和接收以下块：</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>发送</th>
<th>接收</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>块1</td>
<td>块0</td>
</tr>
<tr>
<td>1</td>
<td>块2</td>
<td>块1</td>
</tr>
<tr>
<td>2</td>
<td>块3</td>
<td>块2</td>
</tr>
<tr>
<td>3</td>
<td>块4</td>
<td>块3</td>
</tr>
<tr>
<td>4</td>
<td>块0</td>
<td>块4</td>
</tr>
</tbody>
</table>
<p>allgather 的第一次迭代中的数据传输如下。</p>
<p></p>
<p>第一次迭代完成后，每个 GPU 都会有最终数组的两个块。在接下来的迭代中，该过程继续一直到最后，最终每个 GPU 将拥有整个数组的完全累加值。</p>
<h5 id="5232-全部迭代">5.2.3.2 全部迭代</h5>
<p>下面系列图展示了所有数据传输和中间结果，从第一次迭代开始，一直持续到全部收集完成。</p>
<p>Allgather 数据传输（迭代 1）
</p>
<p>Allgather 数据传输（迭代 2）如下：
</p>
<p>Allgather 数据传输（迭代 3）：</p>
<p></p>
<p>Allgather 数据传输（迭代 4）：</p>
<p></p>
<p>所有全部转移后的最终状态。</p>
<p></p>
<h4 id="524-horovod-架构图">5.2.4 Horovod 架构图</h4>
<p>工作原理也可以借助<a href="https://www.uber.com/blog/manifold-open-source/"target="_blank" rel="external nofollow noopener noreferrer">Horovod<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>的发布帖子 来看看。</p>
<p></p>
<h4 id="525-百度思路">5.2.5 百度思路</h4>
<p>或者我们从百度的源码中也可以直接看到思路，现在摘录给大家。</p>
<p>具体代码参见 <a href="https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/baidu-research/tensorflow-allreduce/commit/66d5b855e90b0949e9fa5cca5599fd729a70e874#diff-3d530d590e551619acd776cfe7eaff06R517<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cm">/* Perform a ring allreduce on the data. Allocate the necessary output tensor and
</span></span></span><span class="line"><span class="cl"><span class="cm"> * store it in the output parameter.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Assumes that all MPI processes are doing an allreduce of the same tensor,
</span></span></span><span class="line"><span class="cl"><span class="cm"> * with the same dimensions.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * A ring allreduce is a bandwidth-optimal way to do an allreduce. To do the allreduce,
</span></span></span><span class="line"><span class="cl"><span class="cm"> * the nodes involved are arranged in a ring:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                   .--0--.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                  /       \
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                 3         1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                  \       /
</span></span></span><span class="line"><span class="cl"><span class="cm"> *                   *--2--*
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Each node always sends to the next clockwise node in the ring, and receives
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  from the previous one.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  The allreduce is done in two parts: a scatter-reduce and an allgather. In
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  the scatter reduce, a reduction is done, so that each node ends up with a
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  chunk of the final output tensor which has contributions from all other
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  nodes.  In the allgather, those chunks are distributed among all the nodes,
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  so that all nodes have the entire output tensor.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Both of these operations are done by dividing the input tensor into N
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  evenly sized chunks (where N is the number of nodes in the ring).
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  The scatter-reduce is done in N-1 steps. In the ith step, node j will send
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  the (j - i)th chunk and receive the (j - i - 1)th chunk, adding it in to
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  its existing data for that chunk. For example, in the first iteration with
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  the ring depicted above, you will have the following transfers:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  In the second iteration, you&#39;ll have the following transfers:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  After this iteration, Node 2 has 3 of the four contributions to Segment 0.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  The last iteration has the following transfers:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  After this iteration, Node 3 has the fully accumulated Segment 0; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  has the fully accumulated Segment 1; and so on. The scatter-reduce is complete.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Next, the allgather distributes these fully accumululated chunks across all nodes.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  Communication proceeds in the same ring, once again in N-1 steps. At the ith step,
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  node j will send chunk (j - i + 1) and receive chunk (j - i). For example, at the
</span></span></span><span class="line"><span class="cl"><span class="cm"> *  first iteration, the following transfers will occur:
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 0:  Node 3 --&gt; Node 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 1:  Node 0 --&gt; Node 1
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 2:  Node 1 --&gt; Node 2
</span></span></span><span class="line"><span class="cl"><span class="cm"> *      Segment 3:  Node 2 --&gt; Node 3
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * After the first iteration, Node 0 will have a fully accumulated Segment 0
</span></span></span><span class="line"><span class="cl"><span class="cm"> * (from Node 3) and Segment 1. In the next iteration, Node 0 will send its
</span></span></span><span class="line"><span class="cl"><span class="cm"> * just-received Segment 0 onward to Node 1, and receive Segment 3 from Node 3.
</span></span></span><span class="line"><span class="cl"><span class="cm"> * After this has continued for N - 1 iterations, all nodes will have a the fully
</span></span></span><span class="line"><span class="cl"><span class="cm"> * accumulated tensor.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Each node will do (N-1) sends for the scatter-reduce and (N-1) sends for the allgather.
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Each send will contain K / N bytes, if there are K bytes in the original tensor on every node.
</span></span></span><span class="line"><span class="cl"><span class="cm"> * Thus, each node sends and receives 2K(N - 1)/N bytes of data, and the performance of the allreduce
</span></span></span><span class="line"><span class="cl"><span class="cm"> * (assuming no latency in connections) is constrained by the slowest interconnect between the nodes.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="53-区别">5.3 区别</h3>
<p><strong>在中等规模模型情况下，all-reduce 更适合。当规模巨大时候则应该使用参数服务器</strong>。</p>
<p>参数服务器 适合的是高维稀疏模型训练，它利用的是维度稀疏的特点，每次 pull or push 只更新有效的值。但是深度学习模型是典型的dense场景，embedding做的就是把稀疏变成稠密。所以这种 pull or push 的不太适合。而 网络通信上更优化的 all-reduce 适合中等规模的深度学习。</p>
<p>又比如由于推荐搜索领域模型的 Embedding 层规模庞大以及训练数据样本长度不固定等原因，导致容易出现显存不足和卡间同步时间耗费等问题，所以 all-reduce 架构很少被用于搜索推荐领域。</p>
<p>至此，背景知识已经介绍完毕，下一篇我们开始介绍 Horovod 的使用。</p>
<p>reference:
[1] <a href="https://www.cnblogs.com/rossiXYZ/p/14856464.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/rossiXYZ/p/14856464.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>