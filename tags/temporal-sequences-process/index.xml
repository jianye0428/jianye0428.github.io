<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>temporal sequences process - 标签 - yejian's blog</title><link>https://jianye0428.github.io/tags/temporal-sequences-process/</link><description>temporal sequences process - 标签 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Thu, 28 Dec 2023 21:50:25 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/tags/temporal-sequences-process/" rel="self" type="application/rss+xml"/><item><title>长短期记忆网络 -- LSTM</title><link>https://jianye0428.github.io/posts/lstm/</link><pubDate>Thu, 28 Dec 2023 21:50:25 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/lstm/</guid><description><![CDATA[<h2 id="一传统的bp网络和cnn网络">一、传统的BP网络和CNN网络</h2>
<p>BP网络和CNN网络没有时间维，和传统的机器学习算法理解起来相差无几，CNN在处理彩色图像的3通道时，也可以理解为叠加多层，图形的三维矩阵当做空间的切片即可理解，写代码的时候照着图形一层层叠加即可。如下图是一个普通的BP网络和CNN网络。</p>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;
  display: inline-block;
  color: #999;
  padding: 2px;">BP Network</div>
</center>
<center>
  
  <br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;
  display: inline-block;
  color: #999;
  padding: 2px;">CNN Network</div>
</center>
<br>
<p>图中的隐含层、卷积层、池化层、全连接层等，都是实际存在的，一层层前后叠加，在空间上很好理解，因此在写代码的时候，基本就是看图写代码，比如用keras就是：</p>
<div class="highlight" id="id-1"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 示例代码，没有实际意义</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>  <span class="c1"># 添加卷积层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>         <span class="c1"># 添加池化层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>                          <span class="c1"># 添加dropout层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>  <span class="c1"># 添加卷积层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>         <span class="c1"># 添加池化层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>                          <span class="c1"># 添加dropout层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">....</span>   <span class="c1"># 添加其他卷积操作</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>                            <span class="c1"># 拉平三维数组为2维数组</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>        <span class="n">添加普通的全连接层</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">....</span>  <span class="c1"># 训练网络</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="二lstm网络">二、LSTM网络</h2>
<p>当我们在网络上搜索看LSTM结构的时候，看最多的是下面这张图：</p>
<p></p>
<p>这是RNN循环神经网络经典的结构图，LSTM只是对隐含层节点A做了改进，整体结构不变，因此本文讨论的也是这个结构的可视化问题。</p>
<p>中间的A节点隐含层，左边是表示只有一层隐含层的LSTM网络，所谓LSTM循环神经网络就是在时间轴上的循环利用，在时间轴上展开后得到右图。</p>
<p>看左图，很多同学以为LSTM是单输入、单输出，只有一个隐含神经元的网络结构，看右图，以为LSTM是多输入、多输出，有多个隐含神经元的网络结构，A的数量就是隐含层节点数量。</p>
<p>WTH？思维转不过来啊。这就是传统网络和空间结构的思维。</p>
<p><strong>实际上，右图中，我们看Xt表示序列，下标t是时间轴，所以，A的数量表示的是时间轴的长度，是同一个神经元在不同时刻的状态(Ht)，不是隐含层神经元个数。</strong></p>
<p>我们知道，LSTM网络在训练时会使用上一时刻的信息，加上本次时刻的输入信息来共同训练。</p>
<p>举个简单的例子：在第一天我生病了(初始状态H0)，然后吃药(利用输入信息X1训练网络)，第二天好转但是没有完全好(H1)，再吃药(X2),病情得到好转(H2),如此循环往复知道病情好转。因此，输入Xt是吃药，时间轴T是吃多天的药，隐含层状态是病情状况。因此我还是我，只是不同状态的我。</p>
<p>实际上，LSTM的网络是这样的：
</p>
<p>上面的图表示包含2个隐含层的LSTM网络，在T=1时刻看，它是一个普通的BP网络，在T=2时刻看也是一个普通的BP网络，只是沿时间轴展开后，T=1训练的隐含层信息H,C会被传递到下一个时刻T=2，如下图所示。上图中向右的五个常常的箭头，所指的也是隐含层状态在时间轴上的传递。</p>
<p></p>
<p>注意，图中H表示隐藏层状态，C是遗忘门，后面会讲解它们的维度。</p>
<h2 id="三lstm的输入结构">三、LSTM的输入结构</h2>
<p>为了更好理解LSTM结构，还必须理解LSTM的数据输入情况。仿照3通道图像的样子，再加上时间轴后的多样本的多特征的不同时刻的数据立方体如下图所示:</p>
<p></p>
<p>右边的图是我们常见模型的输入，比如XGBOOST，lightGBM，决策树等模型，输入的数据格式都是这种(N<em>F)的矩阵，而左边是加上时间轴后的数据立方体，也就是时间轴上的切片，它的维度是(N</em>T*F),第一维度是样本数，第二维度是时间，第三维度是特征数，如下图所示：</p>
<p></p>
<p>这样的数据立方体很多，比如天气预报数据，把样本理解成城市，时间轴是日期，特征是天气相关的降雨风速PM2.5等，这个数据立方体就很好理解了。在NLP里面，一句话会被embedding成一个矩阵，词与词的顺序是时间轴T，索引多个句子的embedding三维矩阵如下图所示：</p>
<p></p>
<h2 id="四pytorch中的lstm">四、pytorch中的LSTM</h2>
<h3 id="41-pytorch中定义的lstm模型">4.1 pytorch中定义的LSTM模型</h3>
<p>pytorch中定义的LSTM模型的参数如下</p>
<div class="highlight" id="id-2"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">参数有</span><span class="err">：</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">input_size</span><span class="err">：</span><span class="n">x的特征维度</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">hidden_size</span><span class="err">：</span><span class="n">隐藏层的特征维度</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">num_layers</span><span class="err">：</span><span class="n">lstm隐层的层数</span><span class="err">，</span><span class="n">默认为1</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">bias</span><span class="err">：</span><span class="n">False则bihbih</span><span class="o">=</span><span class="mi">0</span><span class="n">和bhhbhh</span><span class="o">=</span><span class="mf">0.</span> <span class="n">默认为True</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">batch_first</span><span class="err">：</span><span class="n">True则输入输出的数据格式为</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">dropout</span><span class="err">：</span><span class="n">除最后一层</span><span class="err">，</span><span class="n">每一层的输出都进行dropout</span><span class="err">，</span><span class="n">默认为</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">bidirectional</span><span class="err">：</span><span class="n">True则为双向lstm默认为False</span></span></span></code></pre></td></tr></table>
</div>
</div><p>结合前面的图形，我们一个个看。</p>
<p>(1)input_size：x的特征维度，就是数据立方体中的F，在NLP中就是一个词被embedding后的向量长度，如下图所示：</p>
<p></p>
<p>(2)hidden_size：隐藏层的特征维度(隐藏层神经元个数)，如下图所示，我们有两个隐含层，每个隐藏层的特征维度都是5。注意，<strong>非双向LSTM的输出维度等于隐藏层的特征维度</strong>。</p>
<p></p>
<p>(3)num_layers：lstm隐层的层数，上面的图我们定义了2个隐藏层。
(4)batch_first：用于定义输入输出维度，后面再讲。
(5)bidirectional：是否是双向循环神经网络，如下图是一个双向循环神经网络，因此在使用双向LSTM的时候我需要特别注意，正向传播的时候有(Ht, Ct),反向传播也有(Ht&rsquo;, Ct&rsquo;),前面我们说了非双向LSTM的输出维度等于隐藏层的特征维度，而<strong>双向LSTM的输出维度是隐含层特征数<em>2，而且H,C的维度是时间轴长度</em>2</strong>。</p>
<p></p>
<h3 id="42-喂给lstm的数据格式">4.2 喂给LSTM的数据格式</h3>
<p>pytorch中LSTM的输入数据格式默认如下：</p>
<div class="highlight" id="id-3"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">参数有</span><span class="err">：</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">seq_len</span><span class="err">：</span><span class="n">序列长度</span><span class="err">，</span><span class="n">在NLP中就是句子长度</span><span class="err">，</span><span class="n">一般都会用pad_sequence补齐长度</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">batch</span><span class="err">：</span><span class="n">每次喂给网络的数据条数</span><span class="err">，</span><span class="n">在NLP中就是一次喂给网络多少个句子</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">input_size</span><span class="err">：</span><span class="n">特征维度</span><span class="err">，</span><span class="n">和前面定义网络结构的input_size一致</span><span class="err">。</span></span></span></code></pre></td></tr></table>
</div>
</div><p>前面也说到，如果LSTM的参数 batch_first=True，则要求输入的格式是：</p>
<div class="highlight" id="id-4"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>刚好调换前面两个参数的位置。其实这是比较好理解的数据形式，下面以NLP中的embedding向量说明如何构造LSTM的输入。</p>
<p>之前我们的embedding矩阵如下图：</p>
<p></p>
<p>如果把batch放在第一位，则三维矩阵的形式如下：</p>
<p></p>
<p>其转换过程如下图所示：</p>
<p></p>
<p>看懂了吗，这就是输入数据的格式，是不是很简单。
LSTM的另外两个输入是 h0 和 c0，可以理解成网络的初始化参数，用随机数生成即可。</p>
<div class="highlight" id="id-5"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">h0</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c0</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">参数</span><span class="err">：</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">num_layers</span><span class="err">：</span><span class="n">隐藏层数</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">num_directions</span><span class="err">：</span><span class="n">如果是单向循环网络</span><span class="err">，</span><span class="n">则num_directions</span><span class="o">=</span><span class="mi">1</span><span class="err">，</span><span class="n">双向则num_directions</span><span class="o">=</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">batch</span><span class="err">：</span><span class="n">输入数据的batch</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">hidden_size</span><span class="err">：</span><span class="n">隐藏层神经元个数</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意，如果我们定义的input格式是：</p>
<div class="highlight" id="id-6"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>则H和C的格式也是要变的：</p>
<div class="highlight" id="id-7"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">h0</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c0</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-lstm的output格式">4.3 LSTM的output格式</h3>
<p>LSTM的输出是一个tuple，如下：</p>
<div class="highlight" id="id-8"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output</span><span class="p">,(</span><span class="n">ht</span><span class="p">,</span> <span class="n">ct</span><span class="p">)</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">output</span><span class="p">:</span> <span class="n">最后一个状态的隐藏层的神经元输出</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">ht</span><span class="err">：</span><span class="n">最后一个状态的隐含层的状态值</span>
</span></span><span class="line"><span class="cl">  <span class="o">-</span> <span class="n">ct</span><span class="err">：</span><span class="n">最后一个状态的隐含层的遗忘门值</span></span></span></code></pre></td></tr></table>
</div>
</div><p>output的默认维度是：</p>
<div class="highlight" id="id-9"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ht</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ct</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>和input的情况类似，如果我们前面定义的input格式是：</p>
<div class="highlight" id="id-10"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">input</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>则ht和ct的格式也是要变的：</p>
<div class="highlight" id="id-11"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ht</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ct</span><span class="p">(</span><span class="n">batc</span><span class="err">，</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>说了这么多，我们回过头来看看ht和ct在哪里，请看下图：</p>
<p></p>
<p>output在哪里？请看下图：</p>
<p></p>
<h2 id="五lstm和其他网络组合">五、LSTM和其他网络组合</h2>
<p>还记得吗，output的维度等于隐藏层神经元的个数，即hidden_size，在一些时间序列的预测中，会在output后，接上一个全连接层，全连接层的输入维度等于LSTM的hidden_size，之后的网络处理就和BP网络相同了，如下图：</p>
<p></p>
<p>用pytorch实现上面的结构：</p>
<div class="highlight" id="id-12"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RegLSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">RegLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 定义LSTM</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_num_layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 定义回归层网络，输入的特征维度等于LSTM的输出，输出维度为1</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">ht</span><span class="p">,</span><span class="n">ct</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span></span></span></code></pre></td></tr></table>
</div>
</div><p>当然，有些模型则是将输出当做另一个LSTM的输入，或者使用隐藏层ht,ct的信息进行建模，不一而足。
好了，以上就是我对LSTM的一些学习心得，看完记得关注点赞。</p>
<p>REF:
<a href="https://mp.weixin.qq.com/s?__biz=MzU1OTYzNjg5OQ==&amp;mid=2247545117&amp;idx=1&amp;sn=670ba155d94b229d39c5bf0bf20239d5&amp;chksm=fc1639d1cb61b0c72434a00454b2af8f9022e7ac3030a4186cda22ef5594ef5994620dc5fd52&amp;mpshare=1&amp;scene=1&amp;srcid=0617kfSozC3sKY1lRjYg1f0u&amp;sharer_shareinfo=6833fdea9df7fee2c423a9474c0928be&amp;sharer_shareinfo_first=6833fdea9df7fee2c423a9474c0928be#rd"target="_blank" rel="external nofollow noopener noreferrer">[1]. 漂亮，LSTM模型结构的可视化<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2].https://zhuanlan.zhihu.com/p/94757947
[3].https://zhuanlan.zhihu.com/p/59862381
[4].https://zhuanlan.zhihu.com/p/36455374
[5].https://www.zhihu.com/question/41949741/answer/318771336
[6].https://blog.csdn.net/android_ruben/article/details/80206792</p>
]]></description></item></channel></rss>