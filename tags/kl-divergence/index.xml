<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>KL Divergence - 标签 - yejian's blog</title><link>https://jianye0428.github.io/tags/kl-divergence/</link><description>KL Divergence - 标签 - yejian's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>18817571704@163.com (Jian YE)</managingEditor><webMaster>18817571704@163.com (Jian YE)</webMaster><lastBuildDate>Sat, 26 Aug 2023 13:59:37 +0800</lastBuildDate><atom:link href="https://jianye0428.github.io/tags/kl-divergence/" rel="self" type="application/rss+xml"/><item><title>KL Divergence 详解</title><link>https://jianye0428.github.io/posts/kldivergence/</link><pubDate>Sat, 26 Aug 2023 13:59:37 +0800</pubDate><author>Jian YE</author><guid>https://jianye0428.github.io/posts/kldivergence/</guid><description><![CDATA[<h2 id="一-基本定义">一、 基本定义</h2>
<p>假设给定事件 $x$, 则我们有以下定义:</p>
<ol>
<li><strong>Probability:</strong>
取值0~1</li>
</ol>
<p>$$p(x) 或 q(x)$$</p>
<ol start="2">
<li>
<p><strong>Information:</strong>
对$p(x)$取对数，加符号得正值
$$I(p)=-\log p(x)$$
概率越高，包含的信息小，因为事件越来越确定。相反，概率越低，包含的信息越多，因为事件具有很大的不确定性。</p>
</li>
<li>
<p><strong>(Shannon)Entropy(信息熵):</strong></p>
</li>
</ol>
<p>$p(x)$对$I(x)$ 平均
$$
\begin{aligned}
H(p)&amp; =\mathbb{E}_{x\sim P}[I(p)]  \
&amp;=\sum p(x)I(p) \
&amp;=-\sum p(x)\log p(x)
\end{aligned}
$$
熵是信息的平均，直观上，Shannon熵是信息在<strong>同一分布</strong>下的平均。</p>
<p>KL散度来源于信息论，信息论的目的是以信息含量来度量数据。信息论的核心概念是信息熵(Entropy)，使用H来表示。概率论中概率分布所含的信息量同样可以使用信息熵来度量。</p>
<div class="details admonition Note open">
    <div class="details-summary admonition-title">
      <i class="icon fa-solid fa-pencil-alt fa-fw" aria-hidden="true"></i>Note<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i>
    </div>
    <div class="details-content">
      <div class="admonition-content">对于任意概率事件，由于其概率$p(x) \geq 0$ 且 $p(x) \leq 0$, 因此 $H(p) \geq 0$.</div>
    </div>
  </div>
<ol start="4">
<li>
<p><strong>Cross-Entropy:</strong>
$p(x)$对$I(q)$平均:
$$
\begin{aligned}
H(p,q)&amp; =\mathbb{E}_{x\sim P}[I(q)]  \
&amp;=\sum p(x)I(q) \
&amp;=-\sum p(x)\log q(x)
\end{aligned}
$$
熵是信息的平均，直观上，交叉熵是信息在不同分布下的平均。</p>
</li>
<li>
<p><strong>KL divergence(Relative entropy/Information gain):</strong>
$$
\begin{aligned}
D_{KL}(p||q)&amp; =H(p,q)-H(p)  \
&amp;=-\sum p(x)\log q(x)+\sum p(x)\log p(x) \
&amp;=-\sum p(x)\log\frac{q(x)}{p(x)} \
&amp;=\sum p(x)\log\frac{p(x)}{q(x)}
\end{aligned}
$$</p>
<ul>
<li>相对熵 = 交叉熵 - shannon熵</li>
<li>非对称$D_{KL}(p||q)\neq D_{KL}(q||p)$，亦不满足三角不等式，故不是距离。</li>
<li>$D_{KL}(p||q)$为$p$相对于$q$，值非负，取零若$p=q$。从公式上看，就是拿$q$替代$p$后熵的变化。</li>
<li>$KL = Kullback-Leibler$</li>
</ul>
</li>
</ol>
<p>所谓KL散度，是指当某分布q(x)被用于近似p(x)时的信息损失。</p>
<p>$$
D(p||q)=\sum_{x\in X}p(x)\log\frac{p(x)}{q(x)}
$$
也就是说，q(x)能在多大程度上表达p(x)所包含的信息，KL散度越大，表达效果越差。</p>
<h2 id="二-kl-divergence-一些性质非正式证明">二、 KL divergence 一些性质（非正式）证明</h2>
<ol>
<li>
<p>非对称性
$$
D_{KL}(p||q)-D_{KL}(q||p)=\sum\left(p(x)+q(x)\right)\log\frac{p(x)}{q(x)}
$$
易知，当$p(x) \neq q(x)$时，上式不为0。故，$D_{KL}(p||q)$和$D_{KL}(q||p)$非对称，是不同的。（此部分侧重于说明它们不是不同的）</p>
</li>
<li>
<p>非负性
$$
\begin{aligned}
-D_{KL}(p||q)&amp; =\sum p(x)\log\frac{q(x)}{p(x)}  \cr
&amp;\leq\log\sum p(x)\frac{q(x)}{p(x)} \cr
&amp;=\log1 \cr
&amp;=0
\end{aligned}
$$
其中，不等式部分使用了<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Jensen%2527s_inequality"target="_blank" rel="external nofollow noopener noreferrer">Jensen&rsquo;s inequality<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p>凹性
$$
\begin{aligned}
&amp;D_{KL}[\lambda p_1(x)+(1-\lambda)p_2(x)||\lambda q_1(x)+(1-\lambda)q_2(x)] \cr
&amp;=\sum\left[\lambda p_1(x)+(1-\lambda)p_2(x)\right]\log\frac{[\lambda p_1(x)+(1-\lambda)p_2(x)]}{[\lambda q_1(x)+(1-\lambda)q_2(x)]} \cr
&amp;\leq\sum\left[\lambda p_1(x)\log\frac{\lambda p_1(x)}{\lambda q_1(x)}+(1-\lambda)p_2(x)\log\frac{(1-\lambda)p_2(x)}{(1-\lambda)q_2(x)}\right] \cr
&amp;=\lambda\sum p_1(x)\log\frac{p_1(x)}{q_1(x)}+(1-\lambda)\sum p_2(x)\log\frac{p_2(x)}{q_2(x)} \cr
&amp;=\lambda D_{KL}[p_1(x)||q_1(x)]+(1-\lambda)D_{KL}[p_2(x)||q_2(x)]
\end{aligned}
$$</p>
</li>
</ol>
<p>其中，不等式部分用到了<a href="https://link.zhihu.com/?target=https%3A//statproofbook.github.io/P/logsum-ineq"target="_blank" rel="external nofollow noopener noreferrer">log sum inequality<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h2 id="三最小化kl-divergence目标函数">三、最小化KL divergence目标函数</h2>
<p>为了方便说明，我们基于定义在某个空间$X$上的分布$P$和$Q$来重写一下KL， 如下所示：
$$
D_{KL}(P||Q)=\mathbb{E}_{x\sim P}[\log\frac{P(X)}{Q(X)}]
$$</p>
<p>假设，$P$为真实的分布函数，我们想要用带参数 $\theta$ 的分布函数 $Q$，即 $Q_{\theta}$ ，去近似。也就是说，通过选取参数$\theta$， 让 $Q_{\theta}$ 和 $P$ 在某种意义上具有相似性。下面，我们分别将选取正向KL和反向KL做为目标函数进行说明。为了方便，我们假设 $P$ 为双峰分布，$Q_{\theta}$ 为正太分布，故 $\theta$ 包含均值和方差两个参数。</p>
<p></p>
<ol>
<li>最小化正向KL目标函数
目标函数如下：
$$
\begin{aligned}
&amp;\arg\min_{\theta}D_{KL}(P||Q_{\theta}) \cr
&amp;=\arg\min_\theta\mathbb{E}<em>{x\sim P}[\log\frac{P(X)}{Q(X)}] \cr
&amp;=\arg\min</em>\theta\mathbb{E}<em>{x\sim P}[-\log Q</em>\theta(X)]-H[P(X)] \cr
&amp;=\arg\min_\theta\mathbb{E}<em>{x\sim P}[-\log Q</em>\theta(X)] \cr
&amp;=\arg\max_\theta\mathbb{E}<em>{x\sim P}[\log Q</em>\theta(X)]
\end{aligned}
$$
从此处可以看出最小化正向KL目标函数，其实是等价于通过 $Q_{\theta}$ 进行最大似然估计。也就是说，数据 $x$ 由 $P$ 产生，基于这些数据，我们选取 $\theta$ 让平均在 $P(X)$ 上的 $\log Q_{\theta}(X)$ 似然函数最大，即:</li>
</ol>
<p><strong>平均P(X)各个峰太，P(X)概率高的地方，$Q_{\theta}(X)$ 概率也要高</strong></p>
<p>所以我们有下图mean-seeking的结果</p>
<p></p>
<p>(你也可以从信息/熵的角度去理解，道理是一样的)</p>
<ol start="2">
<li>最小化反向KL目标函数
目标函数如下：</li>
</ol>
<p>$$
\begin{aligned}
&amp;\arg\min_{\theta}D_{KL}(Q_{\theta}||P) \cr
&amp;=\arg\min_\theta\mathbb{E}<em>{x\sim Q</em>\theta}[\log\frac{Q_\theta}{P(X)}] \cr
&amp;=\arg\min_\theta\mathbb{E}<em>{x\sim Q</em>\theta}[-\log P(X)]-H[Q_\theta(X)] \cr
&amp;=\arg\max_\theta\mathbb{E}<em>{x\sim Q</em>\theta}\left[\log P(X)\right]+H[Q_\theta(X)]
\end{aligned}
$$</p>
<p>此时，我们需要选取参数 $\theta$，让平均在 $Q_{\theta}(X)$ 上的 $\log P(X)$ 似然函数最大;同时，让Shannon熵 $H(Q_{\theta}(X))$
也比较大，即约束 $Q_{\theta}(X)$ 不要过于集中。总的来看，我们有：</p>
<p><strong>平均 $Q_{\theta}(X)$ 各个峰太，$Q_{\theta}(X)$ 概率高的地方，$P(X)$ 概率也要高，但 $Q_{\theta}(X)$ 不能过于集中</strong></p>
<p>可以想象，如果没有 $H[Q_{\theta}(X)]$ 的约束，可能会调整 $\theta$，让 $Q_{\theta}(X)$ 集中于 $P(X)$ 最大的地方，得到的值也会比较大。所以，$H[Q_{\theta}(X)]$ 起到了一个正则化（regularization）的效果。</p>
<p>所以我们有下图mode-seeking 的结果:</p>
<p></p>
<p>正向最小化和反向最小化放在一起对比：</p>
<p></p>
<p>正向和反向最小化 此部分代码来自参考文献3，但在调用logsumexp时，有点问题，故做了一个微小改动，代码放在微信公众号MoData文章最后，如果感兴趣，<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzg4NDY5NTk5NA%3D%3D%26amp%3Bmid%3D2247483795%26amp%3Bidx%3D1%26amp%3Bsn%3D25f1bcf1e725090c22fcf37d181e875e%26amp%3Bchksm%3Dcfb57746f8c2fe50ffefbd47620e447542d43639774fb8f462c97c4953643238e62619db81ba%26token%3D1055919935%26lang%3Dzh_CN%23rd"target="_blank" rel="external nofollow noopener noreferrer">点击此处<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>。</p>
<p><strong>相关参考资料</strong>
[1]. Cover, T. M., and J. A. Thomas. &ldquo;Elements of Information Theory,(2nd edn, 2006).&rdquo; DOI: https://doi. org/10.1002 X 47174882 (2006).
[2]. <a href="https://dibyaghosh.com/blog/probability/kldivergence.html"target="_blank" rel="external nofollow noopener noreferrer">https://dibyaghosh.com/blog/probability/kldivergence.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3]. <a href="https://www.tuananhle.co.uk/notes/r"target="_blank" rel="external nofollow noopener noreferrer">https://www.tuananhle.co.uk/notes/r<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>ref:
[1]. <a href="https://zhuanlan.zhihu.com/p/425693597"target="_blank" rel="external nofollow noopener noreferrer">KL-Divergence详解<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2]. <a href="https://www.zhihu.com/tardis/zm/art/95687720?source_id=1005"target="_blank" rel="external nofollow noopener noreferrer">https://www.zhihu.com/tardis/zm/art/95687720?source_id=1005<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3]. <a href="http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf"target="_blank" rel="external nofollow noopener noreferrer">http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[4]. <a href="https://zhuanlan.zhihu.com/p/372835186"target="_blank" rel="external nofollow noopener noreferrer">KL 进阶<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
]]></description></item></channel></rss>